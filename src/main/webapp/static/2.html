<table border="0" cellpadding="2" cellspacing="0"><tr><td>FN</td><td>Clarivate Analytics Web of Science</td></tr><tr><td>VR</td><td>1.0</td></tr><style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Song, P
   <br>Zheng, WM
   <br>Zhao, L</td>
</tr>

<tr>
<td valign="top">AF </td><td>Song, Peng
   <br>Zheng, Wenming
   <br>Zhao, Li</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>NON-PARALLEL TRAINING FOR VOICE CONVERSION BASED ON ADAPTATION METHOD</td>
</tr>

<tr>
<td valign="top">SO </td><td>2013 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 26-31, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Vancouver, CANADA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; non-parallel training; MAP; Gaussian normalization;
   mean transformation</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we propose a simple and efficient non-parallel training scheme for voice conversion (VC). First, the speaker models are adapted from the background model using maximum a posteriori (MAP) technique. Then, by utilizing the parameters of adapted speaker models, the Gaussian normalization and mean transformation methods are proposed for VC, respectively. In addition, to improve the conversion performance of the proposed methods, a combination approach is further presented. Finally, objective and subjective experiments are carried out to evaluate the performance of the proposed scheme, the results demonstrate that our scheme can obtain comparable performance with the traditional GMM method based on parallel corpus.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Song, Peng; Zhao, Li] Southeast Univ, Sch Informat Sci &amp; Engn, Nanjing
   210096, Jiangsu, Peoples R China.
   <br>[Zheng, Wenming] Southeast Univ, Res Ctr Learning Sci, Nanjing 210096,
   Jiangsu, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Song, P (reprint author), Southeast Univ, Sch Informat Sci &amp; Engn, Nanjing 210096, Jiangsu, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>pengsong@seu.edu.cn; wenming_zheng@seu.edu.cn; zhaoli@seu.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>6</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>6</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>6905</td>
</tr>

<tr>
<td valign="top">EP </td><td>6909</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000329611507013</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Anumanchipalli, GK
   <br>Oliveira, LC
   <br>Black, AW</td>
</tr>

<tr>
<td valign="top">AF </td><td>Anumanchipalli, Gopala Krishna
   <br>Oliveira, Luis C.
   <br>Black, Alan W.</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>A STYLE CAPTURING APPROACH TO F0 TRANSFORMATION IN VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2013 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 26-31, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Vancouver, CANADA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Prosody Transformation; Metrical Foot; Voice Conversion; F0</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we present a new approach to F0 transformation, that can capture aspects of speaking style. Instead of using the traditional 5ms frames as units in transformation, we propose a method that looks at longer phonological regions such as metrical feet. We automatically detect metrical feet in the source speech, and for each of source speaker's feet, we find its phonological correspondence in target speech. We use a statistical phrase accent model to represent the F0 contour, where a 4-dimensional TILT representation is used for the F0 is parameterized over each feet region for the source and target speakers. This forms the parallel data that is the training data for our transformation. We transform the phrase component using simple z-score mapping. We use a joint density Gaussian mixture model to transform the accent contours. Our transformation method generates F0 contours that are significantly more correlated with the target speech than a baseline, frame-based method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Anumanchipalli, Gopala Krishna; Black, Alan W.] Carnegie Mellon Univ,
   Language Technol Inst, Pittsburgh, PA 15213 USA.
   <br>[Anumanchipalli, Gopala Krishna; Oliveira, Luis C.] INESC ID IST Lisboa,
   L2F Spoken English Lab, Lisbon, Portugal.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Anumanchipalli, GK (reprint author), Carnegie Mellon Univ, Language Technol Inst, Pittsburgh, PA 15213 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>gopalakr@cs.cmu.edu; lco@l2f.inesc-id.pt; awb@cs.cmu.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>6915</td>
</tr>

<tr>
<td valign="top">EP </td><td>6919</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000329611507015</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Mohammadi, SH
   <br>Kain, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Mohammadi, Seyed Hamidreza
   <br>Kain, Alexander</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>TRANSMUTATIVE VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2013 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 26-31, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Vancouver, CANADA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; speech transformation; frequency warping</td>
</tr>

<tr>
<td valign="top">AB </td><td>There are two types of voice conversion (VC) systems: generative and transmutative. A generative VC system typically uses a compact parametrization of speech and maps input to output parameters directly; however, the relative low dimensionality of the underlying speech model reduces quality. On the other hand, a transmutative VC system modifies high-dimensional features of a high-fidelity speech model, leaving critical details unmodified. Two versions of transmutative VC approach are implemented and compared to a generative VC approach. The results show that the implemented transmutative VC is significantly better compared to generative VC in terms of quality. The difference between the two VC methods regarding recognition scores are insignificant.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Mohammadi, Seyed Hamidreza; Kain, Alexander] Oregon Hlth &amp; Sci Univ,
   Ctr Spoken Language Understanding, Portland, OR 97201 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Mohammadi, SH (reprint author), Oregon Hlth &amp; Sci Univ, Ctr Spoken Language Understanding, Portland, OR 97201 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>mohammah@ohsu.edu; kaina@ohsu.edu</td>
</tr>

<tr xmlns:date="http://exslt.org/dates-and-times">
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Mohammadi, Seyed Hamidreza</display_name>&nbsp;</font></td><td><font size="3">0000-0002-6892-9241&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>5</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>5</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>6920</td>
</tr>

<tr>
<td valign="top">EP </td><td>6924</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000329611507016</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Percybrooks, W
   <br>Moore, E
   <br>McMillan, C</td>
</tr>

<tr>
<td valign="top">AF </td><td>Percybrooks, Winston
   <br>Moore, Elliot
   <br>McMillan, Correy</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>PHONEME INDEPENDENT HMM VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2013 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 26-31, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Vancouver, CANADA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Phoneme independent; HMM; voice conversion; ABX; MOS</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a voice conversion algorithm based on Hidden Markov Models that does not requires explicit phonetic labeling of the input speech. Additionally, the proposed voice conversion algorithm also uses an excitation estimation algorithm previously presented by the authors to achieve higher speech quality without compromising speaker identity conversion. The performance of the proposed algorithm was compared, using listening tests, with the performance of a recent voice conversion algorithm based on HMM but requiring phonetic labeling. The proposed algorithm was found to achieve equivalent identity conversion scores while improving the perceived quality of the converted speech. Thus, the proposed algorithm was found as a viable alternative for conversion applications where phonetic labeling is not practical.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Percybrooks, Winston; Moore, Elliot; McMillan, Correy] Georgia Inst
   Technol, Savannah, GA 31407 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Percybrooks, W (reprint author), Georgia Inst Technol, 210 Technolgy Circle, Savannah, GA 31407 USA.</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Percybrooks, Winston</display_name>&nbsp;</font></td><td><font size="3">0000-0002-0169-7562&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>6925</td>
</tr>

<tr>
<td valign="top">EP </td><td>6929</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000329611507017</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wu, ZZ
   <br>Xiao, X
   <br>Chng, ES
   <br>Li, HZ</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wu, Zhizheng
   <br>Xiao, Xiong
   <br>Chng, Eng Siong
   <br>Li, Haizhou</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>SYNTHETIC SPEECH DETECTION USING TEMPORAL MODULATION FEATURE</td>
</tr>

<tr>
<td valign="top">SO </td><td>2013 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 26-31, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Vancouver, CANADA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Anti-spoofing attack; synthetic detection; modulation; phase modulation;
   temporal feature</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEAKER; RECOGNITION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion and speaker adaptation techniques present a threat to current state-of-the-art speaker verification systems. To prevent such spoofing attack and enhance the security of speaker verification systems, the development of anti-spoofing techniques to distinguish synthetic and human speech is necessary. In this study, we continue the quest to discriminate synthetic and human speech. Motivated by the facts that current analysis-synthesis techniques operate on frame level and make the frame-by-frame independence assumption, we proposed to adopt magnitude/phase modulation features to detect synthetic speech from human speech. Modulation features derived from magnitude/phase spectrum carry long-term temporal information of speech, and may be able to detect temporal artifacts caused by the frame-by-frame processing in the synthesis of speech signal. From our synthetic speech detection results, the modulation features provide complementary information to magnitude/phase features. The best detection performance is obtained by fusing phase modulation features and phase features, yielding an equal error rate of 0.89%, which is significantly lower than the 1.25% of phase features and 10.98% of MFCC features.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Wu, Zhizheng; Chng, Eng Siong; Li, Haizhou] Nanyang Technol Univ, Sch
   Comp Engn, Singapore 639798, Singapore.
   <br>[Wu, Zhizheng; Xiao, Xiong; Chng, Eng Siong; Li, Haizhou] Nanyang
   Technol Univ, Ternasek Lab, Singapore 639798, Singapore.
   <br>[Li, Haizhou] Inst Infocomm Res, Human Language Technol Dept, Singapore
   639798, Singapore.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Wu, ZZ (reprint author), Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.</td>
</tr>

<tr>
<td valign="top">EM </td><td>wuzz@ntu.edu.sg</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Li, Haizhou</display_name>&nbsp;</font></td><td><font size="3">Q-6438-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Li, Haizhou</display_name>&nbsp;</font></td><td><font size="3">0000-0001-9158-9401&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>39</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>39</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>7234</td>
</tr>

<tr>
<td valign="top">EP </td><td>7238</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000329611507080</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hanilci, C
   <br>Kinnunen, T
   <br>Saeidi, R
   <br>Pohjalainen, J
   <br>Alku, P
   <br>Ertas, F</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hanilci, Cemal
   <br>Kinnunen, Tomi
   <br>Saeidi, Rahim
   <br>Pohjalainen, Jouni
   <br>Alku, Paavo
   <br>Ertas, Figen</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>SPEAKER IDENTIFICATION FROM SHOUTED SPEECH: ANALYSIS AND COMPENSATION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2013 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 26-31, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Vancouver, CANADA</td>
</tr>

<tr>
<td valign="top">DE </td><td>speaker identification; shouted speech</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Text-independent speaker identification is studied using neutral and shouted speech in Finnish to analyze the effect of vocal mode mismatch between training and test utterances. Standard mel-frequency cepstral coefficient (MFCC) features with Gaussian mixture model (GMM) recognizer are used for speaker identification. The results indicate that speaker identification accuracy reduces from perfect (100 %) to 8.71 % under vocal mode mismatch. Because of this dramatic degradation in recognition accuracy, we propose to use a joint density GMM mapping technique for compensating the MFCC features. This mapping is trained on a disjoint emotional speech corpus to create a completely speaker- and speech mode independent emotion-neutralizing mapping. As a result of the compensation, the 8.71 % identification accuracy increases to 32.00 % without degrading the non-mismatched train-test conditions much.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Hanilci, Cemal; Ertas, Figen] Uludag Univ, Dept Elect Engn, Bursa,
   Turkey.
   <br>[Hanilci, Cemal; Kinnunen, Tomi] Univ Eastern Finland, Sch Comp,
   Joensuu, Finland.
   <br>[Saeidi, Rahim] Radboud Univ Nijmegen, Ctr Language &amp; Speech Technol,
   NL-6525 ED Nijmegen, Netherlands.
   <br>[Pohjalainen, Jouni; Alku, Paavo] Aalto Univ, Dept Signal Proc &amp; Acoust,
   Espoo, Finland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hanilci, C (reprint author), Uludag Univ, Dept Elect Engn, Bursa, Turkey.</td>
</tr>

<tr>
<td valign="top">EM </td><td>chanilci@uludag.edu.tr; tkinnu@cs.joensuu.fi; rahim.saeidi@let.ru.nl</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Hanilci, Cemal</display_name>&nbsp;</font></td><td><font size="3">S-4967-2016&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Saeidi, Rahim</display_name>&nbsp;</font></td><td><font size="3">J-5963-2014&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Alku, Paavo</display_name>&nbsp;</font></td><td><font size="3">E-2400-2012&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Saeidi, Rahim</display_name>&nbsp;</font></td><td><font size="3">0000-0002-9084-0091&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>4</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>5</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>8027</td>
</tr>

<tr>
<td valign="top">EP </td><td>8031</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000329611508038</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Aihara, R
   <br>Takashima, R
   <br>Takiguchi, T
   <br>Ariki, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Aihara, Ryo
   <br>Takashima, Ryoichi
   <br>Takiguchi, Tetsuya
   <br>Ariki, Yasuo</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>INDIVIDUALITY-PRESERVING VOICE CONVERSION FOR ARTICULATION DISORDERS
   BASED ON NON-NEGATIVE MATRIX FACTORIZATION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2013 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 26-31, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Vancouver, CANADA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice Conversion; NMF; Articulation Disorders; Voice Reconstruction;
   Assistive Technologies</td>
</tr>

<tr>
<td valign="top">AB </td><td>We present in this paper a voice conversion (VC) method for a person with an articulation disorder resulting from athetoid cerebral palsy. The movement of such speakers is limited by their athetoid symptoms, and their consonants are often unstable or unclear, which makes it difficult for them to communicate. In this paper, exemplar-based spectral conversion using Non-negative Matrix Factorization (NMF) is applied to a voice with an articulation disorder. To preserve the speaker's individuality, we used a combined dictionary that is constructed from the source speaker's vowels and target speaker's consonants. Experimental results indicate that the performance of NMF-based VC is considerably better than conventional GMM-based VC.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Aihara, Ryo; Takashima, Ryoichi; Takiguchi, Tetsuya; Ariki, Yasuo] Kobe
   Univ, Grad Sch Syst Informat, Nada Ku, Kobe, Hyogo 6578501, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Aihara, R (reprint author), Kobe Univ, Grad Sch Syst Informat, Nada Ku, 1-1 Rokikodai, Kobe, Hyogo 6578501, Japan.</td>
</tr>

<tr>
<td valign="top">TC </td><td>9</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>9</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>8037</td>
</tr>

<tr>
<td valign="top">EP </td><td>8040</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000329611508040</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Zhao, SX
   <br>Koh, SN
   <br>Yann, SI
   <br>Luke, KK</td>
</tr>

<tr>
<td valign="top">AF </td><td>Zhao, Sixuan
   <br>Koh, Soo Ngee
   <br>Yann, Soon Ing
   <br>Luke, Kang Kwong</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>FEEDBACK UTTERANCES FOR COMPUTER-ADIED LANGUAGE LEARNING USING ACCENT
   REDUCTION AND VOICE CONVERSION METHOD</td>
</tr>

<tr>
<td valign="top">SO </td><td>2013 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 26-31, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Vancouver, CANADA</td>
</tr>

<tr>
<td valign="top">DE </td><td>CALL; feedback utterances; accent reduction; voice conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>PITCH</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper considers the generation of feedback utterances for speaking skills training of non-native English learners. The proposed feedback is in the form of a combination of the learner's voice and the linguistic gestures, i.e., the prosody or pronunciation, of a native speaker. Both accent reduction method and voice conversion method are employed to generate feedback stimuli. For accent reduction, three speech synthesis methods, namely pitch-synchronous overlap and add (PSOLA), harmonic stochastic model (HSM), and speech transformation and representation by adaptive interpolation of weighted spectrogram (STRAIGHT) are used to reduce the accent of the utterances of English learners. For voice conversion, the teacher's voice is converted to that of the learner and the converted speech is used as a feedback. Objective measurements are employed to assess the nativeness and acoustic quality of the generated stimuli. A feedback scheme which combines the accent reduction and voice conversion methods is also proposed.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Zhao, Sixuan; Koh, Soo Ngee; Yann, Soon Ing] Nanyang Technol Univ, Sch
   Elect &amp; Elect Engn, Singapore 639798, Singapore.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Zhao, SX (reprint author), Nanyang Technol Univ, Sch Elect &amp; Elect Engn, Singapore 639798, Singapore.</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>8208</td>
</tr>

<tr>
<td valign="top">EP </td><td>8212</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000329611508075</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kim, J
   <br>Hong, MC
   <br>Hahn, H</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kim, Jongkuk
   <br>Hong, Min-Cheol
   <br>Hahn, Hernsoo</td>
</tr>

<tr>
<td valign="top">BE </td><td>Zheng, F</td>
</tr>

<tr>
<td valign="top">TI </td><td>On a Voice Conversion by using Prosodic Control</td>
</tr>

<tr>
<td valign="top">SO </td><td>PROCEEDINGS OF THE 2013 INTERNATIONAL CONFERENCE ON ADVANCED COMPUTER
   SCIENCE AND ELECTRONICS INFORMATION (ICACSEI 2013)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Advances in Intelligent Systems Research</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Advanced Computer Science and Electronics
   Information (ICACSEI)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUL 25-26, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Beijing, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>POSLA; Voice conversion; Prosodic; DTW; Mapping; Pitch; Modification</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion is a method that aims to transform the input speech signal such that the output signal will be perceived as produced by another speaker. Speech synthesizers using voice conversion technologies allow developers to create more voices from a single database and users to personalize the synthesizer to speak with any desired voice after a training period. In this paper, we present the method that converts time and pitch scaling using spectral mapping and PSOLA technique with OLA. This new synthesis scheme allows very flexible modifications of the pitch-scale, the time-scale and the spectral envelope characteristics while producing high-quality speech output. This synthesis scheme is thus well suited to voice conversion. Further work will be conducted on a matching method to correspond well with each phonetic information, and larger corpora to assess the robustness of the method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kim, Jongkuk; Hong, Min-Cheol; Hahn, Hernsoo] Soongsil Univ, Dept
   Informat &amp; Telecommun Engn, Seoul 156743, South Korea.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kim, J (reprint author), Soongsil Univ, Dept Informat &amp; Telecommun Engn, Seoul 156743, South Korea.</td>
</tr>

<tr>
<td valign="top">EM </td><td>kokjk@hanmail.net</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">VL </td><td>41</td>
</tr>

<tr>
<td valign="top">BP </td><td>477</td>
</tr>

<tr>
<td valign="top">EP </td><td>481</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000327726600117</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nirmal, J
   <br>Kachare, P
   <br>Patnaik, S
   <br>Zaveri, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nirmal, Jagannath
   <br>Kachare, Pramod
   <br>Patnaik, Suprava
   <br>Zaveri, Mukesh</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Cepstrum Liftering based Voice Conversion using RBF and GMM</td>
</tr>

<tr>
<td valign="top">SO </td><td>2013 INTERNATIONAL CONFERENCE ON COMMUNICATIONS AND SIGNAL PROCESSING
   (ICCSP)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>2nd IEEE International Conference on Communications and Signal
   Processing (ICCSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 03-05, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Adhiparasakthi Engn Coll, Dept Elect &amp; Commun Engn, Melmaruvathur, INDIA</td>
</tr>

<tr>
<td valign="top">HO </td><td>Adhiparasakthi Engn Coll, Dept Elect &amp; Commun Engn</td>
</tr>

<tr>
<td valign="top">DE </td><td>Cepstrum; gaussian mixture model; liftering; radial basis function;
   voice conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>TRANSFORMATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice Conversion is a technique which morphs the speaker dependent acoustical cues of the source speaker to those of the target speaker. Speaker dependent acoustical cues are characterized at different levels such as shape of vocal tract, glottal excitation and long term prosodic parameters. In this paper, low time and high time liftering is applied to the cepstrum to separate the vocal tract and glottal excitation of the speech signal. The Radial Basis Function and Gaussian Mixture Model are developed to capture the mapping functions for modifying the cepstrum based vocal tract and glottal excitation of the source speaker according to a target speaker. The subjective and Objective measures are used to evaluate the comparative performance of RBF and GMM based voice conversion system. Results indicate that the RBF based transformation can be used as an alternative to GMM based model. Subjective evaluation illustrate that the proposed algorithm maintains target voice individuality, quality of the speech signal.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nirmal, Jagannath; Kachare, Pramod] KJ Somaiya Coll Engn, Dept Elect,
   Mumbai, Maharashtra, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nirmal, J (reprint author), KJ Somaiya Coll Engn, Dept Elect, Mumbai, Maharashtra, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jhnirmal1975@gmail.com; pramod_1991@yahoo.com; ssp@eced.svnit.ac.in;
   mazaveri@coed.svnit.ac.in</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Patnaik, Suprava</display_name>&nbsp;</font></td><td><font size="3">0000-0002-7068-5960&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>570</td>
</tr>

<tr>
<td valign="top">EP </td><td>575</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000327328000118</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wu, XH
   <br>Obeng, M
   <br>Wang, J
   <br>Kulas, D</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wu, Xiaohe
   <br>Obeng, Morrison
   <br>Wang, Jing
   <br>Kulas, Daniel</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Survey of Techniques to Add Audio Module to Embedded Systems</td>
</tr>

<tr>
<td valign="top">SO </td><td>2013 PROCEEDINGS OF IEEE SOUTHEASTCON</td>
</tr>

<tr>
<td valign="top">SE </td><td>IEEE SoutheastCon-Proceedings</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE SoutheastCon</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 04-07, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>IEEE Jacksonville Sect, Jacksonville, FL</td>
</tr>

<tr>
<td valign="top">HO </td><td>IEEE Jacksonville Sect</td>
</tr>

<tr>
<td valign="top">DE </td><td>Embedded system; audio module; stand-alone voice chip; programmable
   system on chip; I2C</td>
</tr>

<tr>
<td valign="top">AB </td><td>In many embedded application systems, it is desirable to incorporate an audio module/functionality to the system such that the system can utter meaningful audio messages in a controlled manner. An audio system in the embedded setting may consist of memory module, Compression/Decompression (CODEC) module, Digital-to-Analog Converter (DAC) module, filter module, power amplifier module; and also (if the function of voice recording is needed) the signal conditioning module, Automatic Gain Control (AGC), and Analog-to-Digital Conversion (ADC) module. Depending on the different demands of various application scenarios, different techniques can be used to implement the audio module. If no audio signal processing is required, the approach of using a dedicated, off-the-shelf, voice chip is preferable. In other cases, individual circuits would have to be built for some or all of the peripheral modules required in an audio system. The major advantage of this approach is increased level of flexibility; while the immediate disadvantage is rising development cost and a higher level of system complexity. In this paper, we first outline the general operation principle of a typical audio system module; then, a sample implementation that uses a dedicated voice chip is introduced and explained in great detail so that it can be used by students and application engineers as a functional reference design.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Wu, Xiaohe; Obeng, Morrison; Wang, Jing; Kulas, Daniel] Bethune Cookman
   Univ, Daytona Beach, FL 32114 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Wu, XH (reprint author), Bethune Cookman Univ, Daytona Beach, FL 32114 USA.</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000326283200072</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Dimmock, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Dimmock, Matthew</td>
</tr>

<tr>
<td valign="top">TI </td><td>Converting and Not Converting "Strangers" in Early Modern London</td>
</tr>

<tr>
<td valign="top">SO </td><td>JOURNAL OF EARLY MODERN HISTORY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Conversion; baptism; early modern England; Islam; Judaism; otherness</td>
</tr>

<tr>
<td valign="top">AB </td><td>The baptism of strangers in early modern England is often imagined as a "protocolonial" enterprise. This article explores the structure, contexts, and language of a number of "stranger" baptisms in this period to challenge such a reading. The improvisatory nature of these baptisms, a consequence of the lack of a specific service until 1662, is explored, with particular attention paid to language and structure, and the role of a Calvinist-influenced conception of religious and cultural difference. The article is also concerned with subaltern voices and silence. It concludes with a close examination of the circumstance of the baptizing of a "Turk" (initially named Chinano, then William) in London in 1586, considering the unique structure created for this specific occasion, and arguing that the occasion depends upon Chinano's articulation of the reasons for his conversion before the community of believers to which he seeks access.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Univ Sussex, Brighton BN1 9RH, E Sussex, England.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Dimmock, M (reprint author), Univ Sussex, Brighton BN1 9RH, E Sussex, England.</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">VL </td><td>17</td>
</tr>

<tr>
<td valign="top">IS </td><td>5-6</td>
</tr>

<tr>
<td valign="top">BP </td><td>457</td>
</tr>

<tr>
<td valign="top">EP </td><td>478</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1163/15700658-12342377</td>
</tr>

<tr>
<td valign="top">SC </td><td>History</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000325749300003</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Pribil, J
   <br>Pribilova, A
   <br>Matousek, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Pribil, J.
   <br>Pribilova, A.
   <br>Matousek, J.</td>
</tr>

<tr>
<td valign="top">TI </td><td>Comparison of Formant Features of Male and Female Emotional Speech in
   Czech and Slovak</td>
</tr>

<tr>
<td valign="top">SO </td><td>ELEKTRONIKA IR ELEKTROTECHNIKA</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speech processing; spectral analysis; speech analysis; emotion
   recognition</td>
</tr>

<tr>
<td valign="top">ID </td><td>RECOGNITION</td>
</tr>

<tr>
<td valign="top">AB </td><td>The paper describes analysis and comparison of formant features comprising the first three formant positions together with their 3 -dB bandwidths and the formant tilts. These features were determined from the smoothed spectral envelopes or directly calculated from the complex roots of the LPC polynomial. Subsequently, statistical analysis and comparison of the formant features from emotional speech representing joy, sadness, anger, and a neutral state was performed. In this experiment we use the speech material in the form of sentences uttered by male and female professional speakers in Czech and Slovak languages. For detailed analysis, the derived speech database consisting of manually selected sounds corresponding to the stationary parts of five vowels and two nasals was created. The determined formant positions and their value ranges are in correspondence with the general knowledge for male and female voices. Obtained statistical results and values of parameter ratios will be used for emotional speech conversion or they can also be applied for extension of the text-to-speech system enabling expressive speech production.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Pribil, J.; Matousek, J.] Univ W Bohemia, Fac Sci Appl, Dept Cybernet,
   CZ-30614 Plzen, Czech Republic.
   <br>[Pribil, J.] Slovak Acad Sci, Inst Measurement Sci, SK-84104 Bratislava,
   Slovakia.
   <br>[Pribilova, A.] Slovak Univ Technol Bratislava, FEI, Inst Elect &amp;
   Photon, SK-81219 Bratislava, Slovakia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Pribil, J (reprint author), Univ W Bohemia, Fac Sci Appl, Dept Cybernet, Univ 8, CZ-30614 Plzen, Czech Republic.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jiri.pribil@savba.sk</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Matousek, Jindrich</display_name>&nbsp;</font></td><td><font size="3">C-2146-2011&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Matousek, Jindrich</display_name>&nbsp;</font></td><td><font size="3">0000-0002-7408-7730&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>6</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>6</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">VL </td><td>19</td>
</tr>

<tr>
<td valign="top">IS </td><td>8</td>
</tr>

<tr>
<td valign="top">BP </td><td>83</td>
</tr>

<tr>
<td valign="top">EP </td><td>88</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.5755/j01.eee.19.8.1739</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000325684100018</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Chen, YY
   <br>Bai, YW
   <br>Tsai, CY
   <br>Wang, JF
   <br>Chen, BW</td>
</tr>

<tr>
<td valign="top">AF </td><td>Chen, Yan-You
   <br>Bai, Yu-Wei
   <br>Tsai, Chun-Yu
   <br>Wang, Jhing-Fa
   <br>Chen, Bo-Wei</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice-Customizable Text-To-Speech for Intelligent Home-Care System</td>
</tr>

<tr>
<td valign="top">SO </td><td>1ST INTERNATIONAL CONFERENCE ON ORANGE TECHNOLOGIES (ICOT 2013)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>1st International Conference on Orange Technologies (ICOT)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 12-16, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Natl Cheng Kung Univ (NCKU), Tainan, TAIWAN</td>
</tr>

<tr>
<td valign="top">HO </td><td>Natl Cheng Kung Univ (NCKU)</td>
</tr>

<tr>
<td valign="top">DE </td><td>Text to Speech (TTS); Linear Multivariate Regression (LMR); HMM based
   speech synthesis system (HTS)</td>
</tr>

<tr>
<td valign="top">AB </td><td>In the home care scenarios, the feedback voices are often presented in reading style (neutral); however, users prefer known voices, such as those of family members, friends or celebrities. For this purpose, the speaker adaptation or voice conversion is usually adopted. However, these techniques demand a well-tagged speaker-dependent or parallel corpus, and building such a corpus is a time-consuming task. Moreover, users don't have the ability to deal with such a database by themselves. Therefore, this study presents a voice-customizable text-to-speech system which allows the system voice can be changed easily by users. In this system, parallel corpus generation, voice conversion based on decision tree and linear multivariate regression (LMR), and HMM-based speech synthesis are integrated. The advantage of this system is that the target speaker needs to record only a small amount of speech data according to a pre-designed balanced text corpus. Thus, the voice conversion models can be automatically estimated and stored. In subjective and objective evaluations, the operation convenience or friendliness of the proposed system is better than the baseline system.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Chen, Yan-You; Bai, Yu-Wei; Wang, Jhing-Fa; Chen, Bo-Wei] Natl Cheng
   Kung Univ, Dept Elect Engn, Tainan 70101, Taiwan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Chen, YY (reprint author), Natl Cheng Kung Univ, Dept Elect Engn, 1 Univ Rd, Tainan 70101, Taiwan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>n2896136@mail.ncku.edu.tw</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>239</td>
</tr>

<tr>
<td valign="top">EP </td><td>242</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000323902600060</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Lecourt, AJ</td>
</tr>

<tr>
<td valign="top">AF </td><td>Lecourt, Anne-Juliette</td>
</tr>

<tr>
<td valign="top">TI </td><td>Individual pathways of prior learning accreditation in France From
   individual to collective responsibilities</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERNATIONAL JOURNAL OF MANPOWER</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>France; Accreditation of prior learning; Lifelong learning; Lifelong
   learning policies; Care sector; Capability pathway; Voice; Professional
   development</td>
</tr>

<tr>
<td valign="top">AB </td><td>Purpose - The purpose of this paper is to analyze employees' trajectories within the Accreditation of Prior Experience Learning process (APEL) in France. It seeks to understand how candidates implement this right, the resources and supports required to manage this implementation, and how employer-employee relationships impact on the end result.
   <br>Design/methodology/approach - The paper draws on a new national survey of more than 3,000 employed APEL candidates, most of whom are women working in the care sector.
   <br>Findings - The paper argues that individual pathways within this process are influenced more by the socio-economic issues at stake in a given sector, its certification policies, environmental incentives and employer-employee joint investments than by individual characteristics. All these elements go to configure a "capability pathway", comprising individual resources, rights, and environmental, social and individual conversion factors.
   <br>Practical implications - A better understanding of employers' role and the support they provide during the course of the overall process can help increase the efficiency of lifelong learning. Spaces of mediation at candidates' disposal and real freedom at work, such as exercising one's right to voice and aspiring to development, are determinant.
   <br>Originality/value - Not much is known about how corporate policies affect individual employee pathways within the framework of the Accreditation of Prior Experience Learning (APEL) process in France. The paper contributes to this literature by using a recent survey econometrically investigating the impact of joint employer-employee investment.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Univ Aix Marseille, Inst Labour Econ &amp; Ind Sociol LEST CNRS, Aix En
   Provence, France.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Lecourt, AJ (reprint author), Univ Aix Marseille, Inst Labour Econ &amp; Ind Sociol LEST CNRS, Aix En Provence, France.</td>
</tr>

<tr>
<td valign="top">EM </td><td>anju.lecourt@laposte.net</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">VL </td><td>34</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">BP </td><td>362</td>
</tr>

<tr>
<td valign="top">EP </td><td>381</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1108/IJM-05-2013-0095</td>
</tr>

<tr>
<td valign="top">SC </td><td>Business &amp; Economics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000322356300005</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Song, P
   <br>Zhao, L
   <br>Bao, YQ</td>
</tr>

<tr>
<td valign="top">AF </td><td>Song, Peng
   <br>Zhao, Li
   <br>Bao, Yongqiang</td>
</tr>

<tr>
<td valign="top">TI </td><td>Spectral Mapping Using Kernel Principal Components Regression for Voice
   Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>ARCHIVES OF ACOUSTICS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>spectral mapping; overfitting; oversmoothing; discontinuity; kernel
   principal component regression</td>
</tr>

<tr>
<td valign="top">AB </td><td>The Gaussian mixture model (GMM) method is popular and efficient for voice conversion (VC), but it is often subject to overfitting. In this paper, the principal component regression (PCR) method is adopted for the spectral mapping between source speech and target speech, and the numbers of principal components are adjusted properly to prevent the overfitting. Then, in order to better model the nonlinear relationships between the source speech and target speech, the kernel principal component regression (KPCR) method is also proposed. Moreover, a KPCR combined with GMM method is further proposed to improve the accuracy of conversion. In addition, the discontinuity and oversmoothing problems of the traditional GMM method are also addressed. On the one hand, in order to solve the discontinuity problem, the adaptive median filter is adopted to smooth the posterior probabilities. On the other hand, the two mixture components with higher posterior probabilities for each frame are chosen for VC to reduce the oversmoothing problem. Finally, the objective and subjective experiments are carried out, and the results demonstrate that the proposed approach shows greatly better performance than the GMM method. In the objective tests, the proposed method shows lower cepstral distances and higher identification rates than the GMM method. While in the subjective tests, the proposed method obtains higher scores of preference and perceptual quality.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Song, Peng; Zhao, Li] Southeast Univ, Minist Educ, Key Lab Underwater
   Acoust Signal Proc, Nanjing 210096, Jiangsu, Peoples R China.
   <br>[Bao, Yongqiang] Nanjing Inst Technol, Sch Commun Engn, Nanjing 211167,
   Jiangsu, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Song, P (reprint author), Southeast Univ, Minist Educ, Key Lab Underwater Acoust Signal Proc, Nanjing 210096, Jiangsu, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>pengsongseu@gmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">VL </td><td>38</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>39</td>
</tr>

<tr>
<td valign="top">EP </td><td>45</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.2478/aoa-2013-0005</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000318056800005</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Li, XN
   <br>Li, XF
   <br>Zhang, Z</td>
</tr>

<tr>
<td valign="top">AF </td><td>Li, Xiaoning
   <br>Li, Xiaofeng
   <br>Zhang, Zhuo</td>
</tr>

<tr>
<td valign="top">BE </td><td>Yang, G</td>
</tr>

<tr>
<td valign="top">TI </td><td>Study of Voice Conversion Based on the 405LP</td>
</tr>

<tr>
<td valign="top">SO </td><td>PROCEEDINGS OF THE 2012 INTERNATIONAL CONFERENCE ON COMMUNICATION,
   ELECTRONICS AND AUTOMATION ENGINEERING</td>
</tr>

<tr>
<td valign="top">SE </td><td>Advances in Intelligent Systems and Computing</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Communications, Electronics and Automation
   Engineering</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 23-25, 2012</td>
</tr>

<tr>
<td valign="top">CL </td><td>Xian, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; Prosody transformation; Power PC 405LP</td>
</tr>

<tr>
<td valign="top">ID </td><td>EMOTION RECOGNITION; SPEECH</td>
</tr>

<tr>
<td valign="top">AB </td><td>voice conversion is based on the combination of proposed independent target speaker pitch between the source of the transfer Because, although the conversion is in the field by modifying the residual signal IoSE Time interval to achieve, it is not on cloud / unvoiced decisions need And make the same silent treatment, which, in the expression part of the, IoSE Equivalent pitch information, and in the silent period.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Li, Xiaoning] Chang Chun Normal Univ, Coll Comp Sci &amp; Technol,
   Changchun, Peoples R China.
   <br>[Li, Xiaofeng] Jilin Univ, Changchun, Peoples R China.
   <br>[Zhang, Zhuo] Changchun City Expt High Sch, Changchun, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Li, XN (reprint author), Chang Chun Normal Univ, Coll Comp Sci &amp; Technol, Changchun, Peoples R China.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">VL </td><td>181</td>
</tr>

<tr>
<td valign="top">BP </td><td>909</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Automation &amp; Control Systems; Computer Science; Mathematics;
   Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000315538000128</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Medina-Sancho, G</td>
</tr>

<tr>
<td valign="top">AF </td><td>Medina-Sancho, Gloria</td>
</tr>

<tr>
<td valign="top">TI </td><td>Spaces Recovered by Memory Film Language and Testimony in Parot's
   Estadio Nacional</td>
</tr>

<tr>
<td valign="top">SO </td><td>LATIN AMERICAN PERSPECTIVES</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Memory; Space; Stadium; Testimony; Torture</td>
</tr>

<tr>
<td valign="top">AB </td><td>The Chilean documentary Estadio Nacional (Carmen Luz Parot, 2001) conflates present and past by testifying, through different voices and images, to the National Stadium's conversion into a concentration camp immediately after the September 11, 1973, coup d'etat. Analysis of the way in which the stadium is critically revisited by victims of the dictatorship who were tortured there calls attention to the importance of the representation of space as a sort of archaeological work of memory that allows victims both to work through their traumatic experiences and to denounce the atrocities committed by their captors. In this sense, the film can be seen as a simulacrum of a pending judicial investigation in that it turns the public into a witness of this unfinished trial.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Calif State Univ Fresno, Fresno, CA 93740 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Medina-Sancho, G (reprint author), Calif State Univ Fresno, Fresno, CA 93740 USA.</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PD </td><td>JAN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">VL </td><td>40</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>161</td>
</tr>

<tr>
<td valign="top">EP </td><td>169</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1177/0094582X12460839</td>
</tr>

<tr>
<td valign="top">SC </td><td>Area Studies; Government &amp; Law</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000312009100012</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nakashika, T
   <br>Takashima, R
   <br>Takiguchi, T
   <br>Ariki, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nakashika, Toru
   <br>Takashima, Ryoichi
   <br>Takiguchi, Tetsuya
   <br>Ariki, Yasuo</td>
</tr>

<tr>
<td valign="top">BE </td><td>Bimbot, F
   <br>Cerisara, C
   <br>Fougeron, C
   <br>Gravier, G
   <br>Lamel, L
   <br>Pellegrino, F
   <br>Perrier, P</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Conversion in High-order Eigen Space Using Deep Belief Nets</td>
</tr>

<tr>
<td valign="top">SO </td><td>14TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2013), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>14th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2013)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 25-29, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Lyon, FRANCE</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; deep learning; deep belief nets</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH RECOGNITION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a voice conversion technique using Deep Belief Nets (DBNs) to build high-order eigen spaces of the source/target speakers, where it is easier to convert the source speech to the target speech than in the traditional cepstrum space. DBNs have a deep architecture that automatically discovers abstractions to maximally express the original input features. If we train the DBNs using only the speech of an individual speaker, it can be considered that there is less phonological information and relatively more speaker individuality in the output features at the highest layer. Training the DBNs for a source speaker and a target speaker, we can then connect and convert the speaker individuality abstractions using Neural Networks (NNs). The converted abstraction of the source speaker is then brought back to the cepstrum space using an inverse process of the DBNs of the target speaker. We conducted speaker voice conversion experiments and confirmed the efficacy of our method with respect to subjective and objective criteria, comparing it with the conventional Gaussian Mixture Model -based method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nakashika, Toru; Takashima, Ryoichi] Kobe Univ, Grad Sch Syst Informat,
   1-1 Rokkodai, Kobe, Hyogo, Japan.
   <br>[Takiguchi, Tetsuya; Ariki, Yasuo] Kobe Univ, Org Adv Sci &amp; Technol,
   Kobe, Hyogo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nakashika, T (reprint author), Kobe Univ, Grad Sch Syst Informat, 1-1 Rokkodai, Kobe, Hyogo, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>nakashika@me.cs.scitec.kobe-u.ac.jp;
   takashima@me.cs.scitec.kobe-u.ac.jp; takigu@kobe-u.ac.jp;
   ariki@kobe-u.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>35</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>36</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>369</td>
</tr>

<tr>
<td valign="top">EP </td><td>372</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000395050000077</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Silen, H
   <br>Nurminen, J
   <br>Helander, E
   <br>Gabbouj, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Silen, Hanna
   <br>Nurminen, Jani
   <br>Helander, Elina
   <br>Gabbouj, Moncef</td>
</tr>

<tr>
<td valign="top">BE </td><td>Bimbot, F
   <br>Cerisara, C
   <br>Fougeron, C
   <br>Gravier, G
   <br>Lamel, L
   <br>Pellegrino, F
   <br>Perrier, P</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Conversion for Non-Parallel Datasets Using Dynamic Kernel Partial
   Least Squares Regression</td>
</tr>

<tr>
<td valign="top">SO </td><td>14TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2013), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>14th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2013)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 25-29, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Lyon, FRANCE</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; non-parallel data; kernel partial least squares
   regression; INCA alignment</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion aims at converting speech from one speaker to sound as if it was spoken by another specific speaker. The most popular voice conversion approach based on Gaussian mixture modeling tends to suffer either from model overfitting or over smoothing. To overcome the shortcomings of the traditional approach, we recently proposed to use dynamic kernel partial least squares (DKPLS) regression in the framework of parallel-data voice conversion. However, the availability of parallel training data from both the source and target speaker is not always guaranteed. In this paper, we extend the DKPLS-based conversion approach for non-parallel data by combining it with a well-known INCA alignment algorithm. The listening test results indicate that high-quality conversion can be achieved with the proposed combination. Furthermore, the performance of two variations of INCA are evaluated with both intra-lingual and cross-lingual data.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Silen, Hanna; Nurminen, Jani; Helander, Elina; Gabbouj, Moncef] Tampere
   Univ Technol, Dept Signal Proc, Tampere, Finland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Silen, H (reprint author), Tampere Univ Technol, Dept Signal Proc, Tampere, Finland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>hanna.silen@tut.fi; jani.nurminen@tut.fi; elina.helander@tut.fi;
   moncef.gabbouj@tut.fi</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Gabbouj, Moncef</display_name>&nbsp;</font></td><td><font size="3">G-4293-2014&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Gabbouj, Moncef</display_name>&nbsp;</font></td><td><font size="3">0000-0002-9788-2323&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Helander, Elina</display_name>&nbsp;</font></td><td><font size="3">0000-0002-0076-0590&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>373</td>
</tr>

<tr>
<td valign="top">EP </td><td>377</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000395050000078</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Huckvale, M
   <br>Leff, J
   <br>Williams, G</td>
</tr>

<tr>
<td valign="top">AF </td><td>Huckvale, Mark
   <br>Leff, Julian
   <br>Williams, Geoff</td>
</tr>

<tr>
<td valign="top">BE </td><td>Bimbot, F
   <br>Cerisara, C
   <br>Fougeron, C
   <br>Gravier, G
   <br>Lamel, L
   <br>Pellegrino, F
   <br>Perrier, P</td>
</tr>

<tr>
<td valign="top">TI </td><td>Avatar Therapy: an audio-visual dialogue system for treating auditory
   hallucinations</td>
</tr>

<tr>
<td valign="top">SO </td><td>14TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2013), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>14th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2013)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 25-29, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Lyon, FRANCE</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; facial animation; audio-visual speech</td>
</tr>

<tr>
<td valign="top">ID </td><td>RANDOMIZED CONTROLLED-TRIAL; COGNITIVE-BEHAVIORAL THERAPY;
   SCHIZOPHRENIC-PATIENTS; PSYCHOTIC SYMPTOMS; RESISTANT</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a radical new therapy for persecutory auditory hallucinations ("voices") which are most commonly found in serious mental illnesses such as schizophrenia. In around 30% of patients these symptoms are not alleviated by anti-psychotic medication. This work is designed to tackle the problem created by the inaccessibility of the patients' experience of voices to the clinician. Patients are invited to create an external representation of their dominant voice hallucination using computer speech and animation technology. Customised graphics software is used to create an avatar that gives a face to the voice, while voice morphing software realises it in audio, in real time. The therapist then conducts a dialogue between the avatar and the patient, with a view to gradually bringing the avatar, and ultimately the hallucinatory voice, under the patient's control. Results of a pilot study reported elsewhere indicate that the approach has potential for dramatic improvements in patient control of the voices after a series of only six short sessions. The focus of this paper is on the audio-visual speech technology which delivers the central aspects of the therapy.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Huckvale, Mark; Williams, Geoff] UCL, Dept Speech Hearing &amp; Phonet,
   London, England.
   <br>[Leff, Julian] UCL, Dept Mental Hlth Sci, London, England.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Huckvale, M (reprint author), UCL, Dept Speech Hearing &amp; Phonet, London, England.</td>
</tr>

<tr>
<td valign="top">EM </td><td>m.huckvale@ucl.ac.uk; j.leff@ucl.ac.uk; geoffrey.williams@ucl.ac.uk</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>392</td>
</tr>

<tr>
<td valign="top">EP </td><td>396</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000395050000082</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Evans, N
   <br>Kinnunen, T
   <br>Yamagishi, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Evans, Nicholas
   <br>Kinnunen, Tomi
   <br>Yamagishi, Junichi</td>
</tr>

<tr>
<td valign="top">BE </td><td>Bimbot, F
   <br>Cerisara, C
   <br>Fougeron, C
   <br>Gravier, G
   <br>Lamel, L
   <br>Pellegrino, F
   <br>Perrier, P</td>
</tr>

<tr>
<td valign="top">TI </td><td>Spoofing and countermeasures for automatic speaker verification</td>
</tr>

<tr>
<td valign="top">SO </td><td>14TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2013), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>14th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2013)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 25-29, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Lyon, FRANCE</td>
</tr>

<tr>
<td valign="top">DE </td><td>spoofing; imposture; automatic speaker verification</td>
</tr>

<tr>
<td valign="top">ID </td><td>CHANNEL COMPENSATION; SYNTHETIC SPEECH; VOICE CONVERSION; MODELS;
   VARIABILITY; RECOGNITION; SECURITY; SYSTEMS</td>
</tr>

<tr>
<td valign="top">AB </td><td>It is widely acknowledged that most biometric systems are vulnerable to spoofing, also known as imposture. While vulnerabilities and countermeasures for other biometric modalities have been widely studied, e.g. face verification, speaker verification systems remain vulnerable. This paper describes some specific vulnerabilities studied in the literature and presents a brief survey of recent work to develop spoofing countermeasures. The paper concludes with a discussion on the need for standard datasets, metrics and formal evaluations which are needed to assess vulnerabilities to spoofing in realistic scenarios without prior knowledge.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Evans, Nicholas] EURECOM, Sophia Antipolis, France.
   <br>[Kinnunen, Tomi] Univ Eastern Finland, Joensuu, Finland.
   <br>[Yamagishi, Junichi] Univ Edinburgh, Edinburgh, Midlothian, Scotland.
   <br>[Yamagishi, Junichi] Natl Inst Informat, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Evans, N (reprint author), EURECOM, Sophia Antipolis, France.</td>
</tr>

<tr>
<td valign="top">EM </td><td>evans@eurecom.fr; tomi.kinnunen@uef.fi; jyamagis@inf.ed.ac.uk</td>
</tr>

<tr>
<td valign="top">TC </td><td>27</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>28</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>925</td>
</tr>

<tr>
<td valign="top">EP </td><td>929</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000395050000197</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hautamaki, RG
   <br>Kinnunen, T
   <br>Hautamaki, V
   <br>Leino, T
   <br>Laukkanen, AM</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hautamaki, Rosa Gonzalez
   <br>Kinnunen, Tomi
   <br>Hautamaki, Ville
   <br>Leino, Timo
   <br>Laukkanen, Anne-Maria</td>
</tr>

<tr>
<td valign="top">BE </td><td>Bimbot, F
   <br>Cerisara, C
   <br>Fougeron, C
   <br>Gravier, G
   <br>Lamel, L
   <br>Pellegrino, F
   <br>Perrier, P</td>
</tr>

<tr>
<td valign="top">TI </td><td>I-vectors meet imitators: on vulnerability of speaker verification
   systems against voice mimicry</td>
</tr>

<tr>
<td valign="top">SO </td><td>14TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2013), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>14th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2013)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 25-29, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Lyon, FRANCE</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice imitation; speaker recognition; mimicry attack</td>
</tr>

<tr>
<td valign="top">ID </td><td>ATTACKS; SPEECH</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice imitation is mimicry of another speaker's voice characteristics and speech behavior. Professional voice mimicry can create entertaining, yet realistic sounding target speaker renditions. As mimicry tends to exaggerate prosodic, idiosyncratic and lexical behavior, it is unclear how modem spectral-feature automatic speaker verification systems respond to mimicry "attacks". We study the vulnerability of two well-known speaker recognition systems, traditional Gaussian mixture model universal background model (GMM-UBM) and a state-of-the-art i-vector classifier with cosine scoring. The material consists of one professional Finnish imitator impersonating five well-known Finnish public figures. In a carefully controlled setting, mimicry attack does slightly increase the false acceptance rate for the i-vector system, but generally this is not alarmingly large in comparison to voice conversion or playback attacks.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Hautamaki, Rosa Gonzalez; Kinnunen, Tomi; Hautamaki, Ville] Univ
   Eastern Finland, Sch Comp, Joensuu, Finland.
   <br>[Leino, Timo; Laukkanen, Anne-Maria] Univ Tampere, Sch Educ, Speech &amp;
   Voice Res Lab, Tampere, Finland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hautamaki, RG (reprint author), Univ Eastern Finland, Sch Comp, Joensuu, Finland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>rgonza@cs.uef.fi; tkinnu@cs.uef.fi; villeh@cs.uef.fi; timo.leino@uta.fi;
   Anne-Maria.Laukkanen@uta.fi</td>
</tr>

<tr>
<td valign="top">TC </td><td>12</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>12</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>930</td>
</tr>

<tr>
<td valign="top">EP </td><td>934</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000395050000198</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Alegre, F
   <br>Vipperla, R
   <br>Amehraye, A
   <br>Evans, N</td>
</tr>

<tr>
<td valign="top">AF </td><td>Alegre, Federico
   <br>Vipperla, Ravichander
   <br>Amehraye, Asmaa
   <br>Evans, Nicholas</td>
</tr>

<tr>
<td valign="top">BE </td><td>Bimbot, F
   <br>Cerisara, C
   <br>Fougeron, C
   <br>Gravier, G
   <br>Lamel, L
   <br>Pellegrino, F
   <br>Perrier, P</td>
</tr>

<tr>
<td valign="top">TI </td><td>A new speaker verification spoofing countermeasure based on local binary
   patterns</td>
</tr>

<tr>
<td valign="top">SO </td><td>14TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2013), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>14th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2013)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 25-29, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Lyon, FRANCE</td>
</tr>

<tr>
<td valign="top">DE </td><td>speaker verification; biometrics; imposture; countermeasures; local
   binary patterns</td>
</tr>

<tr>
<td valign="top">ID </td><td>FREQUENCY</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a new countermeasure for the protection of automatic speaker verification systems from spoofed, converted voice signals. The new countermeasure is based on the analysis of a sequence of acoustic feature vectors using Local Binary Patterns (LBPs). Compared to existing approaches the new countermeasure is less reliant on prior knowledge and affords robust protection from not only voice conversion, for which it is optimised, but also spoofing attacks from speech synthesis and artificial signals, all of which otherwise provoke significant increases in false acceptance. The work highlights the difficulty in detecting converted voice and also discusses the need for formal evaluations to develop new countermeasures which are less reliant on prior knowledge and thus more reflective of practical use cases.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Alegre, Federico; Amehraye, Asmaa; Evans, Nicholas] EURECOM, Multimedia
   Commun Dept, Sophia Antipolis, France.
   <br>[Vipperla, Ravichander] Nuance Commun, Cambridge, England.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Alegre, F (reprint author), EURECOM, Multimedia Commun Dept, Sophia Antipolis, France.</td>
</tr>

<tr>
<td valign="top">EM </td><td>alegre@eurecom.fr; ravichander.vipperla@nuance.com; fillatre@eurecom.fr;
   evans@eurecom.fr</td>
</tr>

<tr>
<td valign="top">TC </td><td>9</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>9</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>940</td>
</tr>

<tr>
<td valign="top">EP </td><td>944</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000395050000200</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wu, ZZ
   <br>Larcher, A
   <br>Lee, KA
   <br>Chng, ES
   <br>Kinnunen, T
   <br>Li, HZ</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wu, Zhizheng
   <br>Larcher, Anthony
   <br>Lee, Kong Aik
   <br>Chng, Eng Siong
   <br>Kinnunen, Tomi
   <br>Li, Haizhou</td>
</tr>

<tr>
<td valign="top">BE </td><td>Bimbot, F
   <br>Cerisara, C
   <br>Fougeron, C
   <br>Gravier, G
   <br>Lamel, L
   <br>Pellegrino, F
   <br>Perrier, P</td>
</tr>

<tr>
<td valign="top">TI </td><td>Vulnerability evaluation of speaker verification under voice conversion
   spoofing: the effect of text constraints</td>
</tr>

<tr>
<td valign="top">SO </td><td>14TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2013), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>14th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2013)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 25-29, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Lyon, FRANCE</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speaker verification; text-dependent; text-independent; voice
   conversion; spoofing attack; security</td>
</tr>

<tr>
<td valign="top">ID </td><td>RECOGNITION; SPEECH</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion, a technique to change one's voice to sound like that of another, poses a threat to even high performance speaker verification system. Vulnerability of text-independent speaker verification systems under spoofing attack, using statistical voice conversion technique, was evaluated and confirmed in our previous work. In this paper, we further extend the study to text-dependent speaker verification systems. In particular, we compare both joint density Gaussian mixture model (JD-GMM) and unit-selection (US) spoofing methods and, for the first time, the performances of text-independent and text-dependent speaker verification systems in a single study. We conduct the experiments using RSR2015 database which is recorded using multiple mobile devices. The experimental results indicate that text-dependent speaker verification system tolerates spoofing attacks better than the text-independent counterpart.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Wu, Zhizheng; Chng, Eng Siong; Li, Haizhou] Nanyang Technol Univ, Sch
   Comp Engn, Singapore, Singapore.
   <br>[Wu, Zhizheng; Chng, Eng Siong; Li, Haizhou] Nanyang Technol Univ,
   Temasek Labs NTU, Singapore, Singapore.
   <br>[Larcher, Anthony; Lee, Kong Aik; Li, Haizhou] Inst Infocomm Res, Human
   Language Technol Dept, Singapore, Singapore.
   <br>[Kinnunen, Tomi] Univ Eastern Finland, Sch Comp, Joensuu, Finland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Wu, ZZ (reprint author), Nanyang Technol Univ, Sch Comp Engn, Singapore, Singapore.</td>
</tr>

<tr>
<td valign="top">EM </td><td>wuzz@ntu.edu.sg</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Li, Haizhou</display_name>&nbsp;</font></td><td><font size="3">Q-6438-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Li, Haizhou</display_name>&nbsp;</font></td><td><font size="3">0000-0001-9158-9401&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>4</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>4</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>950</td>
</tr>

<tr>
<td valign="top">EP </td><td>954</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000395050000202</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Azarov, E
   <br>Vashkevich, M
   <br>Likhachov, D
   <br>Petrovsky, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Azarov, Elias
   <br>Vashkevich, Maxim
   <br>Likhachov, Denis
   <br>Petrovsky, Alexander</td>
</tr>

<tr>
<td valign="top">BE </td><td>Bimbot, F
   <br>Cerisara, C
   <br>Fougeron, C
   <br>Gravier, G
   <br>Lamel, L
   <br>Pellegrino, F
   <br>Perrier, P</td>
</tr>

<tr>
<td valign="top">TI </td><td>Real-time Voice Conversion Using Artificial Neural Networks with
   Rectified Linear Units</td>
</tr>

<tr>
<td valign="top">SO </td><td>14TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2013), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>14th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2013)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 25-29, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Lyon, FRANCE</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; artificial neural networks</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents an approach to parametric voice conversion that can be used in real-time entertainment applications. The approach is based on spectral mapping using an artificial neural network (ANN) with rectified linear units (ReLU). To overcome the oversmoothing problem a special network configuration is proposed that utilizes temporal states of the speaker. The speech is represented using the harmonic plus noise model. The parameters of the model are estimated using instantaneous harmonic parameters. Using objective and subjective measures the proposed voice conversion technique is compared to the main alternative approaches.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Azarov, Elias; Vashkevich, Maxim; Likhachov, Denis; Petrovsky,
   Alexander] Belarusian State Univ Informat &amp; Radioelect, Dept Comp Engn,
   6 P Brovky Str, Minsk 220013, BELARUS.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Azarov, E (reprint author), Belarusian State Univ Informat &amp; Radioelect, Dept Comp Engn, 6 P Brovky Str, Minsk 220013, BELARUS.</td>
</tr>

<tr>
<td valign="top">EM </td><td>azarov@bsuir.by; vashkevich@bsuir.by; likhachov@bsuir.by; palex@bsuir.by</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>1031</td>
</tr>

<tr>
<td valign="top">EP </td><td>1035</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000395050000219</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kobayashi, K
   <br>Doi, H
   <br>Toda, T
   <br>Nakano, T
   <br>Goto, M
   <br>Neubig, G
   <br>Sakti, S
   <br>Nakamura, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kobayashi, Kazuhiro
   <br>Doi, Hironori
   <br>Toda, Tomoki
   <br>Nakano, Tomoyasu
   <br>Goto, Masataka
   <br>Neubig, Graham
   <br>Sakti, Sakriani
   <br>Nakamura, Satoshi</td>
</tr>

<tr>
<td valign="top">BE </td><td>Bimbot, F
   <br>Cerisara, C
   <br>Fougeron, C
   <br>Gravier, G
   <br>Lamel, L
   <br>Pellegrino, F
   <br>Perrier, P</td>
</tr>

<tr>
<td valign="top">TI </td><td>An Investigation of Acoustic Features for Singing Voice Conversion based
   on Perceptual Age</td>
</tr>

<tr>
<td valign="top">SO </td><td>14TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2013), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>14th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2013)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 25-29, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Lyon, FRANCE</td>
</tr>

<tr>
<td valign="top">DE </td><td>singing voice; voice conversion; perceptual age; spectral and prosodic
   features; subjective evaluations</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH SYNTHESIS</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we investigate the acoustic features that can be modified to control the perceptual age of a singing voice. Singers can sing expressively by controlling prosody and vocal timbre, but the varieties of voices that singers can produce are limited by physical constraints. Previous work has attempted to overcome this limitation through the use of statistical voice conversion. This technique makes it possible to convert singing voice characteristics of an arbitrary source singer into those of an arbitrary target singer. However, it is still difficult to intuitively control singing voice characteristics by manipulating parameters corresponding to specific physical traits, such as gender and age. In this paper, we focus on controlling the perceived age of the singer and, as a first step, perform an investigation of the factors that play a part in the listener's perception of the singer's age. The experimental results demonstrate that 1) the perceptual age of singing voices corresponds relatively well to the actual age of the singer, 2) speech analysis/synthesis processing and statistical voice conversion processing don't cause adverse effects on the perceptual age of singing voices, and 3) prosodic features have a larger effect on the perceptual age than spectral features.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kobayashi, Kazuhiro; Doi, Hironori; Toda, Tomoki; Neubig, Graham;
   Sakti, Sakriani; Nakamura, Satoshi] Nara Inst Sci &amp; Technol NAIST, Grad
   Sch Informat Sci, Ikoma, Nara, Japan.
   <br>[Nakano, Tomoyasu; Goto, Masataka] Natl Inst Adv Ind Sci &amp; Technol,
   Tsukuba, Ibaraki, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kobayashi, K (reprint author), Nara Inst Sci &amp; Technol NAIST, Grad Sch Informat Sci, Ikoma, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>kazuhiro-k@is.naist.jp; hironori-d@is.naist.jp; tomoki@is.naist.jp;
   t.nakano@aist.go.jp; m.goto@aist.go.jp; neubig@is.naist.jp;
   ssakti@is.naist.jp; s-nakamura@is.naist.jp</td>
</tr>

<tr xmlns:date="http://exslt.org/dates-and-times">
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Nakano, Tomoyasu</display_name>&nbsp;</font></td><td><font size="3">A-8670-2013&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Goto, Masataka</display_name>&nbsp;</font></td><td><font size="3">K-8205-2012&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Nakano, Tomoyasu</display_name>&nbsp;</font></td><td><font size="3">0000-0001-8014-2209&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Goto, Masataka</display_name>&nbsp;</font></td><td><font size="3">0000-0003-1167-0977&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>1056</td>
</tr>

<tr>
<td valign="top">EP </td><td>1060</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000395050000224</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Doi, H
   <br>Toda, T
   <br>Nakano, T
   <br>Goto, M
   <br>Nakamura, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Doi, Hironori
   <br>Toda, Tomoki
   <br>Nakano, Tomoyasu
   <br>Goto, Masataka
   <br>Nakamura, Satoshi</td>
</tr>

<tr>
<td valign="top">BE </td><td>Bimbot, F
   <br>Cerisara, C
   <br>Fougeron, C
   <br>Gravier, G
   <br>Lamel, L
   <br>Pellegrino, F
   <br>Perrier, P</td>
</tr>

<tr>
<td valign="top">TI </td><td>Evaluation of a Singing Voice Conversion Method Based on Many-to-Many
   Eigenvoice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>14TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2013), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>14th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2013)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 25-29, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Lyon, FRANCE</td>
</tr>

<tr>
<td valign="top">DE </td><td>singing voice; voice conversion; eigenvoice conversion;
   singing-to-singing synthesis; performance evaluation</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we evaluate our proposed singing voice conversion method from various perspectives. To enable singers to freely control their voice timbre of singing voice, we have proposed a singing voice conversion method based on many-to-many eigenvoice conversion (EVC) that enables to convert the voice timbre of an arbitrary source singer into that of another arbitrary target singer using a probabilistic model. Furthermore, to easily develop training data consisting of multiple parallel data sets between a single reference singer and many other singers, a technique for efficiently and effectively generating the parallel data sets from nonparallel singing voice data sets of many singers using a singing-to-singing synthesis system have been proposed. However, we have never conducted sufficient investigations into the effectiveness of these proposed methods. In this paper, we conduct both objective and subjective evaluations to carefully investigate the effectiveness of proposed methods. Moreover, the differences between singing voice conversion and speaking voice conversion are also analyzed. Experimental results show that our proposed method succeeds in enabling people to control their own voice timbre by using only an extremely small amount of the target singing voice.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Doi, Hironori; Toda, Tomoki; Nakamura, Satoshi] Nara Inst Sci &amp;
   Technol, Grad Sch Informat Sci, Nara, Japan.
   <br>[Nakano, Tomoyasu; Goto, Masataka] Natl Inst Adv Ind Sci &amp; Technol,
   Ibaraki, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Doi, H (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>hironori-d@is.naist.jp; tomoki@is.naist.jp; t.nakano@aist.go.jp;
   m.goto@aist.go.jp; s-nakamura@is.naist.jp</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Goto, Masataka</display_name>&nbsp;</font></td><td><font size="3">K-8205-2012&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Nakano, Tomoyasu</display_name>&nbsp;</font></td><td><font size="3">A-8670-2013&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Goto, Masataka</display_name>&nbsp;</font></td><td><font size="3">0000-0003-1167-0977&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Nakano, Tomoyasu</display_name>&nbsp;</font></td><td><font size="3">0000-0001-8014-2209&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>1066</td>
</tr>

<tr>
<td valign="top">EP </td><td>1070</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000395050000226</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Raitio, T
   <br>Suni, A
   <br>Pohjalainen, J
   <br>Airaksinen, M
   <br>Vainio, M
   <br>Alku, P</td>
</tr>

<tr>
<td valign="top">AF </td><td>Raitio, Tuomo
   <br>Suni, Antti
   <br>Pohjalainen, Jouni
   <br>Airaksinen, Manu
   <br>Vainio, Martti
   <br>Alku, Paavo</td>
</tr>

<tr>
<td valign="top">BE </td><td>Bimbot, F
   <br>Cerisara, C
   <br>Fougeron, C
   <br>Gravier, G
   <br>Lamel, L
   <br>Pellegrino, F
   <br>Perrier, P</td>
</tr>

<tr>
<td valign="top">TI </td><td>Analysis and Synthesis of Shouted Speech</td>
</tr>

<tr>
<td valign="top">SO </td><td>14TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2013), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>14th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2013)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 25-29, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Lyon, FRANCE</td>
</tr>

<tr>
<td valign="top">DE </td><td>shouting; speech analysis; speech synthesis</td>
</tr>

<tr>
<td valign="top">ID </td><td>AMPLITUDE QUOTIENT; LINEAR PREDICTION; VOCAL INTENSITY; VOICE;
   INTELLIGIBILITY; ALGORITHM</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this study, the acoustic properties of shouted speech are analyzed in relation to normal speech, and various synthesis techniques for shouting are investigated. The analysis shows large differences between the two styles, which induces difficulties in synthesis. Analysis-synthesis experiments show that the use of spectral estimation methods that are not biased by the sparse harmonics of shouted speech is beneficial. The synthesis of shouting is performed through adaptation and voice conversion. Subjective evaluation of synthesis reveals that, despite quality degradation, the impression of shouting and use of vocal effort is fairly well preserved. In addition, the use of specific spectral estimation methods is found to be beneficial also in adaptation.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Raitio, Tuomo; Pohjalainen, Jouni; Airaksinen, Manu; Alku, Paavo] Aalto
   Univ, Dept Signal Proc &amp; Acoust, Espoo, Finland.
   <br>[Suni, Antti; Vainio, Martti] Univ Helsinki, Dept Behav Sci, Helsinki,
   Finland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Raitio, T (reprint author), Aalto Univ, Dept Signal Proc &amp; Acoust, Espoo, Finland.</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Alku, Paavo</display_name>&nbsp;</font></td><td><font size="3">E-2400-2012&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>1543</td>
</tr>

<tr>
<td valign="top">EP </td><td>1547</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000395050000324</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Azarov, E
   <br>Vashkevich, M
   <br>Petrovsky, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Azarov, Elias
   <br>Vashkevich, Maxim
   <br>Petrovsky, Alexander</td>
</tr>

<tr>
<td valign="top">BE </td><td>Bimbot, F
   <br>Cerisara, C
   <br>Fougeron, C
   <br>Gravier, G
   <br>Lamel, L
   <br>Pellegrino, F
   <br>Perrier, P</td>
</tr>

<tr>
<td valign="top">TI </td><td>Instantaneous Harmonic Representation of Speech Using Multicomponent
   Sinusoidal Excitation</td>
</tr>

<tr>
<td valign="top">SO </td><td>14TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2013), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>14th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2013)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 25-29, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Lyon, FRANCE</td>
</tr>

<tr>
<td valign="top">DE </td><td>harmonic representation of speech; speech morphing</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper introduces a framework for parametric speech modeling that can be used in various speech applications such as text-to-speech synthesis, voice conversion etc. In order to reduce impact of pitch variations the harmonic analysis is done in the warped time scale that is aligned with instantaneous pitch values. It is assumed that each harmonic has its own periodic excitation source that evolves in time and can be modeled as a sum of several sinusoidal components with close frequencies. The parameters of the excitation components are estimated using a modified instantaneous Prony's method. The proposed analysis/synthesis technique is compared with TANDEM-STRAIGHT.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Azarov, Elias; Vashkevich, Maxim; Petrovsky, Alexander] Belarusian
   State Univ Informat &amp; Radioelect, Dept Comp Engn, 6 P Brovky Str, Minsk
   220013, BELARUS.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Azarov, E (reprint author), Belarusian State Univ Informat &amp; Radioelect, Dept Comp Engn, 6 P Brovky Str, Minsk 220013, BELARUS.</td>
</tr>

<tr>
<td valign="top">EM </td><td>azarov@bsuir.by; vashkevich@bsuir.by; palex@bsuir.by</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>1696</td>
</tr>

<tr>
<td valign="top">EP </td><td>1700</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000395050000355</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Azarov, E
   <br>Vashkevich, M
   <br>Likhachov, D
   <br>Petrovsky, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Azarov, Elias
   <br>Vashkevich, Maxim
   <br>Likhachov, Denis
   <br>Petrovsky, Alexander</td>
</tr>

<tr>
<td valign="top">BE </td><td>Bimbot, F
   <br>Cerisara, C
   <br>Fougeron, C
   <br>Gravier, G
   <br>Lamel, L
   <br>Pellegrino, F
   <br>Perrier, P</td>
</tr>

<tr>
<td valign="top">TI </td><td>Real-time and Non-real-time Voice Conversion Systems with Web Interfaces</td>
</tr>

<tr>
<td valign="top">SO </td><td>14TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2013), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>14th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2013)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 25-29, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Lyon, FRANCE</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; VoIP; instantaneous speech parameters; neural networks</td>
</tr>

<tr>
<td valign="top">AB </td><td>Two speech processing systems have been developed for realtime and non-real-time voice conversion. Using the real-time processing the user can apply conversion during voice over IP (VoIP) calls imitating identity of a specified target speaker. Non-real-time processing system converts prerecorded audio books read by a professional reader imitating voice of the user. Both systems require some speech samples of the user for training. The training procedures are similar for both systems however the user is considered as a source speaker in the first case and as a target speaker in the second. For parametric representation of speech we use a speech model based on instantaneous harmonic parameters with multicomponent sinusoidal excitation. The voice conversion itself is made using artificial neural networks (ANN) with rectified linear units. Here we demonstrate implementations of the voice conversion systems with dedicated web interfaces and iPhone application.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Azarov, Elias; Vashkevich, Maxim; Likhachov, Denis; Petrovsky,
   Alexander] Belarusian State Univ Informat &amp; Radioelect, Dept Comp Engn,
   6,Brovky str, Minsk 220013, BELARUS.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Azarov, E (reprint author), Belarusian State Univ Informat &amp; Radioelect, Dept Comp Engn, 6,Brovky str, Minsk 220013, BELARUS.</td>
</tr>

<tr>
<td valign="top">EM </td><td>azarov@bsuir.by; vashkevich@bsuir.by; likhachov@bsuir.by; palex@bsuir.by</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>2661</td>
</tr>

<tr>
<td valign="top">EP </td><td>2662</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000395050001072</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hueber, T
   <br>Bailly, G
   <br>Badin, P
   <br>Elisei, F</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hueber, Thomas
   <br>Bailly, Gerard
   <br>Badin, Pierre
   <br>Elisei, Frederic</td>
</tr>

<tr>
<td valign="top">BE </td><td>Bimbot, F
   <br>Cerisara, C
   <br>Fougeron, C
   <br>Gravier, G
   <br>Lamel, L
   <br>Pellegrino, F
   <br>Perrier, P</td>
</tr>

<tr>
<td valign="top">TI </td><td>Speaker Adaptation of an Acoustic-Articulatory Inversion Model using
   Cascaded Gaussian Mixture Regressions</td>
</tr>

<tr>
<td valign="top">SO </td><td>14TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2013), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>14th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2013)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 25-29, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Lyon, FRANCE</td>
</tr>

<tr>
<td valign="top">DE </td><td>articulatory inversion; voice conversion; speaker adaptation; GMM;
   computer assisted pronunciation training; biofeedback</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH PRODUCTION-MODEL</td>
</tr>

<tr>
<td valign="top">AB </td><td>The article presents a method for adapting a GMM-based acoustic-articulatory inversion model trained on a reference speaker to another speaker. The goal is to estimate the articulatory trajectories in the geometrical space of a reference speaker from the speech audio signal of another speaker. This method is developed in the context of a system of visual biofeedback, aimed at pronunciation training. This system provides a speaker with visual information about his/her own articulation, via a 3D orofacial clone. In previous work, we proposed to use GMM-based voice conversion for speaker adaptation. Acoustic-articulatory mapping was achieved in 2 consecutive steps: 1) converting the spectral trajectories of the target speaker (i.e. the system user) into spectral trajectories of the reference speaker (voice conversion), and 2) estimating the most likely articulatory trajectories of the reference speaker from the converted spectral features (acoustic-articulatory inversion). In this work, we propose to combine these two steps into the same statistical mapping framework, by fusing multiple regressions based on trajectory GMM and maximum likelihood criterion (MLE). The proposed technique is compared to two standard speaker adaptation techniques based respectively on MAP and MLLR.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Hueber, Thomas; Bailly, Gerard; Badin, Pierre; Elisei, Frederic] U
   Stendhal, CNRS, UMR5216, GIPSA Lab,INP,UJF, F-5216 Grenoble, France.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hueber, T (reprint author), U Stendhal, CNRS, UMR5216, GIPSA Lab,INP,UJF, F-5216 Grenoble, France.</td>
</tr>

<tr>
<td valign="top">EM </td><td>thomas.hueber@gipsa-lab.grenoble-inp.fr;
   gerard.bailly@gipsa-lab.grenoble-inp.fr;
   pierre.badin@gipsa-lab.grenoble-inp.fr;
   frederic.elisei@gipsa-lab.grenoble-inp.fr</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>2752</td>
</tr>

<tr>
<td valign="top">EP </td><td>2756</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000395050001093</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Chen, LH
   <br>Ling, ZH
   <br>Song, Y
   <br>Dai, LR</td>
</tr>

<tr>
<td valign="top">AF </td><td>Chen, Ling-Hui
   <br>Ling, Zhen-Hua
   <br>Song, Yan
   <br>Dai, Li-Rong</td>
</tr>

<tr>
<td valign="top">BE </td><td>Bimbot, F
   <br>Cerisara, C
   <br>Fougeron, C
   <br>Gravier, G
   <br>Lamel, L
   <br>Pellegrino, F
   <br>Perrier, P</td>
</tr>

<tr>
<td valign="top">TI </td><td>Joint Spectral Distribution Modeling Using Restricted Boltzmann Machines
   for Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>14TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2013), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>14th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2013)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 25-29, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Lyon, FRANCE</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice Conversion; Restricted Boltzmann Machine; Spectral envelope</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a new spectral modeling and conversion method for voice conversion. In contrast to the conventional Gaussian mixture model (GMM) based methods, we use restricted Boltzmann machines (RBMs) as probability density models to model the joint distributions of source and target spectral features. The Gaussian distribution in each mixture of GMM is replaced by an RBM, which can better capture the inter-dimensional and inter-speaker correlations within the joint spectral features. Spectral conversion is performed by the maximum conditional output probability criterion. Our experimental results show that the similarity and naturalness of the proposed method are significantly improved comparing with the conventional GMM based method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Chen, Ling-Hui; Ling, Zhen-Hua; Song, Yan; Dai, Li-Rong] Univ Sci &amp;
   Technol China, Natl Engn Lab Speech &amp; Language Informat Proc, Hefei,
   Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Chen, LH (reprint author), Univ Sci &amp; Technol China, Natl Engn Lab Speech &amp; Language Informat Proc, Hefei, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>chenlh@mail.ustc.edu.cn; zhling@ustc.edu.cn; songy@ustc.edu.cn;
   lrdail@ustc.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>7</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>7</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>3051</td>
</tr>

<tr>
<td valign="top">EP </td><td>3055</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000395050001154</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wu, ZZ
   <br>Virtanen, T
   <br>Kinnunen, T
   <br>Chng, ES
   <br>Li, HZ</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wu, Zhizheng
   <br>Virtanen, Tuomas
   <br>Kinnunen, Tomi
   <br>Chng, Eng Siong
   <br>Li, Haizhou</td>
</tr>

<tr>
<td valign="top">BE </td><td>Bimbot, F
   <br>Cerisara, C
   <br>Fougeron, C
   <br>Gravier, G
   <br>Lamel, L
   <br>Pellegrino, F
   <br>Perrier, P</td>
</tr>

<tr>
<td valign="top">TI </td><td>Exemplar-based unit selection for voice conversion utilizing temporal
   information</td>
</tr>

<tr>
<td valign="top">SO </td><td>14TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2013), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>14th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2013)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 25-29, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Lyon, FRANCE</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; unit selection; multi-frame exemplar; temporal
   information</td>
</tr>

<tr>
<td valign="top">ID </td><td>ARTIFICIAL NEURAL-NETWORKS; SPEECH RECOGNITION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Although temporal information of speech has been shown to play an important role in perception, most of the voice conversion approaches assume the speech frames are independent of each other, thereby ignoring the temporal information. In this study, we improve conventional unit selection approach by using exemplars which span multiple frames as base units, and also take temporal information constraint into voice conversion by using overlapping frames to generate speech parameters. This approach thus provides more stable concatenation cost and avoids discontinuity problem in conventional unit selection approach. The proposed method also keeps away from the over-smoothing problem in the mainstream joint density Gaussian mixture model (JD-GMM) based conversion method by directly using target speaker's training data for synthesizing the converted speech. Both objective and subjective evaluations indicate that our proposed method outperforms JD-GMM and conventional unit selection methods.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Wu, Zhizheng; Chng, Eng Siong; Li, Haizhou] Nanyang Technol Univ,
   Singapore, Singapore.
   <br>[Virtanen, Tuomas] Tampere Univ Technol, Tampere, Finland.
   <br>[Kinnunen, Tomi] Univ Eastern Finland, Joensuu, Finland.
   <br>[Li, Haizhou] Inst Infocomm Res, Singapore, Singapore.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Wu, ZZ (reprint author), Nanyang Technol Univ, Singapore, Singapore.</td>
</tr>

<tr>
<td valign="top">EM </td><td>wuzz@ntu.edu.sg</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Li, Haizhou</display_name>&nbsp;</font></td><td><font size="3">Q-6438-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>3056</td>
</tr>

<tr>
<td valign="top">EP </td><td>3060</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000395050001155</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hwang, HT
   <br>Tsao, Y
   <br>Wang, HM
   <br>Wang, YR
   <br>Chen, SH</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hwang, Hsin-Te
   <br>Tsao, Yu
   <br>Wang, Hsin-Min
   <br>Wang, Yih-Ru
   <br>Chen, Sin-Horng</td>
</tr>

<tr>
<td valign="top">BE </td><td>Bimbot, F
   <br>Cerisara, C
   <br>Fougeron, C
   <br>Gravier, G
   <br>Lamel, L
   <br>Pellegrino, F
   <br>Perrier, P</td>
</tr>

<tr>
<td valign="top">TI </td><td>Alleviating the Over-Smoothing Problem in GMM-Based Voice Conversion
   with Discriminative Training</td>
</tr>

<tr>
<td valign="top">SO </td><td>14TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2013), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>14th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2013)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 25-29, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Lyon, FRANCE</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; discriminative training; GMM</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we propose a discriminative training (DT) method to alleviate the muffled sound effect caused by over smoothing in the Gaussian mixture model (GMM)-based voice conversion (VC). For the conventional GMM-based VC, we often observed a large degree of ambiguities among acoustic classes (generative classes), determined by the source feature vectors for generating the converted feature vectors, causing the "muffled sound" effect on the converted voice. The proposed DT method is applied to refine the parameters in the maximum likelihood (ML)-trained joint density GMM (JDGMM) in the training stage to reduce the ambiguities among acoustic classes (generative classes) to alleviate the muffled sound effect. Experimental results demonstrate that the DT method significantly enhances the discriminative power between acoustic classes (generative classes) in the objective evaluation and effectively alleviates the muffled sound effect in the subjective evaluation.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Hwang, Hsin-Te; Wang, Yih-Ru; Chen, Sin-Horng] Natl Chiao Tung Univ,
   Dept Elect &amp; Comp Engn, Hsinchu, Taiwan.
   <br>[Tsao, Yu] Acad Sinica, Res Ctr Informat Technol Innovat, Taipei, Taiwan.
   <br>[Hwang, Hsin-Te; Wang, Hsin-Min] Acad Sinica, Inst Informat Sci, Taipei,
   Taiwan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hwang, HT (reprint author), Natl Chiao Tung Univ, Dept Elect &amp; Comp Engn, Hsinchu, Taiwan.; Hwang, HT (reprint author), Acad Sinica, Inst Informat Sci, Taipei, Taiwan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>hwanght@iis.sinica.edu.tw; yu.tsao@citi.sinica.edu.tw;
   whm@iis.sinica.edu.tw; yrwang@cc.nctu.edu.tw; schen@mail.nctu.edu.tw</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>3061</td>
</tr>

<tr>
<td valign="top">EP </td><td>3065</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000395050001156</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Tanaka, K
   <br>Toda, T
   <br>Neubig, G
   <br>Sakti, S
   <br>Nakamura, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Tanaka, Kou
   <br>Toda, Tomoki
   <br>Neubig, Graham
   <br>Sakti, Sakriani
   <br>Nakamura, Satoshi</td>
</tr>

<tr>
<td valign="top">BE </td><td>Bimbot, F
   <br>Cerisara, C
   <br>Fougeron, C
   <br>Gravier, G
   <br>Lamel, L
   <br>Pellegrino, F
   <br>Perrier, P</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Hybrid Approach to Electrolaryngeal Speech Enhancement Based on
   Spectral Subtraction and Statistical Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>14TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2013), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>14th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2013)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 25-29, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Lyon, FRANCE</td>
</tr>

<tr>
<td valign="top">DE </td><td>speaking-aid; electrolaryngeal speech; spectral subtraction; voice
   conversion; hybrid approach</td>
</tr>

<tr>
<td valign="top">AB </td><td>We present a hybrid approach to improving naturalness of electrolaryngeal (EL) speech while minimizing degradation in intelligibility. An electrolarynx is a device that artificially generates excitation sounds to enable laryngectomees to produce EL speech. Although proficient laryngectomees can produce quite intelligible EL speech, it sounds very unnatural due to the mechanical excitation produced by the device. Moreover, the excitation sounds produced by the device often leak outside, adding noise to EL speech. To address these issues, previous work has proposed methods for EL speech enhancement through either noise reduction or voice conversion. The former usually causes no degradation in intelligibility but yields only small improvements in naturalness as the mechanical excitation sounds remain essentially unchanged. On the other hand, the latter method significantly improves naturalness of EL speech using spectral and excitation parameters of natural voices converted from acoustic parameters of EL speech, but it usually causes degradation in intelligibility owing to errors in conversion. We propose a hybrid method using the noise reduction method for enhancing spectral parameters and voice conversion method for predicting excitation parameters. The experimental results demonstrate the proposed method yields significant improvements in naturalness compared with EL speech while keeping intelligibility high enough.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Tanaka, Kou; Toda, Tomoki; Neubig, Graham; Sakti, Sakriani; Nakamura,
   Satoshi] Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Tanaka, K (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ko-t@is.naist.jp; tomoki@is.naist.jp; neubig@is.naist.jp;
   ssakti@is.naist.jp; s-nakamura@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>3066</td>
</tr>

<tr>
<td valign="top">EP </td><td>3070</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000395050001157</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Moriguchi, T
   <br>Toda, T
   <br>Sano, M
   <br>Sato, H
   <br>Neubig, G
   <br>Sakti, S
   <br>Nakamura, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Moriguchi, Takuto
   <br>Toda, Tomoki
   <br>Sano, Motoaki
   <br>Sato, Hiroshi
   <br>Neubig, Graham
   <br>Sakti, Sakriani
   <br>Nakamura, Satoshi</td>
</tr>

<tr>
<td valign="top">BE </td><td>Bimbot, F
   <br>Cerisara, C
   <br>Fougeron, C
   <br>Gravier, G
   <br>Lamel, L
   <br>Pellegrino, F
   <br>Perrier, P</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Digital Signal Processor Implementation of Silent/Electrolaryngeal
   Speech Enhancement based on Real-Time Statistical Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>14TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2013), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>14th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2013)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 25-29, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Lyon, FRANCE</td>
</tr>

<tr>
<td valign="top">DE </td><td>statistical voice conversion; real-time processing; reduction of
   computational cost; DSP; non-audible murmur; electrolaryngeal speech</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we present a digital signal processor (DSP) implementation of real-time statistical voice conversion (VC) for silent speech enhancement and electrolaryngeal speech enhancement. As a silent speech interface, we focus on non audible murmur (NAM), which can be used in situations where audible speech is not acceptable. Electrolaryngeal speech is one of the typical types of alaryngeal speech produced by an alternative speaking method for laryngectornees. However, the sound quality of NAM and electrolaryngeal speech suffers from lack of naturalness. VC has proven to be one of the promising approaches to address this problem, and it has been successfully implemented on devices with sufficient computational resources. An implementation on devices that are highly portable but have limited computational resources would greatly contribute to its practical use. In this paper we further implement real-time VC on a DSP. To implement the two speech enhancement systems based on real-time VC, one from NAM to a whispered voice and the other from electrolaryngeal speech to a natural voice, we propose several methods for reducing computational cost while preserving conversion accuracy. We conduct experimental evaluations and show that real-time VC is capable of running on a DSP with little degradation.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Moriguchi, Takuto; Toda, Tomoki; Neubig, Graham; Sakti, Sakriani;
   Nakamura, Satoshi] Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara,
   Japan.
   <br>[Sano, Motoaki; Sato, Hiroshi] Foster Elect Co Ltd, Akishima, Tokyo,
   Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Moriguchi, T (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>takuto-m@is.naist.jp; tomoki@is.naist.jp; m_sano@foster.co.jp;
   hrssato@foster.co.jp; neubig@is.naist.jp; ssakti@is.naist.jp;
   s-nakamura@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>3071</td>
</tr>

<tr>
<td valign="top">EP </td><td>3075</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000395050001158</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Aryal, S
   <br>Felps, D
   <br>Gutierrez-Osuna, R</td>
</tr>

<tr>
<td valign="top">AF </td><td>Aryal, Sandesh
   <br>Felps, Daniel
   <br>Gutierrez-Osuna, Ricardo</td>
</tr>

<tr>
<td valign="top">BE </td><td>Bimbot, F
   <br>Cerisara, C
   <br>Fougeron, C
   <br>Gravier, G
   <br>Lamel, L
   <br>Pellegrino, F
   <br>Perrier, P</td>
</tr>

<tr>
<td valign="top">TI </td><td>Foreign Accent Conversion through Voice Morphing</td>
</tr>

<tr>
<td valign="top">SO </td><td>14TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2013), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>14th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2013)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 25-29, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Lyon, FRANCE</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice morphing; accent conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEAKER IDENTIFICATION; SPEECH</td>
</tr>

<tr>
<td valign="top">AB </td><td>We present a voice morphing strategy that can be used to generate a continuum of.accent transformations between a foreign speaker and a native speaker. The approach performs a cepstral decomposition of speech into spectral slope and spectral detail. Accent conversions are then generated by combining the spectral slope of the foreign speaker with a morph of the spectral detail of the native speaker. Spectral morphing is achieved by representing the spectral detail through pulse density modulation and averaging pulses in a pair -wise fashion. The technique is validated on parallel recordings from two ARCTIC speakers using both objective and subjective measures of acoustic quality, speaker identity and foreign accent.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Aryal, Sandesh; Felps, Daniel; Gutierrez-Osuna, Ricardo] Texas A&amp;M
   Univ, Dept Comp Sci &amp; Engn, College Stn, TX 77843 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Aryal, S (reprint author), Texas A&amp;M Univ, Dept Comp Sci &amp; Engn, College Stn, TX 77843 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>sandesh@cse.tamu.edu; dlfelps@cse.tamu.edu; rgutier@cse.tamu.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>4</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>4</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>3076</td>
</tr>

<tr>
<td valign="top">EP </td><td>3080</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000395050001159</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Aihara, R
   <br>Takashima, R
   <br>Takiguchi, T
   <br>Ariki, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Aihara, Ryo
   <br>Takashima, Ryoichi
   <br>Takiguchi, Tetsuya
   <br>Ariki, Yasuo</td>
</tr>

<tr>
<td valign="top">BE </td><td>Bimbot, F
   <br>Cerisara, C
   <br>Fougeron, C
   <br>Gravier, G
   <br>Lamel, L
   <br>Pellegrino, F
   <br>Perrier, P</td>
</tr>

<tr>
<td valign="top">TI </td><td>Exemplar-based Individuality-Preserving Voice Conversion for
   Articulation Disorders in Noisy Environments</td>
</tr>

<tr>
<td valign="top">SO </td><td>14TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2013), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>14th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2013)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 25-29, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Lyon, FRANCE</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice Conversion; NMF; Articulation Disorders; Noise Robustness;
   Assistive Technologies</td>
</tr>

<tr>
<td valign="top">AB </td><td>We present in this paper a noise robust voice conversion (VC) method for a person with an articulation disorder resulting from athetoid cerebral palsy. The movements of such speakers are limited by their athetoid symptoms, and their consonants are often unstable or unclear, which makes it difficult for them to communicate. In this paper, exemplar-based spectral conversion using Non-negative Matrix Factorization (NMF) is applied to a voice with an articulation disorder in real noisy environments. In this paper, in order to deal with background noise, an input noisy source signal is decomposed into the clean source exemplars and noise exemplars by NMF. Also, to preserve the speaker's individuality, we use a combined dictionary that was constructed from the source speaker's vowels and target speaker's consonants. The effectiveness of this method was confirmed by comparing its effectiveness with that of a conventional Gaussian Mixture Model (GMM)-based method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Aihara, Ryo; Takashima, Ryoichi; Takiguchi, Tetsuya; Ariki, Yasuo] Kobe
   Univ, Grad Sch Syst Informat, Kobe, Hyogo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Aihara, R (reprint author), Kobe Univ, Grad Sch Syst Informat, Kobe, Hyogo, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>aihara@me.cs.scitec.kobe-u.ac.jp; takashima@me.cs.scitec.kobe-u.ac.jp;
   takigu@kobe-u.ac.jp; ariki@kobe-u.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>3604</td>
</tr>

<tr>
<td valign="top">EP </td><td>3608</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000395050001268</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ahangar, M
   <br>Ghorbandoost, M
   <br>Sheikhzadeh, H
   <br>Raahemifar, K
   <br>Shahrebabaki, AS
   <br>Amini, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ahangar, Mohsen
   <br>Ghorbandoost, Mostafa
   <br>Sheikhzadeh, Hamid
   <br>Raahemifar, Kaamran
   <br>Shahrebabaki, Abdoreza Sabzi
   <br>Amini, Jamal</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Conversion Based on State Space Model and Considering Global
   Variance</td>
</tr>

<tr>
<td valign="top">SO </td><td>2013 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION
   TECHNOLOGY (IEEE ISSPIT 2013)</td>
</tr>

<tr>
<td valign="top">SE </td><td>IEEE International Symposium on Signal Processing and Information
   Technology</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Symposium on Signal Processing and Information
   Technology (ISSPIT)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 12-15, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Athens, GREECE</td>
</tr>

<tr>
<td valign="top">DE </td><td>State space model; global variance; voice conversion</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion based on State Space Model (SSM) has been recently proposed to address the discontinuity problem in the traditional frame-based voice conversion by considering the spectral envelope evolutions. However, the results are over-smoothed. To resolve this problem, in this paper we propose a new procedure for integrating the global variance constraint into the SSM-based voice conversion. Moreover, unlike the SSM-based method, we allow the state-vector order to be higher than the feature-vector order. Experimental results verify that the proposed method significantly improves the performance of the SSM-based voice conversion in terms of speaker individuality and speech quality. Our experiments also show that the proposed method outperforms the well-known Maximum Likelihood estimation method that considers the Global Variance in terms of speech quality.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ahangar, Mohsen; Ghorbandoost, Mostafa; Sheikhzadeh, Hamid;
   Shahrebabaki, Abdoreza Sabzi; Amini, Jamal] Amirkabir Univ Technol, Dept
   Elect Engn, Tehran, Iran.
   <br>[Raahemifar, Kaamran] Ryerson Univ, Dept Elect &amp; Comp Engn, Toronto, ON
   M5B 2K3, Canada.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ahangar, M (reprint author), Amirkabir Univ Technol, Dept Elect Engn, POB 15875-4413,424 Hafez Ave, Tehran, Iran.</td>
</tr>

<tr>
<td valign="top">EM </td><td>Ahangar@aut.ac.ir; m.ghorbandoost@aut.ac.ir; hsheikh@aut.ac.ir;
   kraahemi@ee.ryerson.ca; rezasabzi@aut.ac.ir; jamal.amini@aut.ac.ir</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>416</td>
</tr>

<tr>
<td valign="top">EP </td><td>421</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000353563200070</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Amini, J
   <br>Shahrebabaki, AS
   <br>Shokouhi, N
   <br>Sheikhzadeh, H
   <br>Raahemifa, K
   <br>Eslami, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Amini, Jamal
   <br>Shahrebabaki, Abdoreza Sabzi
   <br>Shokouhi, Navid
   <br>Sheikhzadeh, Hamid
   <br>Raahemifa, Kaamran
   <br>Eslami, Mehdi</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Speech Analysis/Synthesis by Gaussian Mixture Approximation of the
   Speech Spectrum for Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>2013 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION
   TECHNOLOGY (IEEE ISSPIT 2013)</td>
</tr>

<tr>
<td valign="top">SE </td><td>IEEE International Symposium on Signal Processing and Information
   Technology</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Symposium on Signal Processing and Information
   Technology (ISSPIT)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 12-15, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Athens, GREECE</td>
</tr>

<tr>
<td valign="top">DE </td><td>Analysis/Synthesis; Feature Extraction; Voice Conversion; GMM; STRAIGHT</td>
</tr>

<tr>
<td valign="top">ID </td><td>FREQUENCY</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion typically employs spectral features to convert a source voice to a target voice. In this paper, we propose a simple method of fitting the STRAIGHT spectrum with Gaussian mixture (GM) models for speech analysis/synthesis and spectral modification. The mean values of the Gaussians are pre-determined based on Mel-frequency spacing. The standard deviations are also adaptively adjusted using the constant-Q principle and the spectrum amplitudes. Finally, the weights of the Gaussians are determined by sampling the log-spectrum at Mel-frequencies. The proposed analysis/synthesis method (MFLS-GM) is employed for speech analysis/synthesis and voice conversion. Subjective evaluations employing MOS and ABX demonstrate superior performance of the voice conversion using the MFLS-GM compared to systems employing MFCC features. The computation cost of the proposed analysis/synthesis method is also much lower than those based on MFCC.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Amini, Jamal; Shahrebabaki, Abdoreza Sabzi; Shokouhi, Navid;
   Sheikhzadeh, Hamid; Eslami, Mehdi] Amirkabir Univ Technol, Dept Elect
   Engn, Tehran, Iran.
   <br>[Raahemifa, Kaamran] Ryerson Univ, Dept Elect &amp; Comp Engn, Toronto, ON
   M5B 2K3, Canada.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Amini, J (reprint author), Amirkabir Univ Technol, Dept Elect Engn, POB 15875-4413,424 Hafez Ave, Tehran, Iran.</td>
</tr>

<tr>
<td valign="top">EM </td><td>Jamal.amini@aut.ac.ir; rezasabzi@aut.ac.ir; navid.shokouhi@utdallas.edu;
   hsheikh@aut.ac.ir; kraahemi@ee.ryerson.ca; meslami@eetd.kntu.ac.ir</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>428</td>
</tr>

<tr>
<td valign="top">EP </td><td>433</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000353563200072</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Fujii, T
   <br>Aihara, R
   <br>Takashima, R
   <br>Takiguchi, T
   <br>Ariki, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Fujii, Takao
   <br>Aihara, Ryo
   <br>Takashima, Ryoichi
   <br>Takiguchi, Tetsuya
   <br>Ariki, Yasuo</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Conversion based on Non-negative Matrix Factorization in Noisy
   Environments</td>
</tr>

<tr>
<td valign="top">SO </td><td>2013 IEEE/SICE INTERNATIONAL SYMPOSIUM ON SYSTEM INTEGRATION (SII)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE/SICE International Symposium on System Integration (SII)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 15-17, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Kobe, JAPAN</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a voice conversion (VC) technique for noisy environments. We prepared parallel exemplars (dictionary) that consist of the source and target exemplars, which have the same texts uttered by the source and target speakers. The input source signal is decomposed into the source exemplars, noise exemplars obtained from the input signal, and their weights (activities). Then, the converted signal is obtained by calculating the linear combination of the target exemplars and the weights which are calculated using the source exemplars. In the proposed method, a Gaussian Mixture Model (GMM)-based conversion method is also applied to the feature vectors generated by the sparse coding in order to compensate a mismatch between the weights of source and target exemplars. The effectiveness of this method was confirmed by comparing its effectiveness with that of a conventional method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Fujii, Takao; Aihara, Ryo; Takashima, Ryoichi; Takiguchi, Tetsuya;
   Ariki, Yasuo] Kobe Univ, Grad Sch Syst Informat, Nada Ku, Kobe, Hyogo
   6578501, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Fujii, T (reprint author), Kobe Univ, Grad Sch Syst Informat, Nada Ku, 1-1 Rokkodai, Kobe, Hyogo 6578501, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>fujii@me.cs.scitec.kobe-u.ac.jp; aihara@me.cs.scitec.kobe-u.ac.jp;
   takigu@kobe-u.ac.jp; ariki@kobe-u.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>495</td>
</tr>

<tr>
<td valign="top">EP </td><td>498</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000352950900086</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sun, XJ
   <br>Zhang, XW
   <br>Yang, JB
   <br>Cao, TY</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sun, Xinjian
   <br>Zhang, Xiongwei
   <br>Yang, Jibin
   <br>Cao, Tieyong</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Conversion Using Bilinear Model Integrated with Joint GMM-based
   Classification</td>
</tr>

<tr>
<td valign="top">SO </td><td>2013 INTERNATIONAL CONFERENCE ON INFORMATION SCIENCE AND TECHNOLOGY
   (ICIST)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Information Science and Technology</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Information Science and Technology (ICIST)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 23-25, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Yangzhou, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">AB </td><td>Bilinear Model (BM) can express both characteristics within a speaker (style) and phonemes across speakers (content) independently in a speech database. It has a successful application in voice conversion (VC) by extrapolation. However, extrapolation suffers an undesired repetition of BM building and a large-scale estimation of parameters. To tackle these problems, we propose to enhance the normal BM-based VC scheme by integrating a joint Gaussian Mixture Model (GMM)-based classification, assuming that the GMM components correspond to the quasi-phoneme content classes. The enhanced scheme not only optimizes the VC algorithm in computation, but also improves the quality of speech compared to the normal BM-based one, as well as traditional GMM-based mapping system in evaluation experiments.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Sun, Xinjian; Zhang, Xiongwei; Yang, Jibin; Cao, Tieyong] PLA Univ Sci
   &amp; Technol, Coll Commun Engn, Nanjing, Jiangsu, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sun, XJ (reprint author), PLA Univ Sci &amp; Technol, Coll Commun Engn, Nanjing, Jiangsu, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>sunxj99@gmail.com; xwzhang@public1.ptt.js.cn; yjbice@sina.com;
   cty_ice@163.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>1225</td>
</tr>

<tr>
<td valign="top">EP </td><td>1228</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000363478300266</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Gu, HY
   <br>Chang, JW</td>
</tr>

<tr>
<td valign="top">AF </td><td>Gu, Hung-Yan
   <br>Chang, Jia-Wei</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>A VOICE CONVERSION METHOD MAPPING SEGMENTED FRAMES WITH LINEAR
   MULTIVARIATE REGRESSION</td>
</tr>

<tr>
<td valign="top">SO </td><td>PROCEEDINGS OF 2013 INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND
   CYBERNETICS (ICMLC), VOLS 1-4</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Machine Learning and Cybernetics</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Machine Learning and Cybernetics (ICMLC)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUL 14-17, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Tianjin, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; Linear multivariate regression; Gaussian mixture
   model; Discrete cepstrum coefficients</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we study a different spectral mapping mechanism based on linear multivariate regression (LMR). Such LMR based spectral mapping methods are intended to alleviate the problem of spectral over-smoothing usually encountered by a GMM based method. First, we derive a solution formula to determine the best LMR mapping matrix. Then, for experimental evaluation, we record a parallel corpus, and adopt discrete cepstrum coefficients (DCC) as the spectral features. Next, we label and segment the recorded sentences into the speech units of syllable initials and finals. Hence, an LMR mapping matrix is trained for each syllable initial or final type. In terms of these LMR mapping matrices, we construct a voice conversion system. According to the measured average conversion errors, our system when using the mapping method, LMR_F, can indeed outperform a conventional GMM based voice conversion system. In addition, listening tests are conducted. The results show that the converted speech by our system is slightly better than that converted by a conventional GMM based system.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Gu, Hung-Yan; Chang, Jia-Wei] Natl Taiwan Univ Sci &amp; Technol, Dept Comp
   Sci &amp; Informat Engn, Taipei 106, Taiwan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Gu, HY (reprint author), Natl Taiwan Univ Sci &amp; Technol, Dept Comp Sci &amp; Informat Engn, Taipei 106, Taiwan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>guhy@mail.ntust.edu.tw; m9815064@mail.ntust.edu.tw</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>1136</td>
</tr>

<tr>
<td valign="top">EP </td><td>1141</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000366853800155</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Huang, YC
   <br>Wu, CH
   <br>Chao, YT</td>
</tr>

<tr>
<td valign="top">AF </td><td>Huang, Yi-Chin
   <br>Wu, Chung-Hsien
   <br>Chao, Yu-Ting</td>
</tr>

<tr>
<td valign="top">TI </td><td>Personalized Spectral and Prosody Conversion Using Frame-Based Codeword
   Distribution and Adaptive CRF</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Conditional random field; frame alignment; principal component analysis;
   prosodic boundary; voice conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION; SPEECH SYNTHESIS; MAXIMUM-LIKELIHOOD; ALGORITHM</td>
</tr>

<tr>
<td valign="top">AB </td><td>This study proposes a voice conversion-based approach to personalized text-to-speech (TTS) synthesis. The conversion functions, trained using a small parallel corpus with source and target speech data, can impose the voice characteristics of a target speaker on an existing synthesizer. Frame alignment between a pair of sentences in the parallel corpus is generally used for training voice conversion functions. However, with incorrect alignment, the resultant conversion functions may generate unacceptable conversion results. Traditional frame alignment using minimal spectral distance between the frame-based feature vectors of the source and the target phone sequences can be imprecise because the voice properties of the source and target phones inherently differ. In the proposed method, feature vectors of the parallel corpus are transformed into codewords in an eigenspace. A more precise frame alignment can be obtained by integrating the codeword occurrence distributions into distance estimation. In addition to the spectral property, a prosodic word/phrase boundary prediction model was constructed using an adaptive conditional random field (CRF) to generate personalized prosodic information. Objective and subjective tests were conducted to evaluate the performance of the proposed approach. The experimental results showed that the proposed voice conversion method, based on distribution-based alignment and prosodic word boundary detection, can improve the speech quality and speaker similarity of the converted speech. Compared to other methods, the evaluation results verified the improved performance of the proposed method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Huang, Yi-Chin; Wu, Chung-Hsien; Chao, Yu-Ting] Natl Cheng Kung Univ,
   Dept Comp Sci &amp; Informat Engn, Tainan 701, Taiwan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Huang, YC (reprint author), Natl Cheng Kung Univ, Dept Comp Sci &amp; Informat Engn, Tainan 701, Taiwan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ychin.huang@gmail.com; chwu@csie.ncku.edu.tw; rita70924@gmail.com</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Wu, Chung-Hsien</display_name>&nbsp;</font></td><td><font size="3">E-7970-2013&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>7</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>7</td>
</tr>

<tr>
<td valign="top">PD </td><td>JAN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">VL </td><td>21</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>49</td>
</tr>

<tr>
<td valign="top">EP </td><td>60</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASL.2012.2213247</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000310393200005</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wu, ZZ
   <br>Kinnunen, T
   <br>Chng, ES
   <br>Li, HZ</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wu, Zhizheng
   <br>Kinnunen, Tomi
   <br>Chng, Eng Siong
   <br>Li, Haizhou</td>
</tr>

<tr>
<td valign="top">TI </td><td>Mixture of Factor Analyzers Using Priors From Non-Parallel Speech for
   Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE SIGNAL PROCESSING LETTERS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; prior knowledge; factor analysis; mixture of factor
   analyzers</td>
</tr>

<tr>
<td valign="top">AB </td><td>A robust voice conversion function relies on a large amount of parallel training data, which is difficult to collect in practice. To tackle the sparse parallel training data problem in voice conversion, this paper describes a mixture of factor analyzers method which integrates prior knowledge from non-parallel speech into the training of conversion function. The experiments on CMU ARCTIC corpus show that the proposed method improves the quality and similarity of converted speech. With both objective and subjective evaluations, we show the proposed method outperforms the baseline GMM method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Wu, Zhizheng; Chng, Eng Siong; Li, Haizhou] Nanyang Technol Univ, Sch
   Comp Engn, Singapore 639798, Singapore.
   <br>[Wu, Zhizheng; Chng, Eng Siong] Nanyang Technol Univ, Temasek Lab NTU,
   Singapore 639798, Singapore.
   <br>[Kinnunen, Tomi] Univ Eastern Finland, Sch Comp, Joensuu, Finland.
   <br>[Li, Haizhou] Inst Infocomm Res, Singapore 138632, Singapore.
   <br>[Li, Haizhou] Univ New S Wales, Sydney, NSW 2052, Australia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Wu, ZZ (reprint author), Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.</td>
</tr>

<tr>
<td valign="top">EM </td><td>wuzz@ntu.edu.sg; tkinnu@cs.joensuu.fi; aseschng@ntu.edu.sg;
   hli@i2r.a-star.edu.sg</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Li, Haizhou</display_name>&nbsp;</font></td><td><font size="3">Q-6438-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Li, Haizhou</display_name>&nbsp;</font></td><td><font size="3">0000-0001-9158-9401&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>10</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>12</td>
</tr>

<tr>
<td valign="top">PD </td><td>DEC</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">VL </td><td>19</td>
</tr>

<tr>
<td valign="top">IS </td><td>12</td>
</tr>

<tr>
<td valign="top">BP </td><td>914</td>
</tr>

<tr>
<td valign="top">EP </td><td>917</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/LSP.2012.2225615</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000310988100003</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Rao, KS</td>
</tr>

<tr>
<td valign="top">AF </td><td>Rao, Krothapalli Sreenivasa</td>
</tr>

<tr>
<td valign="top">TI </td><td>Unconstrained Pitch Contour Modification Using Instants of Significant
   Excitation</td>
</tr>

<tr>
<td valign="top">SO </td><td>CIRCUITS SYSTEMS AND SIGNAL PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Pitch contour modification; Excitation source; LP residual; Instants of
   significant excitation (epochs); Pitch period; Voice conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>LINEAR PREDICTION; TIME-SCALE; SPEECH; EXTRACTION; TRANSFORMATION;
   FEATURES</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper proposes a flexible method for pitch contour modification using the instants of significant excitation of the vocal tract system during the production of speech. The instants of significant excitation correspond to the instants of glottal closure (epochs) in the case of voiced speech, and to some random excitations like onset of burst in the case of nonvoiced speech. Instants of significant excitation are computed from the Linear Prediction (LP) residual of speech signals by using the property of average group-delay of minimum phase signals. The modification of pitch contour is achieved by manipulating the LP residual with the help of the knowledge of the instants of significant excitation. The modified residual is used to excite the time-varying filter, whose parameters are derived from the original speech signal. Perceptual quality of the synthesized speech is good, and is without any significant distortion. The proposed method is evaluated using waveforms, spectrograms and listening tests. Listening tests are performed on voice conversion application, where the source speaker's pitch contour is modified by the proposed method according to the target speaker's pitch contour. The performance of the proposed method is compared with Linear Prediction Pitch Synchronous Overlap and Add (LP-PSOLA) method using listening tests, for the voice conversion application.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Indian Inst Technol, Sch Informat Technol, Kharagpur 721302, W Bengal,
   India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Rao, KS (reprint author), Indian Inst Technol, Sch Informat Technol, Kharagpur 721302, W Bengal, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ksrao@iitkgp.ac.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>6</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>6</td>
</tr>

<tr>
<td valign="top">PD </td><td>DEC</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">VL </td><td>31</td>
</tr>

<tr>
<td valign="top">IS </td><td>6</td>
</tr>

<tr>
<td valign="top">BP </td><td>2133</td>
</tr>

<tr>
<td valign="top">EP </td><td>2152</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/s00034-012-9428-8</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000310230600016</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Laskar, RH
   <br>Chakrabarty, D
   <br>Talukdar, FA
   <br>Rao, KS
   <br>Banerjee, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Laskar, R. H.
   <br>Chakrabarty, D.
   <br>Talukdar, F. A.
   <br>Rao, K. Sreenivasa
   <br>Banerjee, K.</td>
</tr>

<tr>
<td valign="top">TI </td><td>Comparing ANN and GMM in a voice conversion framework</td>
</tr>

<tr>
<td valign="top">SO </td><td>APPLIED SOFT COMPUTING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Artificial neural networks; Gaussian mixture models; Prosody; Pitch
   contour; Intonation patterns; Duration patterns; Energy profiles;
   Residual modification</td>
</tr>

<tr>
<td valign="top">ID </td><td>FEATURES; SPEECH; TRANSFORMATION; PREDICTION; EXCITATION; NETWORKS;
   SPECTRUM</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we present a comparative analysis of artificial neural networks (ANNs) and Gaussian mixture models (GMMs) for design of voice conversion system using line spectral frequencies (LSFs) as feature vectors. Both the ANN and GMM based models are explored to capture nonlinear mapping functions for modifying the vocal tract characteristics of a source speaker according to a desired target speaker. The LSFs are used to represent the vocal tract transfer function of a particular speaker. Mapping of the intonation patterns (pitch contour) is carried out using a codebook based model at segmental level. The energy profile of the signal is modified using a fixed scaling factor defined between the source and target speakers at the segmental level. Two different methods for residual modification such as residual copying and residual selection methods are used to generate the target residual signal. The performance of ANN and GMM based voice conversion (VC) system are conducted using subjective and objective measures. The results indicate that the proposed ANN-based model using LSFs feature set may be used as an alternative to state-of-the-art GMM-based models used to design a voice conversion system. (C) 2012 Elsevier B.V. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Laskar, R. H.; Talukdar, F. A.; Banerjee, K.] Natl Inst Technol
   Silchar, Dept Elect &amp; Commun Engn, Silchar 788010, Assam, India.
   <br>[Chakrabarty, D.] Indian Inst Technol Guwahati, Dept Elect &amp; Commun
   Engn, Gauhati 781039, Assam, India.
   <br>[Rao, K. Sreenivasa] IIT Kharagpur, Sch Informat Technol, Kharagpur
   721302, W Bengal, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Laskar, RH (reprint author), Natl Inst Technol Silchar, Dept Elect &amp; Commun Engn, Silchar 788010, Assam, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>rabul18@yahoo.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>15</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>16</td>
</tr>

<tr>
<td valign="top">PD </td><td>NOV</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">VL </td><td>12</td>
</tr>

<tr>
<td valign="top">IS </td><td>11</td>
</tr>

<tr>
<td valign="top">BP </td><td>3332</td>
</tr>

<tr>
<td valign="top">EP </td><td>3342</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.asoc.2012.05.027</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000308666500003</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Toda, T
   <br>Nakagiri, M
   <br>Shikano, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Toda, Tomoki
   <br>Nakagiri, Mikihiro
   <br>Shikano, Kiyohiro</td>
</tr>

<tr>
<td valign="top">TI </td><td>Statistical Voice Conversion Techniques for Body-Conducted Unvoiced
   Speech Enhancement</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Silent speech; body-conducted unvoiced speech; voice conversion;
   nonaudible murmur; whispered voice</td>
</tr>

<tr>
<td valign="top">ID </td><td>TRANSMITTED SPEECH; RECOGNITION</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we present statistical approaches to enhance body-conducted unvoiced speech for silent speech communication. A body-conductive microphone called nonaudible murmur (NAM) microphone is effectively used to detect very soft unvoiced speech such as NAM or a whispered voice while keeping speech sounds emitted outside almost inaudible. However, body-conducted unvoiced speech is difficult to use in human-to-human speech communication because it sounds unnatural and less intelligible owing to the acoustic change caused by body conduction. To address this issue, voice conversion (VC) methods from NAM to normal speech (NAM-to-Speech) and to a whispered voice (NAM-to-Whisper) are proposed, where the acoustic features of body-conducted unvoiced speech are converted into those of natural voices in a probabilistic manner using Gaussian mixture models (GMMs). Moreover, these methods are extended to convert not only NAM but also a body-conducted whispered voice (BCW) as another type of body-conducted unvoiced speech. Several experimental evaluations are conducted to demonstrate the effectiveness of the proposed methods. The experimental results show that 1) NAM-to-Speech effectively improves intelligibility but it causes degradation of naturalness owing to the difficulty of estimating natural fundamental frequency contours from unvoiced speech; 2) NAM-to-Whisper significantly outperforms NAM-to-Speech in terms of both intelligibility and naturalness; and 3) a single conversion model capable of converting both NAM and BCW is effectively developed in our proposed VC methods.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Toda, Tomoki; Shikano, Kiyohiro] Nara Inst Sci &amp; Technol, Grad Sch
   Informat Sci, Nara 6300192, Japan.
   <br>[Nakagiri, Mikihiro] Panasonic Corp, Osaka 5718501, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Toda, T (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara 6300192, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>tomoki@is.naist.jp; shikano@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>56</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>59</td>
</tr>

<tr>
<td valign="top">PD </td><td>NOV</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">VL </td><td>20</td>
</tr>

<tr>
<td valign="top">IS </td><td>9</td>
</tr>

<tr>
<td valign="top">BP </td><td>2505</td>
</tr>

<tr>
<td valign="top">EP </td><td>2517</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASL.2012.2205241</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000308108900001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Tsai, WH
   <br>Lee, HC</td>
</tr>

<tr>
<td valign="top">AF </td><td>Tsai, Wei-Ho
   <br>Lee, Hsin-Chieh</td>
</tr>

<tr>
<td valign="top">TI </td><td>Singer Identification Based on Spoken Data in Voice Characterization</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Model adaptation; singer identification (SID); speaker identification
   (SPID); voice conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>SINGING-VOICE; SPEAKER VERIFICATION; RECOGNITION; MODELS; ROBUST;
   TUTORIAL; FEATURES; SPEECH</td>
</tr>

<tr>
<td valign="top">AB </td><td>Currently existing singer identification (SID) methods follow the framework of speaker identification (SPID), which requires that singing data be collected beforehand to establish each singer's voice characteristics. This framework, however, is unsuitable for many SID applications, because acquiring solo a cappella from each singer is usually not as feasible as collecting spoken data in SPID applications. Since a cappella data are difficult to acquire, many studies have tried to improve SID accuracies when only accompanied singing data are available for training; but, the improvements are not always satisfactory. Recognizing that spoken data are usually available easily, this work investigates the possibility of characterizing singers' voices using the spoken data instead of their singing data. Unfortunately, our experiment found it difficult to replace singing data fully by using spoken data in singer voice characterization, due to the significant difference between singing and speech voice for most people. Thus, we propose two alternative solutions based on the use of few singing data. The first solution aims at adapting a speech-derived model to cover singing voice characteristics. The second solution attempts to establish the relationships between speech and singing using a transformation, so that an unknown test singing clip can be converted into its speech counterpart and then identified using speech-derived models; or alternatively, training data can be converted from speech into singing to generate a singer model capable of matching test singing clips. Our experiments conducted using a 20-singer database validate the proposed solutions.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Tsai, Wei-Ho; Lee, Hsin-Chieh] Natl Taipei Univ Technol, Dept Elect
   Engn, Taipei 10608, Taiwan.
   <br>[Tsai, Wei-Ho; Lee, Hsin-Chieh] Natl Taipei Univ Technol, Grad Inst Comp
   &amp; Commun Engn, Taipei 10608, Taiwan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Tsai, WH (reprint author), Natl Taipei Univ Technol, Dept Elect Engn, Taipei 10608, Taiwan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>whtsai@ntut.edu.tw</td>
</tr>

<tr>
<td valign="top">TC </td><td>6</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>6</td>
</tr>

<tr>
<td valign="top">PD </td><td>OCT</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">VL </td><td>20</td>
</tr>

<tr>
<td valign="top">IS </td><td>8</td>
</tr>

<tr>
<td valign="top">BP </td><td>2291</td>
</tr>

<tr>
<td valign="top">EP </td><td>2300</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASL.2012.2201473</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000307840300003</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Erro, D
   <br>Sainz, I
   <br>Navas, E
   <br>Hernaez, I</td>
</tr>

<tr>
<td valign="top">AF </td><td>Erro, D.
   <br>Sainz, I.
   <br>Navas, E.
   <br>Hernaez, I.</td>
</tr>

<tr>
<td valign="top">TI </td><td>Efficient spectral envelope estimation from harmonic speech signals</td>
</tr>

<tr>
<td valign="top">SO </td><td>ELECTRONICS LETTERS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">AB </td><td>Modern speech synthesis and conversion systems based on statistical methods require vocoders that allow accurate analysis and high-quality reconstruction of speech signals. Reliable spectral envelope estimation and parameterisation is one of the key parts of the vocoding process. Moreover, its efficiency is essential for these systems to be practical in online applications such as speech-to-speech translation or live voice conversion. The simple and fast spectral analysis procedure that is presented in this reported work gives satisfactory results in terms of accuracy and proves useful in vocoding applications.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Erro, D.; Sainz, I.; Navas, E.; Hernaez, I.] Univ Basque Country UPV
   EHU, Aholab Signal Proc Lab, Bilbao 48013, Spain.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Erro, D (reprint author), Univ Basque Country UPV EHU, Aholab Signal Proc Lab, Alda Urquijo S-N, Bilbao 48013, Spain.</td>
</tr>

<tr>
<td valign="top">EM </td><td>derro@aholab.ehu.es</td>
</tr>

<tr xmlns:date="http://exslt.org/dates-and-times">
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Hernaez Rioja, Inmaculada</display_name>&nbsp;</font></td><td><font size="3">K-8303-2012&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Erro, Daniel</display_name>&nbsp;</font></td><td><font size="3">H-7043-2015&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Navas, Eva</display_name>&nbsp;</font></td><td><font size="3">H-4317-2013&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Hernaez Rioja, Inmaculada</display_name>&nbsp;</font></td><td><font size="3">0000-0003-4447-7575&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Erro, Daniel</display_name>&nbsp;</font></td><td><font size="3">0000-0003-0954-6942&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Navas, Eva</display_name>&nbsp;</font></td><td><font size="3">0000-0003-3804-4984&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PD </td><td>AUG 2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">VL </td><td>48</td>
</tr>

<tr>
<td valign="top">IS </td><td>16</td>
</tr>

<tr>
<td valign="top">BP </td><td>1019</td>
</tr>

<tr>
<td valign="top">EP </td><td>1020</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1049/el.2012.0756</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000307645300035</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Saito, D
   <br>Watanabe, S
   <br>Nakamura, A
   <br>Minematsu, N</td>
</tr>

<tr>
<td valign="top">AF </td><td>Saito, Daisuke
   <br>Watanabe, Shinji
   <br>Nakamura, Atsushi
   <br>Minematsu, Nobuaki</td>
</tr>

<tr>
<td valign="top">TI </td><td>Statistical Voice Conversion Based on Noisy Channel Model</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Joint density model; noisy channel model; probabilistic integration;
   speaker model; voice conversion (VC)</td>
</tr>

<tr>
<td valign="top">ID </td><td>HIDDEN MARKOV-MODELS; SPEECH RECOGNITION; SPEAKER ADAPTATION; DYNAMIC
   FEATURES; TRANSFORMATIONS; VERIFICATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper describes a novel framework of voice conversion effectively using both a joint density model and a speaker model. In voice conversion studies, approaches based on the Gaussian mixture model (GMM) with probabilistic densities of joint vectors of a source and a target speakers are widely used to estimate a transform function between both the speakers. However, to achieve sufficient quality, these approaches require a parallel corpus which contains plenty of utterances with the same linguistic content spoken by both the speakers. In addition, the joint density GMM methods often suffer from overtraining effects when the amount of training data is small. To compensate for these problems, we propose a voice conversion framework, which integrates the speaker GMM of the target with the joint density model using a noisy channel model. The proposed method trains the joint density model with a few parallel utterances, and the speaker model with nonparallel data of the target, independently. It can ease the burden on the source speaker. Experiments demonstrate the effectiveness of the proposed method, especially when the amount of the parallel corpus is small.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Watanabe, Shinji; Nakamura, Atsushi] NTT Corp, NTT Commun Sci Labs,
   Kyoto 6190237, Japan.
   <br>[Saito, Daisuke] Univ Tokyo, Dept Elect Engn &amp; Informat Syst, Tokyo
   1138656, Japan.
   <br>[Minematsu, Nobuaki] Univ Tokyo, Dept Informat &amp; Commun Engn, Tokyo
   1130033, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Saito, D (reprint author), Univ Tokyo, Dept Informat Phys &amp; Comp, Tokyo 1130033, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>dsaito@hil.t.u-tokyo.ac.jp; shinjiw@ieee.org;
   nakamura.atsushi@lab.ntt.co.jp; mine@gavo.t.u-tokyo.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>10</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>13</td>
</tr>

<tr>
<td valign="top">PD </td><td>AUG</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">VL </td><td>20</td>
</tr>

<tr>
<td valign="top">IS </td><td>6</td>
</tr>

<tr>
<td valign="top">BP </td><td>1784</td>
</tr>

<tr>
<td valign="top">EP </td><td>1794</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASL.2012.2188628</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000302532000010</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Timmer, K
   <br>Vahid-Gharavi, N
   <br>Schiller, NO</td>
</tr>

<tr>
<td valign="top">AF </td><td>Timmer, Kalinka
   <br>Vahid-Gharavi, Narges
   <br>Schiller, Niels O.</td>
</tr>

<tr>
<td valign="top">TI </td><td>Reading aloud in Persian: ERP evidence for an early locus of the masked
   onset priming effect</td>
</tr>

<tr>
<td valign="top">SO </td><td>BRAIN AND LANGUAGE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Reading aloud; Masked onset priming effect; Dual-route cascaded model;
   Speech planning; Grapheme-to-phoneme conversion; Persian</td>
</tr>

<tr>
<td valign="top">ID </td><td>VISUAL WORD RECOGNITION; PHONOLOGICAL PROCESSING SKILLS; LANGUAGE
   PRODUCTION; TIME-COURSE; BRAIN POTENTIALS; FREQUENCY WORDS; SPEECH;
   TASK; MODEL</td>
</tr>

<tr>
<td valign="top">AB </td><td>The current study investigates reading aloud words in Persian, a language that does not mark all its vowels in the script. Behaviorally, a masked onset priming effect (MOPE) was revealed for transparent words, with faster speech onset latencies in the phoneme-matching condition (i.e. phonological prime and target onset overlap; e.g. (sic) /s(sic):l/; 'year' - (sic) /sot/; 'voice') than the phoneme-mismatching condition (e.g. (sic)/t(sic):b/ 'swing' - (sic) /sot/; 'voice'). For opaque target words (e.g. (sic) /solh/: 'peace'), no such effect was found. However, event-related potentials (ERPs) did reveal an amplitude difference between the two prime conditions in the 80-160 ms time window for transparent as well as opaque words. Only for the former, this effect continued into the 300-480 ms time window. This finding constrains the time course of the MOPE and suggests the simultaneous activation of both the non-lexical grapheme-to-phoneme and the lexical route in the dual-route cascaded (DRC) model. (c) 2012 Elsevier Inc. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Timmer, Kalinka] Leiden Univ, Fac Social Sci, Cognit Psychol Unit, Dept
   Psychol,Ctr Linguist, NL-2300 RB Leiden, Netherlands.
   <br>[Timmer, Kalinka; Schiller, Niels O.] LIBC, Leiden, Netherlands.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Timmer, K (reprint author), Leiden Univ, Fac Social Sci, Cognit Psychol Unit, Dept Psychol,Ctr Linguist, NL-2300 RB Leiden, Netherlands.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ktimmer@fsw.leidenuniv.nl</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Timmer, Kalinka</display_name>&nbsp;</font></td><td><font size="3">B-2138-2019&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Schiller, Niels O.</display_name>&nbsp;</font></td><td><font size="3">A-9481-2008&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Timmer, Kalinka</display_name>&nbsp;</font></td><td><font size="3">0000-0003-2842-0843&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Schiller, Niels O.</display_name>&nbsp;</font></td><td><font size="3">0000-0002-0392-7608&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>12</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>13</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUL</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">VL </td><td>122</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>34</td>
</tr>

<tr>
<td valign="top">EP </td><td>41</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.bandl.2012.04.013</td>
</tr>

<tr>
<td valign="top">SC </td><td>Audiology &amp; Speech-Language Pathology; Linguistics; Neurosciences &amp;
   Neurology; Psychology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000305597300004</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wester, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wester, Mirjam</td>
</tr>

<tr>
<td valign="top">TI </td><td>Talker discrimination across languages</td>
</tr>

<tr>
<td valign="top">SO </td><td>SPEECH COMMUNICATION</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Human speech perception; Talker discrimination; Cross-language</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE IDENTIFICATION; SPEECH; RECOGNITION; SPEAKERS</td>
</tr>

<tr>
<td valign="top">AB </td><td>This study investigated the extent to which listeners are able to discriminate between bilingual talkers in three language pairs - English-German, English-Finnish and English-Mandarin. Native English listeners were presented with two sentences spoken by bilingual talkers and were asked to judge whether they thought the sentences were spoken by the same person. Equal amounts of cross-language and matched-language trials were presented. The results show that native English listeners are able to carry out this task well; achieving percent correct levels at well above chance for all three language pairs. Previous research has shown this for English German, this research shows listeners also extend this to Finnish and Mandarin, languages that are quite distinct from English from a genetic and phonetic similarity perspective. However, listeners are significantly less accurate on cross-language talker trials (English foreign) than on matched-language trials (English English and foreign foreign). Understanding listeners' behaviour in cross-language talker discrimination using natural speech is the first step in developing principled evaluation techniques for synthesis systems in which the goal is for the synthesised voice to sound like the original speaker, for instance, in speech-to-speech translation systems, voice conversion and reconstruction. (C) 2012 Elsevier B.V. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Univ Edinburgh, Ctr Speech Technol Res, Edinburgh EH8 9AB, Midlothian,
   Scotland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Wester, M (reprint author), Univ Edinburgh, Ctr Speech Technol Res, 10 Crichton St, Edinburgh EH8 9AB, Midlothian, Scotland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>mwester@inf.ed.ac.uk</td>
</tr>

<tr>
<td valign="top">TC </td><td>16</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>16</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUL</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">VL </td><td>54</td>
</tr>

<tr>
<td valign="top">IS </td><td>6</td>
</tr>

<tr>
<td valign="top">BP </td><td>781</td>
</tr>

<tr>
<td valign="top">EP </td><td>790</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.specom.2012.01.006</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000303908400007</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Walling, AD
   <br>Dickson, GM</td>
</tr>

<tr>
<td valign="top">AF </td><td>Walling, Anne D.
   <br>Dickson, Gretchen M.</td>
</tr>

<tr>
<td valign="top">TI </td><td>Hearing Loss in Older Adults</td>
</tr>

<tr>
<td valign="top">SO </td><td>AMERICAN FAMILY PHYSICIAN</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">ID </td><td>RANDOMIZED CONTROLLED-TRIAL; QUALITY-OF-LIFE; AUDITORY REHABILITATION;
   IMPAIRMENT; PEOPLE; NOISE; EPIDEMIOLOGY; PREVALENCE; EDUCATION; PROGRAM</td>
</tr>

<tr>
<td valign="top">AB </td><td>Hearing loss affects approximately one-third of adults 61 to 70 years of age and more than 80 percent of those older than 85 years. Men usually experience greater hearing loss and have earlier onset compared with women. The most common type is age-related hearing loss; however, many conditions can interfere with the conduction of sound vibrations to the inner ear and their conversion to electrical impulses for conduction to the brain. Screening for hearing loss is recommended in adults older than 50 to 60 years. Office screening tests include the whispered voice test and audioscopy. Older patients who admit to having difficulty hearing may be referred directly for audiometry. The history can identify risk factors for hearing loss, especially noise exposure and use of ototoxic medications. Examination of the auditory canal and tympanic membrane can identify causes of conductive hearing loss. Audiometric testing is required to confirm hearing loss. Adults presenting with idiopathic sudden sensorineural hearing loss should be referred for urgent assessment. Management of hearing loss is based on addressing underlying causes, especially obstructions (including cerumen) and ototoxic medications. Residual hearing should be optimized by use of hearing aids, assistive listening devices, and rehabilitation programs. Surgical implants are indicated for selected patients. Major barriers to improved hearing in older adults include lack of recognition of hearing loss; perception that hearing loss is a normal part of aging or is not amenable to treatment; and patient nonadherence with hearing aids because of stigma, cost, inconvenience, disappointing initial results, or other factors. (Am Fam Physician. 2012;85 (12):1150-1156. Copyright (C) 2012 American Academy of Family Physicians.)</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Walling, Anne D.; Dickson, Gretchen M.] Univ Kansas, Sch Med, Wichita,
   KS 67214 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Walling, AD (reprint author), Univ Kansas, Sch Med, 1010 N Kansas, Wichita, KS 67214 USA.</td>
</tr>

<tr>
<td valign="top">TC </td><td>44</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>44</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUN 15</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">VL </td><td>85</td>
</tr>

<tr>
<td valign="top">IS </td><td>12</td>
</tr>

<tr>
<td valign="top">BP </td><td>1150</td>
</tr>

<tr>
<td valign="top">EP </td><td>1156</td>
</tr>

<tr>
<td valign="top">SC </td><td>General &amp; Internal Medicine</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000306868600005</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Godoy, E
   <br>Rosec, O
   <br>Chonavel, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Godoy, Elizabeth
   <br>Rosec, Olivier
   <br>Chonavel, Thierry</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Conversion Using Dynamic Frequency Warping With Amplitude Scaling,
   for Parallel or Nonparallel Corpora</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Frequency warping; Gaussian mixture model (GMM); nonparallel corpora;
   spectral envelope; Voice Conversion (VC)</td>
</tr>

<tr>
<td valign="top">ID </td><td>TRANSFORMATION; SPEECH; MODEL</td>
</tr>

<tr>
<td valign="top">AB </td><td>In Voice Conversion (VC), the speech of a source speaker is modified to resemble that of a particular target speaker. Currently, standard VC approaches use Gaussian mixture model (GMM)-based transformations that do not generate high-quality converted speech due to "over-smoothing" resulting from weak links between individual source and target frame parameters. Dynamic Frequency Warping (DFW) offers an appealing alternative to GMM-based methods, as more spectral details are maintained in transformation; however, the speaker timbre is less successfully converted because spectral power is not adjusted explicitly. Previous work combines separate GMM- and DFW-transformed spectral envelopes for each frame. This paper proposes a more effective DFW-based approach that 1) does not rely on the baseline GMM methods, and 2) functions on the acoustic class level. To adjust spectral power, an amplitude scaling function is used that compares the average target and warped source log spectra for each acoustic class. The proposed DFW with Amplitude scaling (DFWA) outperforms standard GMM and hybrid GMM-DFW methods for VC in terms of both speech quality and timbre conversion, as is confirmed in extensive objective and subjective testing. Furthermore, by not requiring time-alignment of source and target speech, DFWA is able to perform equally well using parallel or nonparallel corpora, as is demonstrated explicitly.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Godoy, Elizabeth; Rosec, Olivier] Orange Labs, TECH ASAP VOICE, F-22307
   Lannion, France.
   <br>[Chonavel, Thierry] Telecom Bretagne, Signal &amp; Commun Dept, Inst
   Telecom, UMR CNRS Lab STICC 3192, F-29238 Brest 3, France.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Godoy, E (reprint author), Orange Labs, TECH ASAP VOICE, F-22307 Lannion, France.</td>
</tr>

<tr>
<td valign="top">EM </td><td>godoyec@gmail.com; olivier.rosec@or-ange.com;
   thierry.chonavel@telecom-bretagne.eu</td>
</tr>

<tr>
<td valign="top">TC </td><td>57</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>59</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAY</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">VL </td><td>20</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">BP </td><td>1313</td>
</tr>

<tr>
<td valign="top">EP </td><td>1323</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASL.2011.2177820</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000300846100006</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nakamura, T
   <br>Sugiura, K
   <br>Nagai, T
   <br>Iwahashi, N
   <br>Toda, T
   <br>Okada, H
   <br>Omori, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nakamura, Tomoaki
   <br>Sugiura, Komei
   <br>Nagai, Takayuki
   <br>Iwahashi, Naoto
   <br>Toda, Tomoki
   <br>Okada, Hiroyuki
   <br>Omori, Takashi</td>
</tr>

<tr>
<td valign="top">TI </td><td>Learning Novel Objects for Extended Mobile Manipulation</td>
</tr>

<tr>
<td valign="top">SO </td><td>JOURNAL OF INTELLIGENT &amp; ROBOTIC SYSTEMS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Mobile manipulation; Object learning; Object recognition;
   Out-of-vocabulary; RoboCup@Home</td>
</tr>

<tr>
<td valign="top">ID </td><td>SEGMENTATION; PLATFORM; CUTS</td>
</tr>

<tr>
<td valign="top">AB </td><td>We propose a method for learning novel objects from audio visual input. The proposed method is based on two techniques: out-of-vocabulary (OOV) word segmentation and foreground object detection in complex environments. A voice conversion technique is also involved in the proposed method so that the robot can pronounce the acquired OOV word intelligibly. We also implemented a robotic system that carries out interactive mobile manipulation tasks, which we call "extended mobile manipulation", using the proposed method. In order to evaluate the robot as a whole, we conducted a task "Supermarket" adopted from the RoboCup@Home league as a standard task for real-world applications. The results reveal that our integrated system works well in real-world applications.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nakamura, Tomoaki; Nagai, Takayuki] Univ Electrocommun, Chofu, Tokyo
   182, Japan.
   <br>[Sugiura, Komei; Iwahashi, Naoto] Natl Inst Informat &amp; Communicat
   Technol, Sora Ku, Kyoto, Japan.
   <br>[Toda, Tomoki] Nara Inst Sci &amp; Technol, Nara, Japan.
   <br>[Okada, Hiroyuki; Omori, Takashi] Tamagawa Univ, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nakamura, T (reprint author), Univ Electrocommun, 1-5-1 Chofugaoka, Chofu, Tokyo 182, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>naka_t@apple.ee.uec.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>7</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>7</td>
</tr>

<tr>
<td valign="top">PD </td><td>APR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">VL </td><td>66</td>
</tr>

<tr>
<td valign="top">IS </td><td>1-2</td>
</tr>

<tr>
<td valign="top">SI </td><td>SI</td>
</tr>

<tr>
<td valign="top">BP </td><td>187</td>
</tr>

<tr>
<td valign="top">EP </td><td>204</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/s10846-011-9605-1</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Robotics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000300706800012</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Lemarquand, G
   <br>Ravaud, R
   <br>Shahosseini, I
   <br>Lemarquand, V
   <br>Moulin, J
   <br>Lefeuvre, E</td>
</tr>

<tr>
<td valign="top">AF </td><td>Lemarquand, G.
   <br>Ravaud, R.
   <br>Shahosseini, I.
   <br>Lemarquand, V.
   <br>Moulin, J.
   <br>Lefeuvre, E.</td>
</tr>

<tr>
<td valign="top">TI </td><td>MEMS electrodynamic loudspeakers for mobile phones</td>
</tr>

<tr>
<td valign="top">SO </td><td>APPLIED ACOUSTICS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>MEMS; Loudspeaker; Acoustic transducer; Voice coil motor</td>
</tr>

<tr>
<td valign="top">ID </td><td>IRONLESS LOUDSPEAKER; PERMANENT-MAGNETS; MICROSPEAKER; MOTORS</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a MEMS structure of electrodynamic loudspeakers dedicated to mobile phone applications. The major goals are to obtain a high electroacoustic conversion efficiency and a high fidelity acoustic quality. The originalities lie in a rigid silicon membrane and in its suspension by a set of silicon beams. The moving coil is a planar copper microcoil electroplated on the silicon membrane whose microstructure was optimized for providing both rigidity and lightness of the mobile part.
   <br>This paper presents different magnetic structures of the motor for this MEMS loudspeaker. These structures are ironless, only made out of permanent magnets which are bonded on the substrate. They are studied and optimized thanks to analytical formulations of the magnetic field created by the permanent magnets. Results are presented for a deep RIE etched 7.5 mm radius silicon membrane structured with 40 stiffening ribs and on a 30 pm thick microcoil with 35 turns. (C) 2011 Elsevier Ltd. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Lemarquand, G.; Ravaud, R.] Univ Maine, LAUM, UMR CNRS 6613, F-72085 Le
   Mans 9, France.
   <br>[Lemarquand, V.] Univ Toulouse, LAPLACE, UMR CNRS 5213, IUT Figeac,
   Toulouse, France.
   <br>[Shahosseini, I.; Moulin, J.; Lefeuvre, E.] Univ Paris 11, IEF, UMR CNRS
   8622, Paris, France.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Lemarquand, G (reprint author), Univ Maine, LAUM, UMR CNRS 6613, Ave Olivier Messiaen, F-72085 Le Mans 9, France.</td>
</tr>

<tr>
<td valign="top">EM </td><td>guy.lemarquand@ieee.org</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Lefeuvre, Elie</display_name>&nbsp;</font></td><td><font size="3">D-8390-2013&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Lefeuvre, Elie</display_name>&nbsp;</font></td><td><font size="3">0000-0002-2482-240X&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>12</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>12</td>
</tr>

<tr>
<td valign="top">PD </td><td>APR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">VL </td><td>73</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">BP </td><td>379</td>
</tr>

<tr>
<td valign="top">EP </td><td>385</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.apacoust.2011.10.013</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000300266000010</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Helander, E
   <br>Silen, H
   <br>Virtanen, T
   <br>Gabbouj, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Helander, Elina
   <br>Silen, Hanna
   <br>Virtanen, Tuomas
   <br>Gabbouj, Moncef</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Conversion Using Dynamic Kernel Partial Least Squares Regression</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Kernel methods; partial least squares regression; voice conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>F0</td>
</tr>

<tr>
<td valign="top">AB </td><td>A drawback of many voice conversion algorithms is that they rely on linear models and/or require a lot of tuning. In addition, many of them ignore the inherent time-dependency between speech features. To address these issues, we propose to use dynamic kernel partial least squares (DKPLS) technique to model nonlinearities as well as to capture the dynamics in the data. The method is based on a kernel transformation of the source features to allow non-linear modeling and concatenation of previous and next frames to model the dynamics. Partial least squares regression is used to find a conversion function that does not overfit to the data. The resulting DKPLS algorithm is a simple and efficient algorithm and does not require massive tuning. Existing statistical methods proposed for voice conversion are able to produce good similarity between the original and the converted target voices but the quality is usually degraded. The experiments conducted on a variety of conversion pairs show that DKPLS, being a statistical method, enables successful identity conversion while achieving a major improvement in the quality scores compared to the state-of-the-art Gaussian mixture-based model. In addition to enabling better spectral feature transformation, quality is further improved when aperiodicity and binary voicing values are converted using DKPLS with auxiliary information from spectral features.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Helander, Elina; Silen, Hanna; Virtanen, Tuomas; Gabbouj, Moncef]
   Tampere Univ Technol, Dept Signal Proc, Tampere 33720, Finland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Helander, E (reprint author), Tampere Univ Technol, Dept Signal Proc, Tampere 33720, Finland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>elina.helander@tut.fi; hanna.silen@tut.fi; tuomas.virtanen@tut.fi;
   moncef.gabbouj@tut.fi</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Gabbouj, Moncef</display_name>&nbsp;</font></td><td><font size="3">G-4293-2014&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Gabbouj, Moncef</display_name>&nbsp;</font></td><td><font size="3">0000-0002-9788-2323&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Helander, Elina</display_name>&nbsp;</font></td><td><font size="3">0000-0002-0076-0590&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>71</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>75</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">VL </td><td>20</td>
</tr>

<tr>
<td valign="top">IS </td><td>3</td>
</tr>

<tr>
<td valign="top">BP </td><td>806</td>
</tr>

<tr>
<td valign="top">EP </td><td>817</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASL.2011.2165944</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000304170300008</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sun, J
   <br>Zhang, XW
   <br>Cao, TY
   <br>Yang, JB
   <br>Sun, XJ</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sun, Jian
   <br>Zhang, Xiongwei
   <br>Cao, Tieyong
   <br>Yang, Jibin
   <br>Sun, Xinjian</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice conversion including contextual information</td>
</tr>

<tr>
<td valign="top">SO </td><td>INFORMATION-AN INTERNATIONAL INTERDISCIPLINARY JOURNAL</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; frame-by-frame; contextual information; artificial
   neural network</td>
</tr>

<tr>
<td valign="top">ID </td><td>ARTIFICIAL NEURAL-NETWORKS; TRANSFORMATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Most of the current Voice Conversion (VC) methods perform the conversion on the basis of a frame-by-frame statistical model. In these models the contextual information which lies in adjacent frames and adjacent phones is usually ignored. In addition, the impacts of different kinds of correlation information for the VC have not been studied before. In this paper, the impacts of some most relevant contextual parameters on conversion performance are evaluated. Then an Artificial Neural Network (ANN) based VC system which incorporates the contextual information is proposed. Finally the validity of incorporating this contextual information for VC is demonstrated by experimental results.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Sun, Jian; Sun, Xinjian] PLA Univ Sci &amp; Tech, Inst Commun Engn, Nanjing
   210007, Jiangsu, Peoples R China.
   <br>[Zhang, Xiongwei; Cao, Tieyong; Yang, Jibin] PLA Univ Sci &amp; Tech, Inst
   Command Automat, Nanjing 210007, Jiangsu, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sun, J (reprint author), PLA Univ Sci &amp; Tech, Inst Commun Engn, 2 Biao Ying, Nanjing 210007, Jiangsu, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>sunjian001@gmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>FEB</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">VL </td><td>15</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>751</td>
</tr>

<tr>
<td valign="top">EP </td><td>762</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000302190200027</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Yuan, HL
   <br>Xu, PP</td>
</tr>

<tr>
<td valign="top">AF </td><td>Yuan, Hailin
   <br>Xu, Pingping</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Designing of the Digital Voice Recording System on SOPC</td>
</tr>

<tr>
<td valign="top">SO </td><td>2012 IEEE FIFTH INTERNATIONAL CONFERENCE ON ADVANCED COMPUTATIONAL
   INTELLIGENCE (ICACI)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE 5th International Conference on Advanced Computational Intelligence
   (ICACI)</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 18-20, 2012</td>
</tr>

<tr>
<td valign="top">CL </td><td>Nanjing, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper introduced a scheme of design an embedded digital voice recording system on SOPC technology. By configure NiosII soft core CPU and some corresponding interface modules on a PFGA to construct an embedded system' s hardware, and combine software programming to controlling audio encode and decode IC WM8731 and SDRAM, system has realized A/D, D/A conversion, saving and replaying of audio signal. Due to using the SOPC and DMA technology, the system has high design flexibility and good expansibility and quick data processing speed.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Yuan, Hailin] Hubei Inst Nationalities, Dept Comp Sci, Enshi 445000,
   Hubei, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Yuan, HL (reprint author), Hubei Inst Nationalities, Dept Comp Sci, Enshi 445000, Hubei, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>edahenry@163.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">BP </td><td>1213</td>
</tr>

<tr>
<td valign="top">EP </td><td>1215</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000327197800260</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sainz, I
   <br>Erro, D
   <br>Navas, E
   <br>Hernaez, I
   <br>Sanchez, J
   <br>Saratxaga, I
   <br>Odriozola, I</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sainz, I.
   <br>Erro, D.
   <br>Navas, E.
   <br>Hernaez, I.
   <br>Sanchez, J.
   <br>Saratxaga, I.
   <br>Odriozola, I.</td>
</tr>

<tr>
<td valign="top">BE </td><td>Calzolari, N
   <br>Choukri, K
   <br>Declerck, T
   <br>Dogan, MU
   <br>Maegaard, B
   <br>Mariani, J
   <br>Odijk, J
   <br>Piperidis, S</td>
</tr>

<tr>
<td valign="top">TI </td><td>Versatile Speech Databases for High Quality Synthesis for Basque</td>
</tr>

<tr>
<td valign="top">SO </td><td>LREC 2012 - EIGHTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND
   EVALUATION</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>8th International Conference on Language Resources and Evaluation (LREC)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 21-27, 2012</td>
</tr>

<tr>
<td valign="top">CL </td><td>Istanbul, TURKEY</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speech Corpus; Speech Synthesis; Evaluation</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents three new speech databases for standard Basque. They are designed primarily for corpus-based synthesis but each database has its specific purpose: 1) AhoSyn: high quality speech synthesis (recorded also in Spanish), 2) AhoSpeakers: voice conversion and 3) AhoEmo3: emotional speech synthesis. The whole corpus design and the recording process are described with detail. Once the databases were collected all the data was automatically labelled and annotated. Then, an HMM-based TTS voice was built and subjectively evaluated. The results of the evaluation are pretty satisfactory: 3.70 MOS for Basque and 3.44 for Spanish. Therefore, the evaluation assesses the quality of this new speech resource and the validity of the automated processing presented.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Sainz, I.; Erro, D.; Navas, E.; Hernaez, I.; Sanchez, J.; Saratxaga,
   I.; Odriozola, I.] Univ Basque Country, Dept Elect &amp; Telecommun, Fac
   Engn, Aholab, Bilbao 48013, Spain.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sainz, I (reprint author), Univ Basque Country, Dept Elect &amp; Telecommun, Fac Engn, Aholab, Bilbao 48013, Spain.</td>
</tr>

<tr>
<td valign="top">EM </td><td>inaki@aholab.ehu.es; derro@aholab.ehu.es; eva@aholab.ehu.es;
   inma@aholab.ehu.es; ion@aholab.ehu.es; ibon@aholab.ehu.es;
   igor@aholab.ehu.es</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Erro, Daniel</display_name>&nbsp;</font></td><td><font size="3">H-7043-2015&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Hernaez Rioja, Inmaculada</display_name>&nbsp;</font></td><td><font size="3">K-8303-2012&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Saratxaga, Ibon</display_name>&nbsp;</font></td><td><font size="3">H-6423-2015&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Erro, Daniel</display_name>&nbsp;</font></td><td><font size="3">0000-0003-0954-6942&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Hernaez Rioja, Inmaculada</display_name>&nbsp;</font></td><td><font size="3">0000-0003-4447-7575&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Saratxaga, Ibon</display_name>&nbsp;</font></td><td><font size="3">0000-0001-7282-2765&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">BP </td><td>3308</td>
</tr>

<tr>
<td valign="top">EP </td><td>3312</td>
</tr>

<tr>
<td valign="top">SC </td><td>Linguistics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000323927703061</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Garcia-Casademont, E
   <br>Bonafonte, A
   <br>Moreno, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Garcia-Casademont, Emilia
   <br>Bonafonte, Antonio
   <br>Moreno, Asuncion</td>
</tr>

<tr>
<td valign="top">BE </td><td>Calzolari, N
   <br>Choukri, K
   <br>Declerck, T
   <br>Dogan, MU
   <br>Maegaard, B
   <br>Mariani, J
   <br>Odijk, J
   <br>Piperidis, S</td>
</tr>

<tr>
<td valign="top">TI </td><td>Building Synthetic Voices in the METANET Framework</td>
</tr>

<tr>
<td valign="top">SO </td><td>LREC 2012 - EIGHTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND
   EVALUATION</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>8th International Conference on Language Resources and Evaluation (LREC)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 21-27, 2012</td>
</tr>

<tr>
<td valign="top">CL </td><td>Istanbul, TURKEY</td>
</tr>

<tr>
<td valign="top">DE </td><td>META-NET; speech synthesis; multilingual synthetic voices; Festival TTS;
   unit-selection; statistical synthesis</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH SYNTHESIS</td>
</tr>

<tr>
<td valign="top">AB </td><td>METANET(4)U is a European project aiming at supporting language technology for European languages and multilingualism. It is a project in the META-NET Network of Excellence, a cluster of projects aiming at fostering the mission of META, which is the Multilingual Europe Technology Alliance, dedicated to building the technological foundations of a multilingual European information society. This paper describe the resources produced at our lab to provide Synthethic voices. Using existing 10h corpus for a male and a female Spanish speakers, voices have been developed to be used in Festival, both with unit-selection and with statistical-based technologies. Furthermore, using data produced for supporting research on intra and inter-lingual voice conversion, four bilingual voices (English/Spanish) have been developed. The paper describes these resources which are available through META. Furthermore, an evaluation is presented to compare different synthesis techniques, influence of amount of data in statistical speech synthesis and the effect of sharing data in bilingual voices.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Garcia-Casademont, Emilia; Bonafonte, Antonio; Moreno, Asuncion] Univ
   Politecn Cataluna, E-08028 Barcelona, Spain.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">BP </td><td>3322</td>
</tr>

<tr>
<td valign="top">EP </td><td>3326</td>
</tr>

<tr>
<td valign="top">SC </td><td>Linguistics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000323927703064</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wang, MM
   <br>Wen, MM
   <br>Hirose, K
   <br>Minematsu, N</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wang, Miaomiao
   <br>Wen, Miaomiao
   <br>Hirose, Keikichi
   <br>Minematsu, Nobuaki</td>
</tr>

<tr>
<td valign="top">BE </td><td>Ma, Q
   <br>Ding, H
   <br>Hirst, D</td>
</tr>

<tr>
<td valign="top">TI </td><td>Emotional Voice Conversion for Mandarin using Tone Nucleus Model - Small
   Corpus and High Efficiency</td>
</tr>

<tr>
<td valign="top">SO </td><td>PROCEEDINGS OF THE 6TH INTERNATIONAL CONFERENCE ON SPEECH PROSODY, VOLS
   I AND II</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>6th International Conference on Speech Prosody</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 22-25, 2012</td>
</tr>

<tr>
<td valign="top">CL </td><td>Shanghai, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Emotional voice conversion; Mandarin; Tone nucleus</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH; RECOGNITION</td>
</tr>

<tr>
<td valign="top">AB </td><td>The GMM-based spectral conversion techniques were applied to emotion conversion but it was found that spectral transformation alone is not sufficient for conveying the required target emotion. In this paper, we adopt the tone nucleus model to carry the most important information of tones and represent F-0 contour for Mandarin speech. And then tone nucleus part is converted to emotional speech from neutral ones. The tone nuclei variations are modeled by the classification and regression tree (CART) and dynamic programming. Compared with previous prosody transforming methods, the proposed method 1) uses only the tone nucleus part of each syllable rather than the whole F-0 contour to avoid the data sparseness problems in emotion conversion; 2) builds mapping functions for well-chosen tone nucleus model parameters to better capture Mandarin tonal and emotional information. Using only a modest amount of training data, the perceptual accuracy achieved by our method was shown to be comparable to that obtained by a professional speaker.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Wang, Miaomiao] Toshiba China R&amp;D Ctr, Beijing, Peoples R China.
   <br>[Wen, Miaomiao] Carnegie Mellon Univ, Language Technol Inst, Pittsburgh,
   PA 15213 USA.
   <br>[Hirose, Keikichi; Minematsu, Nobuaki] Univ Tokyo, Grad Sch Informat Sci
   &amp; Technol, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Wang, MM (reprint author), Toshiba China R&amp;D Ctr, Beijing, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>wangmiaomiao@rdc.toshiba.com.cn; mwen@cs.cmu.edu;
   hirose@gavo.t.u-tokyo.ac.jp; mine@gavo.t.u-tokyo.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">BP </td><td>163</td>
</tr>

<tr>
<td valign="top">EP </td><td>166</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science; Linguistics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000325160200041</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wu, LF
   <br>Zhang, LH</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wu Lifang
   <br>Zhang Linghua</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Voice Conversion System Based on the Harmonic plus Noise Excitation
   and Gaussian Mixture Model</td>
</tr>

<tr>
<td valign="top">SO </td><td>PROCEEDINGS OF THE 2012 SECOND INTERNATIONAL CONFERENCE ON
   INSTRUMENTATION &amp; MEASUREMENT, COMPUTER, COMMUNICATION AND CONTROL
   (IMCCC 2012)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>2nd International Conference on Instrumentation and Measurement,
   Computer, Communication and Control (IMCCC)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 08-10, 2012</td>
</tr>

<tr>
<td valign="top">CL </td><td>Harbin Inst Technol, Harbin, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">HO </td><td>Harbin Inst Technol</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice Conversion; Harmonic plus Noise Model; Residual; Gaussian Mixture
   Model</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper proposes an algorithm to extract the excitation source based on the Harmonic plus Noise Model, which decomposes the source voice to harmonic components and random noise component. By The proposed algorithm, the LPC parameters of the harmonic components are extracted through linear prediction method, and then the LPC inverse filter is used to get the harmonic residual signal as the excitation source. This excitation source avoids artificial modifications and contains more speaker personality characteristic. Finally the synthesized speech is superimposed on the random noise component compensation. Experiments demonstrate that proposed algorithm improves the target tendentiousness and naturalness of the synthesized speech.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Wu Lifang; Zhang Linghua] Nanjing Univ Posts &amp; Telecommun, Coll Commun
   &amp; Informat Engn, Nanjing, Jiangsu, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Wu, LF (reprint author), Nanjing Univ Posts &amp; Telecommun, Coll Commun &amp; Informat Engn, Nanjing, Jiangsu, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>wulifang_1988@sina.com; zhanglh@njupt.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">BP </td><td>1575</td>
</tr>

<tr>
<td valign="top">EP </td><td>1578</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/IMCCC.2012.367</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering; Instruments &amp; Instrumentation</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000324691100362</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hwang, HT
   <br>Tsao, Y
   <br>Wang, HM
   <br>Wang, YR
   <br>Chen, SH</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hwang, Hsin-Te
   <br>Tsao, Yu
   <br>Wang, Hsin-Min
   <br>Wang, Yih-Ru
   <br>Chen, Sin-Horng</td>
</tr>

<tr>
<td valign="top">GP </td><td>International Speech Communications Association</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Study of Mutual Information for GMM-Based Spectral Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>13TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION 2012 (INTERSPEECH 2012), VOLS 1-3</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>13th Annual Conference of the
   International-Speech-Communication-Association</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 09-13, 2012</td>
</tr>

<tr>
<td valign="top">CL </td><td>Portland, OR</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; mutual information; GMM</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>The Gaussian mixture model (GMM)-based method has dominated the field of voice conversion (VC) for last decade. However, the converted spectra are excessively smoothed and thus produce muffled converted sound. In this study, we improve the speech quality by enhancing the dependency between the source (natural sound) and converted feature vectors (converted sound). It is believed that enhancing this dependency can make the converted sound closer to the natural sound. To this end, we propose an integrated maximum a posteriori and mutual information (MAPMI) criterion for parameter generation on spectral conversion. Experimental results demonstrate that the quality of converted speech by the proposed MAPMI method outperforms that by the conventional method in terms of formal listening test.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Hwang, Hsin-Te; Wang, Yih-Ru; Chen, Sin-Horng] Natl Chiao Tung Univ,
   Dept Elect Engn, Hsinchu, Taiwan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hwang, HT (reprint author), Natl Chiao Tung Univ, Dept Elect Engn, Hsinchu, Taiwan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>hwanght@iis.sinica.edu.tw; yu.tsao@citi_sinica.edu.tw;
   whm@iis.sinica.edu.tw; yrwang@cc.nctu_edu_tw; schen@mail.nctu.edu.tw</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">BP </td><td>78</td>
</tr>

<tr>
<td valign="top">EP </td><td>81</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000320827200020</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Li, N
   <br>Qiao, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Li, Na
   <br>Qiao, Yu</td>
</tr>

<tr>
<td valign="top">GP </td><td>International Speech Communications Association</td>
</tr>

<tr>
<td valign="top">TI </td><td>Bayesian Mixture of Probabilistic Linear Regressions for Voice
   Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>13TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION 2012 (INTERSPEECH 2012), VOLS 1-3</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>13th Annual Conference of the
   International-Speech-Communication-Association</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 09-13, 2012</td>
</tr>

<tr>
<td valign="top">CL </td><td>Portland, OR</td>
</tr>

<tr>
<td valign="top">DE </td><td>Bayesian linear regression; mixture of probabilistic regressions; voice
   conversion</td>
</tr>

<tr>
<td valign="top">AB </td><td>The objective of voice conversion is to transform the voice of one speaker to make it sound like another. The GMM-based statistical mapping technique has been proved to be an efficient method for converting voices [1, 2]. In a recent work [3], we generalized this technique to Mixture of Probabilistic Linear Regressions (MPLR) by using general mixture model of source vectors. In this paper, we improve MPLR by considering a prior for the transformation parameters of linear regressions, which leads to Bayesian Mixture of Probabilistic Linear Regressions (BMPLR). BMPLR has the effectiveness and robustness of Bayesian inference. Especially when the number of training data is limited and the mixture number is larger, BMPLR can largely relieve the overfitting problem. This paper presents two formulations for BMPLR, depending on how to model noise in probabilistic regression function. In addition, we derive equations for MAP estimation of transformation parameters. We examine the proposed method on voice conversion of Japanese utterances. The experimental results exhibit that BMPLR achieves better performance than MPLR.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Li, Na; Qiao, Yu] Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen
   Key Lab CVPR, Shenzhen, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Li, N (reprint author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen Key Lab CVPR, Shenzhen, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>na.li@siat.ac.cn; yu.qiao@siat.ac.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">BP </td><td>82</td>
</tr>

<tr>
<td valign="top">EP </td><td>85</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000320827200021</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Erro, D
   <br>Navas, E
   <br>Hernaez, I</td>
</tr>

<tr>
<td valign="top">AF </td><td>Erro, Daniel
   <br>Navas, Eva
   <br>Hernaez, Inma</td>
</tr>

<tr>
<td valign="top">GP </td><td>International Speech Communications Association</td>
</tr>

<tr>
<td valign="top">TI </td><td>Iterative MMSE Estimation of Vocal Tract Length Normalization Factors
   for Voice Transformation</td>
</tr>

<tr>
<td valign="top">SO </td><td>13TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION 2012 (INTERSPEECH 2012), VOLS 1-3</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>13th Annual Conference of the
   International-Speech-Communication-Association</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 09-13, 2012</td>
</tr>

<tr>
<td valign="top">CL </td><td>Portland, OR</td>
</tr>

<tr>
<td valign="top">DE </td><td>vocal tract length normalization; voice conversion; frequency warping
   plus amplitude scaling; speech synthesis</td>
</tr>

<tr>
<td valign="top">ID </td><td>CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>We present a method that determines the optimal configuration of a bilinear vocal tract length normalization function to transform the frequency axis of one voice according to a specific target voice. Given a number of parallel utterances of the involved speakers, the single parameter of this function can be calculated through an iterative procedure by minimizing an objective error measure defined in the cepstral domain. This method is also applicable when multiple warping classes are considered, and it can be complemented with amplitude correction filters. The resulting physically motivated cepstral transformation results in highly satisfactory conversion accuracy and improved quality with respect to standard satistical systems.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Erro, Daniel; Navas, Eva; Hernaez, Inma] Univ Basque Country UPV EHU,
   AHOLAB, Bilbao, Spain.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Erro, D (reprint author), Univ Basque Country UPV EHU, AHOLAB, Bilbao, Spain.</td>
</tr>

<tr>
<td valign="top">EM </td><td>derro@aholab.ehu.es; eva@aholab.ehu.es; inmal@aholab.ehu.es</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Hernaez Rioja, Inmaculada</display_name>&nbsp;</font></td><td><font size="3">K-8303-2012&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Erro, Daniel</display_name>&nbsp;</font></td><td><font size="3">H-7043-2015&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Navas, Eva</display_name>&nbsp;</font></td><td><font size="3">H-4317-2013&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Hernaez Rioja, Inmaculada</display_name>&nbsp;</font></td><td><font size="3">0000-0003-4447-7575&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Erro, Daniel</display_name>&nbsp;</font></td><td><font size="3">0000-0003-0954-6942&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Navas, Eva</display_name>&nbsp;</font></td><td><font size="3">0000-0003-3804-4984&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">BP </td><td>86</td>
</tr>

<tr>
<td valign="top">EP </td><td>89</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000320827200022</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Percybrooks, W
   <br>Moore, E</td>
</tr>

<tr>
<td valign="top">AF </td><td>Percybrooks, Winston
   <br>Moore, Elliot</td>
</tr>

<tr>
<td valign="top">GP </td><td>International Speech Communications Association</td>
</tr>

<tr>
<td valign="top">TI </td><td>A HMM approach to residual estimation for high resolution voice
   conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>13TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION 2012 (INTERSPEECH 2012), VOLS 1-3</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>13th Annual Conference of the
   International-Speech-Communication-Association</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 09-13, 2012</td>
</tr>

<tr>
<td valign="top">CL </td><td>Portland, OR</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; residual estimation; HMM; MOS test; ABX test</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion systems aim to process speech from a source speaker so it would be perceived as spoken by a target speaker. This paper presents a procedure to improve high resolution voice conversion by modifying the algorithm used for residual estimation. The proposed residual estimation algorithm exploits the temporal dependencies between residuals in consecutive speech frames using a hidden Markov model. A previous residual estimation technique based on Gaussian mixtures is used as comparison. Both algorithms are subjected to tests to measure perceived identity conversion and converted speech quality. It was found that the proposed algorithm generates converted speech with significantly better quality without degraded identity conversion performance with respect to the baseline, working particularly well for female target speakers and cross-gender conversions.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Percybrooks, Winston; Moore, Elliot] Georgia Inst Technol, Sch Elect &amp;
   Comp Engn, Savannah, GA USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Percybrooks, W (reprint author), Georgia Inst Technol, Sch Elect &amp; Comp Engn, Savannah, GA USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>wpercybrooks@gatech.edu; em80@mail.gatech.edu</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Percybrooks, Winston</display_name>&nbsp;</font></td><td><font size="3">0000-0002-0169-7562&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">BP </td><td>90</td>
</tr>

<tr>
<td valign="top">EP </td><td>93</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000320827200023</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Toda, T
   <br>Muramatsu, T
   <br>Banno, H</td>
</tr>

<tr>
<td valign="top">AF </td><td>Toda, Tomoki
   <br>Muramatsu, Takashi
   <br>Banno, Hideki</td>
</tr>

<tr>
<td valign="top">GP </td><td>International Speech Communications Association</td>
</tr>

<tr>
<td valign="top">TI </td><td>Implementation of Computationally Efficient Real-Time Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>13TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION 2012 (INTERSPEECH 2012), VOLS 1-3</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>13th Annual Conference of the
   International-Speech-Communication-Association</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 09-13, 2012</td>
</tr>

<tr>
<td valign="top">CL </td><td>Portland, OR</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; real-time processing; low-delay conversion;
   computational efficiency</td>
</tr>

<tr>
<td valign="top">ID </td><td>DYNAMIC FEATURES; SPEECH; MODELS</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents an implementation of real-time processing of statistical voice conversion (VC) based on Gaussian mixture models (GMMs). To develop VC applications for enhancing our human-to-human speech communication, it is essential to implement real-time conversion processing. Moreover, it is useful to reduce computational complexity of the conversion processing for making VC applications available even in limited resources. In this paper, we propose a real-time VC method based on a low-delay conversion algorithm considering dynamic features and a global variance. Moreover, we also propose a computationally efficient VC method based on rapid source feature extraction and diagonalization of full covariance matrices. Some experimental results are presented to show that the proposed methods work reasonably well.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Toda, Tomoki; Muramatsu, Takashi] Nara Inst Sci &amp; Technol, Grad Sch
   Informat Sci, Nara 6300192, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Toda, T (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, 8916-5 Takayama Cho, Nara 6300192, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>tomoki@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>9</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>9</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">BP </td><td>94</td>
</tr>

<tr>
<td valign="top">EP </td><td>97</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000320827200024</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Saito, D
   <br>Minematsu, N
   <br>Hirose, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Saito, Daisuke
   <br>Minematsu, Nobuaki
   <br>Hirose, Keikichi</td>
</tr>

<tr>
<td valign="top">GP </td><td>International Speech Communications Association</td>
</tr>

<tr>
<td valign="top">TI </td><td>Effects of Speaker Adaptive Training on Tensor-based Arbitrary Speaker
   Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>13TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION 2012 (INTERSPEECH 2012), VOLS 1-3</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>13th Annual Conference of the
   International-Speech-Communication-Association</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 09-13, 2012</td>
</tr>

<tr>
<td valign="top">CL </td><td>Portland, OR</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; Gaussian mixture model; eigenvoice; Tucker
   decomposition; speaker adaptive training</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION; SPEECH RECOGNITION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper introduces speaker adaptive training techniques to tensor-based arbitrary speaker conversion. In voice conversion studies, realization of conversion from/to an arbitrary speaker's voice is one of the important objectives. For this purpose, eigen-voice conversion (EVC), which is based on an eigenvoice Gaussian mixture model (EV-GMM), was proposed. Although the EVC can effectively construct the conversion model for arbitrary target speakers using only a few utterances, increase of the utterances used to construct the conversion model does not always improve the conversion performance. This is because the EV-GMM method has an inherent problem in representation of GMM supervectors. We previously proposed tensor-based speaker space as a solution for this problem, and realized more flexible control of speaker characteristics. In this paper, to aim larger improvement of the performance of VC, speaker adaptive training and tensor-based speaker representation are integrated. The proposed method can construct the flexible and precise conversion model, and experimental results of one-to-many voice conversion demonstrate the effectiveness of the proposed approach.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Saito, Daisuke; Hirose, Keikichi] Univ Tokyo, Grad Sch Informat Sci &amp;
   Technol, Tokyo 1138654, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Saito, D (reprint author), Univ Tokyo, Grad Sch Informat Sci &amp; Technol, Tokyo 1138654, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>dsaito@hil.t.u-tokyo.ac.jp; mine@gavo.t.u-tokyo.ac.jp;
   hirose@gavo.t.u-tokyo.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">BP </td><td>98</td>
</tr>

<tr>
<td valign="top">EP </td><td>101</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000320827200025</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hueber, T
   <br>Ben-Youssef, A
   <br>Bailly, G
   <br>Badin, P
   <br>Elisei, F</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hueber, Thomas
   <br>Ben-Youssef, Atef
   <br>Bailly, Gerard
   <br>Badin, Pierre
   <br>Elisei, Frederic</td>
</tr>

<tr>
<td valign="top">GP </td><td>International Speech Communications Association</td>
</tr>

<tr>
<td valign="top">TI </td><td>Cross-speaker Acoustic-to-Articulatory Inversion using Phone-based
   Trajectory HAM for Pronunciation Training</td>
</tr>

<tr>
<td valign="top">SO </td><td>13TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION 2012 (INTERSPEECH 2012), VOLS 1-3</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>13th Annual Conference of the
   International-Speech-Communication-Association</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 09-13, 2012</td>
</tr>

<tr>
<td valign="top">CL </td><td>Portland, OR</td>
</tr>

<tr>
<td valign="top">DE </td><td>acoustic-to-articulatory inversion; intelligent tutoring systems;
   pronunciation training; trajectory HMM; voice conversion; talking head</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH PRODUCTION-MODEL; ADAPTATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>The article presents a statistical mapping approach for cross-speaker acoustic-to-articulatory inversion. The goal is to estimate the most likely articulatory trajectories for a reference speaker from the speech audio signal of another speaker. This approach is developed in the framework of our system of visual articulatory feedback developed for computer-assisted pronunciation training applications (CAPT). The proposed technique is based on the joint modeling of articulatory and acoustic features, for each phonetic class, using full-covariance trajectory HMM. The acoustic-to-articulatory inversion is achieved in 2 steps: 1) finding the most likely HMM state sequence from the acoustic observations; 2) inferring the articulatory trajectories from both the decoded state sequence and the acoustic observations. The problem of speaker adaptation is addressed using a voice conversion approach, based on trajectory GMM.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Hueber, Thomas; Ben-Youssef, Atef; Bailly, Gerard; Badin, Pierre;
   Elisei, Frederic] UMR 5216 CNRS INP UJF U Stendhal, GIPSA Lab, Grenoble,
   France.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hueber, T (reprint author), UMR 5216 CNRS INP UJF U Stendhal, GIPSA Lab, Grenoble, France.</td>
</tr>

<tr>
<td valign="top">EM </td><td>thomas.hueber@gipsa-lab.grenoble-inp.fr;
   atef.benyoussef@gipsa-lab.grenoble-inp.fr;
   gerard.bailly@gipsa-lab.grenoble-inp.fr;
   pierre.badin@gipsa-lab.grenoble-inp.fr;
   frederic.elisei@gipsa-lab.grenoble-inp.fr</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">BP </td><td>782</td>
</tr>

<tr>
<td valign="top">EP </td><td>785</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000320827200196</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wu, ZZ
   <br>Chng, ES
   <br>Li, HZ</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wu, Zhizheng
   <br>Chng, Eng Siong
   <br>Li, Haizhou</td>
</tr>

<tr>
<td valign="top">GP </td><td>International Speech Communications Association</td>
</tr>

<tr>
<td valign="top">TI </td><td>Detecting Converted Speech and Natural Speech for anti-Spoofing Attack
   in Speaker Recognition</td>
</tr>

<tr>
<td valign="top">SO </td><td>13TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION 2012 (INTERSPEECH 2012), VOLS 1-3</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>13th Annual Conference of the
   International-Speech-Communication-Association</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 09-13, 2012</td>
</tr>

<tr>
<td valign="top">CL </td><td>Portland, OR</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speaker verification; voice conversion; anti-spoofing attack; synthetic
   speech detection; phase spectrum</td>
</tr>

<tr>
<td valign="top">ID </td><td>HUMAN LISTENING TESTS; TIME PHASE SPECTRUM</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion techniques present a threat to speaker verification systems. To enhance the security of speaker verification systems, We study how to automatically distinguish natural speech and synthetic/converted speech. Motivated by the research on phase spectrum in speech perception, in this study, we propose to use features derived from phase spectrum to detect converted speech. The features are tested under three different training situations of the converted speech detector: a) only Gaussian mixture model (GMM) based converted speech data are available; b) only unit-selection based converted speech data are available; c) no converted speech data are available for training converted speech model. Experiments conducted on the National Institute of Standards and Technology (NIST) 2006 speaker recognition evaluation (SRE) corpus show that the performance of the features derived from phase spectrum outperform the mel-frequency cepstral coefficients (MFCCs) tremendously: even without converted speech for training, the equal error rate (EER) is reduced from 20.20% of MFCCs to 2.35%.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Wu, Zhizheng; Chng, Eng Siong; Li, Haizhou] Nanyang Technol Univ, Sch
   Comp Engn, Singapore 639798, Singapore.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Wu, ZZ (reprint author), Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.</td>
</tr>

<tr>
<td valign="top">EM </td><td>wuzz@ntu.edu.sg; aseschng@ntu.edu.sg; hli@i2r.a-star.edu.sg</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Li, Haizhou</display_name>&nbsp;</font></td><td><font size="3">Q-6438-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Li, Haizhou</display_name>&nbsp;</font></td><td><font size="3">0000-0001-9158-9401&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">BP </td><td>1698</td>
</tr>

<tr>
<td valign="top">EP </td><td>1701</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000320827200425</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Bollepalli, B
   <br>Black, AW
   <br>Prahallad, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Bollepalli, Bajibabu
   <br>Black, Alan W.
   <br>Prahallad, Kishore</td>
</tr>

<tr>
<td valign="top">GP </td><td>International Speech Communications Association</td>
</tr>

<tr>
<td valign="top">TI </td><td>Modeling a Noisy-channel for Voice Conversion Using Articulatory
   Features</td>
</tr>

<tr>
<td valign="top">SO </td><td>13TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION 2012 (INTERSPEECH 2012), VOLS 1-3</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>13th Annual Conference of the
   International-Speech-Communication-Association</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 09-13, 2012</td>
</tr>

<tr>
<td valign="top">CL </td><td>Portland, OR</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; articulatory features; noisy-channel model;
   speaker-independent representation</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we propose modeling a noisy-channel for the task of voice conversion (VC). We have used the artificial neural networks (ANN) to capture speaker-specific characteristics of a target speaker which avoid the need for any training utterance from a source speaker. We use articulatory features (AFs) as a canonical form or speaker-independent representation of a speech signal. Our studies show that AFs contain a significant amount of speaker information in their trajectories. Suitable techniques are proposed to normalize the speaker-specific information in AF trajectories and the resultant AFs are used in voice conversion. The results of voice conversion evaluated using objective and subjective measures confirm that AFs can be used as a canonical form in nosiy-channel to capture speaker-specific characteristics of a target speaker.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Bollepalli, Bajibabu; Prahallad, Kishore] Int Inst Informat Technol,
   Speech &amp; Vis Lab, Hyderabad, Andhra Pradesh, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Bollepalli, B (reprint author), Int Inst Informat Technol, Speech &amp; Vis Lab, Hyderabad, Andhra Pradesh, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>bajibabu.b@research.iiit.ac.in; awb@cs.cmu.edu; kishore@iiit.ac.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">BP </td><td>2199</td>
</tr>

<tr>
<td valign="top">EP </td><td>2202</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000320827201105</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Aihara, R
   <br>Takashima, R
   <br>Takiguchi, T
   <br>Ariki, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Aihara, Ryo
   <br>Takashima, Ryoichi
   <br>Takiguchi, Tetsuya
   <br>Ariki, Yasuo</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Consonant Enhancement for Articulation Disorders Based on Non-negative
   Matrix Factorization</td>
</tr>

<tr>
<td valign="top">SO </td><td>2012 ASIA-PACIFIC SIGNAL AND INFORMATION PROCESSING ASSOCIATION ANNUAL
   SUMMIT AND CONFERENCE (APSIPA ASC)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Asia-Pacific Signal and Information Processing Association Annual Summit
   and Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Annual Summit and Conference of the
   Asia-Pacific-Signal-and-Information-Processing-Association (APSIPA)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 03-06, 2012</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hollywood, CA</td>
</tr>

<tr>
<td valign="top">AB </td><td>We present consonant enhancement on a voice for a person with articulation disorders resulting from athetoid cerebral palsy. The movement of such speakers is limited by their athetoid symptoms, and their consonants are often unstable or unclear, which makes it difficult for them to communicate. Speech recognition for articulation disorders has been studied; however, its recognition rate is still lower than that of physically unimpaired persons. In this paper, an exemplar-based spectral conversion using Non-negative Matrix Factorization (NMF) is applied to consonant enhancement of a voice with articulation disorders. The source speaker's spectrum is easily converted into a well-ordered speaker's spectrum. Its effectiveness is examined for voice quality and clarity of consonants for a person with articulation disorders.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Aihara, Ryo; Takashima, Ryoichi; Takiguchi, Tetsuya; Ariki, Yasuo] Kobe
   Univ, Grad Sch Syst Informat, Kobe, Hyogo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Aihara, R (reprint author), Kobe Univ, Grad Sch Syst Informat, Kobe, Hyogo, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>aihara@me.cs.scitec.kobe-u.ac.jp; takashima@me.cs.scitec.kobe-u.ac.jp;
   takigu@kobe-u.ac.jp; ariki@kobe-u.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000319456200054</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Doi, H
   <br>Toda, T
   <br>Nakano, T
   <br>Goto, M
   <br>Nakamura, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Doi, Hironori
   <br>Toda, Tomoki
   <br>Nakano, Tomoyasu
   <br>Goto, Masataka
   <br>Nakamura, Satoshi</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Singing Voice Conversion Method Based on Many-to-Many Eigenvoice
   Conversion and Training Data Generation Using a Singing-to-Singing
   Synthesis System</td>
</tr>

<tr>
<td valign="top">SO </td><td>2012 ASIA-PACIFIC SIGNAL AND INFORMATION PROCESSING ASSOCIATION ANNUAL
   SUMMIT AND CONFERENCE (APSIPA ASC)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Asia-Pacific Signal and Information Processing Association Annual Summit
   and Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Annual Summit and Conference of the
   Asia-Pacific-Signal-and-Information-Processing-Association (APSIPA)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 03-06, 2012</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hollywood, CA</td>
</tr>

<tr>
<td valign="top">AB </td><td>The voice quality (identity) of singing voices is usually fixed in each singer. To overcome this limitation and enable singers to freely change their voice quality using signal-processing technologies, we propose a singing voice conversion method based on many-to-many eigenvoice conversion (EVC) that can convert the voice quality of an arbitrary source singer into that of another arbitrary target singer. Previous EVC-based methods required parallel data consisting of song pairs of a single reference singer and many prestored target singers for training a voice conversion model, but it was difficult to record such data. Our proposed method therefore uses a singing-to-singing synthesis system called VocaListener to generate parallel data by imitating singing voices of many prestored target singers with the system's singing voices. Experimental results show that our method succeeded in enabling people to sing a song with the voice quality of a different target singer even if only an extremely small amount of the target singing voice is available.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Doi, Hironori; Toda, Tomoki; Nakamura, Satoshi] Nara Inst Sci &amp; Technol
   NAIST, Grad Sch Informat Sci, Nara, Japan.
   <br>Natl Inst Adv Ind Sci &amp; Technol, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Doi, H (reprint author), Nara Inst Sci &amp; Technol NAIST, Grad Sch Informat Sci, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>hironori-d@is.naist.jp; tomoki@is.naist.jp; t.nakano@aist.go.jp;
   m.goto@aist.go.jp; s-nakamura@is.naist.jp</td>
</tr>

<tr xmlns:date="http://exslt.org/dates-and-times">
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Goto, Masataka</display_name>&nbsp;</font></td><td><font size="3">K-8205-2012&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Nakano, Tomoyasu</display_name>&nbsp;</font></td><td><font size="3">A-8670-2013&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Goto, Masataka</display_name>&nbsp;</font></td><td><font size="3">0000-0003-1167-0977&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Nakano, Tomoyasu</display_name>&nbsp;</font></td><td><font size="3">0000-0001-8014-2209&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000319456200049</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sawada, K
   <br>Tagami, Y
   <br>Tamura, S
   <br>Takehara, M
   <br>Hayamizu, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sawada, Kohei
   <br>Tagami, Yoji
   <br>Tamura, Satoshi
   <br>Takehara, Masanori
   <br>Hayamizu, Satoru</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Statistical Voice Conversion using GA-based Informative Feature</td>
</tr>

<tr>
<td valign="top">SO </td><td>2012 ASIA-PACIFIC SIGNAL AND INFORMATION PROCESSING ASSOCIATION ANNUAL
   SUMMIT AND CONFERENCE (APSIPA ASC)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Asia-Pacific Signal and Information Processing Association Annual Summit
   and Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Annual Summit and Conference of the
   Asia-Pacific-Signal-and-Information-Processing-Association (APSIPA)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 03-06, 2012</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hollywood, CA</td>
</tr>

<tr>
<td valign="top">AB </td><td>In order to make voice conversion (VC) robust to noise, we propose VC using GA-based informative feature (GIF), by adding an extraction process of GIF to a conventional VC. GIF is proposed as a feature that can be applied not only in pattern recognition but also in relative tasks. In speech recognition, furthermore, GIF could improve recognition accuracy in noise environment. We evaluated the performances of VC using spectral segmental features (conventional method) and GIF, respectively. Objective experimental result indicates that in noise environments, the proposed method was better than the conventional method. Subjective experiment was also conducted to compare the performances. These results show that application of GIF to VC was effective.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Sawada, Kohei; Tagami, Yoji; Tamura, Satoshi; Takehara, Masanori;
   Hayamizu, Satoru] Gifu Univ, Dept Informat Sci, Gifu, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sawada, K (reprint author), Gifu Univ, Dept Informat Sci, Gifu, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>kouhei@asr.info.gifu-u.ac.jp; tagami@asr.info.gifu-u.ac.jp;
   tamura@info.gifu-u.ac.jp; takehara@asr.info.gifu-u.ac.jp;
   hayamizu@gifu-u.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000319456200031</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wu, ZZ
   <br>Kinnunen, T
   <br>Chng, ES
   <br>Li, HZ
   <br>Ambikairajah, E</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wu, Zhizheng
   <br>Kinnunen, Tomi
   <br>Chng, Eng Siong
   <br>Li, Haizhou
   <br>Ambikairajah, Eliathamby</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>A study on spoofing attack in state-of-the-art speaker verification: the
   telephone speech case</td>
</tr>

<tr>
<td valign="top">SO </td><td>2012 ASIA-PACIFIC SIGNAL AND INFORMATION PROCESSING ASSOCIATION ANNUAL
   SUMMIT AND CONFERENCE (APSIPA ASC)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Asia-Pacific Signal and Information Processing Association Annual Summit
   and Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Annual Summit and Conference of the
   Asia-Pacific-Signal-and-Information-Processing-Association (APSIPA)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 03-06, 2012</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hollywood, CA</td>
</tr>

<tr>
<td valign="top">ID </td><td>SUPPORT VECTOR MACHINES; VOICE</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion technique, which modifies one speaker's (source) voice to sound like another speaker (target), presents a threat to automatic speaker verification. In this paper, we first present new results of evaluating the vulnerability of current state-of-the-art speaker verification systems: Gaussian mixture model with joint factor analysis (GMM-JFA) and probabilistic linear discriminant analysis (PLDA) systems, against spoofing attacks. The spoofing attacks are simulated by two voice conversion techniques: Gaussian mixture model based conversion and unit selection based conversion. To reduce false acceptance rate caused by spoofing attack, we propose a general anti-spoofing attack framework for the speaker verification systems, where a converted speech detector is adopted as a post-processing module for the speaker verification system's acceptance decision. The detector decides whether the accepted claim is human speech or converted speech.
   <br>A subset of the core task in the NIST SRE 2006 corpus is used to evaluate the vulnerability of speaker verification system and the performance of converted speech detector. The results indicate that both conversion techniques can increase the false acceptance rate of GMM-JFA and PLDA system, while the converted speech detector can reduce the false acceptance rate from 31.54% and 41.25% to 1.64% and 1.71% for GMM-JFA and PLDA system on unit-selection based converted speech, respectively.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Wu, Zhizheng; Chng, Eng Siong; Li, Haizhou] Nanyang Technol Univ, Sch
   Comp Engn, Singapore 639798, Singapore.
   <br>[Wu, Zhizheng] Nanyang Technol Univ, Temasek Labs NTU, Singapore,
   Singapore.
   <br>[Wu, Zhizheng; Kinnunen, Tomi] Univ Eastern Finland, Sch Comp, Joensuu,
   Finland.
   <br>[Li, Haizhou] Inst Infocomm Res, Human Language Technol Dept, Singapore,
   Singapore.
   <br>[Li, Haizhou; Ambikairajah, Eliathamby] Univ New South Wales, Sch Elect
   Engn &amp; Telecommun, Sydney, NSW, Australia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Wu, ZZ (reprint author), Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.</td>
</tr>

<tr>
<td valign="top">EM </td><td>wuzz@ntu.edu.sg; aseschng@ntu.edu.sg; hli@i2r.a-star.edu.sg</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Li, Haizhou</display_name>&nbsp;</font></td><td><font size="3">Q-6438-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000319456200146</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Xie, WC
   <br>Zhang, LH</td>
</tr>

<tr>
<td valign="top">AF </td><td>Xie Weichao
   <br>Zhang Linghua</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Vocal Tract Spectrum Transformation Based on Clustering in Voice
   Conversion System</td>
</tr>

<tr>
<td valign="top">SO </td><td>PROCEEDING OF THE IEEE INTERNATIONAL CONFERENCE ON INFORMATION AND
   AUTOMATION</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Information and Automation (ICIA)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUN 06-08, 2012</td>
</tr>

<tr>
<td valign="top">CL </td><td>Shenyang, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice Conversion; Spectrum Transformation; Cluster; K-Means algorithm;
   Gaussian Mixture Model (GMM)</td>
</tr>

<tr>
<td valign="top">AB </td><td>By the conventional vocal tract spectrum transformation based on Gaussian Mixture Model (GMM), the transformation rule is not very accurate because of the large amount of voice data which is time-varying and non-stationary. This paper mainly studies a method of spectrum transformation based on clustering algorithm. First of all, the training data are classified into several clusters and each cluster is trained relatively to get a more accurate transformation rule. And in the stage of transformation, the source parameters of each frame are classified into one cluster, and then are converted by the transformation rule of that cluster. In this paper, K-means algorithm is used as the clustering method to classified data. Experiment results show that proposed method based on clustering is better than the transformation by conventional GMM, especially the one by K-Means algorithm with 20 centers is the best one.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Xie Weichao; Zhang Linghua] Nanjing Univ Posts &amp; Telecommun, Coll
   Telecommun &amp; Informat Engn, Nanjing 210003, Jiangsu, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Xie, WC (reprint author), Nanjing Univ Posts &amp; Telecommun, Coll Telecommun &amp; Informat Engn, Nanjing 210003, Jiangsu, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>1010010504@njupt.edu.cn; zhanglh@njupt.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">BP </td><td>236</td>
</tr>

<tr>
<td valign="top">EP </td><td>240</td>
</tr>

<tr>
<td valign="top">SC </td><td>Automation &amp; Control Systems; Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000318899300042</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Toda, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Toda, Tomoki</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Statistical Approach to Voice Conversion and Its Applications for
   Augmented Human Communication</td>
</tr>

<tr>
<td valign="top">SO </td><td>2012 8TH INTERNATIONAL SYMPOSIUM ON CHINESE SPOKEN LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>8th International Symposium on Chinese Spoken Language Processing
   (ISCSLP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 05-08, 2012</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hong Kong, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion is a technique for modifying speech acoustics, converting nonlinguistic information to any form we want while preserving the linguistic content. One of the most popular approaches to voice conversion is based on statistical processing, which is capable of extracting complex conversion functions from a parallel speech data set consisting of utterance pairs of the source and the target voices. Although this technique was originally studied in the context of speaker conversion, which converts the voice of a certain speaker (the source speaker) to sound like that of another speaker(the target speaker), it has great potential to achieve various applications beyond speaker conversion.
   <br>In this tutorial, we first overview basic approaches to statistical voice conversion especially focusing on conversion methods that do not require any linguistic input. After reviewing frame-by-frame conversion as a standard method, we take a careful look at a state-of-the-art trajectory-based conversion method that is capable of using statistics calculated over an utterance to effectively reproduce natural speech parameter trajectories, and analyze which problems are well addressed in this method. Furthermore, we look at a technique that extends this trajectory-based conversion method to achieve a lower conversion delay, which enables to use state-of-the-art voice conversion in real-time applications.
   <br>Real-time applications of voice conversion have a potential to enhance our human-to-human speech communication to overcome barriers such as physical constraints causing vocal disorders, and environmental constraints that do not allow for producing and conveying intelligible speech. In this tutorial, we look at body-conducted speech enhancement and speaking-aids for total laryngectomees as examples of voice conversion applications. The basic conversion algorithm is effectively applied to mapping problems between various different types of speech parameters. From these examples, we learn how to apply voice conversion techniques to individual speech applications to successfully augment human communication.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Nara Inst Sci &amp; Technol, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Toda, T (reprint author), Nara Inst Sci &amp; Technol, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">BP </td><td>XXIX</td>
</tr>

<tr>
<td valign="top">EP </td><td>XXX</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000316984700006</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hwang, HT
   <br>Tsao, Y
   <br>Wang, HM
   <br>Wang, YR
   <br>Chen, SH</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hwang, Hsin-Te
   <br>Tsao, Yu
   <br>Wang, Hsin-Min
   <br>Wang, Yih-Ru
   <br>Chen, Sin-Horng</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>EXPLORING MUTUAL INFORMATION FOR GMM-BASED SPECTRAL CONVERSION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2012 8TH INTERNATIONAL SYMPOSIUM ON CHINESE SPOKEN LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>8th International Symposium on Chinese Spoken Language Processing
   (ISCSLP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 05-08, 2012</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hong Kong, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; mutual information; GMM</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we propose a maximum mutual information (MMI) training criterion to refine the parameters of the joint density GMM (JDGMM) set to tackle the over-smoothing issue in voice conversion (VC). Conventionally, the maximum likelihood (ML) criterion is used to train a JDGMM set, which characterizes the joint property of the source and target feature vectors. The MMI training criterion, on the other hand, updates the parameters of the JDGMM set to increase its capability on modeling the dependency between the source and target feature vectors, and thus to make the converted sounds closer to the natural ones. The subjective listening test demonstrates that the quality and individuality of the converted speech by the proposed ML followed by MMI (ML+MMI) training method is better that by the ML training method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Hwang, Hsin-Te; Wang, Yih-Ru; Chen, Sin-Horng] Natl Chiao Tung Univ,
   Dept Elect &amp; Comp Engn, Hsinchu, Taiwan.
   <br>[Tsao, Yu] Acad Sinica, Res Ctr Informat Technol Innovat, Taipei, Taiwan.
   <br>[Hwang, Hsin-Te; Wang, Hsin-Min] Acad Sinica, Inst Informat Sci, Taipei,
   Taiwan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hwang, HT (reprint author), Natl Chiao Tung Univ, Dept Elect &amp; Comp Engn, Hsinchu, Taiwan.</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">BP </td><td>50</td>
</tr>

<tr>
<td valign="top">EP </td><td>54</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000316984700018</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Li, N
   <br>Qiao, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Li, Na
   <br>Qiao, Yu</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Conversion using Bayesian Mixture of Probabilistic Linear
   Regressions and Dynamic Kernel Features</td>
</tr>

<tr>
<td valign="top">SO </td><td>2012 8TH INTERNATIONAL SYMPOSIUM ON CHINESE SPOKEN LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>8th International Symposium on Chinese Spoken Language Processing
   (ISCSLP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 05-08, 2012</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hong Kong, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>dynamic kernel features; Bayesian inference; voice conversion; mixture
   of probabilistic linear regressions</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion can be formulated as finding a mapping function which transforms the features of a source speaker to those of the target speaker. Gaussian mixture model (GMM)-based conversion techniques [1, 2] have been widely used in voice conversion due to its effectiveness and efficiency. In a recent work [3], we generalized GMM-based mapping to Mixture of Probabilistic Linear Regressions (MPLR). But both GMM based mapping and MPLR are subjected to overfitting problem especially when the training utterances are sparse, and both ignore the inherent time-dependency among speech features. This paper addresses this problem by introducing dynamic kernel features and conducting Bayesian analysis for MPLR. The dynamic kernel features are calculated as kernel transformations of current, previous and next frames, which can model both the nonlinearities and dynamics in the features. We further develop Maximum a Posterior (MAP) inference to alleviate the overfitting problem by introducing prior on the parameters of kernel transformation. Our experimental results exhibit that the proposed methods achieve better performance compared to the MPLR based model.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Li, Na] Northwestern Polytech Univ, Xian 710072, Peoples R China.
   <br>[Li, Na; Qiao, Yu] Shenzhen Inst Adv Technol, Shenzhen key Iab CVPR,
   CAS, Shenzhen, Peoples R China.
   <br>[Qiao, Yu] Chinese Univ Hong Kong, Hong Kong, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Li, N (reprint author), Northwestern Polytech Univ, Xian 710072, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>na.li@siat.ac.cn; yu.qiao@siat.ac.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">BP </td><td>69</td>
</tr>

<tr>
<td valign="top">EP </td><td>73</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000316984700022</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Takashima, R
   <br>Takiguchi, T
   <br>Ariki, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Takashima, Ryoichi
   <br>Takiguchi, Tetsuya
   <br>Ariki, Yasuo</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>EXEMPLAR-BASED VOICE CONVERSION IN NOISY ENVIRONMENT</td>
</tr>

<tr>
<td valign="top">SO </td><td>2012 IEEE WORKSHOP ON SPOKEN LANGUAGE TECHNOLOGY (SLT 2012)</td>
</tr>

<tr>
<td valign="top">SE </td><td>IEEE Workshop on Spoken Language Technology</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE Workshop on Spoken Language Technology (SLT)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 02-05, 2012</td>
</tr>

<tr>
<td valign="top">CL </td><td>Miami, FL</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; exemplar-based; sparse coding; non-negative matrix
   factorization; noise robustness</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a voice conversion (VC) technique for noisy environments, where parallel exemplars are introduced to encode the source speech signal and synthesize the target speech signal. The parallel exemplars (dictionary) consist of the source exemplars and target exemplars, having the same texts uttered by the source and target speakers. The input source signal is decomposed into the source exemplars, noise exemplars obtained from the input signal, and their weights (activities). Then, by using the weights of the source exemplars, the converted signal is constructed from the target exemplars. We carried out speaker conversion tasks using clean speech data and noise-added speech data. The effectiveness of this method was confirmed by comparing its effectiveness with that of a conventional Gaussian Mixture Model (GMM)-based method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Takashima, Ryoichi; Takiguchi, Tetsuya; Ariki, Yasuo] Kobe Univ, Grad
   Sch Syst Informat, Kobe, Hyogo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Takashima, R (reprint author), Kobe Univ, Grad Sch Syst Informat, Kobe, Hyogo, Japan.</td>
</tr>

<tr>
<td valign="top">TC </td><td>44</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>44</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">BP </td><td>313</td>
</tr>

<tr>
<td valign="top">EP </td><td>317</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000317182800055</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Zorila, TC
   <br>Erro, D
   <br>Hernaez, I</td>
</tr>

<tr>
<td valign="top">AF </td><td>Zorila, Tudor-Catalin
   <br>Erro, Daniel
   <br>Hernaez, Inma</td>
</tr>

<tr>
<td valign="top">BE </td><td>Toledano, DT
   <br>Gimenez, AO
   <br>Teixeira, A
   <br>Rodriguez, JG
   <br>Gomez, LH
   <br>Hernandez, RSS
   <br>Castro, DR</td>
</tr>

<tr>
<td valign="top">TI </td><td>Improving the Quality of Standard GMM-Based Voice Conversion Systems by
   Considering Physically Motivated Linear Transformations</td>
</tr>

<tr>
<td valign="top">SO </td><td>ADVANCES IN SPEECH AND LANGUAGE TECHNOLOGIES FOR IBERIAN LANGUAGES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Communications in Computer and Information Science</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Spanish Speech Technology Workshop/Iberian SLTech Workshop</td>
</tr>

<tr>
<td valign="top">CY </td><td>NOV 21-23, 2012</td>
</tr>

<tr>
<td valign="top">CL </td><td>Univ Autonoma Madrid, ATVS Biometr Res Grp, Madrid, SPAIN</td>
</tr>

<tr>
<td valign="top">HO </td><td>Univ Autonoma Madrid, ATVS Biometr Res Grp</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; Gaussian mixture models; dynamic frequency warping;
   amplitude scaling; linear transformation</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a new method to train traditional voice conversion functions based on Gaussian mixture models, linear transforms and cepstral parameterization. Instead of using statistical criteria, this method calculates a set of linear transforms that represent physically meaningful spectral modifications such as frequency warping and amplitude scaling. Our experiments indicate that the proposed training method leads to significant improvements in the average quality of the converted speech with respect to traditional statistical methods. This is achieved without modifying the input/output parameters or the shape of the conversion function.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Zorila, Tudor-Catalin] POLITEHN Univ Bucharest UPB, Bucharest, Romania.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Zorila, TC (reprint author), POLITEHN Univ Bucharest UPB, Bucharest, Romania.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ztudorc@gmail.com; derro@aholab.ehu.es; inma@aholab.ehu.es</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Erro, Daniel</display_name>&nbsp;</font></td><td><font size="3">H-7043-2015&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Hernaez Rioja, Inmaculada</display_name>&nbsp;</font></td><td><font size="3">K-8303-2012&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Erro, Daniel</display_name>&nbsp;</font></td><td><font size="3">0000-0003-0954-6942&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Hernaez Rioja, Inmaculada</display_name>&nbsp;</font></td><td><font size="3">0000-0003-4447-7575&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>9</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>9</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">VL </td><td>328</td>
</tr>

<tr>
<td valign="top">BP </td><td>30</td>
</tr>

<tr>
<td valign="top">EP </td><td>39</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000316029900004</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sun, XJ
   <br>Zhang, XW
   <br>Cao, TY
   <br>Yang, JB
   <br>Sun, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sun, Xinjian
   <br>Zhang, Xiongwei
   <br>Cao, Tieyong
   <br>Yang, Jibin
   <br>Sun, Jian</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Conversion Using A Two-Factor Gaussian Process Latent Variable
   Model</td>
</tr>

<tr>
<td valign="top">SO </td><td>PRZEGLAD ELEKTROTECHNICZNY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; style and content; separation; Gaussian Process Latent
   Variable Model</td>
</tr>

<tr>
<td valign="top">ID </td><td>ARTIFICIAL NEURAL-NETWORKS; TRANSFORMATION; REGRESSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a novel strategy for voice conversion by solving style and content separation task using a two-factor Gaussian Process Latent Variable Model (GP-LVM). A generative model for speech is developed by interaction of style and content, which represent the voice individual characteristics and semantic information respectively. The interaction is captured by a GP-LVM with two latent variables, as well as a GP mapping to observation. Then, for a given collection of labelled observations, the separation task is accomplished by fitting the model with Maximum Likelihood method. Finally, voice conversion is implemented by style alternation, and the desired speech is reconstructed with the decomposed target speaker style and the source speech content using the learned model as a prior. Both objective and subjective test results show the advantage of the proposed method compared to the traditional GMM-based mapping system with limited size of training data. Furthermore, experimental results indicate that the GP-LVM with nonlinear kernel functions behaves better than that with linear ones for voice conversion due to its ability of better capturing the interaction between style and content, and rich varieties of the two factors in a training set also help to improve the conversion performance.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Sun, Xinjian; Sun, Jian] PLA Univ Sci &amp; Tech, Inst Commun Engn,
   Postgrad Team 2, Nanjing 210007, Jiangsu, Peoples R China.
   <br>[Zhang, Xiongwei; Cao, Tieyong; Yang, Jibin] PLA Univ Sci &amp; Tech, Inst
   Command Automat, Nanjing 210007, Jiangsu, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sun, XJ (reprint author), PLA Univ Sci &amp; Tech, Inst Commun Engn, Postgrad Team 2, Biaoyin 2,Yudao St, Nanjing 210007, Jiangsu, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>sunxj99@hotmail.com; xwzhang@public1.ptt.js.cn; cty_ice@163.com;
   yjbice@sina.com; sunjian001@gmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">VL </td><td>88</td>
</tr>

<tr>
<td valign="top">IS </td><td>12A</td>
</tr>

<tr>
<td valign="top">BP </td><td>318</td>
</tr>

<tr>
<td valign="top">EP </td><td>324</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000314689300068</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Yang, M
   <br>Que, DS
   <br>Li, B</td>
</tr>

<tr>
<td valign="top">AF </td><td>Yang, Man
   <br>Que, Dashun
   <br>Li, Bei</td>
</tr>

<tr>
<td valign="top">BE </td><td>Wang, FL
   <br>Lei, JS
   <br>Lau, RWH
   <br>Zhang, JX</td>
</tr>

<tr>
<td valign="top">TI </td><td>Design and Implementation of Voice Conversion System Based on GMM and
   ANN</td>
</tr>

<tr>
<td valign="top">SO </td><td>MULTIMEDIA AND SIGNAL PROCESSING</td>
</tr>

<tr>
<td valign="top">SE </td><td>Communications in Computer and Information Science</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>2nd International Conference on Multimedia and Signal Processing (CMSP
   2012)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 07-09, 2012</td>
</tr>

<tr>
<td valign="top">CL </td><td>Shanghai, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; GMM; ANN; residual; pitch</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion has become one current researching hotspot in speech signal processing. The article designs and establishes a voice conversion system based on Gaussian mixture model (GMM) and Artificial Neural Network (ANN) after researching the existing voice conversion algorithms. The core is to obtain respectively three mapping rules by training spectral envelope and its residual with GMM, and pitch contrail with BP network. And the voice is transformed according to the three mapping rules above. Finally, the algorithm simulation, the system implementation and algorithm performance evaluating of voice conversion is completed. The theoretic analysis and simulating results reveal that the algorithm and the system of voice conversion are effective.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Yang, Man; Que, Dashun; Li, Bei] Wuhan Univ Technol, Informat Engn
   Coll, Wuhan 430063, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Yang, M (reprint author), Wuhan Univ Technol, Informat Engn Coll, Wuhan 430063, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>pipayang0503@163.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">VL </td><td>346</td>
</tr>

<tr>
<td valign="top">BP </td><td>624</td>
</tr>

<tr>
<td valign="top">EP </td><td>631</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000313664700079</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Janke, M
   <br>Wand, M
   <br>Nakamura, K
   <br>Schultz, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Janke, Matthias
   <br>Wand, Michael
   <br>Nakamura, Keigo
   <br>Schultz, Tanja</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>FURTHER INVESTIGATIONS ON EMG-TO-SPEECH CONVERSION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2012 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech and Signal Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 25-30, 2012</td>
</tr>

<tr>
<td valign="top">CL </td><td>Kyoto, JAPAN</td>
</tr>

<tr>
<td valign="top">DE </td><td>Silent Speech; Electromyography; Speech Synthesis; Voice Conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Our study deals with a Silent Speech Interface based on mapping surface electromyographic (EMG) signals to speech waveforms. Electromyographic signals recorded from the facial muscles capture the activity of the human articulatory apparatus and therefore allow to retrace speech, even when no audible signal is produced. The mapping of EMG signals to speech is done via a Gaussian mixture model (GMM)-based conversion technique.
   <br>In this paper, we follow the lead of EMG-based speech-to-text systems and apply two major recent technological advances to our system, namely, we consider session-independent systems, which are robust against electrode repositioning, and we show that mapping the EMG signal to whispered speech creates a better speech signal than a mapping to normally spoken speech. We objectively evaluate the performance of our systems using a spectral distortion measure.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Janke, Matthias; Wand, Michael; Nakamura, Keigo; Schultz, Tanja]
   Karlsruhe Inst Technol KIT, Cognit Syst Lab, Karlsruhe, Germany.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Janke, M (reprint author), Karlsruhe Inst Technol KIT, Cognit Syst Lab, Karlsruhe, Germany.</td>
</tr>

<tr>
<td valign="top">EM </td><td>matthias.janke@kit.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>7</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>7</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">BP </td><td>365</td>
</tr>

<tr>
<td valign="top">EP </td><td>368</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000312381400090</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kinnunen, T
   <br>Wu, ZZ
   <br>Lee, KA
   <br>Sedlak, F
   <br>Chng, ES
   <br>Li, HZ</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kinnunen, Tomi
   <br>Wu, Zhi-Zheng
   <br>Lee, Kong Aik
   <br>Sedlak, Filip
   <br>Chng, Eng Siong
   <br>Li, Haizhou</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Vulnerability of Speaker Verification Systems Against Voice Conversion
   Spoofing Attacks: the Case of Telephone Speech</td>
</tr>

<tr>
<td valign="top">SO </td><td>2012 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech and Signal Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 25-30, 2012</td>
</tr>

<tr>
<td valign="top">CL </td><td>Kyoto, JAPAN</td>
</tr>

<tr>
<td valign="top">DE </td><td>speaker verification; voice conversion; security</td>
</tr>

<tr>
<td valign="top">ID </td><td>SUPPORT VECTOR MACHINES; TRANSFORMATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion - the methodology of automatically converting one's utterances to sound as if spoken by another speaker - presents a threat for applications relying on speaker verification. We study vulnerability of text-independent speaker verification systems against voice conversion attacks using telephone speech. We implemented a voice conversion systems with two types of features and nonparallel frame alignment methods and five speaker verification systems ranging from simple Gaussian mixture models (GMMs) to state-of-the-art joint factor analysis (JFA) recognizer. Experiments on a subset of NIST 2006 SRE corpus indicate that the JFA method is most resilient against conversion attacks. But even it experiences more than 5-fold increase in the false acceptance rate from 3.24 % to 17.33 %.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kinnunen, Tomi; Sedlak, Filip] UEF, Sch Comp, Joensuu, Finland.
   <br>[Wu, Zhi-Zheng; Chng, Eng Siong] Nanyang Technol Univ, Sch Comp Engn,
   Singapore, Singapore.
   <br>[Lee, Kong Aik; Li, Haizhou] Inst Infocomm Res I2R, Human Language
   Technol Dept, Singapore, Singapore.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kinnunen, T (reprint author), UEF, Sch Comp, Joensuu, Finland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>tkinnu@cs.joensuu.fi; wuzz@ntu.edu.sg; kalee@i2r.a-star.edu.sg</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Li, Haizhou</display_name>&nbsp;</font></td><td><font size="3">Q-6438-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Li, Haizhou</display_name>&nbsp;</font></td><td><font size="3">0000-0001-9158-9401&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>65</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>67</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">BP </td><td>4401</td>
</tr>

<tr>
<td valign="top">EP </td><td>4404</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000312381404118</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Yamamoto, K
   <br>Toda, T
   <br>Doi, H
   <br>Saruwatari, H
   <br>Shikano, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Yamamoto, Kenzo
   <br>Toda, Tomoki
   <br>Doi, Hironori
   <br>Saruwatari, Hiroshi
   <br>Shikano, Kiyohiro</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>STATISTICAL APPROACH TO VOICE QUALITY CONTROL IN ESOPHAGEAL SPEECH
   ENHANCEMENT</td>
</tr>

<tr>
<td valign="top">SO </td><td>2012 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech and Signal Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 25-30, 2012</td>
</tr>

<tr>
<td valign="top">CL </td><td>Kyoto, JAPAN</td>
</tr>

<tr>
<td valign="top">DE </td><td>Esophageal speech; speech enhancement; voice conversion; voice quality
   control; kernel regression</td>
</tr>

<tr>
<td valign="top">ID </td><td>CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper describes a voice quality control method in statistical esophageal speech enhancement. Esophageal speech is produced by one of the alternative speaking methods for laryngectomees. Its naturalness and intelligibility are much lower than those of natural voices and its voice quality sounds similar even if uttered by different laryngectomees. These issues are alleviated by a statistical voice conversion method from esophageal speech into normal speech (ES-to-Speech) based on eigenvoices. This method is capable of determining converted voice quality using a few target voice samples. In this paper, we propose ES-to-Speech using regression techniques to make it possible to manually control the converted voice quality by manipulating a few intuitively controllable parameters even if no target voice sample is available. The effectiveness of the proposed method is confirmed by experimental evaluations.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Yamamoto, Kenzo; Toda, Tomoki; Doi, Hironori; Saruwatari, Hiroshi;
   Shikano, Kiyohiro] Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara,
   Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Yamamoto, K (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>tomoki@is.naist.jp; hironori-d@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>5</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>5</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">BP </td><td>4497</td>
</tr>

<tr>
<td valign="top">EP </td><td>4500</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000312381404142</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Cen, L
   <br>Dong, MH
   <br>Chan, P</td>
</tr>

<tr>
<td valign="top">AF </td><td>Cen, Ling
   <br>Dong, Minghui
   <br>Chan, Paul</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>TEMPLATE-BASED PERSONALIZED SINGING VOICE SYNTHESIS</td>
</tr>

<tr>
<td valign="top">SO </td><td>2012 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech and Signal Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 25-30, 2012</td>
</tr>

<tr>
<td valign="top">CL </td><td>Kyoto, JAPAN</td>
</tr>

<tr>
<td valign="top">DE </td><td>Singing voice synthesis; music score; pitch; spectrum; alignment</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, a template-based personalized singing voice synthesis method is proposed. It generates singing voices by means of conversion from the narrated lyrics of a song with the use of template recordings. The template voices are parallel speaking and singing voices recorded from professional singers, which are used to derive the transformation models for acoustic feature conversion. When converting a new instance of speech, its acoustic features are modified to approximate those of the actual singing voice based on the transformation models. Since the pitch contour of the synthesized singing is derived from an actual singing voice, it is more natural than modifying a step contour to implement pitch fluctuations such as overshoot and vibrato. It has been shown from the subjective tests that nearly natural singing quality with the preservation of the timbre can be achieved with the help of our method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Cen, Ling; Dong, Minghui; Chan, Paul] ASTAR, Inst Infocomm Res,
   Singapore 138632, Singapore.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Cen, L (reprint author), ASTAR, Inst Infocomm Res, 1 Fusionopolis Way, Singapore 138632, Singapore.</td>
</tr>

<tr>
<td valign="top">TC </td><td>6</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>6</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">BP </td><td>4509</td>
</tr>

<tr>
<td valign="top">EP </td><td>4512</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000312381404145</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Popa, V
   <br>Silen, H
   <br>Nurminen, J
   <br>Gabbouj, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Popa, Victor
   <br>Silen, Hanna
   <br>Nurminen, Jani
   <br>Gabbouj, Moncef</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>LOCAL LINEAR TRANSFORMATION FOR VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2012 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech and Signal Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 25-30, 2012</td>
</tr>

<tr>
<td valign="top">CL </td><td>Kyoto, JAPAN</td>
</tr>

<tr>
<td valign="top">DE </td><td>Gaussian Mixture Model (GMM); Line Spectral Frequencies (LSF); Local
   Linear Transformation (LLT)</td>
</tr>

<tr>
<td valign="top">AB </td><td>Many popular approaches to spectral conversion involve linear transformations determined for particular acoustic classes and compute the converted result as a linear combination between different local transformations in an attempt to ensure a continuous conversion. These methods often produce over-smoothed spectra and parameter tracks. The proposed method computes an individual linear transformation for every feature vector based on a small neighborhood in the acoustic space thus preserving local details. The method effectively reduces the over-smoothing by eliminating undesired contributions from acoustically remote regions. The method is evaluated in listening tests against the well-known Gaussin Mixture Model based conversion, representative of the class of methods involving linear transformations. Perceptual results indicate a clear preference for the proposed scheme.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Popa, Victor; Silen, Hanna; Gabbouj, Moncef] Tampere Univ Technol, Dept
   Signal Proc, FIN-33101 Tampere, Finland.
   <br>[Nurminen, Jani] Nokia, Tampere, Finland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Popa, V (reprint author), Tampere Univ Technol, Dept Signal Proc, FIN-33101 Tampere, Finland.</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Gabbouj, Moncef</display_name>&nbsp;</font></td><td><font size="3">G-4293-2014&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Gabbouj, Moncef</display_name>&nbsp;</font></td><td><font size="3">0000-0002-9788-2323&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>6</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>6</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">BP </td><td>4517</td>
</tr>

<tr>
<td valign="top">EP </td><td>4520</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000312381404147</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Goto, M
   <br>Nakano, T
   <br>Kajita, S
   <br>Matsusaka, Y
   <br>Nakaoka, S
   <br>Yokoi, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Goto, Masataka
   <br>Nakano, Tomoyasu
   <br>Kajita, Shuuji
   <br>Matsusaka, Yosuke
   <br>Nakaoka, Shin'ichiro
   <br>Yokoi, Kazuhito</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>VOCALISTENER AND VOCAWATCHER: IMITATING A HUMAN SINGER BY USING SIGNAL
   PROCESSING</td>
</tr>

<tr>
<td valign="top">SO </td><td>2012 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech and Signal Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 25-30, 2012</td>
</tr>

<tr>
<td valign="top">CL </td><td>Kyoto, JAPAN</td>
</tr>

<tr>
<td valign="top">DE </td><td>Music; singing information processing; singing synthesis; singing robot</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE; CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we describe three singing information processing systems, VocaListener, VocaListener2, and VocaWatcher, that imitate singing expressions of the voice and face of a human singer. VocaListener can synthesize natural singing voices by analyzing and imitating the pitch and dynamics of the human singing. VocaListener2 imitates temporal timbre changes in addition to the pitch and dynamics. In synchronization with the synthesized singing voices, VocaWatcher can generate realistic facial motions of a humanoid robot, the HRP-4C, by analyzing and imitating facial motions of a human singing that are recorded by a single video camera. These systems that focus on "imitation" are not only promising for representing human-like naturalness, but also useful for providing intuitive control means.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Goto, Masataka; Nakano, Tomoyasu; Kajita, Shuuji; Matsusaka, Yosuke;
   Nakaoka, Shin'ichiro; Yokoi, Kazuhito] Natl Inst Adv Ind Sci &amp; Technol,
   Tsukuba, Ibaraki, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Goto, M (reprint author), Natl Inst Adv Ind Sci &amp; Technol, Tsukuba, Ibaraki, Japan.</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Kajita, Shuuji</display_name>&nbsp;</font></td><td><font size="3">M-5010-2016&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Nakaoka, Shin'ichiro</display_name>&nbsp;</font></td><td><font size="3">M-5396-2016&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Yokoi, Kazuhito</display_name>&nbsp;</font></td><td><font size="3">K-2046-2012&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Goto, Masataka</display_name>&nbsp;</font></td><td><font size="3">K-8205-2012&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Nakano, Tomoyasu</display_name>&nbsp;</font></td><td><font size="3">A-8670-2013&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Kajita, Shuuji</display_name>&nbsp;</font></td><td><font size="3">0000-0001-8188-2209&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Nakaoka, Shin'ichiro</display_name>&nbsp;</font></td><td><font size="3">0000-0002-2346-1251&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Yokoi, Kazuhito</display_name>&nbsp;</font></td><td><font size="3">0000-0003-3942-2027&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Goto, Masataka</display_name>&nbsp;</font></td><td><font size="3">0000-0003-1167-0977&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Nakano, Tomoyasu</display_name>&nbsp;</font></td><td><font size="3">0000-0001-8014-2209&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>5</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>5</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">BP </td><td>5393</td>
</tr>

<tr>
<td valign="top">EP </td><td>5396</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000312381405117</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Biasi, R
   <br>Andrighettoni, M
   <br>Angerer, G
   <br>Mair, C
   <br>Pescoller, D
   <br>Lazzarini, P
   <br>Anaclerio, E
   <br>Mantegazza, M
   <br>Gallieni, D
   <br>Vernet, E
   <br>Arsenault, R
   <br>Madec, PY
   <br>Duhoux, P
   <br>Riccardi, A
   <br>Xompero, M
   <br>Briguglio, R
   <br>Manetti, M
   <br>Morandini, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Biasi, R.
   <br>Andrighettoni, M.
   <br>Angerer, G.
   <br>Mair, C.
   <br>Pescoller, D.
   <br>Lazzarini, P.
   <br>Anaclerio, E.
   <br>Mantegazza, M.
   <br>Gallieni, D.
   <br>Vernet, E.
   <br>Arsenault, R.
   <br>Madec, P. -Y.
   <br>Duhoux, P.
   <br>Riccardi, A.
   <br>Xompero, M.
   <br>Briguglio, R.
   <br>Manetti, M.
   <br>Morandini, M.</td>
</tr>

<tr>
<td valign="top">BE </td><td>Ellerbroek, BL
   <br>Marchetti, E
   <br>Veran, JP</td>
</tr>

<tr>
<td valign="top">TI </td><td>VLT Deformable Secondary Mirror: integration and electromechanical tests
   results</td>
</tr>

<tr>
<td valign="top">SO </td><td>ADAPTIVE OPTICS SYSTEMS III</td>
</tr>

<tr>
<td valign="top">SE </td><td>Proceedings of SPIE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Conference on Adaptive Optics Systems III</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUL 01-06, 2012</td>
</tr>

<tr>
<td valign="top">CL </td><td>Amsterdam, NETHERLANDS</td>
</tr>

<tr>
<td valign="top">DE </td><td>VLT DSM; VLT AOF; adaptive optics; adaptive mirrors; deformable mirrors;
   adaptive secondary; voice-coil; massive control</td>
</tr>

<tr>
<td valign="top">AB </td><td>The VLT Deformable secondary is planned to be installed on the VLT UT#4 as part of the telescope conversion into the Adaptive Optics test Facility (AOF). The adaptive unit is based on the well proven contactless, voice coil motor technology that has been already successfully implemented in the MMT, LBT and Magellan adaptive secondaries, and is considered a promising technical choice for the forthcoming ELT-generation adaptive correctors, like the E-ELT M4 and the GMT ASM. The VLT adaptive unit has been recently assembled after the completion of the manufacturing and modular test phases. In this paper, we present the most relevant aspects of the system integration and report the preliminary results of the electromechanical tests performed on the unit. This test campaign is a typical major step foreseen in all similar systems built so far: thanks to the metrology embedded in the system, that allows generating time-dependent stimuli and recording in real time the position of the controlled mirror on all actuators, typical dynamic response quality parameters like modal settling time, overshoot and following error can be acquired without employing optical measurements. In this way the system dynamic and some aspect of its thermal and long term stability can be fully characterized before starting the optical tests and calibrations.</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Morandini, Marco</display_name>&nbsp;</font></td><td><font size="3">E-5117-2012&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Mantegazza, Paolo</display_name>&nbsp;</font></td><td><font size="3">E-9254-2012&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Morandini, Marco</display_name>&nbsp;</font></td><td><font size="3">0000-0002-1992-9077&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Mantegazza, Paolo</display_name>&nbsp;</font></td><td><font size="3">0000-0002-5979-9190&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Briguglio, Runa</display_name>&nbsp;</font></td><td><font size="3">0000-0002-0495-0543&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Xompero, Marco</display_name>&nbsp;</font></td><td><font size="3">0000-0002-5565-084X&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Riccardi, Armando</display_name>&nbsp;</font></td><td><font size="3">0000-0001-5460-2929&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>11</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>11</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">VL </td><td>8447</td>
</tr>

<tr>
<td valign="top">AR </td><td>84472G</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1117/12.927087</td>
</tr>

<tr>
<td valign="top">SC </td><td>Optics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000312387300079</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Benisty, H
   <br>Malah, D
   <br>Crammer, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Benisty, H.
   <br>Malah, D.
   <br>Crammer, K.</td>
</tr>

<tr>
<td valign="top">TI </td><td>MODULAR GLOBAL VARIANCE ENHANCEMENT FOR VOICE CONVERSION SYSTEMS</td>
</tr>

<tr>
<td valign="top">SO </td><td>2012 PROCEEDINGS OF THE 20TH EUROPEAN SIGNAL PROCESSING CONFERENCE
   (EUSIPCO)</td>
</tr>

<tr>
<td valign="top">SE </td><td>European Signal Processing Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>20th European Signal Processing Conference (EUSIPCO)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 27-31, 2012</td>
</tr>

<tr>
<td valign="top">CL </td><td>Bucharest, ROMANIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Global Variance (GV); Gaussian Mixture Model (GMM); Log Spectral
   Distance (LSD); Voice Conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH SYNTHESIS; MODEL</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion systems aim to transform sentences said by one speaker, to sound as if another speaker had said them. Many statistically trained conversion methods produce muffled synthesized outputs due to over-smoothing of the converted spectra. To deal with the muffling effect, conversion methods integrated with Global Variance (GV) enhancement, have been proposed. In order to gain the benefits of GV enhancement, the user is restricted to apply one of these methods as a conversion method.
   <br>We propose a new GV enhancement method designed independently of any specific conversion scheme and applied as a post-processing block. The extent of GV enhancement is controlled through the allowed spectral distance between the enhanced and the originally converted output, as specified by the user. Listening tests showed that the proposed method improves both quality and similarity to the target of the examined converted sentences, outperforming other enhancement approaches that we evaluated.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Benisty, H.; Malah, D.; Crammer, K.] Technion Israel Inst Technol, Dept
   Elect Engn, IL-32000 Haifa, Israel.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Benisty, H (reprint author), Technion Israel Inst Technol, Dept Elect Engn, IL-32000 Haifa, Israel.</td>
</tr>

<tr>
<td valign="top">EM </td><td>hadasbe@tx.technion.ac.il; malah@ee.technion.ac.il;
   koby@ee.technion.ac.il</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">BP </td><td>370</td>
</tr>

<tr>
<td valign="top">EP </td><td>374</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000310623800075</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Zhou, Y
   <br>Zhang, LH</td>
</tr>

<tr>
<td valign="top">AF </td><td>Zhou Ying
   <br>Zhang Ling-hua</td>
</tr>

<tr>
<td valign="top">BE </td><td>Chen, BJ
   <br>Hu, V
   <br>Kong, D</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Time-scale Alternation Method in GMM Voice Conversion System</td>
</tr>

<tr>
<td valign="top">SO </td><td>ELECTRONIC INFORMATION AND ELECTRICAL ENGINEERING</td>
</tr>

<tr>
<td valign="top">SE </td><td>Advances in Intelligent Systems Research</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Electronic Information and Electrical
   Engineering</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUN 15-17, 2012</td>
</tr>

<tr>
<td valign="top">CL </td><td>Serv Acad Conf Ctr, Changsha, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">HO </td><td>Serv Acad Conf Ctr</td>
</tr>

<tr>
<td valign="top">DE </td><td>GMM; STRAIGHT; time-scale; voice conversion</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper improves and presents an advanced method of the voice conversion system based on GMM models by changing the time-scale of speech. The STRAIGHT model is adopted to extract the spectrum features, and the GMM models are trained to generate the conversion function. The spectrum features of a source speech will be converted by the conversion function. The time-scale of speech is changed by extracting the converted features and adding to the spectrum. The conversion voice was evaluated by subjective and objective measurements. The results confirm that the transformed speech not only approximates the characteristics of the target speaker, but also more natural and more intelligible.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Zhou Ying; Zhang Ling-hua] Nanjing Univ Posts &amp; Telecommun, Coll
   Telecommun &amp; Informat Engn, Nanjing, Jiangsu, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Zhou, Y (reprint author), Nanjing Univ Posts &amp; Telecommun, Coll Telecommun &amp; Informat Engn, Nanjing, Jiangsu, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>Y001090512@njupt.edu.cn; zhanglh@njupt.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">VL </td><td>19</td>
</tr>

<tr>
<td valign="top">BP </td><td>161</td>
</tr>

<tr>
<td valign="top">EP </td><td>164</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000310490800040</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wu, XH
   <br>Obeng, M
   <br>Wang, J
   <br>Kulas, D</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wu, Xiaohe
   <br>Obeng, Morrison
   <br>Wang, Jing
   <br>Kulas, Daniel</td>
</tr>

<tr>
<td valign="top">BE </td><td>Sundaram, KB
   <br>Wu, D</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Survey of Techniques to Add Audio Module to Embedded Systems</td>
</tr>

<tr>
<td valign="top">SO </td><td>2012 PROCEEDINGS OF IEEE SOUTHEASTCON</td>
</tr>

<tr>
<td valign="top">SE </td><td>IEEE SoutheastCon-Proceedings</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE SoutheastCon</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 15-18, 2012</td>
</tr>

<tr>
<td valign="top">CL </td><td>Orlando, FL</td>
</tr>

<tr>
<td valign="top">DE </td><td>Embedded system; audio module; stand-alone voice chip; programmable
   system on chip; I2C</td>
</tr>

<tr>
<td valign="top">AB </td><td>In many embedded application systems, it is desirable to incorporate an audio module/functionality to the system such that the system can utter meaningful audio messages in a controlled manner. An audio system in the embedded setting may consist of memory module, Compression/Decompression (CODEC) module, Digital-to-Analog Converter (DAC) module, filter module, power amplifier module; and also (if the function of voice recording is needed) the signal conditioning module, Automatic Gain Control (AGC), and Analog-to-Digital Conversion (ADC) module. Depending on the different demands of various application scenarios, different techniques can be used to implement the audio module. If no audio signal processing is required, the approach of using a dedicated, off-the-shelf, voice chip is preferable. In other cases, individual circuits would have to be built for some or all of the peripheral modules required in an audio system. The major advantage of this approach is increased level of flexibility; while the immediate disadvantage is rising development cost and a higher level of system complexity. In this paper, we first outline the general operation principle of a typical audio system module; then, a sample implementation that uses a dedicated voice chip is introduced and explained in great detail so that it can be used by students and application engineers as a functional reference design.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Wu, Xiaohe; Obeng, Morrison; Wang, Jing; Kulas, Daniel] Bethune Cookman
   Univ, Daytona Beach, FL 32114 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Wu, XH (reprint author), Bethune Cookman Univ, Daytona Beach, FL 32114 USA.</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000309205400069</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Song, P
   <br>Jin, Y
   <br>Zhao, L
   <br>Zou, CR</td>
</tr>

<tr>
<td valign="top">AF </td><td>Song, Peng
   <br>Jin, Yun
   <br>Zhao, Li
   <br>Zou, Cairong</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Conversion Based on Hybrid SVR and GMM</td>
</tr>

<tr>
<td valign="top">SO </td><td>ARCHIVES OF ACOUSTICS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; support vector regression; Gaussian mixture model; F0
   prediction; speaker-specific information</td>
</tr>

<tr>
<td valign="top">ID </td><td>FREQUENCY; SPECTRUM</td>
</tr>

<tr>
<td valign="top">AB </td><td>A novel VC (voice conversion) method based on hybrid SVR (support vector regression) and GMM (Gaussian mixture model) is presented in the paper, the mapping abilities of SVR and GMM are exploited to map the spectral features of the source speaker to those of target ones. A new strategy of F0 transformation is also presented, the F0s are modeled with spectral features in a joint GMM and predicted from the converted spectral features using the SVR method. Subjective and objective tests are carried out to evaluate the VC performance; experimental results show that the converted speech using the proposed method can obtain a better quality than that using the state-of-the-art GMM method. Meanwhile, a VC method based on non-parallel data is also proposed, the speaker-specific information is investigated using the SVR method and preliminary subjective experiments demonstrate that the proposed method is feasible when a parallel corpus is not available.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Song, Peng; Zhao, Li; Zou, Cairong] Southeast Univ, Minist Educ, Key
   Lab Underwater Acoust Signal Proc, Nanjing 210096, Jiangsu, Peoples R
   China.
   <br>[Jin, Yun] Xuzhou Normal Univ, Sch Phys &amp; Elect Engn, Xuzhou 221116,
   Peoples R China.
   <br>[Jin, Yun] Southeast Univ, Minist Educ, Key Lab Child Dev &amp; Learning
   Sci, Nanjing 210096, Jiangsu, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Song, P (reprint author), Southeast Univ, Minist Educ, Key Lab Underwater Acoust Signal Proc, Nanjing 210096, Jiangsu, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>pengsongseu@gmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">VL </td><td>37</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>143</td>
</tr>

<tr>
<td valign="top">EP </td><td>149</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.2478/v10168-012-0020-9</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000306394600002</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Demenko, G
   <br>Cecko, R
   <br>Szymanski, M
   <br>Owsianny, M
   <br>Francuzik, P
   <br>Lange, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Demenko, Grazyna
   <br>Cecko, Robert
   <br>Szymanski, Marcin
   <br>Owsianny, Mariusz
   <br>Francuzik, Piotr
   <br>Lange, Marek</td>
</tr>

<tr>
<td valign="top">BE </td><td>Dziech, A
   <br>Czyzewski, A</td>
</tr>

<tr>
<td valign="top">TI </td><td>Polish Speech Dictation System as an Application of Voice Interfaces</td>
</tr>

<tr>
<td valign="top">SO </td><td>MULTIMEDIA COMMUNICATIONS, SERVICES AND SECURITY</td>
</tr>

<tr>
<td valign="top">SE </td><td>Communications in Computer and Information Science</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>5th International Conference on Multimedia Communications, Services and
   Security, MCSS 2012</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 31-JUN 01, 2012</td>
</tr>

<tr>
<td valign="top">CL </td><td>Krakow, POLAND</td>
</tr>

<tr>
<td valign="top">DE </td><td>speech recognition; dictation; voice interfaces</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents the results of the project realized at PSNC and supported by The Polish Ministry of Science and Higher Education - "Integrated system of automatic speech-to-text conversion based on linguistic modeling designed in the environment of the analysis and legal documentation workflow for the needs of homeland security", aiming at developing a Polish speech dictation (or Large Vocabulary Continuous Speech Recognition, LVCSR) system designed with the use of a phonetically controlled large vocabulary speech corpus and a large text corpora. The functions of the resulting system are outlined, the software architecture is presented briefly, then the example applications are demonstrated and the recognition results are discussed.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Demenko, Grazyna; Cecko, Robert; Szymanski, Marcin; Owsianny, Mariusz;
   Francuzik, Piotr; Lange, Marek] Polish Acad Sci, Poznan Supercomp &amp;
   Networking Ctr, Poznan, Poland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Demenko, G (reprint author), Polish Acad Sci, Poznan Supercomp &amp; Networking Ctr, Poznan, Poland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>grazyna.demenko@speechlabs.pl; cecko@man.poznan.pl;
   marcin.szymanski@speechlabs.pl; mariusz.owsianny@speechlabs.pl;
   piotr.francuzik@speechlabs.pl; marek.lange@speechlabs.pl</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Demenko, Grazyna</display_name>&nbsp;</font></td><td><font size="3">0000-0003-0849-6533&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">VL </td><td>287</td>
</tr>

<tr>
<td valign="top">BP </td><td>68</td>
</tr>

<tr>
<td valign="top">EP </td><td>76</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000306990300007</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Borhaug, FB</td>
</tr>

<tr>
<td valign="top">AF </td><td>Borhaug, Frederique Brossard</td>
</tr>

<tr>
<td valign="top">TI </td><td>Rethinking Antiracist Education in the Light of the Capability Approach</td>
</tr>

<tr>
<td valign="top">SO </td><td>JOURNAL OF HUMAN DEVELOPMENT AND CAPABILITIES</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Anti-racist education; Civic education; French curriculum; Norwegian
   curriculum; Capability approach</td>
</tr>

<tr>
<td valign="top">AB </td><td>In anti-racist education, it is important to reflect on the pitfalls of the ideal of equality in order to obtain greater social justice. Too narrow an understanding of equality without an acknowledgment of the individual's ethnic identity provides fragile grounds when dealing with the social and cultural recognition of minority pupils. The capability approach contributes to discussing theoretically the anti-racist dialectical aim of equality and difference. In assessing the individual's capability to combine his/her own substantive freedoms within his/her specific living, it also gives the possibility to evaluate the concrete social arrangements and resources available to minority pupils. If considering the curriculum as a resource, French and Norwegian school discourses in civic education offer too few conversion possibilities for minority youth to expand their capabilities in the multicultural society and to use their own voice in order to build self-governance based on self-mobilization and self-articulation of concrete cultural and social aspirations.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>NLA Univ Coll, Bergen, Norway.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Borhaug, FB (reprint author), NLA Univ Coll, Bergen, Norway.</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">VL </td><td>13</td>
</tr>

<tr>
<td valign="top">IS </td><td>3</td>
</tr>

<tr>
<td valign="top">SI </td><td>SI</td>
</tr>

<tr>
<td valign="top">BP </td><td>397</td>
</tr>

<tr>
<td valign="top">EP </td><td>413</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1080/19452829.2012.679646</td>
</tr>

<tr>
<td valign="top">SC </td><td>Development Studies</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000306145000005</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>MacLeod, G
   <br>Johnstone, C</td>
</tr>

<tr>
<td valign="top">AF </td><td>MacLeod, Gordon
   <br>Johnstone, Craig</td>
</tr>

<tr>
<td valign="top">TI </td><td>Stretching Urban Renaissance: Privatizing Space, Civilizing Place,
   Summoning 'Community'</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERNATIONAL JOURNAL OF URBAN AND REGIONAL RESEARCH</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Accumulation by Dispossession; Gentrification; Punitive Urbanism; United
   Kingdom; England</td>
</tr>

<tr>
<td valign="top">ID </td><td>BUSINESS IMPROVEMENT DISTRICTS; ANTISOCIAL-BEHAVIOR; SOCIAL-CONTROL;
   POSTINDUSTRIAL CITY; HOUSING MANAGEMENT; NEOLIBERAL CITY; WELFARE-STATE;
   UNITED-STATES; PENAL STATE; GENTRIFICATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>In common with many countries in recent years, visions of an urban renaissance have been instrumental in guiding a transformation of England's cities, enabling a boom in economic development and urban living. However, while critics voice concerns about the renascent downtowns being increasingly privatized and inscribed through displacement-inducing gentrification, a seemingly inexorable rise in inequality also prompts misgivings about the social and geographical reach of any purported renaissance. Appreciative of this, the New Labour government introduced as part of its sustainable communities plan an initiative called Housing Market Renewal, designed to reconnect distressed areas of low demand to the vibrant city centres. However, the extent to which this endeavour to stretch the frontier of England's urban renaissance is premised upon a fundamentalist faith in private property inclines us to delineate it as an archetypical case of late-neoliberalizing accumulation by dispossession that licenses state-orchestrated gentrification. We go on to consider how the landscape conversions precipitated by the renaissance vision have been convened alongside an unprecedented expansion of policies for crime control, designed to instil a particular version of civility within the urban and suburban vernacular. The article thereby reveals how politically orchestrated endeavours to induce an urban renaissance appear to be increasingly intertwined with gentrification and a punitive urbanism, and how this chimes with experiences across many parts of the urban world.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[MacLeod, Gordon] Univ Durham, Dept Geog, Durham DH1 3LE, England.
   <br>[Johnstone, Craig] Univ Brighton, Sch Appl Social Sci, Brighton BN1 9PH,
   E Sussex, England.</td>
</tr>

<tr>
<td valign="top">RP </td><td>MacLeod, G (reprint author), Univ Durham, Dept Geog, Durham DH1 3LE, England.</td>
</tr>

<tr>
<td valign="top">EM </td><td>Gordon.MacLeod@durham.ac.uk; R.C.Johnstone@brighton.ac.uk</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Marra, Giulia</display_name>&nbsp;</font></td><td><font size="3">L-8303-2014&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>29</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>29</td>
</tr>

<tr>
<td valign="top">PD </td><td>JAN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">VL </td><td>36</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>1</td>
</tr>

<tr>
<td valign="top">EP </td><td>28</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1111/j.1468-2427.2011.01067.x</td>
</tr>

<tr>
<td valign="top">SC </td><td>Geography; Public Administration; Urban Studies</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000298477900001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Comi, G
   <br>De Stefano, N
   <br>Freedman, MS
   <br>Barkhof, F
   <br>Polman, CH
   <br>Uitdehaag, BMJ
   <br>Casset-Semanaz, F
   <br>Hennessy, B
   <br>Moraga, MS
   <br>Rocak, S
   <br>Stubinski, B
   <br>Kappos, L</td>
</tr>

<tr>
<td valign="top">AF </td><td>Comi, Giancarlo
   <br>De Stefano, Nicola
   <br>Freedman, Mark S.
   <br>Barkhof, Frederik
   <br>Polman, Chris H.
   <br>Uitdehaag, Bernard M. J.
   <br>Casset-Semanaz, Florence
   <br>Hennessy, Brian
   <br>Moraga, Margaretha Stam
   <br>Rocak, Sanda
   <br>Stubinski, Bettina
   <br>Kappos, Ludwig</td>
</tr>

<tr>
<td valign="top">TI </td><td>Comparison of two dosing frequencies of subcutaneous interferon beta-1a
   in patients with a first clinical demyelinating event suggestive of
   multiple sclerosis (REFLEX): a phase 3 randomised controlled trial</td>
</tr>

<tr>
<td valign="top">SO </td><td>LANCET NEUROLOGY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">ID </td><td>DIAGNOSTIC-CRITERIA; NATURAL-HISTORY; DISABILITY; DEFINITE; BENEFIT;
   GUIDELINES; CONVERSION; SUBGROUPS; RISK; MS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Background In patients presenting with a first clinical demyelinating event that is suggestive of multiple sclerosis (MS), treatment with interferon beta can delay the occurrence of further attacks and the onset of MS. We investigated the effects of two dosing frequencies of subcutaneous interferon beta-la in patients with a first clinical demyelinating event.
   <br>Methods We undertook a multicentre phase 3 study (REbif FLEXible dosing in early MS [REFLEX]) that included patients (aged 18-50 years) with a single clinical event suggestive of MS, and at least two clinically silent T2 lesions on brain MRI. Participants were randomly assigned in a 1:1:1 ratio by use of a centralised interactive voice response system to receive the serum-free formulation of subcutaneous interferon beta-la 44 mu g three times a week or once a week (plus placebo twice a week for masking), or placebo three times a week for up to 24 months. Patients and physicians were masked to group allocation. The primary endpoint was time to a diagnosis of MS as defined by the 2005 McDonald criteria and the main secondary endpoint was time to clinically definite MS (CDMS) as defined by the Poser criteria. Analysis was by intention to treat. The study is registered with ClinicalTrials.gov, number NCT00404352.
   <br>Findings 517 patients were randomly assigned (171 to subcutaneous interferon beta-la three times a week, 175 to subcutaneous interferon beta-la once a week, and 171 to placebo) and 515 were treated. The 2-year cumulative probability of McDonald MS was significantly lower in patients treated with subcutaneous interferon beta-1a (three times a week 62.5%, p&lt;0.0001, hazard ratio [HR] 0.49 [95% CI 0.38-0.64]; once a week 75.5%, p=0.008, HR 0.69 [0.54-0.87]) versus placebo (85.8%). 2-year rates of conversion to CDMS were lower for both interferon beta-1a dosing regimens (three times a week 20.6%, p=0.0004, HR 0.48 [0.31-0.73]; once a week 21.6%, p=0.0023, HR 0.53 [0.35-0.79]) than for placebo (37.5%). Adverse events were within the established profile for subcutaneous interferon beta-1a.
   <br>Interpretation Both regimens of subcutaneous interferon beta-la delayed clinical relapses and subclinical disease activity. The potential differences between the regimens warrant longer-term study.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Comi, Giancarlo] Osped San Raffaele, Dept Neurol, I-20132 Milan, Italy.
   <br>[De Stefano, Nicola] Univ Siena, Dept Neurol &amp; Behav Sci, I-53100 Siena,
   Italy.
   <br>[Freedman, Mark S.] Ottawa Gen Hosp, Multiple Sclerosis Res Unit,
   Ottawa, ON K1H 8L6, Canada.
   <br>[Barkhof, Frederik] Vrije Univ Amsterdam Med Ctr, Diagnost Radiol &amp;
   Image Anal Ctr, Amsterdam, Netherlands.
   <br>[Polman, Chris H.; Uitdehaag, Bernard M. J.] Vrije Univ Amsterdam Med
   Ctr, Dept Neurol, Amsterdam, Netherlands.
   <br>[Casset-Semanaz, Florence; Hennessy, Brian; Moraga, Margaretha Stam;
   Rocak, Sanda; Stubinski, Bettina] Merck Serono SA, Geneva, Switzerland.
   <br>[Kappos, Ludwig] Univ Basel Hosp, Dept Biomed, CH-4031 Basel,
   Switzerland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Comi, G (reprint author), Osped San Raffaele, Dept Neurol, Via Olgettina 60, I-20132 Milan, Italy.</td>
</tr>

<tr>
<td valign="top">EM </td><td>g.comi@hsr.it</td>
</tr>

<tr xmlns:date="http://exslt.org/dates-and-times">
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>de stefano, nicola</display_name>&nbsp;</font></td><td><font size="3">0000-0003-4930-7639&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>110</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>114</td>
</tr>

<tr>
<td valign="top">PD </td><td>JAN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">VL </td><td>11</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>33</td>
</tr>

<tr>
<td valign="top">EP </td><td>41</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/S1474-4422(11)70262-9</td>
</tr>

<tr>
<td valign="top">SC </td><td>Neurosciences &amp; Neurology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000298513700017</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Zhu, CL
   <br>Yu, YB</td>
</tr>

<tr>
<td valign="top">AF </td><td>Zhu, Chunlei
   <br>Yu, Yibiao</td>
</tr>

<tr>
<td valign="top">BE </td><td>Baozong, Y
   <br>Qiuqi, R
   <br>Xiaofang, T</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice conversion with UBM and speaker-specific model adaptation</td>
</tr>

<tr>
<td valign="top">SO </td><td>PROCEEDINGS OF 2012 IEEE 11TH INTERNATIONAL CONFERENCE ON SIGNAL
   PROCESSING (ICSP) VOLS 1-3</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Signal Processing</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE 11th International Conference on Signal Processing (ICSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 21-25, 2012</td>
</tr>

<tr>
<td valign="top">CL </td><td>Beijing, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; non-parallel corpus; non-joint training; UBM; MAP</td>
</tr>

<tr>
<td valign="top">AB </td><td>Traditional voice conversion algorithms are usually based on parallel speech corpus and joint training, but it is difficult to obtain parallel data and inflexible to extend system in practical application. This paper presents a non-parallel and non-joint training algorithm for voice conversion using Universal Background Model (UBM) and Maximum a Posteriori (MAP) adaptation approach. First of all, a UBM is trained reflecting the speaker-independent statistical distribution of features using non-parallel speech samples of all speakers, then with the UBM acting as the prior model, every speaker-specific model is derived by using new parameter estimation based on MAP adaptation. Experimental results show that the proposed method achieves equivalent conversion performance comparing to traditional parallel corpus based method and has more flexible system extension ability.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Zhu, Chunlei; Yu, Yibiao] Soochow Univ, Sch Elect &amp; Informat Engn,
   Suzhou, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Zhu, CL (reprint author), Soochow Univ, Sch Elect &amp; Informat Engn, Suzhou, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>zhuchun_lei1988@126.com; yuyb@suda.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">BP </td><td>553</td>
</tr>

<tr>
<td valign="top">EP </td><td>556</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000349102800127</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kishimoto, M
   <br>Toda, T
   <br>Doi, H
   <br>Sakti, S
   <br>Nakamura, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kishimoto, Mayumi
   <br>Toda, Tomoki
   <br>Doi, Hironori
   <br>Sakti, Sakriani
   <br>Nakamura, Satoshi</td>
</tr>

<tr>
<td valign="top">BE </td><td>Baozong, Y
   <br>Qiuqi, R
   <br>Xiaofang, T</td>
</tr>

<tr>
<td valign="top">TI </td><td>Model Training Using Parallel Data with Mismatched Pause Positions in
   Statistical Esophageal Speech Enhancement</td>
</tr>

<tr>
<td valign="top">SO </td><td>PROCEEDINGS OF 2012 IEEE 11TH INTERNATIONAL CONFERENCE ON SIGNAL
   PROCESSING (ICSP) VOLS 1-3</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Signal Processing</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE 11th International Conference on Signal Processing (ICSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 21-25, 2012</td>
</tr>

<tr>
<td valign="top">CL </td><td>Beijing, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>esophageal speech enhancement; eigenvoice conversion; training data;
   mismatched pause position</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>As one of the speaking aid techniques for laryngectomees, an esophageal speech enhancement method based on eigenvoice conversion has been proposed. In this method, conversion models are trained using utterance pairs of esophageal speech uttered by a laryngectomee and normal speech uttered by many normal speakers. Recording of normal speech of which pause positions correspond to those of esophageal speech is effective to develop well-designed training data for building the conversion models but it requires an enormous amount of time and expensive costs. In this paper, we propose a method capable of effectively using normal speech data including mismatched pause positions as training data by alleviating their impact on the conversion models. The experimental results demonstrate that the proposed method yields significant improvements in both speech quality and conversion accuracy for speaker individuality (i.e., speaker identity).</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kishimoto, Mayumi; Toda, Tomoki; Doi, Hironori; Sakti, Sakriani;
   Nakamura, Satoshi] Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara
   6300101, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kishimoto, M (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara 6300101, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>tomoki@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">BP </td><td>590</td>
</tr>

<tr>
<td valign="top">EP </td><td>594</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000349102800135</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Saito, D
   <br>Minematsu, N
   <br>Hirose, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Saito, Daisuke
   <br>Minematsu, Nobuaki
   <br>Hirose, Keikichi</td>
</tr>

<tr>
<td valign="top">BE </td><td>Baozong, Y
   <br>Qiuqi, R
   <br>Xiaofang, T</td>
</tr>

<tr>
<td valign="top">TI </td><td>Tensor-based Speaker Space Construction for Arbitrary Speaker Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>PROCEEDINGS OF 2012 IEEE 11TH INTERNATIONAL CONFERENCE ON SIGNAL
   PROCESSING (ICSP) VOLS 1-3</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Signal Processing</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE 11th International Conference on Signal Processing (ICSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 21-25, 2012</td>
</tr>

<tr>
<td valign="top">CL </td><td>Beijing, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; Gaussian mixture model; tensor analysis; Tucker
   decomposition</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION; SPEECH RECOGNITION; ADAPTATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper describes a novel approach to flexible control of speaker characteristics using tensor representation of speaker space. In voice conversion studies, realization of conversion from/to an arbitrary speaker's voice is one of the important objectives. For this purpose, eigenvoice conversion (EVC) based on an eigenvoice Gaussian mixture model (EV-GMM) was proposed. In the EVC, a speaker space is constructed based on GMM supervectors which are high-dimensional vectors derived by concatenating the mean vectors of each of the speaker GMMs. In the speaker space, each speaker is represented by a small number of weight parameters of eigen-supervectors. In this paper, we revisit construction of the speaker space by introducing the tensor analysis of training data set. In our approach, each speaker is represented as a matrix of which the row and the column respectively correspond to the Gaussian component and the dimension of the mean vector, and the speaker space is derived by the tensor analysis of the set of the matrices. Our approach can solve an inherent problem of supervector representation, and it improves the performance of voice conversion. Experimental results of one-to-many voice conversion demonstrate the effectiveness of the proposed approach.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Saito, Daisuke; Minematsu, Nobuaki; Hirose, Keikichi] Univ Tokyo,
   Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Saito, D (reprint author), Univ Tokyo, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>dsaito@hil.t.u-tokyo.ac.jp; mine@gavo.t.u-tokyo.ac.jp;
   hirose@gavo.t.u-tokyo.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">BP </td><td>595</td>
</tr>

<tr>
<td valign="top">EP </td><td>598</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000349102800136</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nakamura, K
   <br>Toda, T
   <br>Saruwatari, H
   <br>Shikano, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nakamura, Keigo
   <br>Toda, Tomoki
   <br>Saruwatari, Hiroshi
   <br>Shikano, Kiyohiro</td>
</tr>

<tr>
<td valign="top">TI </td><td>Speaking-aid systems using GMM-based voice conversion for
   electrolaryngeal speech</td>
</tr>

<tr>
<td valign="top">SO </td><td>SPEECH COMMUNICATION</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Electrolaryngeal speech; Voice conversion; Speaking-aid system; Speech
   enhancement; Airpressure sensor; Silence excitation; Non-audible murmur;
   Laryngectomee</td>
</tr>

<tr>
<td valign="top">ID </td><td>MAXIMUM-LIKELIHOOD; LARYNGECTOMY; CANCER</td>
</tr>

<tr>
<td valign="top">AB </td><td>An electrolarynx (EL) is a medical device that generates sound source signals to provide laryngectomees with a voice. In this article we focus on two problems of speech produced with an EL (EL speech). One problem is that EL speech is extremely unnatural and the other is that sound source signals with high energy are generated by an EL, and therefore, the signals often annoy surrounding people. To address these two problems, in this article we propose three speaking-aid systems that enhance three different types of EL speech signals: EL speech, EL speech using an air-pressure sensor (EL-air speech), and silent EL speech. The air-pressure sensor enables a laryngectomee to manipulate the F-0 contours of EL speech using exhaled air that flows from the tracheostoma. Silent EL speech is produced with a new sound source unit that generates signals with extremely low energy. Our speaking-aid systems address the poor quality of EL speech using voice conversion (VC), which transforms acoustic features so that it appears as if the speech is uttered by another person. Our systems estimate spectral parameters, F-0 and aperiodic components independently. The result of experimental evaluations demonstrates that the use of an air-pressure sensor dramatically improves F-0 estimation accuracy. Moreover, it is revealed that the converted speech signals are preferred to source EL speech. (C) 2011 Elsevier B.V. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nakamura, Keigo; Toda, Tomoki; Saruwatari, Hiroshi; Shikano, Kiyohiro]
   Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Ikoma, Nara 6300192,
   Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nakamura, K (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, 8916-5 Takayama Cho, Ikoma, Nara 6300192, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>kei_go@nifty.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>59</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>60</td>
</tr>

<tr>
<td valign="top">PD </td><td>JAN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">VL </td><td>54</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>134</td>
</tr>

<tr>
<td valign="top">EP </td><td>146</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.specom.2011.07.007</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000295745800010</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Patel, RR
   <br>Donohue, KD
   <br>Johnson, WC
   <br>Archer, SM</td>
</tr>

<tr>
<td valign="top">AF </td><td>Patel, Rita R.
   <br>Donohue, Kevin D.
   <br>Johnson, Weston C.
   <br>Archer, Sanford M.</td>
</tr>

<tr>
<td valign="top">TI </td><td>Laser Projection Imaging for Measurement of Pediatric Voice</td>
</tr>

<tr>
<td valign="top">SO </td><td>LARYNGOSCOPE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Laser projection system; high-speed digital imaging; pediatric voice;
   pediatric vocal fold length</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOCAL FOLD; PUBERTAL LARYNX; LAMINA PROPRIA; STROBOSCOPY</td>
</tr>

<tr>
<td valign="top">AB </td><td>Objectives/Hypothesis: The aim of the study was to present the development of a miniature laser projection endoscope and to quantify vocal fold length and vibratory amplitude of the pediatric glottis using high-speed digital imaging coupled with the laser endoscope.
   <br>Study Design: For this prospective study, absolute measurement of entire vocal fold length, membranous length of the vocal fold, and vibratory amplitude during phonation were obtained in one child (9 years old), one adult male (36 years old), and one adult female (20 years old) with the use of high-speed digital imaging, coupled with a custom-developed laser projection endoscope.
   <br>Methods: The laser projection system consists of a module slip-fit sleeve with two 3-mW 650-nm laser diodes in horizontal orientation separated by a distance of 5 mm. Calibration involved projecting the laser onto grid patterns at depths ranging from 6 to 10 cm and tilt angles of 15 to -5 degrees to obtain pixel-to-millimeter conversion templates. Measurements of vocal fold length and vibratory amplitude were extracted based on methods of image processing.
   <br>Results: The system demonstrated a method for estimating vocal fold length and vibratory amplitude with a single laser point with high measurement precision. First measurements of vocal fold length (6.8 mm) and vibratory amplitude (0.25 mm) during phonation in a pediatric participant are reported.
   <br>Conclusions: The proposed laser projection system can be used to obtain absolute length and vibratory measurements of the pediatric glottis. The projection system can be used with stroboscopy or high-speed digital imaging systems with a 70-degree rigid endoscope.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Patel, Rita R.] Univ Kentucky, Dept Rehabil Sci, Div Commun Sci &amp;
   Disorders, Lexington, KY 40536 USA.
   <br>[Donohue, Kevin D.; Johnson, Weston C.] Univ Kentucky, Dept Elect &amp; Comp
   Engn, Lexington, KY 40536 USA.
   <br>[Archer, Sanford M.] Univ Kentucky, Dept Surg, Div Otolaryngol Head &amp;
   Neck Surg, Lexington, KY 40536 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Patel, RR (reprint author), Univ Kentucky, Dept Rehabil Sci, Div Commun Sci &amp; Disorders, 900 S Limestone,120 D,Charles T Wethington Bldg, Lexington, KY 40536 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>rita.patel@uky.edu</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Donohue, Kevin</display_name>&nbsp;</font></td><td><font size="3">C-3444-2012&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>15</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>15</td>
</tr>

<tr>
<td valign="top">PD </td><td>NOV</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">VL </td><td>121</td>
</tr>

<tr>
<td valign="top">IS </td><td>11</td>
</tr>

<tr>
<td valign="top">BP </td><td>2411</td>
</tr>

<tr>
<td valign="top">EP </td><td>2417</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1002/lary.22325</td>
</tr>

<tr>
<td valign="top">SC </td><td>Research &amp; Experimental Medicine; Otorhinolaryngology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000296714800026</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Rao, KS</td>
</tr>

<tr>
<td valign="top">AF </td><td>Rao, K. Sreenivasa</td>
</tr>

<tr>
<td valign="top">TI </td><td>Role of neural network models for developing speech systems</td>
</tr>

<tr>
<td valign="top">SO </td><td>SADHANA-ACADEMY PROCEEDINGS IN ENGINEERING SCIENCES</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Feedforward neural network (FFNN); autoassociative neural network
   (AANN); prosody models; speaker recognition; language identification;
   voice conversion; Emotion recognition; Dialect identification</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION; SIGNIFICANT EXCITATION; EMOTION RECOGNITION; AMERICAN
   ENGLISH; EXTRACTION; INSTANTS; FEATURES; CLASSIFICATION; TRANSFORMATION;
   PREDICTION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper discusses the application of neural networks for developing different speech systems. Prosodic parameters of speech at syllable level depend on positional, contextual and phonological features of the syllables. In this paper, neural networks are explored to model the prosodic parameters of the syllables from their positional, contextual and phonological features. The prosodic parameters considered in this work are duration and sequence of pitch (F-0) values of the syllables. These prosody models are further examined for applications such as text to speech synthesis, speech recognition, speaker recognition and language identification. Neural network models in voice conversion system are explored for capturing the mapping functions between source and target speakers at source, system and prosodic levels. We have also used neural network models for characterizing the emotions present in speech. For identification of dialects in Hindi, neural network models are used to capture the dialect specific information from spectral and prosodic features of speech.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Indian Inst Technol, Sch Informat Technol, Kharagpur 721302, W Bengal,
   India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Rao, KS (reprint author), Indian Inst Technol, Sch Informat Technol, Kharagpur 721302, W Bengal, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ksrao@iitkgp.ac.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>19</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>19</td>
</tr>

<tr>
<td valign="top">PD </td><td>OCT</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">VL </td><td>36</td>
</tr>

<tr>
<td valign="top">IS </td><td>5</td>
</tr>

<tr>
<td valign="top">SI </td><td>SI</td>
</tr>

<tr>
<td valign="top">BP </td><td>783</td>
</tr>

<tr>
<td valign="top">EP </td><td>836</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/s12046-011-0047-z</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000298279400011</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Weber, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Weber, Till</td>
</tr>

<tr>
<td valign="top">TI </td><td>Exit, Voice, and Cyclicality: A Micrologic of Midterm Effects in
   European Parliament Elections</td>
</tr>

<tr>
<td valign="top">SO </td><td>AMERICAN JOURNAL OF POLITICAL SCIENCE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">ID </td><td>PRESIDENTIAL POPULARITY; CONGRESSIONAL ELECTIONS; PARTY PREFERENCE;
   MODEL; VOTE; ELECTORATE; BEHAVIOR; LOYALTY; DECLINE; BRITAIN</td>
</tr>

<tr>
<td valign="top">AB </td><td>Very few theories of democratic elections can claim to overarch the field. One of them that has not been given due regard, I suggest, is Albert Hirschman's Exit, Voice, and Loyalty. I aim to exploit the integrative capacity of this general framework in a model of typical "midterm" effects occurring through the electoral cycle. The model unites such diverse phenomena as antigovernment swings, declining turnout, protest voting, conversion, and alienation. An empirical test with comparative survey data from elections to the European Parliament reveals that the role of strategic voting in the form of voice is limited. Instead, processes of de- and realignment in the form of exit dominate a picture of European Parliament elections beyond the widespread conception of "second-order" irrelevance. More generally, the "cyclical" view on voting behavior suggests systematic links between short-run midterm effects and long-run electoral change.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>European Univ Inst, Dept Polit &amp; Social Sci, I-50014 Florence, Italy.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Weber, T (reprint author), European Univ Inst, Dept Polit &amp; Social Sci, Via Roccettini 9, I-50014 Florence, Italy.</td>
</tr>

<tr>
<td valign="top">EM </td><td>till.weber@eui.eu</td>
</tr>

<tr>
<td valign="top">TC </td><td>22</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>22</td>
</tr>

<tr>
<td valign="top">PD </td><td>OCT</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">VL </td><td>55</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">BP </td><td>906</td>
</tr>

<tr>
<td valign="top">EP </td><td>921</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1111/j.1540-5907.2011.00535.x</td>
</tr>

<tr>
<td valign="top">SC </td><td>Government &amp; Law</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000296029900012</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Van Vleet, KE</td>
</tr>

<tr>
<td valign="top">AF </td><td>Van Vleet, Krista E.</td>
</tr>

<tr>
<td valign="top">TI </td><td>On Devils and the Dissolution of Sociality: Andean Catholics Voicing
   Ambivalence in Neoliberal Bolivia</td>
</tr>

<tr>
<td valign="top">SO </td><td>ANTHROPOLOGICAL QUARTERLY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Andes; Christianity; death; devil; narrative; neoliberalism; religion</td>
</tr>

<tr>
<td valign="top">ID </td><td>CONTINUITY THINKING; ECUADORIAN ANDES; CHRISTIANITY; CAPITALISM;
   NARRATIVES; COMMUNITY; POLITICS</td>
</tr>

<tr>
<td valign="top">AB </td><td>In the Andean highlands of Bolivia, people sometimes express their ambivalence over the religious conversion of family and community members through stories about evangelical Protestants who have been possessed by Santuku or the devil. The article analyzes these narratives as part of a larger genre of devil stories and as a window onto the multiple ways Andean Catholics link migration, religious conversion, and death in the context of broader neoliberal transformations. From the perspective of those "left behind" - Catholic family and community members conversion empties the future. Nevertheless, the necessary labor of dissolving or reconfiguring social relationships is undertaken by both Catholics and evangelical Protestants and sheds light on the production of sociality in 21st century Bolivia.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Bowdoin Coll, Brunswick, ME 04011 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Van Vleet, KE (reprint author), Bowdoin Coll, Brunswick, ME 04011 USA.</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PD </td><td>FAL</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">VL </td><td>84</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">BP </td><td>835</td>
</tr>

<tr>
<td valign="top">EP </td><td>864</td>
</tr>

<tr>
<td valign="top">SC </td><td>Anthropology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000297720000003</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Song, P
   <br>Bao, YQ
   <br>Zhao, L
   <br>Zou, CR</td>
</tr>

<tr>
<td valign="top">AF </td><td>Song, P.
   <br>Bao, Y. Q.
   <br>Zhao, L.
   <br>Zou, C. R.</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice conversion using support vector regression</td>
</tr>

<tr>
<td valign="top">SO </td><td>ELECTRONICS LETTERS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">AB </td><td>A new voice conversion method based on support vector regression (SVR) is proposed, and the mapping abilities of a multi-dimensional SVR are exploited to perform the mapping of spectral features of a source speaker to that of a target speaker. A novel mixed kernel is presented to improve the mapping performance, the correlations between frames of the source speaker are considered to overcome the discontinuities of converted speech, and an adaptive median filter is adopted in the conversion phase to smooth the converted spectral parameter trajectory. Experimental results show that the proposed method outperforms the state-of-the-art Gaussian mixture model based method, it can achieve high similarity between converted and target speakers, and has good quality and naturalness.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Song, P.; Bao, Y. Q.; Zhao, L.; Zou, C. R.] Southeast Univ, Key Lab
   Underwater Acoust Signal Proc, Minist Educ, Nanjing 210096, Peoples R
   China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Song, P (reprint author), Southeast Univ, Key Lab Underwater Acoust Signal Proc, Minist Educ, Nanjing 210096, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>pengsongseu@gmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>13</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>13</td>
</tr>

<tr>
<td valign="top">PD </td><td>SEP 1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">VL </td><td>47</td>
</tr>

<tr>
<td valign="top">IS </td><td>18</td>
</tr>

<tr>
<td valign="top">BP </td><td>1045</td>
</tr>

<tr>
<td valign="top">EP </td><td>U1586</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1049/el.2011.1851</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000294457600027</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nose, T
   <br>Kobayashi, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nose, Takashi
   <br>Kobayashi, Takao</td>
</tr>

<tr>
<td valign="top">TI </td><td>Speaker-independent HMM-based voice conversion using adaptive
   quantization of the fundamental frequency</td>
</tr>

<tr>
<td valign="top">SO </td><td>SPEECH COMMUNICATION</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; Hidden Markov model (HMM); HMM-based speech synthesis;
   Speaker-independent model; Fundamental frequency quantization; Prosody
   conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH SYNTHESIS</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper describes a speaker-independent HMM-based voice conversion technique that incorporates context-dependent prosodic symbols obtained using adaptive quantization of the fundamental frequency (F0). In the HMM-based conversion of our previous study, the input utterance of a source speaker is decoded into phonetic and prosodic symbol sequences, and the converted speech is generated using the decoded information from the pre-trained target speaker's phonetically and prosodically context-dependent H M M. In our previous work, we generated the F0 symbol by quantizing the average log F0 value of each phone using the global mean and variance calculated from the training data. In the current study, these statistical parameters are obtained from each utterance itself, and this adaptive method improves the F0 conversion performance of the conventional one. We also introduce a speaker-independent model for decoding the input speech and model adaptation for training the target speaker's model in order to reduce the required amount of training data under a condition where the phonetic transcription is available for the input speech. Objective and subjective experimental results for Japanese speech demonstrate that the adaptive quantization method gives better F0 conversion performance than the conventional one. Moreover, our technique with only ten sentences of the target speaker's adaptation data outperforms the conventional GMM-based one using parallel data of 200 sentences. (C) 2011 Elsevier B.V. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nose, Takashi; Kobayashi, Takao] Tokyo Inst Technol, Interdisciplinary
   Grad Sch Sci &amp; Engn, Yokohama, Kanagawa 2268502, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nose, T (reprint author), Tokyo Inst Technol, Interdisciplinary Grad Sch Sci &amp; Engn, Yokohama, Kanagawa 2268502, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>takashi.nose@ip.titech.ac.jp; takao.ko-bayashi@ip.titech.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>5</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>5</td>
</tr>

<tr>
<td valign="top">PD </td><td>SEP</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">VL </td><td>53</td>
</tr>

<tr>
<td valign="top">IS </td><td>7</td>
</tr>

<tr>
<td valign="top">BP </td><td>973</td>
</tr>

<tr>
<td valign="top">EP </td><td>985</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.specom.2011.05.001</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000292675800002</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Tolley, N
   <br>Arora, A
   <br>Palazzo, F
   <br>Garas, G
   <br>Dhawan, R
   <br>Cox, J
   <br>Darzi, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Tolley, Neil
   <br>Arora, Asit
   <br>Palazzo, Fausto
   <br>Garas, George
   <br>Dhawan, Ranju
   <br>Cox, Jeremy
   <br>Darzi, Ara</td>
</tr>

<tr>
<td valign="top">TI </td><td>Robotic-Assisted Parathyroidectomy: A Feasibility Study</td>
</tr>

<tr>
<td valign="top">SO </td><td>OTOLARYNGOLOGY-HEAD AND NECK SURGERY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>robotic surgery; da Vinci surgical system; targeted parathyroidectomy;
   parathyroid adenoma; patient-reported outcome measures (PROMs); quality
   of life (QoL); scar cosmesis</td>
</tr>

<tr>
<td valign="top">ID </td><td>MINIMALLY INVASIVE PARATHYROIDECTOMY; VISUAL ANALOG SCALE; PRIMARY
   HYPERPARATHYROIDISM; ENDOSCOPIC THYROIDECTOMY; SURGERY; EXPERIENCE;
   RESECTION; BENEFITS; QUALITY; ADENOMA</td>
</tr>

<tr>
<td valign="top">AB </td><td>Objective. Targeted parathyroidectomy is the gold standard for localized parathyroid disease. A robotic-assisted approach has not been investigated. The aim was to assess the feasibility of a robotic technique that avoids a neck scar.
   <br>Study Design. Feasibility study.
   <br>Setting. Tertiary referral center.
   <br>Subjects and Methods. Eleven patients with primary hyperparathyroidism were prospectively evaluated. Triple modality concordant localization was a prerequisite. All patients underwent robotic-assisted parathyroidectomy (RAP). Outcome variables assessed were operative time, voice change, biochemical cure, and histopathological confirmation. Patient-reported outcome measures (PROMs) included subjective assessment of pain and scar cosmesis, Voice Handicap Index 2, and EQ-5D quality-of-life assessment. Mean follow-up was 6 months (range, 3-12 months).
   <br>Results. The parathyroid adenoma was successfully excised in all cases with negligible blood loss (&lt;5 mL). There was 1 conversion. There was no voice change in any case. Robot docking time plateaued to 10 minutes after 8 cases. Mean exposure and console times (31 and 51 minutes, respectively) were affected by body habitus. The mean visual analog scale for scar cosmesis was 75% on the first postoperative day, improving to 92% at 6 months and 95% at 1 year. Pain scores decreased to 8% at 2 weeks. All 5 EQ-5D quality-of-life parameters significantly improved following surgery.
   <br>Conclusion. The robotic approach is feasible for performing targeted parathyroidectomy that avoids a neck scar. The clinical efficacy and cost-effectiveness of the robotic approach compared with conventional targeted parathyroidectomy warrant further evaluation to establish if this represents a viable alternative to the existing targeted techniques.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Tolley, Neil; Arora, Asit; Garas, George] Imperial Coll Healthcare NHS
   Trust, St Marys Hosp, Dept Otorhinolaryngol &amp; Head &amp; Neck Surg, London
   W2 1NY, England.
   <br>[Palazzo, Fausto] Imperial Coll Healthcare NHS Trust, Hammersmith Hosp,
   Dept Endocrine &amp; Thyroid Surg, London W2 1NY, England.
   <br>[Dhawan, Ranju] Imperial Coll Healthcare NHS Trust, St Marys Hosp, Dept
   Radiol, London W2 1NY, England.
   <br>[Cox, Jeremy] Imperial Coll Healthcare NHS Trust, St Marys Hosp, Dept
   Endocrinol, London W2 1NY, England.
   <br>[Darzi, Ara] Univ London Imperial Coll Sci Technol &amp; Med, St Marys Hosp,
   Dept Biosurg &amp; Surg Technol, London, England.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Arora, A (reprint author), Imperial Coll Healthcare NHS Trust, St Marys Hosp, Dept Otorhinolaryngol &amp; Head &amp; Neck Surg, Praed St, London W2 1NY, England.</td>
</tr>

<tr>
<td valign="top">EM </td><td>asitarora@doctors.org.uk</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Garas, George</display_name>&nbsp;</font></td><td><font size="3">0000-0001-7468-3287&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>25</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>25</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">VL </td><td>144</td>
</tr>

<tr>
<td valign="top">IS </td><td>6</td>
</tr>

<tr>
<td valign="top">BP </td><td>859</td>
</tr>

<tr>
<td valign="top">EP </td><td>866</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1177/0194599811402152</td>
</tr>

<tr>
<td valign="top">SC </td><td>Otorhinolaryngology; Surgery</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000293998800007</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Deary, V
   <br>Miller, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Deary, Vincent
   <br>Miller, Tracy</td>
</tr>

<tr>
<td valign="top">TI </td><td>Reconsidering the role of psychosocial factors in functional dysphonia</td>
</tr>

<tr>
<td valign="top">SO </td><td>CURRENT OPINION IN OTOLARYNGOLOGY &amp; HEAD AND NECK SURGERY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>cognitive behavioural therapy; conversion; functional dysphonia;
   medically unexplained symptoms; psychogenic dysphonia</td>
</tr>

<tr>
<td valign="top">ID </td><td>CHRONIC-FATIGUE-SYNDROME; UNEXPLAINED PHYSICAL SYMPTOMS;
   COGNITIVE-BEHAVIORAL THERAPY; VOICE DISORDERS; SOMATOFORM DISORDERS;
   PRIMARY-CARE; LIFE EVENTS; PERSONALITY; POPULATION; IMPACT</td>
</tr>

<tr>
<td valign="top">AB </td><td>Purpose of review
   <br>Functional dysphonia, defined as alteration or loss of voice in the absence of physical pathology, is known to be associated with a variety of psychosocial factors including anxiety, depression and reduced quality of life. Models of functional dysphonia have tended to conceptualize the voice disorder as being the result of a failure to acknowledge and/or express this associated distress. The current literature was reviewed to identify psychosocial factors that predispose to, precipitate and perpetuate functional dysphonia and to assess the evidence for these models.
   <br>Recent findings
   <br>Recent studies have identified evidence of genetic susceptibility, occupational susceptibility, a history of sexual and/or physical abuse and perfectionism as being predisposing factors. Precipitants include life events, frequency of vocal use and infections. General fatigue is identified as being a potential perpetuating factor. A recent novel theoretical model of functional dysphonia is reported, which proposes deficits in emotional processing as the core process in voice loss.
   <br>Summary
   <br>Current research confirms that functional dysphonia is associated with multiple psychosocial factors. However, these findings are shown to be true of other medically unexplained symptoms in which vocal problems are absent. It is argued that, whilst intuitively appealing, there is insufficient evidence to support the popular notion that the loss of voice is the consequence of unexpressed emotion.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Deary, Vincent] Northumbria Univ, Sch Life Sci, Dept Psychol, Newcastle
   Upon Tyne NE1 8ST, Tyne &amp; Wear, England.
   <br>[Miller, Tracy] Freeman Rd Hosp, Dept Speech Voice &amp; Swallowing,
   Newcastle Upon Tyne, Tyne &amp; Wear, England.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Deary, V (reprint author), Northumbria Univ, Sch Life Sci, Dept Psychol, Room 147,Northumberland Bldg, Newcastle Upon Tyne NE1 8ST, Tyne &amp; Wear, England.</td>
</tr>

<tr>
<td valign="top">EM </td><td>vincent.deary@unn.ac.uk</td>
</tr>

<tr>
<td valign="top">TC </td><td>15</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>16</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">VL </td><td>19</td>
</tr>

<tr>
<td valign="top">IS </td><td>3</td>
</tr>

<tr>
<td valign="top">BP </td><td>150</td>
</tr>

<tr>
<td valign="top">EP </td><td>154</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1097/MOO.0b013e328346494d</td>
</tr>

<tr>
<td valign="top">SC </td><td>Otorhinolaryngology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000290514100003</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Pyon, CU
   <br>Woo, JY
   <br>Park, SC</td>
</tr>

<tr>
<td valign="top">AF </td><td>Pyon, Chong Un
   <br>Woo, Ji Young
   <br>Park, Sang Chan</td>
</tr>

<tr>
<td valign="top">TI </td><td>Service improvement by business process management using customer
   complaints in financial service industry</td>
</tr>

<tr>
<td valign="top">SO </td><td>EXPERT SYSTEMS WITH APPLICATIONS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Business process management (BPM); Customer complaints management system
   (CCMS); Voice of the Customer (VOC); Call centers; Financial service
   industry</td>
</tr>

<tr>
<td valign="top">AB </td><td>In financial service industry, service improvement should be considered from process viewpoint and customer viewpoint because the value creation is ultimately linked with internal business processes on the back office and customers are involved as a co-producer of value. In this perspective, customer complaints through call centers are adequate to support the analysis for service improvement in financial service industry. In this study, we propose a web-based decision support system for business process management employing customer complaints, namely Voice of the Customer (VOC), and its handling data for service improvement. It involves VOC conversion for data enrichment and includes analysis of summarization, exception and comparison. The proposed system is evaluated on a major credit card company in South Korea. (C) 2010 Elsevier Ltd. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Woo, Ji Young] Univ Arizona, Eller Coll Management, Tucson, AZ 85721
   USA.
   <br>[Pyon, Chong Un] Korea Adv Inst Sci &amp; Technol, Dept Ind &amp; Syst Engn,
   Taejon 305701, South Korea.
   <br>[Park, Sang Chan] Kyung Hee Univ, Coll Business Adm, Seoul 130701, South
   Korea.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Woo, JY (reprint author), Univ Arizona, Eller Coll Management, 1130 E Helen St, Tucson, AZ 85721 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>pcu@kaist.ac.kr; jywoo@kaist.ac.kr; sangchan@khu.ac.kr</td>
</tr>

<tr>
<td valign="top">TC </td><td>16</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>16</td>
</tr>

<tr>
<td valign="top">PD </td><td>APR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">VL </td><td>38</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">BP </td><td>3267</td>
</tr>

<tr>
<td valign="top">EP </td><td>3279</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.eswa.2010.08.112</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering; Operations Research &amp; Management Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000286904600039</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Fujii, S
   <br>Watanabe, K
   <br>Ota, M
   <br>Yamagishi, S
   <br>Kunisaki, C
   <br>Osada, S
   <br>Ike, H
   <br>Ichikawa, Y
   <br>Endo, I
   <br>Shimada, H</td>
</tr>

<tr>
<td valign="top">AF </td><td>Fujii, Shoichi
   <br>Watanabe, Kazuteru
   <br>Ota, Mitsuyoshi
   <br>Yamagishi, Shigeru
   <br>Kunisaki, Chikara
   <br>Osada, Shunichi
   <br>Ike, Hideyuki
   <br>Ichikawa, Yasushi
   <br>Endo, Itaru
   <br>Shimada, Hiroshi</td>
</tr>

<tr>
<td valign="top">TI </td><td>Solo Surgery in Laparoscopic Colectomy: A Case-matched Study Comparing
   Robotic and Human Scopist</td>
</tr>

<tr>
<td valign="top">SO </td><td>HEPATO-GASTROENTEROLOGY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Laparoscopic-assisted colectomy; Solo-surgery; Voice-controlled robotic
   arm; Colon-lifting method</td>
</tr>

<tr>
<td valign="top">ID </td><td>CAMERA-HOLDER; AESOP; PERFORMANCE; EFFICIENCY; SYSTEM; ARM; AID</td>
</tr>

<tr>
<td valign="top">AB </td><td>Background/Aims: Recent technical developments have enabled solo surgery in laparoscopic surgery. Our experience of solo surgery using the voice-guided robotic arm in laparoscopic colectomy for colorectal cancer was analyzed.
   <br>Methodology: The colon-lifting method was used in this study. The laparoscope was handled by AESOP3000 (TM). The colon was retracted anteriorly by the thread that passed through the mesocolon. This method enables lymphadenectomy by stretching of feeding vessels and obviates the need for an assistant. The short-term outcomes and survival between robotic arm and human scopist in a series of laparoscopic colectomies were compared with a case-matched control study.
   <br>Results: The numbers of both group patients were 11 respectively. There was no conversion to open surgery in both groups. The operation time (Robotic vs. Human=269 min. vs. 265) and laparoscopic time (209 us. 212) were not significant differences. There were also no significant differences in the bleeding, the morbidity rate and the numbers of dissected lymph nodes between the two groups. The five-year overall (81.8% vs. 72.7%) and disease-free (72.7% vs. 62.3%) survivals showed no significant differences.
   <br>Conclusions: Laparoscopic solo-surgery in colectomy is safe and feasible, without any deterioration of the curative potential of the procedure.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Fujii, Shoichi] Yokohama City Univ, Dept Surg, Gastroenterol Ctr,
   Minami Ku, Yokohama, Kanagawa 2320024, Japan.
   <br>[Yamagishi, Shigeru; Ike, Hideyuki; Endo, Itaru; Shimada, Hiroshi]
   Yokohama City Univ, Grad Sch Med, Dept Surg Gastroenterol, Yokohama,
   Kanagawa 2320024, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Fujii, S (reprint author), Yokohama City Univ, Dept Surg, Gastroenterol Ctr, Minami Ku, 4-57 Urafunecho, Yokohama, Kanagawa 2320024, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>u0970047@urahp.yokohama-cu.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>4</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>4</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAR-APR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">VL </td><td>58</td>
</tr>

<tr>
<td valign="top">IS </td><td>106</td>
</tr>

<tr>
<td valign="top">BP </td><td>406</td>
</tr>

<tr>
<td valign="top">EP </td><td>410</td>
</tr>

<tr>
<td valign="top">SC </td><td>Gastroenterology &amp; Hepatology; Surgery</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000291084600026</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Helmy, T
   <br>Hassan, MM
   <br>Sarfraz, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Helmy, Tarek
   <br>Hassan, Mohammad M.
   <br>Sarfraz, Muhammad</td>
</tr>

<tr>
<td valign="top">TI </td><td>A hybrid computational model for an automated image descriptor for
   visually impaired users</td>
</tr>

<tr>
<td valign="top">SO </td><td>COMPUTERS IN HUMAN BEHAVIOR</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Classification; Image analysis and descriptor</td>
</tr>

<tr>
<td valign="top">ID </td><td>CATEGORIZATION; RETRIEVAL</td>
</tr>

<tr>
<td valign="top">AB </td><td>Nowadays, with the development of high-quality software, most presentations contain images. This makes a problem for visually impaired people, as there is a support for text-to-voice conversion but not for image-to-voice. For documents which combine images and text, we propose a hybrid model to make a meaningful and easily recognizable descriptor for images in three main categories (statistical, geometrical and non-geometrical). First, a neural classifier is trained, by mining the associated texts using advanced concepts, so that it can assign each document to a specific category. Then, a similarity matching with that category's annotated templates is performed for images in every other category. We have made a classifier by using novel features based on color projection and able to differentiate geometrical images from ordinary images. Thus we have significantly improved the similarity matching, to achieve more accurate descriptions of images for visually impaired users. An important feature of the proposed model is that its specific matching techniques, suitable for a particular category, can be easily integrated and developed for other categories. (C) 2010 Elsevier Ltd. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Helmy, Tarek] King Fahd Univ Petr &amp; Minerals, Coll Comp Sci &amp; Engn,
   Dhahran 31261, Saudi Arabia.
   <br>[Hassan, Mohammad M.] King Fahd Univ Petr &amp; Minerals, Dammam Community
   Coll, Dhahran 31261, Saudi Arabia.
   <br>[Sarfraz, Muhammad] Kuwait Univ, Dept Informat Sci, Safat 13060, Kuwait.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Helmy, T (reprint author), King Fahd Univ Petr &amp; Minerals, Coll Comp Sci &amp; Engn, Mail Box 413, Dhahran 31261, Saudi Arabia.</td>
</tr>

<tr>
<td valign="top">EM </td><td>helmy@kfupm.edu.sa; mdmandi@kfupm.edu.sa; sarfraz@cfw.kuniv.edu</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Helmy, Tarek</display_name>&nbsp;</font></td><td><font size="3">B-5967-2015&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">VL </td><td>27</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>677</td>
</tr>

<tr>
<td valign="top">EP </td><td>693</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.chb.2010.04.018</td>
</tr>

<tr>
<td valign="top">SC </td><td>Psychology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000287622000008</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wilhelm, T
   <br>Metzig, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wilhelm, Thomas
   <br>Metzig, Andreas</td>
</tr>

<tr>
<td valign="top">TI </td><td>Endoscopic Minimally Invasive Thyroidectomy (eMIT): A Prospective
   Proof-of-Concept Study in Humans</td>
</tr>

<tr>
<td valign="top">SO </td><td>WORLD JOURNAL OF SURGERY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">ID </td><td>NATURAL ORIFICE SURGERY; NECK-SURGERY; BREAST APPROACH; SUBTOTAL
   PARATHYROIDECTOMY; ENDOCRINE SURGERY; RESECTION; ACCESS; NERVE</td>
</tr>

<tr>
<td valign="top">AB </td><td>We have developed a new approach for endoscopic minimally invasive thyroidectomy (eMIT) in anatomical studies. Safety and feasibility were demonstrated in an animal study and then the eMIT technique was applied for the first time successfully in humans on the 18 March 2009.
   <br>In a prospective study, we performed this eMIT technique on eight patients suffering from nodular change of the thyroid gland. All patients were evaluated regarding recurrent laryngeal nerve function, intra- and postoperative complications, and postoperative outcome, particularly with respect to swallowing disorders.
   <br>A total thyroidectomy and a partial resection were performed in four cases each. In three cases, a conversion to open surgery was necessary due to specimen size. No local infection at the incision site or within the cervical spaces occurred within the direct postoperative course. No intraoperative bleeding necessitating conversion to open surgery was observed. In one case, a permanent palsy of the right recurrent laryngeal nerve was noted. Voice function and breathing were not affected. Paresthesia of the mental nerve did not occur in all patients and in those in which it did occur, it resolved within 3 weeks. Mean follow-up time was 10.9 months.
   <br>The experimental development of the eMIT technique has led to its first clinical application in humans. In this prospective proof-of-concept study in humans, the thyroid gland was reached via the transoral endoscopic approach in an anatomically defined layer without any relevant damage to vessels. Limitations to this technique are determined by specimen volume (up to 30 ml) and nodule size (up to 20 mm).</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Wilhelm, Thomas] HELIOS Klinikum Borna, Dept Otolaryngol Head Neck &amp;
   Facial Plast Surg, D-04552 Borna, Germany.
   <br>[Metzig, Andreas] HELIOS Klinikum Borna, Dept Gen Visceral Minimally
   Invas &amp; Vasc Surg, D-04552 Borna, Germany.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Wilhelm, T (reprint author), HELIOS Klinikum Borna, Dept Otolaryngol Head Neck &amp; Facial Plast Surg, Rudolf Virchow Str 2, D-04552 Borna, Germany.</td>
</tr>

<tr>
<td valign="top">EM </td><td>thomas.wilhelm@helios-kliniken.de</td>
</tr>

<tr>
<td valign="top">TC </td><td>74</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>89</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">VL </td><td>35</td>
</tr>

<tr>
<td valign="top">IS </td><td>3</td>
</tr>

<tr>
<td valign="top">BP </td><td>543</td>
</tr>

<tr>
<td valign="top">EP </td><td>551</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/s00268-010-0846-0</td>
</tr>

<tr>
<td valign="top">SC </td><td>Surgery</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000286934700012</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Zen, H
   <br>Nankaku, Y
   <br>Tokuda, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Zen, Heiga
   <br>Nankaku, Yoshihiko
   <br>Tokuda, Keiichi</td>
</tr>

<tr>
<td valign="top">TI </td><td>Continuous Stochastic Feature Mapping Based on Trajectory HMMs</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Gaussian mixture model (GMM)-based mapping; speech recognition;
   trajectory hidden Markov model (HMM); voice conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>ARTICULATORY MOVEMENTS; VOICE CONVERSION; MODEL; ACOUSTICS; SPECTRUM</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper proposes a technique of continuous stochastic feature mapping based on trajectory hidden Markov models (HMMs), which have been derived from HMMs by imposing explicit relationships between static and dynamic features. Although Gaussian mixture model (GMM)- or HMM-based feature-mapping techniques work effectively, their accuracy occasionally degrades due to inappropriate dynamic characteristics caused by frame-by-frame mapping. While the use of dynamic-feature constraints at the mapping stage can alleviate this problem, it also introduces inconsistencies between training and mapping. The technique we propose can eliminate these inconsistencies while retaining the benefits of using dynamic-feature constraints, and it offers entire sequence-level transformation rather than frame-by-frame mapping. The results obtained from speaker-conversion, acoustic-to-articulatory inversion-mapping, and noise-compensation experiments demonstrated that our new approach outperformed the conventional one.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Zen, Heiga; Nankaku, Yoshihiko; Tokuda, Keiichi] Nagoya Inst Technol,
   Dept Comp Sci &amp; Engn, Nagoya, Aichi 4668555, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Zen, H (reprint author), Toshiba Res Europe Ltd, Cambridge Res Lab, Cambridge CB4 0GZ, England.</td>
</tr>

<tr>
<td valign="top">EM </td><td>heiga.zen@crl.toshiba.co.uk; nankaku@sp.nitech.ac.jp;
   tokuda@nitech.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>34</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>37</td>
</tr>

<tr>
<td valign="top">PD </td><td>FEB</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">VL </td><td>19</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>417</td>
</tr>

<tr>
<td valign="top">EP </td><td>430</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASL.2010.2049685</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000291717600017</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Li, JL
   <br>Yang, HW
   <br>Zhang, WZ
   <br>Cai, LH</td>
</tr>

<tr>
<td valign="top">AF </td><td>Li, Jinlong
   <br>Yang, Hongwu
   <br>Zhang, Weizhao
   <br>Cai, Lianhong</td>
</tr>

<tr>
<td valign="top">BE </td><td>Zeng, D</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Lyrics to Singing Voice Synthesis System with Variable Timbre</td>
</tr>

<tr>
<td valign="top">SO </td><td>APPLIED INFORMATICS AND COMMUNICATION, PT 2</td>
</tr>

<tr>
<td valign="top">SE </td><td>Communications in Computer and Information Science</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Applied Informatics and Communication (ICAIC
   2011)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 20-21, 2011</td>
</tr>

<tr>
<td valign="top">CL </td><td>Xian, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Singing voice synthesis; melody control model; timbre modification; GMM;
   MIDI</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we present a singing voice synthesis system, which can convert lyrics to singing voice. As the synthetic song's timbre is too monotonous, a new singing voice morphing algorithm based on GMM (Gaussian Mixture Model) was presented accordingly. The MOS test shows that the average MOS score of synthesized song is above 3.3 before timbre conversion. The professional singer's timbre can be added proportionally by changing the scale factor k in the system. The ABX test demonstrates that the accuracy can be up to 100% in the case of k=0 or k=1, and it can be higher than 64.5% in the case of 0&lt;k&lt;1. The experiments also show the mean of GMM has greater impact on a singer's timbre than weight ratio and covariance.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Li, Jinlong; Yang, Hongwu; Zhang, Weizhao] NW Normal Univ, Dept Phys &amp;
   Elect Engn, Lanzhou, Gansu Province, Peoples R China.
   <br>[Cai, Lianhong] Tsinghua Univ, Dept Comp Sci &amp; Technol, Beijing, Peoples
   R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Li, JL (reprint author), NW Normal Univ, Dept Phys &amp; Elect Engn, Lanzhou, Gansu Province, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>lijinlong_88@sina.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">VL </td><td>225</td>
</tr>

<tr>
<td valign="top">BP </td><td>186</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">PN </td><td>II</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000307481100023</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Licev, L
   <br>Brettsnajdr, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Licev, Lacezar
   <br>Brettsnajdr, Antonin</td>
</tr>

<tr>
<td valign="top">GP </td><td>SGEM</td>
</tr>

<tr>
<td valign="top">TI </td><td>SPEECH ANALYSIS AND SYNTHESIS FOR THE HANDICAPPED</td>
</tr>

<tr>
<td valign="top">SO </td><td>11TH INTERNATIONAL MULTIDISCIPLINARY SCIENTIFIC GEOCONFERENCE (SGEM
   2011), VOL II</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Multidisciplinary Scientific GeoConference-SGEM</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>11th International Multidisciplinary Scientific GeoConference</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUN 20-25, 2011</td>
</tr>

<tr>
<td valign="top">CL </td><td>Albena, BULGARIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>LPC; ACF; speech; synthesis; signal processing</td>
</tr>

<tr>
<td valign="top">AB </td><td>This bachelor thesis is focused on speech synthesis of Czech language in frequency domain. The resulting software solution consists of the analyzer and synthesizer of selected elementary speech units. Speech synthesis is based on the LPC model. The analyzer analyzes an input phoneme unit in WAV format, divides it into microsegmens of length 20 ms and keeps the LPC coefficients, flag (voiced / unvoiced) and the residual prediction error signal. These data are then stored in the X M L file. Further, this thesis deals with methods of speech microsegment analysis - such as determining voice, pitch detection. From the side of resulting speech synthesis it's about reverse conversion of microsegment parameters into the time domain, phonetic transcription, splitting sentences into syllables and their concatenation.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Licev, Lacezar; Brettsnajdr, Antonin] VSB TU Ostrava, Katedra ATR
   352,17 Listopadu Ostrava, Ostrava, Czech Republic.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Licev, L (reprint author), VSB TU Ostrava, Katedra ATR 352,17 Listopadu Ostrava, Ostrava, Czech Republic.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">BP </td><td>581</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Geology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000307366300077</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Pribil, J
   <br>Pribilova, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Pribil, Jiri
   <br>Pribilova, Anna</td>
</tr>

<tr>
<td valign="top">BE </td><td>Esposito, A
   <br>Vinciarelli, A
   <br>Vicsi, K
   <br>Pelachaud, C
   <br>Nijholt, A</td>
</tr>

<tr>
<td valign="top">TI </td><td>Influence of Visual Stimuli on Evaluation of Converted Emotional Speech
   by Listening Tests</td>
</tr>

<tr>
<td valign="top">SO </td><td>ANALYSIS OF VERBAL AND NONVERBAL COMMUNICATION AND ENACTMENT: THE
   PROCESSING ISSUES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Lecture Notes in Computer Science</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>PINK SSPnet-COST 2102 International Conference on Analysis of Verbal and
   Nonverbal Communication and Enactment: The Processing Issues</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 07-10, 2010</td>
</tr>

<tr>
<td valign="top">CL </td><td>Budapest, HUNGARY</td>
</tr>

<tr>
<td valign="top">DE </td><td>emotional voice conversion; visual stimuli; listening test</td>
</tr>

<tr>
<td valign="top">ID </td><td>EXPRESSIONS; RECOGNITION; INTEGRATION; INFORMATION; MODEL</td>
</tr>

<tr>
<td valign="top">AB </td><td>Emotional voice conversion is usually evaluated by subjective listening tests. In our experiment, the sentences of emotionally converted speech were evaluated by the classical listening test accompanied by visual stimuli of affective pictures with emotional content from the International Affective Picture System (IAPS) database. Obtained results for sentences uttered by male and female speakers corresponding to five emotional states (joy, surprise, sadness, anger together with original neutral state) confirm our working hypothesis about influence of visual stimuli on the speech emotion evaluation process.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Pribil, Jiri] Acad Sci Czech Republic, Inst Photon &amp; Elect, Vvi,
   Chaberska 57, CZ-18251 Prague 8, Czech Republic.
   <br>[Pribil, Jiri] SAS, Inst Measurement Sci, SK-84104 Bratislava, Slovakia.
   <br>[Pribilova, Anna] Slovak Univ Technol Bratislava, Inst Elect &amp; Photon,
   Fac Elect Engn &amp; Informat Technol, SK-81219 Bratislava, Slovakia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Pribil, J (reprint author), Acad Sci Czech Republic, Inst Photon &amp; Elect, Vvi, Chaberska 57, CZ-18251 Prague 8, Czech Republic.</td>
</tr>

<tr>
<td valign="top">EM </td><td>Jiri.Pribil@savba.sk; Anna.Pribilova@stuba.sk</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">VL </td><td>6800</td>
</tr>

<tr>
<td valign="top">BP </td><td>378</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000307258000035</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Calzada, A
   <br>Socoro, JC</td>
</tr>

<tr>
<td valign="top">AF </td><td>Calzada, Angel
   <br>Claudi Socoro, Joan</td>
</tr>

<tr>
<td valign="top">BE </td><td>TraviesoGonzalez, CM
   <br>AlonsoHernandez, JB</td>
</tr>

<tr>
<td valign="top">TI </td><td>Vocal Effort Modification through Harmonics Plus Noise Model
   Representation</td>
</tr>

<tr>
<td valign="top">SO </td><td>ADVANCES IN NONLINEAR SPEECH PROCESSING</td>
</tr>

<tr>
<td valign="top">SE </td><td>Lecture Notes in Artificial Intelligence</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>5th International Conference on Nonlinear Speech Processing (NOLISP
   2011)</td>
</tr>

<tr>
<td valign="top">CY </td><td>NOV 07-09, 2011</td>
</tr>

<tr>
<td valign="top">CL </td><td>Univ Las Palmas Gran Canaria, Las Palmas Gran Canaria, SPAIN</td>
</tr>

<tr>
<td valign="top">HO </td><td>Univ Las Palmas Gran Canaria</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice quality; harmonics plus noise model; speech synthesis; vocal
   effort; speech conversion; expressive speech</td>
</tr>

<tr>
<td valign="top">AB </td><td>Harmonics plus Noise Model (HNM) is a well known speech signal representation technique that allows to apply high quality modifications to the signal used in text-to-speech systems providing higher flexibility than its counterpart TD-PSOLA based synthesis systems. In this paper an adaptation of the adaptive pre-emphasis linear prediction technique for modifying the vocal effort, using HNM speech representation, is presented. The proposed transformation methodology is validated using a Copy Re-synthesis strategy on a speech corpora specifically designed with three levels of vocal effort (soft, modal and loud). The results of a perceptual test demonstrate the effectiveness of the proposed technique performing all different vocal effort conversions for the given corpus.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Calzada, Angel; Claudi Socoro, Joan] La Salle Univ Ramon Llull, GTM Grp
   Recerca Tecnol Media, Barcelona 08022, Spain.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Calzada, A (reprint author), La Salle Univ Ramon Llull, GTM Grp Recerca Tecnol Media, C Quatre Camins 2, Barcelona 08022, Spain.</td>
</tr>

<tr>
<td valign="top">EM </td><td>acalzada@salle.url.edu; jclaudi@salle.url.edu</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Socoro, Joan Claudi</display_name>&nbsp;</font></td><td><font size="3">0000-0002-7348-6916&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>4</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>4</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">VL </td><td>7015</td>
</tr>

<tr>
<td valign="top">BP </td><td>96</td>
</tr>

<tr>
<td valign="top">EP </td><td>103</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000306444200013</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Xu, SJ
   <br>Xu, J
   <br>Tang, YJ
   <br>Shen, LK</td>
</tr>

<tr>
<td valign="top">AF </td><td>Xu Shijian
   <br>Xu Jie
   <br>Tang Yijun
   <br>Shen Likai</td>
</tr>

<tr>
<td valign="top">BE </td><td>Zhou, M
   <br>Tan, HH</td>
</tr>

<tr>
<td valign="top">TI </td><td>Design of the Portable Gloves for Sign Language Recognition</td>
</tr>

<tr>
<td valign="top">SO </td><td>ADVANCES IN COMPUTER SCIENCE AND EDUCATION APPLICATIONS, PT II</td>
</tr>

<tr>
<td valign="top">SE </td><td>Communications in Computer and Information Science</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Computer Science and Education (CSE 2011)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUL 09-10, 2011</td>
</tr>

<tr>
<td valign="top">CL </td><td>Qingdao, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>gesture recognition; data collection; portable; data-glove; Embedded
   System</td>
</tr>

<tr>
<td valign="top">AB </td><td>In order to improve the gesture communication ability of the obstacle of hearing people, a portable gesture recognition data-glove is designed, which uses 14 groups of three-axis acceleration sensor- totally and 42 acceleration channels, and Bluetooth to transfer the data to the embedded system. Obtaining and identifying the effective parameters through the real-time analysis of the 42 groups of variables. Based on the identification, gesture converts into semantic conversion, and finally semantic conversion converts into voice, thus to realize the gesture transforms into voice in real-time.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Xu Shijian; Xu Jie; Tang Yijun; Shen Likai] Zhejiang Univ City Coll,
   Sch Informat &amp; Elect Engn, Hangzhou 310005, Zhejiang, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Xu, J (reprint author), Zhejiang Univ City Coll, Sch Informat &amp; Elect Engn, Hangzhou 310005, Zhejiang, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>Xushijian1226@163.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">VL </td><td>202</td>
</tr>

<tr>
<td valign="top">BP </td><td>171</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">PN </td><td>2</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000302337200025</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Suzuki, Y
   <br>Sakata, O
   <br>Qin, L
   <br>Sato, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Suzuki, Yutaka
   <br>Sakata, Osamu
   <br>Qin, Ling
   <br>Sato, Yu</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice-Pulse Conversion Method Based on the Response of the Primary
   Auditory Cortex For Auditory Input-Type BMI by using Electric
   Micro-stimulation of the Primary Auditory Cortex</td>
</tr>

<tr>
<td valign="top">SO </td><td>2011 IEEE REGION 10 CONFERENCE TENCON 2011</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE Region 10 Conference on TENCON</td>
</tr>

<tr>
<td valign="top">CY </td><td>NOV 21-24, 2011</td>
</tr>

<tr>
<td valign="top">CL </td><td>INDONESIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>component; Brain Macine Interface; Primary Auditory Cortex; Vowel; Self
   Organizing Map</td>
</tr>

<tr>
<td valign="top">ID </td><td>COCHLEAR IMPLANT; AWAKE CATS; SENSITIVITY; NEURONS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Brain machine interface (BMI) is one of useful techniques to compensate the deteriorated sensory functions. For the purpose of aid of auditory impediment, we have developed a sound-stimulus signal conversion interface for an auditory BMI that is based on the response of primary auditory cortex (A1) cells. In A1 neuron responses, amplitude-modulated (AM) sound is encoded as spike rate information of the tonic response, and frequency-modulated (FM) sound is encoded as the order of active place in the brain. In signal processing, the sound signals were divided into frequency bands by using a digital filter bank for FM sound. The separate signals were then converted to pulse train signals to deal with AM. In this manner, a voice-pulse conversion method that is based on A1 neuron response to AM and FM sound was constructed. Additionally, in order to examine which frequency band division method was suitable for vowel identification, we got the self-organizing map to learn the number of pulses for each output channel. The result showed that the exponential processing was best. Under the condition, the formant information is left behind and the classification of vowels is possible.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Suzuki, Yutaka] Univ Yamanashi, Ctr Life Sci Res, Chuo Ku, Yamanashi,
   Japan.
   <br>[Sakata, Osamu] Univ Yamanashi, Fac Engn, Yamanashi, Japan.
   <br>[Qin, Ling; Sato, Yu] Univ Yamanashi, Fac Med, Dept Physiol, Yamanashi,
   Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Suzuki, Y (reprint author), Univ Yamanashi, Ctr Life Sci Res, Chuo Ku, Yamanashi, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>yutakas@yamanashi.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">BP </td><td>1225</td>
</tr>

<tr>
<td valign="top">EP </td><td>1229</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000299628600237</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nakano, T
   <br>Goto, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nakano, Tomoyasu
   <br>Goto, Masataka</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>VOCALISTENER2: A SINGING SYNTHESIS SYSTEM ABLE TO MIMIC A USER'S SINGING
   IN TERMS OF VOICE TIMBRE CHANGES AS WELL AS PITCH AND DYNAMICS</td>
</tr>

<tr>
<td valign="top">SO </td><td>2011 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 22-27, 2011</td>
</tr>

<tr>
<td valign="top">CL </td><td>Prague Congress Ctr, Prague, CZECH REPUBLIC</td>
</tr>

<tr>
<td valign="top">HO </td><td>Prague Congress Ctr</td>
</tr>

<tr>
<td valign="top">DE </td><td>Singing synthesis; Voice timbre changes; Singing information processing</td>
</tr>

<tr>
<td valign="top">ID </td><td>CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a singing synthesis system, VocaListener2, that can automatically synthesize a singing voice by mimicking the timbre changes of a user's singing voice. The system is an extension of our previous VocaListener system which deals with only pitch and dynamics. Most previous techniques for manipulating voice timbre have focused on voice conversion and voice morphing, and they cannot deal with the timbre changes during singing. To develop VocaListener2, we constructed a voice timbre space on the basis of various singing voices that are synchronized under pitch, dynamics, and phoneme by using VocaListener. In this space, the timbre changes can be reflected in the synthesized singing voice. The system was evaluated by the Euclidean distance in the space between an estimated result and a ground-truth under closed/open conditions.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nakano, Tomoyasu; Goto, Masataka] Natl Inst Adv Ind Sci &amp; Technol,
   Tsukuba, Ibaraki, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nakano, T (reprint author), Natl Inst Adv Ind Sci &amp; Technol, Tsukuba, Ibaraki, Japan.</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Goto, Masataka</display_name>&nbsp;</font></td><td><font size="3">K-8205-2012&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Nakano, Tomoyasu</display_name>&nbsp;</font></td><td><font size="3">A-8670-2013&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Goto, Masataka</display_name>&nbsp;</font></td><td><font size="3">0000-0003-1167-0977&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Nakano, Tomoyasu</display_name>&nbsp;</font></td><td><font size="3">0000-0001-8014-2209&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>10</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>10</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">BP </td><td>453</td>
</tr>

<tr>
<td valign="top">EP </td><td>456</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering; Imaging Science &amp; Photographic Technology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000296062400114</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nakamura, K
   <br>Janke, M
   <br>Wand, M
   <br>Schultz, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nakamura, Keigo
   <br>Janke, Matthias
   <br>Wand, Michael
   <br>Schultz, Tanja</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>ESTIMATION OF FUNDAMENTAL FREQUENCY FROM SURFACE ELECTROMYOGRAPHIC DATA:
   EMG-TO-F(0)</td>
</tr>

<tr>
<td valign="top">SO </td><td>2011 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 22-27, 2011</td>
</tr>

<tr>
<td valign="top">CL </td><td>Prague Congress Ctr, Prague, CZECH REPUBLIC</td>
</tr>

<tr>
<td valign="top">HO </td><td>Prague Congress Ctr</td>
</tr>

<tr>
<td valign="top">DE </td><td>Electromyography; Voice conversion; Fundamental frequency; Feature
   estimation</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we present our recent studies of F(0) estimation from the surface electromyographic (EMG) data using a Gaussian mixture model (GMM)-based voice conversion (VC) technique, referred to as EMG-to-F(0). In our approach, a support vector machine recognizes individual frames as unvoiced and voiced (U/V), and voiced F(0) contours are discriminated by the trainedGMMbased on the manner of minimum mean-square error. EMG-to-F(0) is experimentally evaluated using three data sets of different speakers. Each data set includes almost 500 utterances. Objective experiments demonstrate that we achieve a correlation coefficient of up to 0.49 between estimated and target F(0) contours with more than 84% U/V decision accuracy, although the results have large variations.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nakamura, Keigo; Janke, Matthias; Wand, Michael; Schultz, Tanja]
   Karlsruhe Inst Technol, Cognit Syst Lab, Karlsruhe, Germany.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nakamura, K (reprint author), Karlsruhe Inst Technol, Cognit Syst Lab, Karlsruhe, Germany.</td>
</tr>

<tr>
<td valign="top">EM </td><td>kei-naka@is.naist.jp; matthias.janke@kit.edu; michael.wand@kit.edu;
   tanja.schultz@kit.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>8</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>8</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">BP </td><td>573</td>
</tr>

<tr>
<td valign="top">EP </td><td>576</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering; Imaging Science &amp; Photographic Technology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000296062400144</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kunikoshi, A
   <br>Qian, Y
   <br>Soong, F
   <br>Minematsu, N</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kunikoshi, Aki
   <br>Qian, Yao
   <br>Soong, Frank
   <br>Minematsu, Nobuaki</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>IMPROVED F0 MODELING AND GENERATION IN VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2011 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 22-27, 2011</td>
</tr>

<tr>
<td valign="top">CL </td><td>Prague Congress Ctr, Prague, CZECH REPUBLIC</td>
</tr>

<tr>
<td valign="top">HO </td><td>Prague Congress Ctr</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice Conversion; v/u decision model; F0 generation; Voicing Strength</td>
</tr>

<tr>
<td valign="top">ID </td><td>HMM</td>
</tr>

<tr>
<td valign="top">AB </td><td>F0 is an acoustic feature that varies largely from one speaker to another. F0 is characterized by a discontinuity in the transition between voiced and unvoiced sounds that presents an obstacle to GMM modeling for use in voice conversion. A Multi-Space Distribution (MSD) [5] can be used to model unvoiced and voiced F0 regions in a linearly weighted mixture. However, the use of two incompatible probabilistic spaces, for example a continuous probability density for voiced observations, and a discrete probability for unvoiced observations, may result in an imprecise voiced/unvoiced (v/u) conversion in a maximum likelihood (ML) sense. In this paper we propose to use voicing strength, characterized by the normalized correlation coefficient magnitude, as calculated from F0 feature extraction, as an additional feature for improving F0 modeling and the v/u decision in the context of voice conversion. The proposed method was evaluated on male-to-female voice conversion tasks in both Mandarin and English. Objective tests showed that the approach is effective in reducing the Root Mean Square Error, while the results for subjective metrics including AB preference and ABX speaker similarity tests also showed gains.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kunikoshi, Aki; Qian, Yao; Soong, Frank] Microsoft Res Asia, Beijing,
   Peoples R China.
   <br>[Kunikoshi, Aki; Minematsu, Nobuaki] Univ Tokyo, Tokyo 1138654, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kunikoshi, A (reprint author), Microsoft Res Asia, Beijing, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>kunikoshi@gavo.t.u-tokyo.ac.jp; yaoqian@microsoft.com;
   frankkps@microsoft.com; mine@gavo.t.u-tokyo.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">BP </td><td>4568</td>
</tr>

<tr>
<td valign="top">EP </td><td>4571</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering; Imaging Science &amp; Photographic Technology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000296062405044</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Saito, D
   <br>Watanabe, S
   <br>Nakamura, A
   <br>Minematsu, N</td>
</tr>

<tr>
<td valign="top">AF </td><td>Saito, Daisuke
   <br>Watanabe, Shinji
   <br>Nakamura, Atsushi
   <br>Minematsu, Nobuaki</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>HIGH ACCURATE MODEL-INTEGRATION-BASED VOICE CONVERSION USING DYNAMIC
   FEATURES AND MODEL STRUCTURE OPTIMIZATION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2011 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 22-27, 2011</td>
</tr>

<tr>
<td valign="top">CL </td><td>Prague Congress Ctr, Prague, CZECH REPUBLIC</td>
</tr>

<tr>
<td valign="top">HO </td><td>Prague Congress Ctr</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; probabilistic integration; dynamic features;
   information criterion</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH RECOGNITION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper combines a parameter generation algorithm and a model optimization approach with the model-integration-based voice conversion (MIVC). We have proposed probabilistic integration of a joint density model and a speaker model to mitigate a requirement of the parallel corpus in voice conversion (VC) based on Gaussian Mixture Model (GMM). As well as the other VC methods, MIVC also suffers from the problems; the degradation of the perceptual quality caused by the discontinuity through the parameter trajectory, and the difficulty to optimize the model structure. To solve the problems, this paper proposes a parameter generation algorithm constrained by dynamic features for the first problem and an information criterion including mutual influences between the joint density model and the speaker model for the second problem. Experimental results show that the first approach improved the performance of VC and the second approach appropriately predicted the optimal number of mixtures of the speaker model for our MIVC.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Saito, Daisuke; Minematsu, Nobuaki] Univ Tokyo, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Saito, D (reprint author), Univ Tokyo, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>dsk_saito@gavo.t.u-tokyo.ac.jp; watanabe@cslab.kecl.ntt.co.jp;
   ats@cslab.kecl.ntt.co.jp; mine@gavo.t.u-tokyo.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">BP </td><td>4576</td>
</tr>

<tr>
<td valign="top">EP </td><td>4579</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering; Imaging Science &amp; Photographic Technology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000296062405046</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Erro, D
   <br>Sainz, I
   <br>Navas, E
   <br>Hernaez, I</td>
</tr>

<tr>
<td valign="top">AF </td><td>Erro, Daniel
   <br>Sainz, Inaki
   <br>Navas, Eva
   <br>Hernaez, Inma</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>HNM-BASED MFCC+F0 EXTRACTOR APPLIED TO STATISTICAL SPEECH SYNTHESIS</td>
</tr>

<tr>
<td valign="top">SO </td><td>2011 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 22-27, 2011</td>
</tr>

<tr>
<td valign="top">CL </td><td>Prague Congress Ctr, Prague, CZECH REPUBLIC</td>
</tr>

<tr>
<td valign="top">HO </td><td>Prague Congress Ctr</td>
</tr>

<tr>
<td valign="top">DE </td><td>speech parameterization; statistical parametric speech synthesis; voice
   conversion; harmonics plus noise model</td>
</tr>

<tr>
<td valign="top">AB </td><td>Currently, the statistical framework based on Hidden Markov Models (HMMs) plays a relevant role in speech synthesis, while voice conversion systems based on Gaussian Mixture Models (GMMs) are almost standard. In both cases, statistical modeling is applied to learn distributions of acoustic vectors extracted from speech signals, each vector containing a suitable parametric representation of one speech frame. The overall performance of the systems is often limited by the accuracy of the underlying speech parameterization and reconstruction method. The method presented in this paper allows accurate MFCC extraction and high-quality reconstruction of speech signals assuming a Harmonics plus Noise Model (HNM). Its suitability for high-quality HMM-based speech synthesis is shown through subjective tests.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Erro, Daniel; Sainz, Inaki; Navas, Eva; Hernaez, Inma] Univ Basque
   Country UPV EHU, AHOLAB Signal Proc Lab, Bilbao, Spain.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Erro, D (reprint author), Univ Basque Country UPV EHU, AHOLAB Signal Proc Lab, Bilbao, Spain.</td>
</tr>

<tr>
<td valign="top">EM </td><td>derro@aholab.ehu.es; inaki@aholab.ehu.es; eva@aholab.ehu.es;
   inma@aholab.ehu.es</td>
</tr>

<tr xmlns:date="http://exslt.org/dates-and-times">
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Hernaez Rioja, Inmaculada</display_name>&nbsp;</font></td><td><font size="3">K-8303-2012&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Erro, Daniel</display_name>&nbsp;</font></td><td><font size="3">H-7043-2015&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Navas, Eva</display_name>&nbsp;</font></td><td><font size="3">H-4317-2013&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Hernaez Rioja, Inmaculada</display_name>&nbsp;</font></td><td><font size="3">0000-0003-4447-7575&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Erro, Daniel</display_name>&nbsp;</font></td><td><font size="3">0000-0003-0954-6942&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Navas, Eva</display_name>&nbsp;</font></td><td><font size="3">0000-0003-3804-4984&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>13</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>13</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">BP </td><td>4728</td>
</tr>

<tr>
<td valign="top">EP </td><td>4731</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering; Imaging Science &amp; Photographic Technology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000296062405084</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Chen, LH
   <br>Ling, ZH
   <br>Dai, LR</td>
</tr>

<tr>
<td valign="top">AF </td><td>Chen, Ling-Hui
   <br>Ling, Zhen-Hua
   <br>Dai, Li-Rong</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>NON-PARALLEL TRAINING FOR VOICE CONVERSION BASED ON FT-GMM</td>
</tr>

<tr>
<td valign="top">SO </td><td>2011 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 22-27, 2011</td>
</tr>

<tr>
<td valign="top">CL </td><td>Prague Congress Ctr, Prague, CZECH REPUBLIC</td>
</tr>

<tr>
<td valign="top">HO </td><td>Prague Congress Ctr</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; Gaussian mixture model; non-parallel training;
   frequency warping</td>
</tr>

<tr>
<td valign="top">ID </td><td>TRANSFORMATIONS</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a non-parallel training algorithm for voice conversion based on feature transform Gaussian mixture model (FT-GMM), which is a mixture model of joint density space of source speaker and target speaker with explicit feature transform modeling. In FT-GMM, the correlations between the distributions of two speakers in each component of the mixture model are not directly modeled, but absorbed into these explicit feature transformations. This makes it possible to extend this model to non-parallel training by simply decomposing it into two sub-models, one for each speaker and optimizing them separatively. A frequency warping process is adopted to compensate performance degradation caused by original spectral distance between source and target speakers. Cross-gender experimental results show that the proposed method achieves comparable performance as parallel training.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Chen, Ling-Hui; Ling, Zhen-Hua; Dai, Li-Rong] Univ Sci &amp; Technol China,
   iFLYTEK Speech Lab, Hefei 230026, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Chen, LH (reprint author), Univ Sci &amp; Technol China, iFLYTEK Speech Lab, Hefei 230026, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>chenlh@mail.ustc.edu.cn; zhling@ustc.edu; lrdai@ustc.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">BP </td><td>5116</td>
</tr>

<tr>
<td valign="top">EP </td><td>5119</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering; Imaging Science &amp; Photographic Technology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000296062405181</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Doi, H
   <br>Nakamura, K
   <br>Toda, T
   <br>Saruwatari, H
   <br>Shikano, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Doi, Hironori
   <br>Nakamura, Keigo
   <br>Toda, Tomoki
   <br>Saruwatari, Hiroshi
   <br>Shikano, Kiyohiro</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>AN EVALUATION OF ALARYNGEAL SPEECH ENHANCEMENT METHODS BASED ON VOICE
   CONVERSION TECHNIQUES</td>
</tr>

<tr>
<td valign="top">SO </td><td>2011 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 22-27, 2011</td>
</tr>

<tr>
<td valign="top">CL </td><td>Prague Congress Ctr, Prague, CZECH REPUBLIC</td>
</tr>

<tr>
<td valign="top">HO </td><td>Prague Congress Ctr</td>
</tr>

<tr>
<td valign="top">DE </td><td>alaryngeal speech; speech enhancement; voice conversion; eigenvoice
   conversion; performance evaluations</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this study, we evaluate our proposed methods for enhancing alaryngeal speech based on statistical voice conversion techniques. Voice conversion based on a Gaussian mixture model has been applied to the conversion of alaryngeal speech into normal speech (AL-to-Speech). Moreover, one-to-many eigenvoice conversion (EVC) has also been applied to AL-to-Speech to enable the recovery of the original voice quality of laryngectomees even if only one arbitrary utterance of the original voice is available. VC/EVC-based AL-to-Speech systems have been developed for several types of alaryngeal speech, such as esophageal speech (ES), electrolaryngeal speech (EL), and body-conducted silent electrolaryngeal speech (silent EL). These proposed systems are compared with each other from various perspectives. The experimental results demonstrate that our proposed systems yield significant enhancement effects on each type of alaryngeal speech.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Doi, Hironori; Nakamura, Keigo; Toda, Tomoki; Saruwatari, Hiroshi;
   Shikano, Kiyohiro] Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara
   6300192, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Doi, H (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara 6300192, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>hironori-d@is.naist.jp; kei-naka@is.naist.jp; tomoki@is.naist.jp;
   sawatari@is.naist.jp; shikano@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>8</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>9</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">BP </td><td>5136</td>
</tr>

<tr>
<td valign="top">EP </td><td>5139</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering; Imaging Science &amp; Photographic Technology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000296062405186</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Shen, J
   <br>Raghunathan, A
   <br>Cheung, SCS
   <br>Patel, R</td>
</tr>

<tr>
<td valign="top">AF </td><td>Shen, Ju
   <br>Raghunathan, Anusha
   <br>Cheung, Sen-Ching S.
   <br>Patel, Rita</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>AUTOMATIC CONTENT GENERATION FOR VIDEO SELF MODELING</td>
</tr>

<tr>
<td valign="top">SO </td><td>2011 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME)</td>
</tr>

<tr>
<td valign="top">SE </td><td>IEEE International Conference on Multimedia and Expo</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Multimedia and Expo (ICME)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUL 11-15, 2011</td>
</tr>

<tr>
<td valign="top">CL </td><td>Univ Ramon Llull, La Salle, Barcelona, SPAIN</td>
</tr>

<tr>
<td valign="top">HO </td><td>Univ Ramon Llull, La Salle</td>
</tr>

<tr>
<td valign="top">DE </td><td>video self modeling; positive feedforward; voice disorder; computational
   multimedia; frame interpolation; voice imitation</td>
</tr>

<tr>
<td valign="top">ID </td><td>SETTINGS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Video self modeling (VSM) is a behavioral intervention technique in which a learner models a target behavior by watching a video of him or herself. Its effectiveness in rehabilitation and education has been repeatedly demonstrated but technical challenges remain in creating video contents that depict previously unseen behaviors. In this paper, we propose a novel system that re-renders new talking-head sequences suitable to be used for VSM treatment of patients with voice disorder. After the raw footage is captured, a new speech track is either synthesized using text-to-speech or selected based on voice similarity from a database of clean speeches. Voice conversion is then applied to match the new speech to the original voice. Time markers extracted from the original and new speech track are used to re-sample the video track for lip synchronization. We use an adaptive re-sampling strategy to minimize motion jitter, and apply bilinear and optical-flow based interpolation to ensure the image quality. Both objective measurements and subjective evaluations demonstrate the effectiveness of the proposed techniques.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Shen, Ju; Raghunathan, Anusha; Cheung, Sen-Ching S.] Univ Kentucky,
   Dept Elect &amp; Comp Engn, Lexington, KY 40506 USA.
   <br>[Patel, Rita] Univ Kentucky, Dept Hlth Sci, Rehabil Sci, Lexington, KY
   40506 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Shen, J (reprint author), Univ Kentucky, Dept Elect &amp; Comp Engn, Lexington, KY 40506 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ju.shen@uky.edu; anusha.raghunathan@uky.edu; sen-ching.cheung@uky.edu;
   rita.patel@uky.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>4</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>4</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000297172100166</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sun, J
   <br>Zhang, XW</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sun, Jian
   <br>Zhang, Xiongwei</td>
</tr>

<tr>
<td valign="top">TI </td><td>Analysis of State-Space Model based Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>PRZEGLAD ELEKTROTECHNICZNY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; State-Space Model (SSM); Linear Multivariate
   Regression (LMR); analysis</td>
</tr>

<tr>
<td valign="top">ID </td><td>TRANSFORMATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>A new State-Space Model (SSM) based voice conversion method has been proposed recently which outperforms the traditional Gaussian Mixture Model (GMM) method. Although the implementation process of the new method has been elaborated, the theoretical essence of this method has not been analysed clearly. In this paper an exhaustive analysis of the SSM based method is given theoretically and experimentally. Through these analysis, much simpler equivalence form and performance upper bound of the new method are obtained. Finally possible improvements are discussed.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Sun, Jian] PLA Univ Sci &amp; Tech, Inst Commun Engn, Postgrad Team 2,
   Nanjing 210007, Peoples R China.
   <br>[Zhang, Xiongwei] PLA Univ Sci &amp; Tech, Inst Command Automat, Nanjing
   210007, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sun, J (reprint author), PLA Univ Sci &amp; Tech, Inst Commun Engn, Postgrad Team 2, Biaoyin 2,Yudao St, Nanjing 210007, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>sunjian001@gmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">VL </td><td>87</td>
</tr>

<tr>
<td valign="top">IS </td><td>10</td>
</tr>

<tr>
<td valign="top">BP </td><td>373</td>
</tr>

<tr>
<td valign="top">EP </td><td>376</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000296743400080</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Elmanfaloty, R
   <br>Korany, N
   <br>Youssef, EA</td>
</tr>

<tr>
<td valign="top">AF </td><td>Elmanfaloty, Rania
   <br>Korany, N.
   <br>Youssef, El-Sayed A.</td>
</tr>

<tr>
<td valign="top">BE </td><td>Xie, Y
   <br>Zheng, Y</td>
</tr>

<tr>
<td valign="top">TI </td><td>Quality of Arabic Utterances Transformed Using Different Residual
   Prediction Techniques</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERNATIONAL CONFERENCE ON GRAPHIC AND IMAGE PROCESSING (ICGIP 2011)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Proceedings of SPIE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Graphic and Image Processing (ICGIP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 01-02, 2011</td>
</tr>

<tr>
<td valign="top">CL </td><td>Cairo, EGYPT</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion (VC) is a process which modifies the speech signal produced by one source speaker so that it sounds like another target speaker. In this paper the transformation is determined by using equal Arabic utterances from source and target speakers; these utterances are time-aligned using dynamic time warping algorithm. A conversion function based on Gaussian mixture model (GMM) is used for transforming the spectral envelope described by line spectral frequencies (LSF) and the residuals are converted using three residual prediction techniques. We also compare between these techniques in the conversion of some Arabic utterances. The quality of the transformed utterances is measured using subjective and objective evaluations.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Elmanfaloty, Rania; Korany, N.; Youssef, El-Sayed A.] Fac Engn
   Alexandria, Dept Elect Engn, Alexandria, Egypt.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Elmanfaloty, R (reprint author), Fac Engn Alexandria, Dept Elect Engn, Alexandria, Egypt.</td>
</tr>

<tr>
<td valign="top">EM </td><td>Rania.elmanfaloty@aiet.edu.eg; nokorany@hotmail.com</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Elmanfaloty, Rania</display_name>&nbsp;</font></td><td><font size="3">J-7725-2015&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">VL </td><td>8285</td>
</tr>

<tr>
<td valign="top">AR </td><td>82854C</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1117/12.913264</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Optics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000295933400155</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kanisha, J
   <br>Balakrishanan, G</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kanisha, Johnny
   <br>Balakrishanan, G.</td>
</tr>

<tr>
<td valign="top">BE </td><td>Meghanathan, N
   <br>Kaushik, BK
   <br>Nagamalai, D</td>
</tr>

<tr>
<td valign="top">TI </td><td>Speech Transaction for Blinds Using Speech-Text-Speech Conversions</td>
</tr>

<tr>
<td valign="top">SO </td><td>ADVANCES IN COMPUTER SCIENCE AND INFORMATION TECHNOLOGY, PT I</td>
</tr>

<tr>
<td valign="top">SE </td><td>Communications in Computer and Information Science</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>1st International Conference on Computer Science and Information
   Technology</td>
</tr>

<tr>
<td valign="top">CY </td><td>JAN 02-04, 2011</td>
</tr>

<tr>
<td valign="top">CL </td><td>Bangalore, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>STS method; Speech Synthesis; Speech recognition</td>
</tr>

<tr>
<td valign="top">AB </td><td>Effective human computer interaction requires speech recognition and voice response. In this paper we present a concatenative Speech-Text-Speech(STS) system and discuss the issues relevant to the development of perfect human-computer interaction. The new STS system allows the visually impaired people to interact with the computer by giving and getting voice commands. Audio samples are collected from the individuals and then transcribed to text. A text file is used,where the meanings for the transcribed texts are stored. In the synthesis phase,the sentences taken from the text file are converted to speech using unit selection synthesis. The proposed method leads to a perfect human-computer interaction</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kanisha, Johnny] Anna Univ, Tiruchchirappalli, Tamil Nadu, India.
   <br>[Balakrishanan, G.] Indra Ganesan Coll Engn, Tiruchchirappalli, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kanisha, J (reprint author), Anna Univ, Tiruchchirappalli, Tamil Nadu, India.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">VL </td><td>131</td>
</tr>

<tr>
<td valign="top">BP </td><td>43</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000288692500005</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sharifzadeh, HR
   <br>Mcloughlin, IV
   <br>Ahmadi, F</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sharifzadeh, H. R.
   <br>Mcloughlin, I. V.
   <br>Ahmadi, F.</td>
</tr>

<tr>
<td valign="top">BE </td><td>Chua, CK
   <br>Lee, MY
   <br>Guan, YL
   <br>Chen, JP
   <br>Luo, KQ
   <br>Lai, CS
   <br>Kwoh, CK
   <br>Lee, JD
   <br>Chian, KS
   <br>Wu, SL</td>
</tr>

<tr>
<td valign="top">TI </td><td>ARTIFICIAL PHONATION FOR PATIENTS SUFFERING VOICE BOX LESIONS</td>
</tr>

<tr>
<td valign="top">SO </td><td>FIRST INTERNATIONAL SYMPOSIUM ON BIOENGINEERING (ISOB 2011), PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>1st International Symposium on Bioengineering</td>
</tr>

<tr>
<td valign="top">CY </td><td>JAN 19, 2011</td>
</tr>

<tr>
<td valign="top">CL </td><td>Nanyang Technol Univ, Singapore, SINGAPORE</td>
</tr>

<tr>
<td valign="top">HO </td><td>Nanyang Technol Univ</td>
</tr>

<tr>
<td valign="top">DE </td><td>Bionic voice; CELP codec; Laryngectomy; Rehabilitation; Speech
   processing</td>
</tr>

<tr>
<td valign="top">AB </td><td>Laryngectomy patients, and others suffering larynx and voice box deficiencies generally cannot speak anything more than hoarse whispers without using voice prostheses or techniques such as oesophageal speech, trachea-oesophageal puncture (TEP) and electrolarynx. However each of these have particular disadvantages that range from clumsy usage to infection risk, and most importantly all suffer from a distinctly robotic-sounding output. This has recently prompted new work on non-surgical and non-invasive alternatives for such patients. An engineering approach for reconstruction of natural sounding speech from the whisper-like speech produced by patients with vocal tract lesions affecting the glottis, aims to meet the long term goal of speech therapists relating to the rehabilitation of normal sounding speech without recourse to surgery.
   <br>This paper presents a solution for the conversion of whispers to normal sounding fully-phonated speech through the use of a modified CELP codec. We present a novel method for spectral enhancement and formant smoothing during the reconstruction process, using a probability mass-density function to identify reliable formant trajectories in whispers, and apply spectral modifications accordingly. The method relies upon the observation that, whilst the pitch generation mechanism of patients with larynx damage is typically unusable, the remaining components of the speech production apparatus maybe largely unaffected. The approach outlined here allows patients to regain their ability to speak with a more natural sounding voice than through alternative methods, by whispering into an external prosthesis which then recreates and outputs reconstructed speech.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Sharifzadeh, H. R.; Mcloughlin, I. V.; Ahmadi, F.] Nanyang Technol
   Univ, Sch Comp Engn, Singapore 639798, Singapore.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sharifzadeh, HR (reprint author), Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.</td>
</tr>

<tr>
<td valign="top">EM </td><td>hami0003@ntu.edu.sg; mcloughlin@ntu.edu.sg; ahmadi@ntu.edu.sg</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>McLoughlin, Ian</display_name>&nbsp;</font></td><td><font size="3">A-3674-2011&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Sharifzadeh, Hamid Reza</display_name>&nbsp;</font></td><td><font size="3">D-8829-2012&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>McLoughlin, Ian</display_name>&nbsp;</font></td><td><font size="3">0000-0001-7111-2008&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Ahmadi, Farzaneh</display_name>&nbsp;</font></td><td><font size="3">0000-0002-0076-0304&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">BP </td><td>235</td>
</tr>

<tr>
<td valign="top">EP </td><td>243</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.3850/978-981-08-7615-9_RE09</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000287943400027</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Saito, D
   <br>Yamamoto, K
   <br>Minematsu, N
   <br>Hirose, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Saito, Daisuke
   <br>Yamamoto, Keisuke
   <br>Minematsu, Nobuaki
   <br>Hirose, Keikichi</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>One-to-Many Voice Conversion Based on Tensor Representation of Speaker
   Space</td>
</tr>

<tr>
<td valign="top">SO </td><td>12TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION 2011 (INTERSPEECH 2011), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>12th Annual Conference of the
   International-Speech-Communication-Association 2011 (INTERSPEECH 2011)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 27-31, 2011</td>
</tr>

<tr>
<td valign="top">CL </td><td>Florence, ITALY</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; Gaussian mixture model; eigenvoice; tensor analysis;
   Tucker decomposition</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH RECOGNITION; ADAPTATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper describes a novel approach to flexible control of speaker characteristics using tensor representation of speaker space. In voice conversion studies, realization of conversion from/to an arbitrary speaker's voice is one of the important objectives. For this purpose, eigenvoice conversion (EVC) based on an eigenvoice Gaussian mixture model (EV-GMM) was proposed. In the EVC, similarly to speaker recognition approaches, a speaker space is constructed based on GMM, supervectors which are high-dimensional vectors derived by concatenating the mean vectors of each of the speaker GMMs. In the speaker space, each speaker is represented by a small number of weight parameters of eigen-supervectors. In this paper, we revisit construction of the speaker space by introducing the tensor analysis of training data set. In our approach, each speaker is represented as a matrix of which the row and the column respectively correspond to the Gaussian component and the dimension of the mean vector, and the speaker space is derived by the tensor analysis of the set of the matrices. Our approach can solve an inherent problem of supervector representation, and it improves the performance of voice conversion. Experimental results of one-to-many voice conversion demonstrate the effectiveness of the proposed approach.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Saito, Daisuke; Yamamoto, Keisuke; Minematsu, Nobuaki; Hirose,
   Keikichi] Univ Tokyo, Grad Sch Informat Sci &amp; Technol, Tokyo 1138654,
   Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Saito, D (reprint author), Univ Tokyo, Grad Sch Informat Sci &amp; Technol, Tokyo 1138654, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>dsk_saito@gavo.t.u-tokyo.ac.jp; yama@gavo.t.u-tokyo.ac.jp;
   mine@gavo.t.u-tokyo.ac.jp; hirose@gavo.t.u-tokyo.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">BP </td><td>660</td>
</tr>

<tr>
<td valign="top">EP </td><td>663</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000316502200167</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Qiao, Y
   <br>Tong, T
   <br>Minematsu, N</td>
</tr>

<tr>
<td valign="top">AF </td><td>Qiao, Yu
   <br>Tong, Tong
   <br>Minematsu, Nobuaki</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Study on Bag of Gaussian Model with Application to Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>12TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION 2011 (INTERSPEECH 2011), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>12th Annual Conference of the
   International-Speech-Communication-Association 2011 (INTERSPEECH 2011)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 27-31, 2011</td>
</tr>

<tr>
<td valign="top">CL </td><td>Florence, ITALY</td>
</tr>

<tr>
<td valign="top">DE </td><td>GMM; bag of Gaussian model; voice conversion; linear regression</td>
</tr>

<tr>
<td valign="top">AB </td><td>The GMM based mapping techniques proved to be an efficient method to find nonlinear regression function between two spaces, and found success in voice conversion. In these methods, a linear transformation is estimated for each Guassian component, and the final conversion function is a weighted summation of all linear transformations. These linear transformations fit well for the samples near to the center of at least one Guassian component, but may not deal well with the samples far from the centers of all Gaussian distributions. To overcome this problem, this paper proposes Bag of Gaussian Model (BGM). BGM model consists of two types of Gaussian distributions, namely basic and complex distributions. Compared with classical GMM, BGM is adaptive for samples. That is for a sample, BGM can select a set of Guassian distributions which fit the sample best. We develop a data-driven method to construct BGM model and show how to estimate regression function with BGM. We carry out experiment on voice conversion tasks. The experimental results exhibit the usefulness of BGM based methods.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Qiao, Yu; Tong, Tong] Chinese Acad Sci, Shenzhen Inst Adv Technol,
   Shenzhen, Peoples R China.
   <br>[Qiao, Yu] Chinese Univ Hong Kong, Hong Kong, Peoples R China.
   <br>[Tong, Tong] Univ Sci &amp; Technol China, Hefei, Peoples R China.
   <br>[Minematsu, Nobuaki] Univ Tokyo, Bunkyo Ku, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Qiao, Y (reprint author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>yu.qiao@siat.ac.cn; ttravel@mail.ustc.edu.cn; mine@gavo.t.u-tokyo.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">BP </td><td>664</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000316502200168</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Li, L
   <br>Nankaku, Y
   <br>Tokuda, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Li, Lei
   <br>Nankaku, Yoshihiko
   <br>Tokuda, Keiichi</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Bayesian Approach to Voice Conversion Based on GMMs Using Multiple
   Model Structures</td>
</tr>

<tr>
<td valign="top">SO </td><td>12TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION 2011 (INTERSPEECH 2011), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>12th Annual Conference of the
   International-Speech-Communication-Association 2011 (INTERSPEECH 2011)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 27-31, 2011</td>
</tr>

<tr>
<td valign="top">CL </td><td>Florence, ITALY</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; speech synthesis; GMM; model structure</td>
</tr>

<tr>
<td valign="top">AB </td><td>A spectral conversion method using multiple Gaussian Mixture Models (GMMs) based on the Bayesian framework is proposed. A typical spectral conversion framework is based on a GMM. However, in this conventional method, a GMM-appropriate number of mixtures is dependent on the amount of training data, and thus the number of mixtures should be determined beforehand. In the proposed method, the variational Bayesian approach is applied to GMM-based voice conversion, and multiple GMMs are integrated as a single statistical model. Appropriate model structures are stochastically selected for each frame based on the Bayesian frame work.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Li, Lei; Nankaku, Yoshihiko; Tokuda, Keiichi] Nagoya Inst Technol, Dept
   Comp Sci &amp; Engn, Nagoya, Aichi, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Li, L (reprint author), Nagoya Inst Technol, Dept Comp Sci &amp; Engn, Nagoya, Aichi, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>li-lei@sp.nitech.ac.jp; nankaku@sp.nitech.ac.jp; tokuda@nitech.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">BP </td><td>668</td>
</tr>

<tr>
<td valign="top">EP </td><td>671</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000316502200169</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Eslami, M
   <br>Sheikhzadeh, H
   <br>Sayadiyan, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Eslami, Mahdi
   <br>Sheikhzadeh, Hamid
   <br>Sayadiyan, Abolghasem</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Quality Improvement of Voice Conversion Systems Based on Trellis
   Structured Vector Quantization</td>
</tr>

<tr>
<td valign="top">SO </td><td>12TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION 2011 (INTERSPEECH 2011), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>12th Annual Conference of the
   International-Speech-Communication-Association 2011 (INTERSPEECH 2011)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 27-31, 2011</td>
</tr>

<tr>
<td valign="top">CL </td><td>Florence, ITALY</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; trellis; vector quantization</td>
</tr>

<tr>
<td valign="top">AB </td><td>Common voice conversion systems employ a spectral / time domain mapping to convert speech from one speaker to another. The speech quality of conversion methods does not sound natural because the spectral / time domain patterns of two speakers' speech do not match completely. In this paper we propose a method that uses inter-frame (dynamic) characteristics in addition to intra-frame characteristics to find the converted speech frames. This method is based on VQ and uses a trellis structure to find the best conversion function. The proposed method provides high quality converted voice, low computational complexity and small trained model size in contrast to other common methods. Subjective and objective evaluations are employed to demonstrate the superiority of the proposed method over the VQ-based and GMM-based methods.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Eslami, Mahdi] Tamin Telecom Co, Dept Commercial, Tehran, Iran.
   <br>[Sheikhzadeh, Hamid; Sayadiyan, Abolghasem] Amirkabir Univ Technol, Dept
   Elect Engn, Tehran, Iran.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Eslami, M (reprint author), Tamin Telecom Co, Dept Commercial, Tehran, Iran.</td>
</tr>

<tr>
<td valign="top">EM </td><td>meslami@eetd.kntu.ac.ir; hsheikh@aut.ac.ir; ee35as@aut.ac.ir</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">BP </td><td>672</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000316502200170</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Benisty, H
   <br>Malah, D</td>
</tr>

<tr>
<td valign="top">AF </td><td>Benisty, Hadas
   <br>Malah, David</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Conversion using GMAT with Enhanced Global Variance</td>
</tr>

<tr>
<td valign="top">SO </td><td>12TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION 2011 (INTERSPEECH 2011), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>12th Annual Conference of the
   International-Speech-Communication-Association 2011 (INTERSPEECH 2011)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 27-31, 2011</td>
</tr>

<tr>
<td valign="top">CL </td><td>Florence, ITALY</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; GMM; Global Variance (GV)</td>
</tr>

<tr>
<td valign="top">ID </td><td>MAXIMUM-LIKELIHOOD; SPEECH SYNTHESIS; ALGORITHM</td>
</tr>

<tr>
<td valign="top">AB </td><td>The goal of voice conversion is to transform a sentence said by one speaker, to sound as if another speaker had said it. The classical conversion based on a Gaussian Mixture Model and several other schemes suggested since, produce muffled sounding outputs, due to excessive smoothing of the spectral envelopes.
   <br>To reduce the muffling effect, enhancement of the Global Variance (GV) of the spectral features was recently suggested. We propose a different approach for GV enhancement, based on the classical conversion formalized as a GV-constrained minimization. Listening tests show that an improvement in quality is achieved by the proposed approach.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Benisty, Hadas; Malah, David] Technion Israel Inst Technol, Dept Elect
   Engn, IL-32000 Haifa, Israel.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Benisty, H (reprint author), Technion Israel Inst Technol, Dept Elect Engn, IL-32000 Haifa, Israel.</td>
</tr>

<tr>
<td valign="top">EM </td><td>hadasbe@tx.technion.ac.il; malah@ee.technion.ac.il</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">BP </td><td>676</td>
</tr>

<tr>
<td valign="top">EP </td><td>679</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000316502200171</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Godoy, E
   <br>Rosec, O
   <br>Chonavel, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Godoy, Elizabeth
   <br>Rosec, Olivier
   <br>Chonavel, Thierry</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Spectral Envelope Transformation using DFW and Amplitude Scaling for
   Voice Conversion with Parallel or Nonparallel Corpora</td>
</tr>

<tr>
<td valign="top">SO </td><td>12TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION 2011 (INTERSPEECH 2011), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>12th Annual Conference of the
   International-Speech-Communication-Association 2011 (INTERSPEECH 2011)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 27-31, 2011</td>
</tr>

<tr>
<td valign="top">CL </td><td>Florence, ITALY</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; DFW; spectral transformation</td>
</tr>

<tr>
<td valign="top">AB </td><td>Dynamic Frequency Warping (DFW) offers an appealing alternative to GMM-based voice conversion, which suffers from "over-smoothing" that hinders speech quality. However, to adjust spectral power after DFW, previous work returns to GMM-transformation. This paper proposes a more effective DFW with amplitude scaling (DFWA) that functions on the acoustic class level and is independent of GMM-transformation. The amplitude scaling compares average target and warped source log amplitude spectra for each class. DFWA outperforms the GMM in terms of both speech quality and timbre conversion, as confirmed in objective and subjective testing. Moreover, DFWA performance is equivalent using parallel or nonparallel corpora.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Godoy, Elizabeth; Rosec, Olivier] Orange Labs, Lannion, France.
   <br>[Chonavel, Thierry] Telecom Bretagne, Brest, France.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Godoy, E (reprint author), Orange Labs, Lannion, France.</td>
</tr>

<tr>
<td valign="top">EM </td><td>godoyec@gmail.com; olivier.rosec@orange-ftgroup.com;
   thierry.chonavel@telecom-bretagne.eu</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">BP </td><td>680</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000316502200172</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Suzuki, Y
   <br>Aikawa, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Suzuki, Yukiko
   <br>Aikawa, Kiyoaki</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Towards Voice-Input Symbolic Pattern Retrieval using Parameter-Based
   Search</td>
</tr>

<tr>
<td valign="top">SO </td><td>12TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION 2011 (INTERSPEECH 2011), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>12th Annual Conference of the
   International-Speech-Communication-Association 2011 (INTERSPEECH 2011)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 27-31, 2011</td>
</tr>

<tr>
<td valign="top">CL </td><td>Florence, ITALY</td>
</tr>

<tr>
<td valign="top">DE </td><td>pattern retrieval; emotional feature; parameter-based; similarity
   measure; voice-input</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper proposes a symbolic pattern retrieval method using emotional feature vectors. Queries and symbolic patterns are represented by emotional vectors composed of eight numerical parameters. Since the proposed method uses numerical vectors close to raw data instead of recognized text, the information loss by data conversion is small. This point is advantageous compared with conventional text-based search such as recent spoken document retrieval approach. Five similarity measures were compared on a test collection. The cos similarity and the Euclidean distance showed the best performance among five similarity measures. OOV analysis clarified several problems for achieving voice-input symbolic pattern retrieval.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Suzuki, Yukiko; Aikawa, Kiyoaki] Tokyo Univ Technol, Sch Media Sci,
   Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Suzuki, Y (reprint author), Tokyo Univ Technol, Sch Media Sci, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>bovo444@gmail.com; aik@media.teu.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">BP </td><td>1128</td>
</tr>

<tr>
<td valign="top">EP </td><td>1131</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000316502200284</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Pilkington, NCV
   <br>Zen, H
   <br>Gales, MJF</td>
</tr>

<tr>
<td valign="top">AF </td><td>Pilkington, Nicholas C. V.
   <br>Zen, Heiga
   <br>Gales, Mark J. F.</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Gaussian Process Experts for Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>12TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION 2011 (INTERSPEECH 2011), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>12th Annual Conference of the
   International-Speech-Communication-Association 2011 (INTERSPEECH 2011)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 27-31, 2011</td>
</tr>

<tr>
<td valign="top">CL </td><td>Florence, ITALY</td>
</tr>

<tr>
<td valign="top">DE </td><td>Gaussian processes; GMM; voice conversion</td>
</tr>

<tr>
<td valign="top">AB </td><td>Conventional approaches to voice conversion typically use a GMM to represent the joint probability density of source and target features. This model is then used to perform spectral conversion between speakers. This approach is reasonably effective but can be prone to overfitting and oversmoothing of the target spectra. This paper proposes an alternative scheme that uses a collection of Gaussian process experts to perform the spectral conversion. Gaussian processes are robust to overfitting and oversmoothing and can predict the target spectra more accurately. Experimental results indicate that the objective performance of voice conversion can be improved using the proposed approach.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Pilkington, Nicholas C. V.; Zen, Heiga; Gales, Mark J. F.] Toshiba Res
   Europe Ltd, Cambridge Res Lab, Cambridge, England.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Pilkington, NCV (reprint author), Toshiba Res Europe Ltd, Cambridge Res Lab, Cambridge, England.</td>
</tr>

<tr>
<td valign="top">EM </td><td>nicholas.pilkington@cl.cam.ac.uk; heiga.zen@crl.toshiba.co.uk;
   mark.gales@crl.toshiba.co.uk</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">BP </td><td>2772</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000316502201182</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hattori, N
   <br>Toda, T
   <br>Kawai, H
   <br>Saruwatari, H
   <br>Shikano, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hattori, Nobuhiko
   <br>Toda, Tomoki
   <br>Kawai, Hisashi
   <br>Saruwatari, Hiroshi
   <br>Shikano, Kiyohiro</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Speaker-Adaptive Speech Synthesis Based on Eigenvoice Conversion and
   Language-Dependent Prosodic Conversion in Speech-to-Speech Translation</td>
</tr>

<tr>
<td valign="top">SO </td><td>12TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION 2011 (INTERSPEECH 2011), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>12th Annual Conference of the
   International-Speech-Communication-Association 2011 (INTERSPEECH 2011)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 27-31, 2011</td>
</tr>

<tr>
<td valign="top">CL </td><td>Florence, ITALY</td>
</tr>

<tr>
<td valign="top">DE </td><td>speech-to-speech translation; speech synthesis; speaker adaptation;
   eigenvoice conversion; prosodic conversion</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper describes a novel approach based on voice conversion (VC) to speaker-adaptive speech synthesis for speech-to-speech translation. Voice quality of translated speech in an output language is usually different from that of an input speaker of the translation system since a text-to-speech system is developed with another speaker's voices in the output language. To render the input speaker's voice quality in the translated speech, we propose a voice quality control method based on one-to-many eigenvoice conversion (EVC) and language-dependent prosodic conversion. Spectral parameters of the translated speech are effectively converted by one-to-many EVC enabling unsupervised speaker adaptation. Moreover, prosodic parameters are modified considering their global differences between the input and output languages. The effectiveness of the proposed method is confirmed by experimental evaluations on cross-lingual VC among Japanese, English, and Chinese.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Hattori, Nobuhiko; Toda, Tomoki; Saruwatari, Hiroshi; Shikano,
   Kiyohiro] Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara, Japan.
   <br>[Toda, Tomoki; Kawai, Hisashi] Natl Inst Informat &amp; Commun Technol,
   Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hattori, N (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>tomoki@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">BP </td><td>2780</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000316502201184</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Perez, J
   <br>Bonafonte, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Perez, Javier
   <br>Bonafonte, Antonio</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Adding Glottal Source Information to Intra-lingual Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>12TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION 2011 (INTERSPEECH 2011), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>12th Annual Conference of the
   International-Speech-Communication-Association 2011 (INTERSPEECH 2011)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 27-31, 2011</td>
</tr>

<tr>
<td valign="top">CL </td><td>Florence, ITALY</td>
</tr>

<tr>
<td valign="top">DE </td><td>glottal modeling; voice conversion; speech analysis; speech synthesis</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper studies the inclusion of glottal source characteristics in voice conversion (VC) systems. We use source/filter decomposition to parametrize the vocal tract using LSF, the glottal source using the LF model, and the aspiration noise using amplitude-modulated high-pass filtered AWGN noise. To evaluate the impact of this new parametrization in VC, we use a reference conversion system that estimates a linear transformation function using a joint target/source model obtained with CART and GMM. The reference system is based on the LPC model, uses LSF to represent the vocal tract and a selection technique for the residual. We use the reference algorithm to build a VC system for each of the three parameter sets. We compared both parametrizations in the framework of an intralingual voice conversion task in Spanish. The results show that the new source/filter representation clearly improves the overall performance, both in terms of speaker identity transformation and voice quality.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Perez, Javier; Bonafonte, Antonio] Univ Politecn Cataluna, Barcelona,
   Spain.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Perez, J (reprint author), Univ Politecn Cataluna, Barcelona, Spain.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">BP </td><td>2784</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000316502201185</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Degottex, G
   <br>Roebel, A
   <br>Rodet, X</td>
</tr>

<tr>
<td valign="top">AF </td><td>Degottex, Gilles
   <br>Roebel, Axel
   <br>Rodet, Xavier</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>PITCH TRANSPOSITION AND BREATHINESS MODIFICATION USING A GLOTTAL SOURCE
   MODEL AND ITS ADAPTED VOCAL-TRACT FILTER</td>
</tr>

<tr>
<td valign="top">SO </td><td>2011 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 22-27, 2011</td>
</tr>

<tr>
<td valign="top">CL </td><td>Prague Congress Ctr, Prague, CZECH REPUBLIC</td>
</tr>

<tr>
<td valign="top">HO </td><td>Prague Congress Ctr</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice transformation; pitch transposition; breathiness; glottal model;
   vocal-tract filter</td>
</tr>

<tr>
<td valign="top">AB </td><td>The transformation of the voiced segments of a speech recording has many applications such as expressivity synthesis or voice conversion. This paper addresses the pitch transposition and the modification of breathiness by means of an analytic description of the deterministic component of the voice source, a glottal model. Whereas this model is dedicated to voice production, most of the current methods can be applied to any pseudo-periodic signals. Using the described method, the synthesized voice is thus expected to better preserve some naturalness compared to a more generic method. Using preference tests, it is shown that this method is preferred for important pitch transposition (e. g. one octave) compared to two state of the art methods. Additionally, it is shown that the breathiness of two male utterances can be controlled.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Degottex, Gilles; Roebel, Axel; Rodet, Xavier] IRCAM, CNRS, UMR9912,
   STMS,Anal Synth Team, F-75004 Paris, France.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Degottex, G (reprint author), IRCAM, CNRS, UMR9912, STMS,Anal Synth Team, 1 Pl Igor Stravinsky, F-75004 Paris, France.</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Degottex, Gilles</display_name>&nbsp;</font></td><td><font size="3">0000-0003-1346-9919&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>10</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>10</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">BP </td><td>5128</td>
</tr>

<tr>
<td valign="top">EP </td><td>5131</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering; Imaging Science &amp; Photographic Technology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000296062405184</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Lanchantin, P
   <br>Rodet, X</td>
</tr>

<tr>
<td valign="top">AF </td><td>Lanchantin, Pierre
   <br>Rodet, Xavier</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>OBJECTIVE EVALUATION OF THE DYNAMIC MODEL SELECTION METHOD FOR SPECTRAL
   VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2011 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 22-27, 2011</td>
</tr>

<tr>
<td valign="top">CL </td><td>Prague Congress Ctr, Prague, CZECH REPUBLIC</td>
</tr>

<tr>
<td valign="top">HO </td><td>Prague Congress Ctr</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; Gaussian Mixture Regression; model selection</td>
</tr>

<tr>
<td valign="top">ID </td><td>TRANSFORMATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Spectral voice conversion is usually performed using a single model selected in order to represent a tradeoff between goodness of fit and complexity. Recently, we proposed a new method for spectral voice conversion, called Dynamic Model Selection (DMS), in which we assumed that the model topology may change over time, depending on the source acoustic features. In this method a set of models with increasing complexity is considered during the conversion of a source speech signal into a target speech signal. During the conversion, the best model is dynamically selected among the models in the set, according to the acoustical features of each source frame. In this paper, we present an objective evaluation demonstrating that this new method improves the conversion by reducing the transformation error compared to methods based on an single model.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Lanchantin, Pierre; Rodet, Xavier] IRCAM, CNRS, UMR9912, STMS,Anal
   Synth Team, F-75004 Paris, France.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Lanchantin, P (reprint author), IRCAM, CNRS, UMR9912, STMS,Anal Synth Team, 1 Pl Igor Stravinsky, F-75004 Paris, France.</td>
</tr>

<tr>
<td valign="top">EM </td><td>lanchantin@ircam.fr; rod@ircam.fr</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>4</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">BP </td><td>5132</td>
</tr>

<tr>
<td valign="top">EP </td><td>5135</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering; Imaging Science &amp; Photographic Technology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000296062405185</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Azarov, E
   <br>Petrovsky, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Azarov, Elias
   <br>Petrovsky, Alexander</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>REAL-TIME VOICE CONVERSION BASED ON INSTANTANEOUS HARMONIC PARAMETERS</td>
</tr>

<tr>
<td valign="top">SO </td><td>2011 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 22-27, 2011</td>
</tr>

<tr>
<td valign="top">CL </td><td>Prague Congress Ctr, Prague, CZECH REPUBLIC</td>
</tr>

<tr>
<td valign="top">HO </td><td>Prague Congress Ctr</td>
</tr>

<tr>
<td valign="top">DE </td><td>Real-time voice conversion; parametric speech representation</td>
</tr>

<tr>
<td valign="top">AB </td><td>The paper presents a voice conversion framework that can be used in real-time applications. The conversion technique is based on hybrid (deterministic/stochastic) parametric speech representation. The conversion approach has been tested in two modifications for narrow-band and wide-band speech signals. Though the real-time requirement adds some significant limitations (frame by frame processing) the approach provides high quality of the reconstructed speech and recognizability of the target speaker's identity due to improved feature mapping. The proposed solution is embedded in a mobile communication system as an entertainment service.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Azarov, Elias; Petrovsky, Alexander] Belarusian State Univ Informat &amp;
   Radioelect, Dept Comp Engn, Minsk, BELARUS.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Azarov, E (reprint author), Belarusian State Univ Informat &amp; Radioelect, Dept Comp Engn, Minsk, BELARUS.</td>
</tr>

<tr>
<td valign="top">EM </td><td>palex@bsuir.by</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">BP </td><td>5140</td>
</tr>

<tr>
<td valign="top">EP </td><td>5143</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering; Imaging Science &amp; Photographic Technology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000296062405187</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Spitz, A
   <br>Moore, AA
   <br>Papaleontiou, M
   <br>Granieri, E
   <br>Turner, BJ
   <br>Reid, MC</td>
</tr>

<tr>
<td valign="top">AF </td><td>Spitz, Aerin
   <br>Moore, Alison A.
   <br>Papaleontiou, Maria
   <br>Granieri, Evelyn
   <br>Turner, Barbara J.
   <br>Reid, M. Carrington</td>
</tr>

<tr>
<td valign="top">TI </td><td>Primary care providers' perspective on prescribing opioids to older
   adults with chronic non-cancer pain: A qualitative study</td>
</tr>

<tr>
<td valign="top">SO </td><td>BMC GERIATRICS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">AB </td><td>Background: The use of opioid medications as treatment for chronic non-cancer pain remains controversial. Little information is currently available regarding healthcare providers' attitudes and beliefs about this practice among older adults. This study aimed to describe primary care providers' experiences and attitudes towards, as well as perceived barriers and facilitators to prescribing opioids as a treatment for chronic pain among older adults.
   <br>Methods: Six focus groups were conducted with a total of 23 physicians and three nurse practitioners from two academically affiliated primary care practices and three community health centers located in New York City. Focus groups were audiotape recorded and transcribed. The data were analyzed using directed content analysis; NVivo software was used to assist in the quantification of identified themes.
   <br>Results: Most participants (96%) employed opioids as therapy for some of their older patients with chronic pain, although not as first-line therapy. Providers cited multiple barriers, including fear of causing harm, the subjectivity of pain, lack of education, problems converting between opioids, and stigma. New barriers included patient/family member reluctance to try an opioid and concerns about opioid abuse by family members/caregivers. Studies confirming treatment benefit, validated tools for assessing risk and/or dosing for comorbidities, improved conversion methods, patient education, and peer support could facilitate opioid prescribing. Participants voiced greater comfort using opioids in the setting of delivering palliative or hospice care versus care of patients with chronic pain, and expressed substantial frustration managing chronic pain.
   <br>Conclusions: Providers perceive multiple barriers to prescribing opioids to older adults with chronic pain, and use these medications cautiously. Establishing the long-term safety and efficacy of these medications, generating improved prescribing methods, and implementing provider and patient educational interventions could help to improve the management of chronic pain in later life.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Spitz, Aerin] Virginia Mason Med Ctr, Dept Internal Med, Seattle, WA
   98101 USA.
   <br>[Moore, Alison A.] Univ Calif Los Angeles, David Geffen Sch Med, Div
   Geriatr, Los Angeles, CA 90095 USA.
   <br>[Papaleontiou, Maria] Univ Michigan, Sch Med, Div Endocrinol, Ann Arbor,
   MI USA.
   <br>[Granieri, Evelyn] Columbia Univ, Div Geriatr Med &amp; Aging, New York, NY
   USA.
   <br>[Turner, Barbara J.] Univ Penn, Dept Internal Med, Philadelphia, PA
   19104 USA.
   <br>[Reid, M. Carrington] Weill Cornell Med Coll, Div Geriatr &amp; Gerontol,
   New York, NY USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Reid, MC (reprint author), Weill Cornell Med Coll, Div Geriatr &amp; Gerontol, New York, NY USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>mcr2004@med.cornell.edu</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Papaleontiou, Maria</display_name>&nbsp;</font></td><td><font size="3">N-8408-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Reid, Cary</display_name>&nbsp;</font></td><td><font size="3">0000-0001-8117-662X&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Papaleontiou, Maria</display_name>&nbsp;</font></td><td><font size="3">0000-0003-2276-0046&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>57</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>57</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">VL </td><td>11</td>
</tr>

<tr>
<td valign="top">AR </td><td>35</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1186/1471-2318-11-35</td>
</tr>

<tr>
<td valign="top">SC </td><td>Geriatrics &amp; Gerontology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000208731700035</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Brown, D
   <br>Macpherson, T
   <br>Ward, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Brown, David
   <br>Macpherson, Tom
   <br>Ward, Jamie</td>
</tr>

<tr>
<td valign="top">TI </td><td>Seeing with sound? Exploring different characteristics of a
   visual-to-auditory sensory substitution device</td>
</tr>

<tr>
<td valign="top">SO </td><td>PERCEPTION</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">ID </td><td>PATTERN-RECOGNITION; MUSICAL FORM; VISION; BLIND; LOCALIZATION;
   BRIGHTNESS; PERCEPTION; IMAGES; SYSTEM</td>
</tr>

<tr>
<td valign="top">AB </td><td>Sensory substitution devices convert live visual images into auditory signals, for example with a web camera (to record the images), a computer (to perform the conversion) and headphones (to listen to the sounds). In a series of three experiments, the performance of one such device ('The vOICe') was assessed under various conditions on blindfolded sighted participants. The main task that we used involved identifying and locating objects placed on a table by holding a webcam (like a flashlight) or wearing it on the head (like a miner's light). Identifying objects on a table was easier with a hand-held device, but locating the objects was easier with a head-mounted device. Brightness converted into loudness was less effective than the reverse contrast (dark being loud), suggesting that performance under these conditions (natural indoor lighting, novice users) is related more to the properties of the auditory signal (ie the amount of noise in it) than the cross-modal association between loudness and brightness. Individual differences in musical memory (detecting pitch changes in two sequences of notes) was related to the time taken to identify or recognise objects, but individual differences in self-reported vividness of visual imagery did not reliably predict performance across the experiments. In general, the results suggest that the auditory characteristics of the device may be more important for initial learning than visual associations.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ward, Jamie] Univ Sussex, Sackler Ctr Consciousness Sci, Brighton BN1
   9QH, E Sussex, England.
   <br>[Macpherson, Tom] Univ Sussex, Sch Psychol, Brighton BN1 9QH, E Sussex,
   England.
   <br>[Brown, David] Univ London, Res Ctr Psychol, London, England.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ward, J (reprint author), Univ Sussex, Sackler Ctr Consciousness Sci, Brighton BN1 9QH, E Sussex, England.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jamiew@sussex.ac.uk</td>
</tr>

<tr xmlns:date="http://exslt.org/dates-and-times">
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Ward, Jamie</display_name>&nbsp;</font></td><td><font size="3">F-3609-2011&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Ward, Jamie</display_name>&nbsp;</font></td><td><font size="3">0000-0001-7007-1902&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>23</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>23</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">VL </td><td>40</td>
</tr>

<tr>
<td valign="top">IS </td><td>9</td>
</tr>

<tr>
<td valign="top">BP </td><td>1120</td>
</tr>

<tr>
<td valign="top">EP </td><td>1135</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1068/p6952</td>
</tr>

<tr>
<td valign="top">SC </td><td>Ophthalmology; Psychology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000297829500007</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Francisco, V
   <br>Gervas, P
   <br>Peinado, F</td>
</tr>

<tr>
<td valign="top">AF </td><td>Francisco, Virginia
   <br>Gervas, Pablo
   <br>Peinado, Federico</td>
</tr>

<tr>
<td valign="top">TI </td><td>Ontological reasoning for improving the treatment of emotions in text</td>
</tr>

<tr>
<td valign="top">SO </td><td>KNOWLEDGE AND INFORMATION SYSTEMS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Affective computing; Emotional annotation; Reasoning; Ontologies</td>
</tr>

<tr>
<td valign="top">AB </td><td>With the advent of affective computing, the task of adequately identifying, representing and processing the emotional connotations of text has acquired importance. Two problems facing this task are addressed in this paper: the composition of sentence emotion from word emotion, and a representation of emotion that allows easy conversion between existing computational representations. The emotion of a sentence of text should be derived by composition of the emotions of the words in the sentence, but no method has been proposed so far to model this compositionality. Of the various existing approaches for representing emotions, some are better suited for some problems and some for others, but there is no easy way of converting from one to another. This paper presents a system that addresses these two problems by reasoning with two ontologies implemented with Semantic Web technologies: one designed to represent word dependency relations within a sentence, and one designed to represent emotions. The ontology of word dependency relies on roles to represent the way emotional contributions project over word dependencies. By applying automated classification of mark-up results in terms of the emotion ontology the system can interpret unrestricted input in terms of a restricted set of concepts for which particular rules are provided. The rules applied at the end of the process provide configuration parameters for a system for emotional voice synthesis.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Francisco, Virginia; Gervas, Pablo; Peinado, Federico] Univ Complutense
   Madrid, Dept Inteligencia Artificial &amp; Ingn Software, Fac Informat,
   Madrid, Spain.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Francisco, V (reprint author), Univ Complutense Madrid, Dept Inteligencia Artificial &amp; Ingn Software, Fac Informat, Madrid, Spain.</td>
</tr>

<tr>
<td valign="top">EM </td><td>virginia@fdi.ucm.es; pgervas@sip.ucm.es; email@federicopeinado.com</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Peinado, Federico</display_name>&nbsp;</font></td><td><font size="3">B-2423-2009&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Gervas, Pablo</display_name>&nbsp;</font></td><td><font size="3">L-9405-2014&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Peinado, Federico</display_name>&nbsp;</font></td><td><font size="3">0000-0002-8893-0020&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Gervas, Pablo</display_name>&nbsp;</font></td><td><font size="3">0000-0003-4906-9837&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>10</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>10</td>
</tr>

<tr>
<td valign="top">PD </td><td>DEC</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">VL </td><td>25</td>
</tr>

<tr>
<td valign="top">IS </td><td>3</td>
</tr>

<tr>
<td valign="top">BP </td><td>421</td>
</tr>

<tr>
<td valign="top">EP </td><td>443</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/s10115-010-0320-1</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000284486700001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kittisuwan, P
   <br>Chanwimaluan, T
   <br>Marukatat, S
   <br>Asdornwised, W</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kittisuwan, Pichid
   <br>Chanwimaluan, Thitiporn
   <br>Marukatat, Sanparith
   <br>Asdornwised, Widhyakorn</td>
</tr>

<tr>
<td valign="top">TI </td><td>IMAGE AND AUDIO-SPEECH DENOISING BASED ON HIGHER-ORDER STATISTICAL
   MODELING OF WAVELET COEFFICIENTS AND LOCAL VARIANCE ESTIMATION</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERNATIONAL JOURNAL OF WAVELETS MULTIRESOLUTION AND INFORMATION
   PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Pearson Type VII random vectors; image denoising; wavelet transforms</td>
</tr>

<tr>
<td valign="top">ID </td><td>BIVARIATE SHRINKAGE; RANDOM VECTORS; TRANSFORM; DOMAIN; NOISE</td>
</tr>

<tr>
<td valign="top">AB </td><td>At first, this paper is concerned with wavelet-based image denoising using Bayesian technique. In conventional denoising process, the parameters of probability density function (PDF) are usually calculated from the first few moments, mean and variance. In the first part of our work, a new image denoising algorithm based on Pearson Type VII random vectors is proposed. This PDF is used because it allows higher-order moments to be incorporated into the noiseless wavelet coefficients' probabilistic model. One of the cruxes of the Bayesian image denoising algorithms is to estimate the variance of the clean image. Here, maximum a posterior (MAP) approach is employed for not only noiseless wavelet-coefficient estimation but also local observed variance acquisition. For the local observed variance estimation, the selection of noisy wavelet-coefficient model, either a Laplacian or a Gaussian distribution, is based upon the corrupted noise power where Gamma distribution is used as a prior for the variance. Evidently, our selection of prior is motivated by analytical and computational tractability. In our experiments, our proposed method gives promising denoising results with moderate complexity. Eventually, our image denoising method can be simply extended to audio/speech processing by forming matrix representation whose rows are formed by time segments of digital speech waveforms. This way, the use of our image denoising methods can be exploited to improve the performance of various audio/speech tasks, e.g., denoised enhancement of voice activity detection to capture voiced speech, significantly needed for speech coding and voice conversion applications. Moreover, one of the voice abnormality detections, called oropharyngeal dysphagia classification, is also required denoising method to improve the signal quality in elderly patients. We provide simple speech examples to demonstrate the prospects of our techniques.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kittisuwan, Pichid; Asdornwised, Widhyakorn] Chulalongkorn Univ, Fac
   Engn, Dept Elect Engn, Bangkok 10330, Thailand.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kittisuwan, P (reprint author), Chulalongkorn Univ, Fac Engn, Dept Elect Engn, Bangkok 10330, Thailand.</td>
</tr>

<tr>
<td valign="top">EM </td><td>pichidkit@yahoo.com; thitiporn.chanwimaluang@nectec.or.th;
   sanparith.marukatat@nectec.or.th; widhyakorn.a@chula.ac.th</td>
</tr>

<tr>
<td valign="top">TC </td><td>11</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>11</td>
</tr>

<tr>
<td valign="top">PD </td><td>NOV</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">VL </td><td>8</td>
</tr>

<tr>
<td valign="top">IS </td><td>6</td>
</tr>

<tr>
<td valign="top">BP </td><td>987</td>
</tr>

<tr>
<td valign="top">EP </td><td>1017</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1142/S0219691310003808</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Mathematics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000284648400011</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Yeh, JF
   <br>Hsu, CH</td>
</tr>

<tr>
<td valign="top">AF </td><td>Yeh, Jui-Feng
   <br>Hsu, Chung-Hua</td>
</tr>

<tr>
<td valign="top">TI </td><td>SUB-SYLLABLE SEGMENT-BASED VOICE CONVERSION USING SPECTRAL BLOCK
   CLUSTERING TRANSFORMATION FUNCTIONS</td>
</tr>

<tr>
<td valign="top">SO </td><td>JOURNAL OF THE CHINESE INSTITUTE OF ENGINEERS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>speech signal processing; voice conversion; voice clustering; voice
   transformation</td>
</tr>

<tr>
<td valign="top">ID </td><td>UNIT SELECTION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a novel framework for voice conversion based on sub-syllable spectral block clustering transformation functions. The speech signal is first transferred to a spectrum by Fast Fourier transform. A sonority measure is used to extract sub-syllable segments from input utterances by computing the energy concentration measure among frequency components. According to the syllable structure of Mandarin, Hidden Markov Model based syllable clustering is used to deal with the variety among different syllables. Dynamic programming is applied to align the spectral blocks of the parallel corpus to constrain the mapping between the spectral unit of the source speaker and that of the listener speaker under the constraint that mapped unities should be constrained to the same sub-syllable and sub-band in the Mel-scale filter bank. A content based image retrieval algorithm is employed to find the target spectral block in the transformation phase. This paper illustrates voice conversion by spectral block transformation that transfers the speech signal of the source speaker to that of the listener. Experimental results show that the proposed method is effective in voice conversion, and the discrimination with regard to speaker identification is better than with traditional approaches. However, there remain additional noises, especially in high frequency components, which reduce the signal quality carried in the transformation phase, due to the fact that speech is not smooth.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Yeh, Jui-Feng; Hsu, Chung-Hua] Natl Chiayi Univ, Dept Comp Sci &amp;
   Informat Engn, Chiayi 60004, Taiwan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Yeh, JF (reprint author), Natl Chiayi Univ, Dept Comp Sci &amp; Informat Engn, Chiayi 60004, Taiwan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ralph@mail.ncyu.edu.tw</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PD </td><td>NOV</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">VL </td><td>33</td>
</tr>

<tr>
<td valign="top">IS </td><td>7</td>
</tr>

<tr>
<td valign="top">BP </td><td>1059</td>
</tr>

<tr>
<td valign="top">EP </td><td>1067</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1080/02533839.2010.9671694</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000285082300012</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Doi, H
   <br>Nakamura, K
   <br>Toda, T
   <br>Saruwatari, H
   <br>Shikano, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Doi, Hironori
   <br>Nakamura, Keigo
   <br>Toda, Tomoki
   <br>Saruwatari, Hiroshi
   <br>Shikano, Kiyohiro</td>
</tr>

<tr>
<td valign="top">TI </td><td>Esophageal Speech Enhancement Based on Statistical Voice Conversion with
   Gaussian Mixture Models</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEICE TRANSACTIONS ON INFORMATION AND SYSTEMS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>laryngectomees; esophageal speech; speech enhancement; voice conversion;
   eigenvoice conversion</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a novel method of enhancing esophageal speech using statistical voice conversion. Esophageal speech is one of the alternative speaking methods for laryngectomees. Although it doesn't require any external devices, generated voices usually sound unnatural compared with normal speech. To improve the intelligibility and naturalness of esophageal speech, we propose a voice conversion method from esophageal speech into normal speech. A spectral parameter and excitation parameters of target normal speech are separately estimated from a spectral parameter of the esophageal speech based on Gaussian mixture models. The experimental results demonstrate that the proposed method yields significant improvements in intelligibility and naturalness. We also apply one-to-many eigenvoice conversion to esophageal speech enhancement to make it possible to flexibly control the voice quality of enhanced speech.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Doi, Hironori; Nakamura, Keigo; Toda, Tomoki; Saruwatari, Hiroshi;
   Shikano, Kiyohiro] Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Ikoma
   6300192, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Doi, H (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Ikoma 6300192, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>hironori-d@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>11</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>11</td>
</tr>

<tr>
<td valign="top">PD </td><td>SEP</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">VL </td><td>E93D</td>
</tr>

<tr>
<td valign="top">IS </td><td>9</td>
</tr>

<tr>
<td valign="top">BP </td><td>2472</td>
</tr>

<tr>
<td valign="top">EP </td><td>2482</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1587/transinf.E93.D.2472</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000282245100014</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nose, T
   <br>Ota, Y
   <br>Kobayashi, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nose, Takashi
   <br>Ota, Yuhei
   <br>Kobayashi, Takao</td>
</tr>

<tr>
<td valign="top">TI </td><td>HMM-Based Voice Conversion Using Quantized F0 Context</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEICE TRANSACTIONS ON INFORMATION AND SYSTEMS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; HMM-based speech synthesis; F0 quantization; prosodic
   context; nonparallel data</td>
</tr>

<tr>
<td valign="top">AB </td><td>We propose a segment-based voice conversion technique using hidden Markov model (HMM)-based speech synthesis with nonparallel training data. In the proposed technique, the phoneme information with durations and a quantized F0 contour are extracted from the input speech of a source speaker, and are transmitted to a synthesis part. In the synthesis part, the quantized F0 symbols are used as prosodic context. A phonetically and prosodically context-dependent label sequence is generated from the transmitted phoneme and the F0 symbols. Then, converted speech is generated from the label sequence with durations using the target speaker's pre-trained context-dependent HMMs. In the model training, the models of the source and target speakers can be trained separately, hence there is no need to prepare parallel speech data of the source and target speakers. Objective and subjective experimental results show that the segment-based voice conversion with phonetic and prosodic contexts works effectively even if the parallel speech data is not available.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nose, Takashi; Ota, Yuhei; Kobayashi, Takao] Tokyo Inst Technol,
   Interdisciplinary Grad Sch Sci &amp; Engn, Yokohama, Kanagawa 2268502, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nose, T (reprint author), Tokyo Inst Technol, Interdisciplinary Grad Sch Sci &amp; Engn, Yokohama, Kanagawa 2268502, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>takashi.nose@ip.titech.ac.jp; yuhei.ota@ip.titech.ac.jp;
   takao.kobayashi@ip.titech.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>7</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>7</td>
</tr>

<tr>
<td valign="top">PD </td><td>SEP</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">VL </td><td>E93D</td>
</tr>

<tr>
<td valign="top">IS </td><td>9</td>
</tr>

<tr>
<td valign="top">BP </td><td>2483</td>
</tr>

<tr>
<td valign="top">EP </td><td>2490</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1587/transinf.E93.D.2483</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000282245100015</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ohtani, Y
   <br>Toda, T
   <br>Saruwatari, H
   <br>Shikano, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ohtani, Yamato
   <br>Toda, Tomoki
   <br>Saruwatari, Hiroshi
   <br>Shikano, Kiyohiro</td>
</tr>

<tr>
<td valign="top">TI </td><td>Improvements of the One-to-Many Eigenvoice Conversion System</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEICE TRANSACTIONS ON INFORMATION AND SYSTEMS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>speech synthesis; eigenvoice conversion; STRAIGHT mixed excitation;
   global variance; adaptive training</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION; ADAPTATION; SPEECH</td>
</tr>

<tr>
<td valign="top">AB </td><td>We have developed a one-to-many eigenvoice conversion (EVC) system that allows us to convert a single source speaker's voice into an arbitrary target speaker's voice using an eigenvoice Gaussian mixture model (EV-GMM). This system is capable of effectively building a conversion model for an arbitrary target speaker by adapting the EV-GMM using only a small amount of speech data uttered by the target speaker in a text-independent manner. However, the conversion performance is still insufficient for the following reasons: I) the excitation signal is not precisely modeled; 2) the oversmoothing of the converted spectrum causes muffled sounds in converted speech; and 3) the conversion model is affected by redundant acoustic variations among a lot of pre-stored target speakers used for building the EV-GMM. In order to address these problems, we apply the following promising techniques to one-to-many EVC: 1) mixed excitation; 2) a conversion algorithm considering global variance; and 3) adaptive training of the EV-GMM. The experimental results demonstrate that the conversion performance of one-to-many EVC is significantly improved by integrating all of these techniques into the one-to-many EVC system.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ohtani, Yamato; Toda, Tomoki; Saruwatari, Hiroshi; Shikano, Kiyohiro]
   Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Ikoma 6300192, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ohtani, Y (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Ikoma 6300192, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>yamato-o@is.naist.jp; tomoki@is.naist.jp; sawatari@is.naist.jp;
   shikano@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PD </td><td>SEP</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">VL </td><td>E93D</td>
</tr>

<tr>
<td valign="top">IS </td><td>9</td>
</tr>

<tr>
<td valign="top">BP </td><td>2491</td>
</tr>

<tr>
<td valign="top">EP </td><td>2499</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1587/transinf.E93.D.2491</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000282245100016</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Koukiadaki, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Koukiadaki, Aristea</td>
</tr>

<tr>
<td valign="top">TI </td><td>The establishment and operation of information and consultation of
   employees' arrangements in a capability-based framework</td>
</tr>

<tr>
<td valign="top">SO </td><td>ECONOMIC AND INDUSTRIAL DEMOCRACY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>capability approach; employee voice; labour law</td>
</tr>

<tr>
<td valign="top">ID </td><td>UK; PARTNERSHIP</td>
</tr>

<tr>
<td valign="top">AB </td><td>Drawing on an evaluative framework inspired by the capability approach, the article assesses the nature and implications of company responses to the British legislation transposing Directive 2002/14/EC on information and consultation rights of employees. Evidence from five case studies in the business services and the financial sectors suggests that the introduction of the Information and Consultation of Employees Regulations 2004 drove the spread of voluntary arrangements. However, the legislation has not promoted so far an effective framework for the development of deliberative procedures between management and labour with the aim of advancing a 'capability for voice'. This is attributed to its institutional design and the limited degree to which extra-legal 'conversion factors' are available in the British industrial relations system.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Koukiadaki, Aristea] Univ Cambridge, Ctr Business Res, Cambridge CB2
   1AG, England.
   <br>[Koukiadaki, Aristea] Univ Cambridge, Darwin Coll, Cambridge CB2 1AG,
   England.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Koukiadaki, A (reprint author), Univ Cambridge, Ctr Business Res, Trumpington St, Cambridge CB2 1AG, England.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ak545@cam.ac.uk</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Koukiadaki, Aristea</display_name>&nbsp;</font></td><td><font size="3">0000-0002-7092-0297&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>9</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>9</td>
</tr>

<tr>
<td valign="top">PD </td><td>AUG</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">VL </td><td>31</td>
</tr>

<tr>
<td valign="top">IS </td><td>3</td>
</tr>

<tr>
<td valign="top">BP </td><td>365</td>
</tr>

<tr>
<td valign="top">EP </td><td>388</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1177/0143831X09351217</td>
</tr>

<tr>
<td valign="top">SC </td><td>Business &amp; Economics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000279970500006</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wu, CH
   <br>Hsia, CC
   <br>Lee, CH
   <br>Lin, MC</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wu, Chung-Hsien
   <br>Hsia, Chi-Chun
   <br>Lee, Chung-Han
   <br>Lin, Mai-Chun</td>
</tr>

<tr>
<td valign="top">TI </td><td>Hierarchical Prosody Conversion Using Regression-Based Clustering for
   Emotional Speech Synthesis</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Emotional speech synthesis; hierarchical prosodic structure; prosody
   conversion; regression-based clustering</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION; MAXIMUM-LIKELIHOOD; INFORMATION; EXTRACTION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents an approach to hierarchical prosody conversion for emotional speech synthesis. The pitch contour of the source speech is decomposed into a hierarchical prosodic structure consisting of sentence, prosodic word, and subsyllable levels. The pitch contour in the higher level is encoded by the discrete Legendre polynomial coefficients. The residual, the difference between the source pitch contour and the pitch contour decoded from the discrete Legendre polynomial coefficients, is then used for pitch modeling at the lower level. For prosody conversion, Gaussian mixture models (GMMs) are used for sentence- and prosodic word-level conversion. At subsyllable level, the pitch feature vectors are clustered via a proposed regression-based clustering method to generate the prosody conversion functions for selection. Linguistic and symbolic prosody features of the source speech are adopted to select the most suitable function using the classification and regression tree for prosody conversion. Three small-sized emotional parallel speech databases with happy, angry, and sad emotions, respectively, were designed and collected for training and evaluation. Objective and subjective evaluations were conducted and the comparison results to the GMM-based method for prosody conversion achieved an improved performance using the hierarchical prosodic structure and the proposed regression-based clustering method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Wu, Chung-Hsien; Lee, Chung-Han; Lin, Mai-Chun] Natl Cheng Kung Univ,
   Dept Comp Sci &amp; Informat Engn, Tainan 701, Taiwan.
   <br>[Hsia, Chi-Chun] Ind Technol Res Inst S, ICT Enabled Healthcare Project,
   Hsinchu 31040, Taiwan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Wu, CH (reprint author), Natl Cheng Kung Univ, Dept Comp Sci &amp; Informat Engn, Tainan 701, Taiwan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>chwu@csie.ncku.edu.tw; shiacj@itri.org.tw; chlee@csie.ncku.edu.tw;
   chun@csie.ncku.edu.tw</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Wu, Chung-Hsien</display_name>&nbsp;</font></td><td><font size="3">E-7970-2013&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>25</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>31</td>
</tr>

<tr>
<td valign="top">PD </td><td>AUG</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">VL </td><td>18</td>
</tr>

<tr>
<td valign="top">IS </td><td>6</td>
</tr>

<tr>
<td valign="top">BP </td><td>1394</td>
</tr>

<tr>
<td valign="top">EP </td><td>1405</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASL.2009.2034771</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000288375700027</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Carter, EJ
   <br>Sharan, L
   <br>Trutoiu, L
   <br>Matthews, I
   <br>Hodgins, JK</td>
</tr>

<tr>
<td valign="top">AF </td><td>Carter, Elizabeth J.
   <br>Sharan, Lavanya
   <br>Trutoiu, Laura
   <br>Matthews, Iain
   <br>Hodgins, Jessica K.</td>
</tr>

<tr>
<td valign="top">TI </td><td>Perceptually Motivated Guidelines for Voice Synchronization in Film</td>
</tr>

<tr>
<td valign="top">SO </td><td>ACM TRANSACTIONS ON APPLIED PERCEPTION</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Documentation; Languages; Multisensory perception and integration; human
   perception and performance; auditory perceptual research; visual
   psychophysics</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH-PERCEPTION</td>
</tr>

<tr>
<td valign="top">AB </td><td>We consume video content in a multitude of ways, including in movie theaters, on television, on DVDs and Blu-rays, online, on smart phones, and on portable media players. For quality control purposes, it is important to have a uniform viewing experience across these various platforms. In this work, we focus on voice synchronization, an aspect of video quality that is strongly affected by current post-production and transmission practices. We examined the synchronization of an actor's voice and lip movements in two distinct scenarios. First, we simulated the temporal mismatch between the audio and video tracks that can occur during dubbing or during broadcast. Next, we recreated the pitch changes that result from conversions between formats with different frame rates. We show, for the first time, that these audio visual mismatches affect viewer enjoyment. When temporal synchronization is noticeably absent, there is a decrease in the perceived performance quality and the perceived emotional intensity of a performance. For pitch changes, we find that higher pitch voices are not preferred, especially for male actors. Based on our findings, we advise that mismatched audio and video signals negatively affect viewer experience.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Carter, Elizabeth J.; Trutoiu, Laura; Hodgins, Jessica K.] Carnegie
   Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.
   <br>[Sharan, Lavanya; Matthews, Iain; Hodgins, Jessica K.] Disney Res
   Pittsburgh, Pittsburgh, PA 15213 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Carter, EJ (reprint author), Carnegie Mellon Univ, Inst Robot, 5000 Forbes Ave, Pittsburgh, PA 15213 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>lizcarter@cmu.edu</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Carter, Elizabeth</display_name>&nbsp;</font></td><td><font size="3">G-6958-2012&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUL</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">VL </td><td>7</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">AR </td><td>23</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1145/1823738.1823741</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000280546500003</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Samy, AK
   <br>Ridgway, D
   <br>Orabi, A
   <br>Suppiah, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Samy, A. K.
   <br>Ridgway, D.
   <br>Orabi, A.
   <br>Suppiah, A.</td>
</tr>

<tr>
<td valign="top">TI </td><td>Minimally invasive, video-assisted thyroidectomy: first experience from
   the United Kingdom</td>
</tr>

<tr>
<td valign="top">SO </td><td>ANNALS OF THE ROYAL COLLEGE OF SURGEONS OF ENGLAND</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Minimally invasive; Thyroidectomy; Education</td>
</tr>

<tr>
<td valign="top">ID </td><td>CONVENTIONAL THYROIDECTOMY; ENDOSCOPIC THYROIDECTOMY; SURGERY; BENEFITS</td>
</tr>

<tr>
<td valign="top">AB </td><td>INTRODUCTION Minimally-invasive, video-assisted thyroidectomy (MIVAT) was developed to reduce scarring/trauma associated with cervical incisions used in open thyroidectomy. Results from various centres have been published internationally but none from the UK. This study reports the first results from the UK and compares them with other centres. We also aim to compare the results of a single-surgeon experience in a small/moderately-sized hospital to those of larger tertiary centres.
   <br>PATIENTS AND METHODS Retrospective analysis of a single surgeon experience in a district general hospital
   <br>RESULTS The cohort was 55 patients (52 female, 3 male), mean age 48 years (range, 21-77 years) who had 64 MIVAT procedures. There were 49 hemithyroidectomies (HTs), 2 isthmusectomy, 4 total thyroidectomies (Us) and 9 completion thyroidectomies (CTs) with median operating time of 86 min (IQR 66-110 min). Individual operating times were HT 85 min (IQR 60-110 min); IT 130 min (IQR 100-140 min) and CT 77 min (IQR 70-98 min). Median operating time was shorter in the second half of this series (76 min vs 92 min; P&lt; 0.001). Length of stay was &lt; 1 day in 92%. Conversions occurred in 6.3% with no haematoma or re-operation. Transient voice change was present in 7 (11%), permanent unilateral recurrent laryngeal nerve palsy in 2 (3%), and transient hypocalcaemia in 2 (3%).
   <br>CONCLUSIONS The first results from the UK are similar to those of other international centres. A single-surgeon practice can obtain results comparable to larger tertiary centres provided there is sufficient case-load. MIVAT is safe and effective, but has a steep learning curve with rapid improvement observed within first 30 cases. Future studies should focus on objective assessment of scar/cosmesis and cost-effectiveness. MIVAT is an acceptable alternative to open surgery in highly selected patients.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Samy, A. K.; Ridgway, D.; Orabi, A.; Suppiah, A.] Diana Princess Wales
   Hosp, Grimsby DN33 2BA, England.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Samy, AK (reprint author), Diana Princess Wales Hosp, Scartho Rd, Grimsby DN33 2BA, England.</td>
</tr>

<tr>
<td valign="top">EM </td><td>aksamy@hotmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>13</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>15</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUL</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">VL </td><td>92</td>
</tr>

<tr>
<td valign="top">IS </td><td>5</td>
</tr>

<tr>
<td valign="top">BP </td><td>379</td>
</tr>

<tr>
<td valign="top">EP </td><td>384</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1308/003588410X12628812459977</td>
</tr>

<tr>
<td valign="top">SC </td><td>Surgery</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000280450200007</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Rao, KS</td>
</tr>

<tr>
<td valign="top">AF </td><td>Rao, K. Sreenivasa</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice conversion by mapping the speaker-specific features using pitch
   synchronous approach</td>
</tr>

<tr>
<td valign="top">SO </td><td>COMPUTER SPEECH AND LANGUAGE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Mapping function; Feedforward neural network (FFNN); Pitch contour;
   Excitation source; LP residual; Instants of significant excitation
   (epochs); Prosody characteristics; Duration and energy patterns; Glottal
   closure; Voice conversion; Objective measures; Mean opinion score (MOS);
   ABX test</td>
</tr>

<tr>
<td valign="top">ID </td><td>SIGNIFICANT EXCITATION; SPEECH SIGNALS; INSTANTS; TRANSFORMATION;
   EXTRACTION</td>
</tr>

<tr>
<td valign="top">AB </td><td>The basic goal of the voice conversion system is to modify the speaker-specific characteristics, keeping the message and the environmental information contained in the speech signal intact. Speaker characteristics reflect in speech at different levels, such as, the shape of the glottal pulse (excitation source characteristics), the shape of the vocal tract (vocal tract system characteristics) and the long-term features (suprasegmental or prosodic characteristics). In this paper, we are proposing neural network models for developing mapping functions at each level. The features used for developing the mapping functions are extracted using pitch synchronous analysis. Pitch synchronous analysis provides the estimation of accurate vocal tract parameters, by analyzing the speech signal independently in each pitch period without influenced by the adjacent pitch cycles. In this work, the instants of significant excitation are used as pitch markers to perform the pitch synchronous analysis. The instants of significant excitation correspond to the instants of glottal closure (epochs) in the case of voiced speech, and to some random excitations like onset of burst in the case of nonvoiced speech. Instants of significant excitation are computed from the linear prediction (LP) residual of speech signals by using the property of average group-delay of minimum phase signals. In this paper, line spectral frequencies (LSFs) are used for representing the vocal tract characteristics, and for developing its associated mapping function. LP residual of the speech signal is viewed as excitation source, and the residual samples around the instant of glottal closure are used for mapping. Prosodic parameters at syllable and phrase levels are used for deriving the mapping function. Source and system level mapping functions are derived pitch synchronously, and the incorporation of target prosodic parameters is performed pitch synchronously using instants of significant excitation. The performance of the voice conversion system is evaluated using listening tests. The prediction accuracy of the mapping functions (neural network models) used at different levels in the proposed voice conversion system is further evaluated using objective measures such as deviation (D(i)), root mean square error (mu(RMSE)) and correlation coefficient (gamma(X,Y)). The proposed approach (i.e., mapping and modification of parameters using pitch synchronous approach) used for voice conversion is shown to be performed better compared to the earlier method (mapping the vocal tract parameters using block processing) proposed by the author. (C) 2009 Elsevier Ltd. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Indian Inst Technol, Sch Informat Technol, Kharagpur 721302, W Bengal,
   India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Rao, KS (reprint author), Indian Inst Technol, Sch Informat Technol, Kharagpur 721302, W Bengal, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ksrao@iitkgp.ac.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>34</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>35</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUL</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">VL </td><td>24</td>
</tr>

<tr>
<td valign="top">IS </td><td>3</td>
</tr>

<tr>
<td valign="top">BP </td><td>474</td>
</tr>

<tr>
<td valign="top">EP </td><td>494</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.csl.2009.03.003</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000277330400005</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Helander, E
   <br>Virtanen, T
   <br>Nurminen, J
   <br>Gabbouj, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Helander, Elina
   <br>Virtanen, Tuomas
   <br>Nurminen, Jani
   <br>Gabbouj, Moncef</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Conversion Using Partial Least Squares Regression</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Gaussian mixture model (GMM); partial least squares regression; voice
   conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEAKER ADAPTATION; ALGORITHM</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion can be formulated as finding a mapping function which transforms the features of the source speaker to those of the target speaker. Gaussian mixture model (GMM)-based conversion is commonly used, but it is subject to overfitting. In this paper, we propose to use partial least squares (PLS)-based transforms in voice conversion. To prevent overfitting, the degrees of freedom in the mapping can be controlled by choosing a suitable number of components. We propose a technique to combine PLS with GMMs, enabling the use of multiple local linear mappings. To further improve the perceptual quality of the mapping where rapid transitions between GMM components produce audible artefacts, we propose to low-pass filter the component posterior probabilities. The conducted experiments show that the proposed technique results in better subjective and objective quality than the baseline joint density GMM approach. In speech quality conversion preference tests, the proposed method achieved 67% preference score against the smoothed joint density GMM method and 84% preference score against the unsmoothed joint density GMM method. In objective tests the proposed method produced a lower Mel-cepstral distortion than the reference methods.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Helander, Elina; Virtanen, Tuomas; Gabbouj, Moncef] Tampere Univ
   Technol, Dept Signal Proc, Tampere 33720, Finland.
   <br>[Nurminen, Jani] Nokia Devices R&amp;D, Tampere 33720, Finland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Helander, E (reprint author), Tampere Univ Technol, Dept Signal Proc, Tampere 33720, Finland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>elina.helander@tut.fi; tuomas.virtanen@tut.fi;
   jani.k.nurminen@nokia.com; moncef.gab-bouj@tut.fi</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Gabbouj, Moncef</display_name>&nbsp;</font></td><td><font size="3">G-4293-2014&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Gabbouj, Moncef</display_name>&nbsp;</font></td><td><font size="3">0000-0002-9788-2323&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Helander, Elina</display_name>&nbsp;</font></td><td><font size="3">0000-0002-0076-0590&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>94</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>98</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUL</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">VL </td><td>18</td>
</tr>

<tr>
<td valign="top">IS </td><td>5</td>
</tr>

<tr>
<td valign="top">BP </td><td>912</td>
</tr>

<tr>
<td valign="top">EP </td><td>921</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASL.2010.2041699</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000278814600002</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Erro, D
   <br>Moreno, A
   <br>Bonafonte, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Erro, Daniel
   <br>Moreno, Asuncion
   <br>Bonafonte, Antonio</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Conversion Based on Weighted Frequency Warping</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Gaussian mixture models (GMMs); harmonic plus stochastic model (HSM);
   speech synthesis; voice conversion; weighted frequency warping</td>
</tr>

<tr>
<td valign="top">AB </td><td>Any modification applied to speech signals has an impact on their perceptual quality. In particular, voice conversion to modify a source voice so that it is perceived as a specific target voice involves prosodic and spectral transformations that produce significant quality degradation. Choosing among the current voice conversion methods represents a trade-off between the similarity of the converted voice to the target voice and the quality of the resulting converted speech, both rated by listeners. This paper presents a new voice conversion method termed Weighted Frequency Warping that has a good balance between similarity and quality. This method uses a time-varying piecewise-linear frequency warping function and an energy correction filter, and it combines typical probabilistic techniques and frequency warping transformations. Compared to standard probabilistic systems, Weighted Frequency Warping results in a significant increase in quality scores, whereas the conversion scores remain almost unaltered. This paper carefully discusses the theoretical aspects of the method and the details of its implementation, and the results of an international evaluation of the new system are also included.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Erro, Daniel; Moreno, Asuncion; Bonafonte, Antonio] Univ Politecn
   Cataluna, TALP Res Ctr, ES-08034 Barcelona, Spain.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Erro, D (reprint author), Univ Basque Country UPV EHU, AhoLab Signal Proc Lab, Bilbao 48013, Spain.</td>
</tr>

<tr>
<td valign="top">EM </td><td>derro@gps.tsc.upc.edu; asun-cion.moreno@upc.edu;
   antonio.bonafonte@upc.edu</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Erro, Daniel</display_name>&nbsp;</font></td><td><font size="3">H-7043-2015&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Moreno, Asuncion</display_name>&nbsp;</font></td><td><font size="3">H-2315-2017&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Erro, Daniel</display_name>&nbsp;</font></td><td><font size="3">0000-0003-0954-6942&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Moreno, Asuncion</display_name>&nbsp;</font></td><td><font size="3">0000-0002-1823-5970&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Bonafonte, Antonio</display_name>&nbsp;</font></td><td><font size="3">0000-0002-6240-9915&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>86</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>93</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUL</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">VL </td><td>18</td>
</tr>

<tr>
<td valign="top">IS </td><td>5</td>
</tr>

<tr>
<td valign="top">BP </td><td>922</td>
</tr>

<tr>
<td valign="top">EP </td><td>931</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASL.2009.2038663</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000278814600003</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Tao, JH
   <br>Zhang, M
   <br>Nurminen, J
   <br>Tian, JL
   <br>Wang, X</td>
</tr>

<tr>
<td valign="top">AF </td><td>Tao, Jianhua
   <br>Zhang, Meng
   <br>Nurminen, Jani
   <br>Tian, Jilei
   <br>Wang, Xia</td>
</tr>

<tr>
<td valign="top">TI </td><td>Supervisory Data Alignment for Text-Independent Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Data alignment; self-organized learning; supervisory phonetic
   restriction; text-independent voice conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>TRANSFORMATION; NETWORKS</td>
</tr>

<tr>
<td valign="top">AB </td><td>We propose new supervisory data alignment methods for text-independent voice conversion which do not need parallel training corpora. Phonetic information is used as a restriction during alignment for mapping the data from the source speaker onto the parameter space of a target speaker. Both linear and nonlinear methods are derived by considering alignment accuracy and topology preservation. For the linear alignment, we consider common phoneme clusters of the source and target space as benchmarks and adapt the source data vector to the target space while maintaining the relative phonetic positions among neighborhood clusters. In order to preserve the topological structure of the source parameter space and improve the stability of conversion and the accuracy of the phonetic mapping, a supervised self-organizing learning algorithm considering phonetic restriction is proposed for iteratively improving the alignment outcome of the previous step. Both the linear and nonlinear methods can also be applied in the cross-lingual case. Evaluation results show that the proposed methods improve the performance of alignment in terms of both alignment accuracy and stability for text-independent voice conversion in intra-lingual and cross-lingual cases.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Tao, Jianhua; Zhang, Meng] Chinese Acad Sci, Inst Automat, Natl Lab
   Pattern Recognit, Beijing 100190, Peoples R China.
   <br>[Nurminen, Jani] Nokia Devices R&amp;D, Tampere 33720, Finland.
   <br>[Tian, Jilei; Wang, Xia] Nokia Res Ctr, Beijing 100176, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Tao, JH (reprint author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jhtao@nlpr.ia.ac.cn; mzhang@nlpr.ia.ac.cn; jani.k.nurminen@nokia.com;
   jilei.tian@nokia.com; xia.s.wang@nokia.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>10</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>10</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUL</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">VL </td><td>18</td>
</tr>

<tr>
<td valign="top">IS </td><td>5</td>
</tr>

<tr>
<td valign="top">BP </td><td>932</td>
</tr>

<tr>
<td valign="top">EP </td><td>943</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASL.2010.2041688</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000278814600004</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Erro, D
   <br>Moreno, A
   <br>Bonafonte, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Erro, Daniel
   <br>Moreno, Asuncion
   <br>Bonafonte, Antonio</td>
</tr>

<tr>
<td valign="top">TI </td><td>INCA Algorithm for Training Voice Conversion Systems From Nonparallel
   Corpora</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Frame alignment; Gaussian mixture model (GMM); nonparallel training
   corpus; text-independent cross-lingual voice conversion</td>
</tr>

<tr>
<td valign="top">AB </td><td>Most existing voice conversion systems, particularly those based on Gaussian mixture models, require a set of paired acoustic vectors from the source and target speakers to learn their corresponding transformation function. The alignment of phonetically equivalent source and target vectors is not problematic when the training corpus is parallel, which means that both speakers utter the same training sentences. However, in some practical situations, such as cross-lingual voice conversion, it is not possible to obtain such parallel utterances. With an aim towards increasing the versatility of current voice conversion systems, this paper proposes a new iterative alignment method that allows pairing phonetically equivalent acoustic vectors from nonparallel utterances from different speakers, even under cross-lingual conditions. This method is based on existing voice conversion techniques, and it does not require any phonetic or linguistic information. Subjective evaluation experiments show that the performance of the resulting voice conversion system is very similar to that of an equivalent system trained on a parallel corpus.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Erro, Daniel; Moreno, Asuncion; Bonafonte, Antonio] Univ Politecn
   Cataluna, TALP Res Ctr, ES-08034 Barcelona, Spain.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Erro, D (reprint author), Univ Basque Country UPV EHU, AhoLab Signal Proc Lab, Bilbao 48013, Spain.</td>
</tr>

<tr>
<td valign="top">EM </td><td>derro@gps.tsc.upc.edu; asun-cion.moreno@upc.edu;
   antonio.bonafonte@upc.edu</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Moreno, Asuncion</display_name>&nbsp;</font></td><td><font size="3">H-2315-2017&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Erro, Daniel</display_name>&nbsp;</font></td><td><font size="3">H-7043-2015&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Moreno, Asuncion</display_name>&nbsp;</font></td><td><font size="3">0000-0002-1823-5970&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Erro, Daniel</display_name>&nbsp;</font></td><td><font size="3">0000-0003-0954-6942&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Bonafonte, Antonio</display_name>&nbsp;</font></td><td><font size="3">0000-0002-6240-9915&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>49</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>54</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUL</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">VL </td><td>18</td>
</tr>

<tr>
<td valign="top">IS </td><td>5</td>
</tr>

<tr>
<td valign="top">BP </td><td>944</td>
</tr>

<tr>
<td valign="top">EP </td><td>953</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASL.2009.2038669</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000278814600005</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Desai, S
   <br>Black, AW
   <br>Yegnanarayana, B
   <br>Prahallad, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Desai, Srinivas
   <br>Black, Alan W.
   <br>Yegnanarayana, B.
   <br>Prahallad, Kishore</td>
</tr>

<tr>
<td valign="top">TI </td><td>Spectral Mapping Using Artificial Neural Networks for Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Artificial neural networks (ANNs); cross-lingual; error correction;
   speaker-specific characteristics; spectral mapping; voice conversion</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we use artificial neural networks (ANNs) for voice conversion and exploit the mapping abilities of an ANN model to perform mapping of spectral features of a source speaker to that of a target speaker. A comparative study of voice conversion using an ANN model and the state-of-the-art Gaussian mixture model (GMM) is conducted. The results of voice conversion, evaluated using subjective and objective measures, confirm that an ANN-based VC system performs as good as that of a GMM-based VC system, and the quality of the transformed speech is intelligible and possesses the characteristics of a target speaker. In this paper, we also address the issue of dependency of voice conversion techniques on parallel data between the source and the target speakers. While there have been efforts to use nonparallel data and speaker adaptation techniques, it is important to investigate techniques which capture speaker-specific characteristics of a target speaker, and avoid any need for source speaker's data either for training or for adaptation. In this paper, we propose a voice conversion approach using an ANN model to capture speaker-specific characteristics of a target speaker and demonstrate that such a voice conversion approach can perform monolingual as well as cross-lingual voice conversion of an arbitrary source speaker.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Desai, Srinivas; Yegnanarayana, B.; Prahallad, Kishore] Int Inst
   Informat Technol, Hyderabad 500032, Andhra Pradesh, India.
   <br>[Black, Alan W.; Prahallad, Kishore] Carnegie Mellon Univ, Language
   Technol Inst, Pittsburgh, PA 15213 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Desai, S (reprint author), Int Inst Informat Technol, Hyderabad 500032, Andhra Pradesh, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>srinivasdesai@research.iiit.ac.in; awb@cs.cmu.edu; yegna@iiit.ac.in;
   kishore@iiit.ac.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>101</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>115</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUL</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">VL </td><td>18</td>
</tr>

<tr>
<td valign="top">IS </td><td>5</td>
</tr>

<tr>
<td valign="top">BP </td><td>954</td>
</tr>

<tr>
<td valign="top">EP </td><td>964</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASL.2010.2047683</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000278814600006</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Turk, O
   <br>Schroder, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Turk, Oytun
   <br>Schroeder, Marc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Evaluation of Expressive Speech Synthesis With Voice Conversion and Copy
   Resynthesis Techniques</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Expressive speech synthesis; prosody; voice conversion; voice quality
   transformation</td>
</tr>

<tr>
<td valign="top">AB </td><td>Generating expressive synthetic voices requires carefully designed databases that contain sufficient amount of expressive speech material. This paper investigates voice conversion and modification techniques to reduce database collection and processing efforts while maintaining acceptable quality and naturalness. In a factorial design, we study the relative contributions of voice quality and prosody as well as the amount of distortions introduced by the respective signal manipulation steps. The unit selection engine in our open source and modular text-to-speech (TTS) framework MARY is extended with voice quality transformation using either GMM-based prediction or vocal tract copy resynthesis. These algorithms are then cross-combined with various prosody copy resynthesis methods. The overall expressive speech generation process functions as a postprocessing step on TTS outputs to transform neutral synthetic speech into aggressive, cheerful, or depressed speech. Cross-combinations of voice quality and prosody transformation algorithms are compared in listening tests for perceived expressive style and quality. The results show that there is a tradeoff between identification and naturalness. Combined modeling of both voice quality and prosody leads to the best identification scores at the expense of lowest naturalness ratings. The fine detail of both voice quality and prosody, as preserved by the copy synthesis, did contribute to a better identification as compared to the approximate models.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Turk, Oytun] Sensory Inc, Portland, OR 97209 USA.
   <br>[Schroeder, Marc] DFKI GmbH Language Technol Lab, Speech Grp, D-66123
   Saarbrucken, Germany.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Turk, O (reprint author), Sensory Inc, Portland, OR 97209 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>oytunturk@gmail.com; marc.schroeder@dfki.de</td>
</tr>

<tr>
<td valign="top">TC </td><td>17</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>18</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUL</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">VL </td><td>18</td>
</tr>

<tr>
<td valign="top">IS </td><td>5</td>
</tr>

<tr>
<td valign="top">BP </td><td>965</td>
</tr>

<tr>
<td valign="top">EP </td><td>973</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASL.2010.2041113</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000278814600007</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Erro, D
   <br>Navas, E
   <br>Hernaez, I
   <br>Saratxaga, I</td>
</tr>

<tr>
<td valign="top">AF </td><td>Erro, Daniel
   <br>Navas, Eva
   <br>Hernaez, Inma
   <br>Saratxaga, Ibon</td>
</tr>

<tr>
<td valign="top">TI </td><td>Emotion Conversion Based on Prosodic Unit Selection</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Emotional speech synthesis; intonation; prosody; unit selection; voice
   conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion has been traditionally focused on spectrum. Current systems lack a solid prosody conversion method suitable for different speaking styles. Recently, the unit selection technique has been applied to transform emotional intonation contours. This paper goes one step beyond: it explores strategies for training and configuring the selection cost function in an emotion conversion application. The proposed system, which uses accent groups as basic intonation units and performs conversion also on phoneme durations and intensity, is evaluated by means of a carefully designed subjective test involving the big six emotions. Although the expressiveness of the converted sentences is still far from that of natural emotional speech, satisfactory results are obtained when different configurations are used for different emotions.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Erro, Daniel; Navas, Eva; Hernaez, Inma; Saratxaga, Ibon] Univ Basque
   Country UPV EHU, Elect &amp; Telecommun Dept, AhoLab Signal Proc Lab,
   E-48013 Bilbao, Spain.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Erro, D (reprint author), Univ Basque Country UPV EHU, Elect &amp; Telecommun Dept, AhoLab Signal Proc Lab, E-48013 Bilbao, Spain.</td>
</tr>

<tr>
<td valign="top">EM </td><td>derro@aholab.ehu.es; eva@aholab.ehu.es; inma@aholab.ehu.es;
   ibon@aholab.ehu.es</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Saratxaga, Ibon</display_name>&nbsp;</font></td><td><font size="3">H-6423-2015&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Hernaez Rioja, Inmaculada</display_name>&nbsp;</font></td><td><font size="3">K-8303-2012&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Navas, Eva</display_name>&nbsp;</font></td><td><font size="3">H-4317-2013&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Erro, Daniel</display_name>&nbsp;</font></td><td><font size="3">H-7043-2015&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Saratxaga, Ibon</display_name>&nbsp;</font></td><td><font size="3">0000-0001-7282-2765&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Hernaez Rioja, Inmaculada</display_name>&nbsp;</font></td><td><font size="3">0000-0003-4447-7575&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Navas, Eva</display_name>&nbsp;</font></td><td><font size="3">0000-0003-3804-4984&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Erro, Daniel</display_name>&nbsp;</font></td><td><font size="3">0000-0003-0954-6942&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>12</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>12</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUL</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">VL </td><td>18</td>
</tr>

<tr>
<td valign="top">IS </td><td>5</td>
</tr>

<tr>
<td valign="top">BP </td><td>974</td>
</tr>

<tr>
<td valign="top">EP </td><td>983</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASL.2009.2038658</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000278814600008</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Yamagishi, J
   <br>Usabaev, B
   <br>King, S
   <br>Watts, O
   <br>Dines, J
   <br>Tian, JL
   <br>Guan, Y
   <br>Hu, RL
   <br>Oura, K
   <br>Wu, YJ
   <br>Tokuda, K
   <br>Karhila, R
   <br>Kurimo, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Yamagishi, Junichi
   <br>Usabaev, Bela
   <br>King, Simon
   <br>Watts, Oliver
   <br>Dines, John
   <br>Tian, Jilei
   <br>Guan, Yong
   <br>Hu, Rile
   <br>Oura, Keiichiro
   <br>Wu, Yi-Jian
   <br>Tokuda, Keiichi
   <br>Karhila, Reima
   <br>Kurimo, Mikko</td>
</tr>

<tr>
<td valign="top">TI </td><td>Thousands of Voices for HMM-Based Speech Synthesis-Analysis and
   Application of TTS Systems Built on Various ASR Corpora</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Automatic speech recognition (ASR); average voice; hidden Markov model
   (HMM)-based speech synthesis; H Triple S (HTS); speaker adaptation;
   speech synthesis; SPEECON database; voice conversion; WSJ database</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEAKER ADAPTATION; ALGORITHM; AVERAGE</td>
</tr>

<tr>
<td valign="top">AB </td><td>In conventional speech synthesis, large amounts of phonetically balanced speech data recorded in highly controlled recording studio environments are typically required to build a voice. Although using such data is a straightforward solution for high quality synthesis, the number of voices available will always be limited, because recording costs are high. On the other hand, our recent experiments with HMM-based speech synthesis systems have demonstrated that speaker-adaptive HMM-based speech synthesis (which uses an "average voice model" plus model adaptation) is robust to non-ideal speech data that are recorded under various conditions and with varying microphones, that are not perfectly clean, and/or that lack phonetic balance. This enables us to consider building high-quality voices on "non-TTS" corpora such as ASR corpora. Since ASR corpora generally include a large number of speakers, this leads to the possibility of producing an enormous number of voices automatically. In this paper, we demonstrate the thousands of voices for HMM-based speech synthesis that we have made from several popular ASR corpora such as the Wall Street Journal (WSJ0, WSJ1, and WSJCAM0), Resource Management, Globalphone, and SPEECON databases. We also present the results of associated analysis based on perceptual evaluation, and discuss remaining issues.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Yamagishi, Junichi; King, Simon; Watts, Oliver] Univ Edinburgh, Ctr
   Speech Technol Res, Edinburgh EH8 9AB, Midlothian, Scotland.
   <br>[Usabaev, Bela] Univ Tubingen, D-72074 Tubingen, Germany.
   <br>[Dines, John] Idiap Res Inst, CH-1920 Martigny, Switzerland.
   <br>[Tian, Jilei; Guan, Yong; Hu, Rile] Nokia Res Ctr, Beijing 100176,
   Peoples R China.
   <br>[Oura, Keiichiro; Tokuda, Keiichi] Nagoya Inst Technol, Dept Comp Sci &amp;
   Engn, Nagoya, Aichi 4668555, Japan.
   <br>[Karhila, Reima; Kurimo, Mikko] Aalto Univ, Adapt Informat Res Ctr,
   FIN-02015 Helsinki, Finland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Yamagishi, J (reprint author), Univ Edinburgh, Ctr Speech Technol Res, Edinburgh EH8 9AB, Midlothian, Scotland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jyamagis@inf.ed.ac.uk; belausabaev@googlemail.com; simon.king@ed.ac.uk;
   o.s.watts@sms.ed.ac.uk; john.dines@idiap.ch; jilei.tian@nokia.com;
   rile.hu@nokia.com; ext-yong.guan@nokia.com; uratec@sp.nitech.ac.jp;
   yijiwu@microsoft.co; tokuda@nitech.ac.jp; rkarhila@james.hut.fi;
   Mikko.Kurimo@tkk.fi</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Kurimo, Mikko</display_name>&nbsp;</font></td><td><font size="3">F-6647-2012&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>King, Simon</display_name>&nbsp;</font></td><td><font size="3">0000-0002-2694-2843&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>38</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>38</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUL</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">VL </td><td>18</td>
</tr>

<tr>
<td valign="top">IS </td><td>5</td>
</tr>

<tr>
<td valign="top">BP </td><td>984</td>
</tr>

<tr>
<td valign="top">EP </td><td>1004</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASL.2010.2045237</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000278814600009</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Watts, O
   <br>Yamagishi, J
   <br>King, S
   <br>Berkling, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Watts, Oliver
   <br>Yamagishi, Junichi
   <br>King, Simon
   <br>Berkling, Kay</td>
</tr>

<tr>
<td valign="top">TI </td><td>Synthesis of Child Speech With HMM Adaptation and Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Children; hidden Markov models (HMMs); speech synthesis</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEAKER ADAPTATION; SYNTHESIS SYSTEM</td>
</tr>

<tr>
<td valign="top">AB </td><td>The synthesis of child speech presents challenges both in the collection of data and in the building of a synthesizer from that data. We chose to build a statistical parametric synthesizer using the hidden Markov model (HMM)-based system HTS, as this technique has previously been shown to perform well for limited amounts of data, and for data collected under imperfect conditions. Six different configurations of the synthesizer were compared, using both speaker-dependent and speaker-adaptive modeling techniques, and using varying amounts of data. For comparison with HMM adaptation, techniques from voice conversion were used to transform existing synthesizers to the characteristics of the target speaker. Speaker-adaptive voices generally outperformed child speaker-dependent voices in the evaluation. HMM adaptation outperformed voice conversion style techniques when using the full target speaker corpus; with fewer adaptation data, however, no significant listener preference for either HMM adaptation or voice conversion methods was found.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Watts, Oliver; Yamagishi, Junichi; King, Simon] Univ Edinburgh, Ctr
   Speech Technol Res, Edinburgh EH8 9YL, Midlothian, Scotland.
   <br>[Berkling, Kay] Inline Internet Online Dienste GmbH, D-76133 Karlsruhe,
   Germany.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Watts, O (reprint author), Univ Edinburgh, Ctr Speech Technol Res, Edinburgh EH8 9YL, Midlothian, Scotland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>O.S.Watts@sms.ed.ac.uk; jyamagis@inf.ed.ac.uk; simon.king@ed.ac.uk;
   kay@berkling.com</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>King, Simon</display_name>&nbsp;</font></td><td><font size="3">0000-0002-2694-2843&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>7</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>7</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUL</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">VL </td><td>18</td>
</tr>

<tr>
<td valign="top">IS </td><td>5</td>
</tr>

<tr>
<td valign="top">BP </td><td>1005</td>
</tr>

<tr>
<td valign="top">EP </td><td>1016</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASL.2009.2035029</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000278814600010</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Felps, D
   <br>Gutierrez-Osuna, R</td>
</tr>

<tr>
<td valign="top">AF </td><td>Felps, Daniel
   <br>Gutierrez-Osuna, Ricardo</td>
</tr>

<tr>
<td valign="top">TI </td><td>Developing Objective Measures of Foreign-Accent Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Accent conversion; foreign accent recognition; speaker recognition;
   voice conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH-QUALITY ASSESSMENT; AMERICAN ENGLISH; INTELLIGIBILITY;
   IDENTIFICATION; TRANSFORM; FEATURES; VOICES; MODELS; VOWELS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Various methods have recently appeared to transform foreign-accented speech into its native-accented counterpart. Evaluation of these accent conversion methods requires extensive listening tests across a number of perceptual dimensions. This article presents three objective measures that may be used to assess the acoustic quality, degree of foreign accent, and speaker identity of accent-converted utterances. Accent conversion generates novel utterances: those of a foreign speaker with a native accent. Therefore, the acoustic quality in accent conversion cannot be evaluated with conventional measures of spectral distortion, which assume that a clean recording of the speech signal is available for comparison. Here we evaluate a single-ended measure of speech quality, ITU-T recommendation P. 563 for narrow-band telephony. We also propose a measure of foreign accent that exploits a weakness of automatic speech recognizers: their sensitivity to foreign accents. Namely, we use phoneme-level match scores given by the HTK recognizer trained on a large number of English American speakers to obtain a measure of native accent. Finally, we propose a measure of speaker identity that projects acoustic vectors (e. g., Mel cepstral, F0) onto the linear discriminant that maximizes separability for a given pair of source and target speakers. The three measures are evaluated on a corpus of accent-converted utterances that had been previously rated through perceptual tests. Our results show that the three measures have a high degree of correlation with their corresponding subjective ratings, suggesting that they may be used to accelerate the development of foreign-accent conversion tools. Applications of these measures in the context of computer assisted pronunciation training and voice conversion are also discussed.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Felps, Daniel; Gutierrez-Osuna, Ricardo] Texas A&amp;M Univ, Dept Comp Sci
   &amp; Engn, College Stn, TX 77843 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Felps, D (reprint author), Texas A&amp;M Univ, Dept Comp Sci &amp; Engn, College Stn, TX 77843 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>dlfelps@cse.tamu.edu; rgutier@cse.tamu.edu</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Gutierrez-Osuna, Ricardo</display_name>&nbsp;</font></td><td><font size="3">0000-0003-2817-2085&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>14</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>14</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUL</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">VL </td><td>18</td>
</tr>

<tr>
<td valign="top">IS </td><td>5</td>
</tr>

<tr>
<td valign="top">BP </td><td>1030</td>
</tr>

<tr>
<td valign="top">EP </td><td>1040</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASL.2009.2038818</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000278814600012</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nakamura, K
   <br>Toda, T
   <br>Saruwatari, H
   <br>Shikano, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nakamura, Keigo
   <br>Toda, Tomoki
   <br>Saruwatari, Hiroshi
   <br>Shikano, Kiyohiro</td>
</tr>

<tr>
<td valign="top">TI </td><td>Evaluation of Extremely Small Sound Source Signals Used in Speaking-Aid
   System with Statistical Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEICE TRANSACTIONS ON INFORMATION AND SYSTEMS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>laryngectomee; speaking aid; electrolarynx; voice conversion; NAM;
   enhancing auditory feedback</td>
</tr>

<tr>
<td valign="top">AB </td><td>We have so far proposed a speaking-aid system for laryngectomees using a statistical voice conversion technique. In the proposed system, artificial speech articulated with extremely small sound source signals is detected with a Non-Audible Murmur (NAM) microphone, and then, the detected artificial speech is converted into more natural voice in a probabilistic manner. Although this system basically allows laryngectomees to speak while keeping the external source signals silent, it is still questionable how much these new sound source signals affect the converted speech quality. In this paper, we investigate the impact of various sound source signals on voice conversion accuracy. Various small sound source signals are designed by changing the spectral envelope and the waveform power independently. We conduct objective and subjective evaluations. The results of these experimental evaluations demonstrate that voice conversion accepts 1) various sound source signals with different spectral envelopes and 2) large degree of power of the sound source signals unless the power of speaking parts is almost equal to that of silence parts. Moreover, we also investigate the effectiveness of enhancing auditory feedback during speaking with the extremely small sound source signals.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nakamura, Keigo; Toda, Tomoki; Saruwatari, Hiroshi; Shikano, Kiyohiro]
   Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Ikoma 6300192, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nakamura, K (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Ikoma 6300192, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>kei-naka@is.naist.jp; tomoki@is.naist.jp; sawatari@is.naist.jp;
   shikano@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>4</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>4</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUL</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">VL </td><td>E93D</td>
</tr>

<tr>
<td valign="top">IS </td><td>7</td>
</tr>

<tr>
<td valign="top">BP </td><td>1909</td>
</tr>

<tr>
<td valign="top">EP </td><td>1917</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1587/transinf.E93.D.1909</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000281342100027</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ohtani, Y
   <br>Toda, T
   <br>Saruwatari, H
   <br>Shikano, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ohtani, Yamato
   <br>Toda, Tomoki
   <br>Saruwatari, Hiroshi
   <br>Shikano, Kiyohiro</td>
</tr>

<tr>
<td valign="top">TI </td><td>Adaptive Training for Voice Conversion Based on Eigenvoices</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEICE TRANSACTIONS ON INFORMATION AND SYSTEMS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>speech synthesis; voice conversion; Gaussian mixture model; eigenvoice;
   adaptive training</td>
</tr>

<tr>
<td valign="top">ID </td><td>ADAPTATION; SPEECH</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we describe a novel model training method for one-to-many eigenvoice conversion (EVC). One-to-many EVC is a technique for converting a specific source speaker's voice into an arbitrary target speaker's voice. An eigenvoice Gaussian mixture model (EV-GMM) is trained in advance using multiple parallel data sets consisting of utterance-pairs of the source speaker and many pre-stored target speakers. The EV-GMM can be adapted to new target speakers using only a few of their arbitrary utterances by estimating a small number of adaptive parameters. In the adaptation process, several parameters of the EV-GMM to be fixed for different target speakers strongly affect the conversion performance of the adapted model. In order to improve the conversion performance in one-to-many EVC, we propose an adaptive training method of the EV-GMM. In the proposed training method, both the fixed parameters and the adaptive parameters are optimized by maximizing a total likelihood function of the EV-GMMs adapted to individual pre-stored target speakers. We conducted objective and subjective evaluations to demonstrate the effectiveness of the proposed training method. The experimental results show that the proposed adaptive training yields significant quality improvements in the converted speech.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ohtani, Yamato; Toda, Tomoki; Saruwatari, Hiroshi; Shikano, Kiyohiro]
   Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Ikoma 6300192, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ohtani, Y (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Ikoma 6300192, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>yamato-o@is.naist.jp; tomoki@is.naist.jp; sawatari@is.naist.jp;
   shikano@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>13</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>13</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">VL </td><td>E93D</td>
</tr>

<tr>
<td valign="top">IS </td><td>6</td>
</tr>

<tr>
<td valign="top">BP </td><td>1589</td>
</tr>

<tr>
<td valign="top">EP </td><td>1598</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1587/transinf.E93.D.1589</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000279250600028</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Gvion, A
   <br>Friedmann, N</td>
</tr>

<tr>
<td valign="top">AF </td><td>Gvion, Aviah
   <br>Friedmann, Naama</td>
</tr>

<tr>
<td valign="top">TI </td><td>Dyscravia: Voicing substitution dysgraphia</td>
</tr>

<tr>
<td valign="top">SO </td><td>NEUROPSYCHOLOGIA</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Dysgraphia; Dyslexia; Hebrew; Dyscravia; Spelling; Voicing</td>
</tr>

<tr>
<td valign="top">ID </td><td>GRAPHEMIC BUFFER DISORDER; DEVELOPMENTAL SURFACE DYSGRAPHIA; LETTER
   POSITION DYSLEXIA; VISUAL WORD RECOGNITION; ACQUIRED DYSGRAPHIA;
   PHONOLOGICAL DYSGRAPHIA; ATTENTIONAL DYSLEXIA; ALZHEIMERS-DISEASE; DEEP
   DYSGRAPHIA; SELECTIVE IMPAIRMENT</td>
</tr>

<tr>
<td valign="top">AB </td><td>We report a new type of dysgraphia, which we term dyscravia. The main error type in dyscravia is substitution of the target letter with a letter that differs only with respect to the voicing feature, such as writing "coat" for "goat", and "vagd" for "fact". Two Hebrew-speaking individuals with acquired dyscravia are reported, TG, a man aged 31, and BG, a woman aged 66. Both had surface dysgraphia in addition to their dyscravia. To describe dyscravia in detail, and to explore the rate and types of errors made in spelling, we administered tests of writing to dictation, written naming, and oral spelling. In writing to dictation, TG made voicing errors on 38% of the words, and BC made 17% voicing errors. Voicing errors also occurred in nonword writing (43% for TG, 56% for BC). The writing performance and the variables that influenced the participants' spelling, as well as the results of the auditory discrimination and repetition tasks indicated that their dyscravia did not result from a deficit in auditory processing, the graphemic buffer, the phonological output lexicon, the phonological output buffer, or the allographic stage. The locus of the deficit is the phoneme-to-grapheme conversion, in a function specialized in the conversion of phonemes' voicing feature into graphemes. Because these participants had surface dysgraphia and were forced to write via the sublexical route, the deficit in voicing was evident in their writing of both words and nonwords. We further examined whether the participants also evinced parallel errors in reading. TG had a selective voicing deficit in writing, and did not show any voicing errors in reading, whereas BC had voicing errors also in the reading of nonwords (i.e., she had dyslegzia in addition to dyscravia). The dissociation TG demonstrated indicated that the voicing feature conversion is separate for reading and writing, and can be impaired selectively in writing. BG's dyslegzia indicates that the grapheme-to-phoneme conversion also includes a function that is sensitive to phonological features such as voicing. Thus the main conclusion of this study is that a separate function of voicing feature conversion exists in the phoneme-to-grapheme conversion route, which may be selectively impaired without deficits in other functions of the conversion route, and without a parallel deficit in reading. (C) 2010 Elsevier Ltd. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Friedmann, Naama] Tel Aviv Univ, Language &amp; Brain Lab, Sch Educ,
   IL-69978 Tel Aviv, Israel.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Friedmann, N (reprint author), Tel Aviv Univ, Language &amp; Brain Lab, Sch Educ, IL-69978 Tel Aviv, Israel.</td>
</tr>

<tr>
<td valign="top">EM </td><td>naamafr@post.tau.ac.il</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Friedmann, Naama</display_name>&nbsp;</font></td><td><font size="3">M-2688-2013&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Friedmann, Naama</display_name>&nbsp;</font></td><td><font size="3">0000-0002-7108-3072&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>7</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>7</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">VL </td><td>48</td>
</tr>

<tr>
<td valign="top">IS </td><td>7</td>
</tr>

<tr>
<td valign="top">BP </td><td>1935</td>
</tr>

<tr>
<td valign="top">EP </td><td>1947</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.neuropsychologia.2010.03.014</td>
</tr>

<tr>
<td valign="top">SC </td><td>Behavioral Sciences; Neurosciences &amp; Neurology; Psychology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000279023100008</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kurchuk, M
   <br>Tsividis, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kurchuk, Mariya
   <br>Tsividis, Yannis</td>
</tr>

<tr>
<td valign="top">TI </td><td>Signal-Dependent Variable-Resolution Clockless A/D Conversion With
   Application to Continuous-Time Digital Signal Processing</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS I-REGULAR PAPERS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Analog-to-digital converter (ADC); continuous-time digital signal
   processing (CT DSP); level-crossing sampling; quantization</td>
</tr>

<tr>
<td valign="top">ID </td><td>DELAY ELEMENT; QUANTIZATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>A variable-resolution (VR) quantizer with input-activity-dependent adjustable resolution is presented. Several potential schemes are discussed; the favored scheme achieves adjustable resolution by level skipping according to the speed of the input. The advantages of a VR analog-to-digital conversion (ADC) are presented with applications in continuous-time (CT) digital signal processing systems. It is shown that a decrease in resolution for fast inputs does not corrupt the in-band spectrum while leading to a reduction in the number of samples produced by a CT ADC. The result is a significant decrease in power dissipation but without in-band performance degradation. Analysis and extensive simulations are provided. Simulations using signals in the voice band show that a power reduction of over 80% is achievable with a VR quantization.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kurchuk, Mariya; Tsividis, Yannis] Columbia Univ, New York, NY 10027
   USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kurchuk, M (reprint author), Columbia Univ, New York, NY 10027 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>maria@cisl.columbia.edu; tsividis@ee.columbia.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>37</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>39</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAY</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">VL </td><td>57</td>
</tr>

<tr>
<td valign="top">IS </td><td>5</td>
</tr>

<tr>
<td valign="top">SI </td><td>SI</td>
</tr>

<tr>
<td valign="top">BP </td><td>982</td>
</tr>

<tr>
<td valign="top">EP </td><td>991</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TCSI.2010.2043987</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000278066200004</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Makki, B
   <br>Hosseini, MN
   <br>Seyyedsalehi, SA
   <br>Sadati, N</td>
</tr>

<tr>
<td valign="top">AF </td><td>Makki, Behrooz
   <br>Hosseini, Mona Noori
   <br>Seyyedsalehi, Seyyed Ali
   <br>Sadati, Nasser</td>
</tr>

<tr>
<td valign="top">TI </td><td>Unaligned training for voice conversion based on a local nonlinear
   principal component analysis approach</td>
</tr>

<tr>
<td valign="top">SO </td><td>NEURAL COMPUTING &amp; APPLICATIONS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Local nonlinear principal component analysis; Unaligned voice
   conversion; Autoassociative neural network</td>
</tr>

<tr>
<td valign="top">ID </td><td>NEURAL-NETWORKS; CURVES</td>
</tr>

<tr>
<td valign="top">AB </td><td>During the past years, various principal component analysis algorithms have been developed. In this paper, a new approach for local nonlinear principal component analysis is proposed which is applied to capture voice conversion (VC). A new structure of autoassociative neural network is designed which not only performs data partitioning but also extracts nonlinear principal components of the clusters. Performance of the proposed method is evaluated by means of two experiments that illustrate its efficiency; at first, performance of the network is described by means of an artificial dataset and then, the developed method is applied to perform VC.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Makki, Behrooz; Hosseini, Mona Noori; Seyyedsalehi, Seyyed Ali]
   Amirkabir Univ Technol, Tehran, Iran.
   <br>[Sadati, Nasser] Sharif Univ Technol, Tehran, Iran.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Makki, B (reprint author), Amirkabir Univ Technol, 15 Dadafarin,Namazzade,Taslimi St,Dibaji Jonubi S, Tehran, Iran.</td>
</tr>

<tr>
<td valign="top">EM </td><td>behrooz.makki@gmail.com; monanoori@gmail.com; ssalehi@aut.ac.ir;
   sadati@sina.sharif.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PD </td><td>APR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">VL </td><td>19</td>
</tr>

<tr>
<td valign="top">IS </td><td>3</td>
</tr>

<tr>
<td valign="top">BP </td><td>437</td>
</tr>

<tr>
<td valign="top">EP </td><td>444</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/s00521-009-0275-x</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000275755000009</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hirahara, T
   <br>Otani, M
   <br>Shimizu, S
   <br>Toda, T
   <br>Nakamura, K
   <br>Nakajima, Y
   <br>Shikano, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hirahara, Tatsuya
   <br>Otani, Makoto
   <br>Shimizu, Shota
   <br>Toda, Tomoki
   <br>Nakamura, Keigo
   <br>Nakajima, Yoshitaka
   <br>Shikano, Kiyohiro</td>
</tr>

<tr>
<td valign="top">TI </td><td>Silent-speech enhancement using body-conducted vocal-tract resonance
   signals</td>
</tr>

<tr>
<td valign="top">SO </td><td>SPEECH COMMUNICATION</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Non-audible murmur; Body-conducted sound; Voice conversion; Talking aids</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>The physical characteristics of weak body-conducted vocal-tract resonance signals called non-audible murmur (NAM) and the acoustic characteristics of three sensors developed for detecting these signals have been investigated. NAM signals attenuate 50 dB at 1 kHz; this attenuation consists of 30-dB full-range attenuation due to air-to-body transmission loss and 10 dB/octave spectral decay due to a sound propagation loss within the body. These characteristics agree with the spectral characteristics of measured NAM signals. The sensors have a sensitivity of between 41 and 58 dB [V/Pa] at I kHz, and the mean signal-to-noise ratio of the detected signals was 15 dB. On the basis of these investigations, three types of silent-speech enhancement systems were developed: (1) simple, direct amplification of weak vocal-tract resonance signals using a wired urethane-elastomer NAM microphone, (2) simple, direct amplification using a wireless urethane-elastomer-duplex NAM microphone, and (3) transformation of the weak vocal-tract resonance signals sensed by a soft-silicone NAM microphone into whispered speech using statistical conversion. Field testing of the systems showed that they enable voice impaired people to communicate verbally using body-conducted vocal-tract resonance signals. Listening tests demonstrated that weak body-conducted vocal-tract resonance sounds can be transformed into intelligible whispered speech sounds. Using these systems, people with voice impairments can re-acquire speech communication with less effort. (C) 2009 Elsevier B.V. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Hirahara, Tatsuya; Otani, Makoto; Shimizu, Shota] Toyama Prefectural
   Univ, Dept Intelligent Syst Design Engn, Toyama 9390398, Japan.
   <br>[Toda, Tomoki; Nakamura, Keigo; Nakajima, Yoshitaka; Shikano, Kiyohiro]
   Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara 6300192, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hirahara, T (reprint author), Toyama Prefectural Univ, Dept Intelligent Syst Design Engn, 5180 Kurokawa, Toyama 9390398, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>hirahara@pu-toyama.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>19</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>22</td>
</tr>

<tr>
<td valign="top">PD </td><td>APR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">VL </td><td>52</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">SI </td><td>SI</td>
</tr>

<tr>
<td valign="top">BP </td><td>301</td>
</tr>

<tr>
<td valign="top">EP </td><td>313</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.specom.2009.12.001</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000276026400004</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Tran, VA
   <br>Bailly, G
   <br>Loevenbruck, H
   <br>Toda, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Tran, Viet-Anh
   <br>Bailly, Gerard
   <br>Loevenbruck, Helene
   <br>Toda, Tomoki</td>
</tr>

<tr>
<td valign="top">TI </td><td>Improvement to a NAM-captured whisper-to-speech system</td>
</tr>

<tr>
<td valign="top">SO </td><td>SPEECH COMMUNICATION</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Non-audible murmur; Whispered speech; Audiovisual voice conversion;
   Silent speech interface</td>
</tr>

<tr>
<td valign="top">ID </td><td>EXTRACTION; PITCH</td>
</tr>

<tr>
<td valign="top">AB </td><td>Exploiting a tissue-conductive sensor a stethoscopic microphone the system developed at NAIST which converts non-audible murmur (NAM) to audible speech by GM M-based statistical mapping is a very promising technique. The quality of the converted speech is however still insufficient for computer-mediated communication, notably because of the poor estimation of F-0 from unvoiced speech and because of impoverished phonetic contrasts. This paper presents our investigations to improve the intelligibility and naturalness of the synthesized speech and first objective and subjective evaluations of the resulting system. The first improvement concerns voicing and F-0 estimation. Instead of using a single GMM for both, we estimate a continuous F-0 using a GMM, trained on target voiced segments only. The continuous F-0 estimation is filtered by a voicing decision computed by a neural network. The objective and subjective improvement is significant. The second improvement concerns the input time window and its dimensionality reduction: we show that the precision of F-0 estimation is also significantly improved by extending the input time window from 90 to 450 ms and by using a Linear Discriminant Analysis (LDA) instead of the original Principal Component Analysis (PCA). Estimation of spectral envelope is also slightly improved with LDA but is degraded with larger time windows. A third improvement consists in adding visual parameters both as input and output parameters. The positive contribution of this information is confirmed by a subjective test. Finally, H M M-based conversion is compared with GMM-based conversion. (C) 2009 Elsevier B.V. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Tran, Viet-Anh; Bailly, Gerard; Loevenbruck, Helene] Grenoble Univ,
   CNRS, UMR 5216, GIPSA Lab, Grenoble, France.
   <br>[Toda, Tomoki] Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara,
   Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Tran, VA (reprint author), Grenoble Univ, CNRS, UMR 5216, GIPSA Lab, Grenoble, France.</td>
</tr>

<tr>
<td valign="top">EM </td><td>viet-anh.tran@gipsa-lab.inpg.fr; gerard.bailly@gipsa-lab.inpg.fr;
   helene.loevenbruck@gipsa-lab.inpg.fr; tomoki@is.naist.jp</td>
</tr>

<tr xmlns:date="http://exslt.org/dates-and-times">
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>bailly, gerard</display_name>&nbsp;</font></td><td><font size="3">0000-0002-6053-0818&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>15</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>18</td>
</tr>

<tr>
<td valign="top">PD </td><td>APR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">VL </td><td>52</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">SI </td><td>SI</td>
</tr>

<tr>
<td valign="top">BP </td><td>314</td>
</tr>

<tr>
<td valign="top">EP </td><td>326</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.specom.2009.11.005</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000276026400005</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Segi, H
   <br>Takou, R
   <br>Seiyama, N
   <br>Takagi, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Segi, Hiroyuki
   <br>Takou, Reiko
   <br>Seiyama, Nobumasa
   <br>Takagi, Tohru</td>
</tr>

<tr>
<td valign="top">TI </td><td>Development of a Prototype Data-Broadcast Receiver with a High-Quality
   Voice Synthesizer</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE TRANSACTIONS ON CONSUMER ELECTRONICS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Data Broadcast Receiver; BML; Voice Synthesizer; Speech Rate Conversion</td>
</tr>

<tr>
<td valign="top">AB </td><td>Here we propose a prototype data-broadcast receiver equipped with a voice synthesizer, which can read out stock prices and stock-price changes from a live data broadcast. Using this receiver, listeners can access their chosen stock information at any time and at an appropriate speech rate. We also propose a high-quality voice synthesizer for use with this receiver. A subjective evaluation confirmed the superiority of this voice synthesizer over commercially available ones(1).</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Segi, Hiroyuki; Takou, Reiko; Takagi, Tohru] NHK Sci &amp; Technol Res
   Labs, Tokyo 1578510, Japan.
   <br>[Seiyama, Nobumasa] NHK Engn Serv Inc, Tokyo 1578540, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Segi, H (reprint author), NHK Sci &amp; Technol Res Labs, Tokyo 1578510, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>segi.h-gs@nhk.or.jp; takou.r-go@nhk.or.jp; seiyama@nes.or.jp;
   takagi.t-fo@nhk.or.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PD </td><td>FEB</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">VL </td><td>56</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>169</td>
</tr>

<tr>
<td valign="top">EP </td><td>174</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TCE.2010.5439141</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000276093200024</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kollbrunner, J
   <br>Menet, AD
   <br>Seifert, E</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kollbrunner, Juerg
   <br>Menet, Anne-Dorine
   <br>Seifert, Eberhard</td>
</tr>

<tr>
<td valign="top">TI </td><td>Psychogenic aphonia: No fixation even after a lengthy period of aphonia</td>
</tr>

<tr>
<td valign="top">SO </td><td>SWISS MEDICAL WEEKLY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>psychogenic aphonia; conversion disorder; symptom-oriented therapy;
   short-term dynamic psychotherapy</td>
</tr>

<tr>
<td valign="top">ID </td><td>FUNCTIONAL APHONIA; VOICE DISORDERS; DYSPHONIA; SYMPTOM</td>
</tr>

<tr>
<td valign="top">AB </td><td>Background: Although psychogenic aphonia is considered to be a conversion disorder, aphonic patients are primarily treated symptomatically. This is because it is considered of overriding importance to elicit a voice quickly to avoid fixation of the aphonia. The aim of this study was to show that, for patients exhibiting the symptom of voicelessness, not eliciting the voice immediately will not lead to a permanent aphonia.
   <br>Methods: Between February 2000 and May 2006, aphonia was diagnosed in 22 patients. Effects of short-term psychodynamic psychotherapeutic intervention and voice therapy were studied in a follow-up of three years, on average.
   <br>Results: Twenty one patients recovered their voices; 6 even before their first medical examination, 13 after an average of 12 weeks and 2 patients after 2 and 3 years respectively. One patient who has been in psychiatric therapy for years, as a result of having suffered serious abuse, failed to regain her voice.
   <br>Conclusions: Even after a lengthy period of aphonia a complete recovery of the voice function is possible in nearly all cases. Countertransference phenomena in therapists are discussed as the possible reason why they usually decide on treatment aimed primarily at dealing with the symptoms.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kollbrunner, Juerg; Menet, Anne-Dorine; Seifert, Eberhard] Univ Bern,
   Inselspital, Dept Otorhinolaryngol Head &amp; Neck Surg, Div Phoniatr,
   CH-3010 Bern, Switzerland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kollbrunner, J (reprint author), Univ Bern, Inselspital, Dept Otorhinolaryngol Head &amp; Neck Surg, Div Phoniatr, CH-3010 Bern, Switzerland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>juerg.kollbrunner@insel.ch</td>
</tr>

<tr>
<td valign="top">TC </td><td>15</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>16</td>
</tr>

<tr>
<td valign="top">PD </td><td>JAN 9</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">VL </td><td>140</td>
</tr>

<tr>
<td valign="top">IS </td><td>1-2</td>
</tr>

<tr>
<td valign="top">BP </td><td>12</td>
</tr>

<tr>
<td valign="top">EP </td><td>17</td>
</tr>

<tr>
<td valign="top">SC </td><td>General &amp; Internal Medicine</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000273928500003</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hiroya, S
   <br>Mochida, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hiroya, Sadao
   <br>Mochida, Takemi</td>
</tr>

<tr>
<td valign="top">GP </td><td>INST SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Phase Equalization-Based Autoregressive Model of Speech Signals</td>
</tr>

<tr>
<td valign="top">SO </td><td>11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>11th Annual Conference of the
   International-Speech-Communication-Association 2010</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 26-30, 2010</td>
</tr>

<tr>
<td valign="top">CL </td><td>Makuhari, JAPAN</td>
</tr>

<tr>
<td valign="top">DE </td><td>LPC; vocal-tract spectrum; phase equalization</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a novel method for estimating a vocal-tract spectrum from speech signals, based on a modeling of excitation signals of voiced speech. A formulation of linear prediction coding with impulse train is derived and applied to the phase-equalized speech signals, which are converted from the original speech signals by phase equalization. Preliminary results show that the proposed method improves the robustness of the estimation of a vocal-tract spectrum and the quality of re-synthesized speech compared with the conventional method. This technique will be useful for speech coding, speech synthesis, and real-time speech conversion.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Hiroya, Sadao; Mochida, Takemi] NTT Corp, NTT Commun Sci Labs, Tokyo,
   Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hiroya, S (reprint author), NTT Corp, NTT Commun Sci Labs, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>hiroya@idea.brl.ntt.co.jp; mochida@idea.brl.ntt.co.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">BP </td><td>42</td>
</tr>

<tr>
<td valign="top">EP </td><td>45</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000294382400007</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Zhuang, X
   <br>Qian, Y
   <br>Soong, F
   <br>Wu, YJ
   <br>Zhang, B</td>
</tr>

<tr>
<td valign="top">AF </td><td>Zhuang, Xin
   <br>Qian, Yao
   <br>Soong, Frank
   <br>Wu, Yijian
   <br>Zhang, Bo</td>
</tr>

<tr>
<td valign="top">GP </td><td>INST SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Formant-based Frequency Warping for Improving Speaker Adaptation in HMM
   TTS</td>
</tr>

<tr>
<td valign="top">SO </td><td>11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>11th Annual Conference of the
   International-Speech-Communication-Association 2010</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 26-30, 2010</td>
</tr>

<tr>
<td valign="top">CL </td><td>Makuhari, JAPAN</td>
</tr>

<tr>
<td valign="top">DE </td><td>speech synthesis; speaker adaptation; frequency warping</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH SYNTHESIS; VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Vocal Tract Length Normalization (VLTN), usually implemented as a frequency warping procedure (e.g. bilinear transformation), has been used successfully to adapt the spectral characteristics to a target speaker in speech recognition. In this study we exploit the same concept of frequency warping but concentrate explicitly on mapping the first four formant frequencies of 5 long vowels from source and target speakers. A universal warping function is thus constructed for improving MLLR-based speaker adaptation performance in TTS. The function first warps the frequency scale of the source speaker's speech data toward that of the target speaker and an HMM of the warped features is trained. Finally, MLLR-based speaker adaptation is applied to the trained HMM for synthesizing the target speaker's speech. When tested on a database of 4,000 sentences (source speaker) and 100 sentences of a male and a female speaker (target speakers), the formant based frequency warping has been found very effective in reducing the objective, log spectral distortion over the system without formant frequency warping. The improvement is also subjectively confirmed in AB preference and ABX speaker similarity listening tests.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Zhuang, Xin; Qian, Yao; Soong, Frank] Microsoft Res Asia, Beijing,
   Peoples R China.
   <br>[Zhuang, Xin; Zhang, Bo] Nankai Univ, Coll Software, Tianjin, Peoples R
   China.
   <br>[Wu, Yijian] Microsoft China, Beijing, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Zhuang, X (reprint author), Microsoft Res Asia, Beijing, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>yaoqian@microsoft.com; frankkps@microsoft.com; yijiwu@microsoft.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>4</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>4</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">BP </td><td>817</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000294382400201</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nakamura, K
   <br>Toda, T
   <br>Saruwatari, H
   <br>Shikano, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nakamura, Keigo
   <br>Toda, Tomoki
   <br>Saruwatari, Hiroshi
   <br>Shikano, Kiyohiro</td>
</tr>

<tr>
<td valign="top">GP </td><td>INST SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>The Use of Air-Pressure Sensor in Electrolaryngeal Speech Enhancement
   Based on Statistical Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 3 AND 4</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>11th Annual Conference of the
   International-Speech-Communication-Association 2010</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 26-30, 2010</td>
</tr>

<tr>
<td valign="top">CL </td><td>Makuhari, JAPAN</td>
</tr>

<tr>
<td valign="top">DE </td><td>Electrolarynx; Air-pressure sensor; Laryngectomee; Voice conversion;
   Speaking-aid</td>
</tr>

<tr>
<td valign="top">AB </td><td>In our previous work, we proposed a speaking-aid system converting electrolaryngeal speech (EL speech) to normal speech using a statistical voice conversion technique. The main weakness of our system is the difficulty of estimating natural contours of the fundamental frequency (F-0) from EL speech including only built-in F-0 contours. This paper proposes another speaking-aid system with an air-pressure sensor to enable laryngectomees to control F-0 contours of the EL speech using their breathing air. The experimental result demonstrates that 1) the correlation coefficient of F-0 contours between the converted and the target speech is improved from 0.58 to 0.78 by the use of the air-pressure sensor and 2) the synthetic speech converted by the proposed system sounds more natural and is more preferred to that by our conventional aid system.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nakamura, Keigo; Toda, Tomoki; Saruwatari, Hiroshi; Shikano, Kiyohiro]
   Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nakamura, K (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>kei-naka@is.naist.jp; tomoki@is.naist.jp; sawatari@is.naist.jp;
   shikano@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">BP </td><td>1628</td>
</tr>

<tr>
<td valign="top">EP </td><td>1631</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000313086500019</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Helander, E
   <br>Silen, H
   <br>Miguez, J
   <br>Gabbouji, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Helander, Elina
   <br>Silen, Hanna
   <br>Miguez, Joaquin
   <br>Gabbouji, Moncef</td>
</tr>

<tr>
<td valign="top">GP </td><td>INST SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Maximum a posteriori voice conversion using sequential Monte Carlo
   methods</td>
</tr>

<tr>
<td valign="top">SO </td><td>11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 3 AND 4</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>11th Annual Conference of the
   International-Speech-Communication-Association 2010</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 26-30, 2010</td>
</tr>

<tr>
<td valign="top">CL </td><td>Makuhari, JAPAN</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; maximum a posteriori; Viterbi algorithm; smoothing;
   particle filter</td>
</tr>

<tr>
<td valign="top">ID </td><td>LEAST-SQUARES REGRESSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Many voice conversion algorithms are based on frame-wise mapping from source features into target features. This ignores the inherent temporal continuity that is present in speech and can degrade the subjective quality. In this paper, we propose to optimize the speech feature sequence after a frame-based conversion algorithm has been applied. In particular, we select the sequence of speech features through the minimization of a cost function that involves both the conversion error and the smoothness of the sequence. The estimation problem is solved using sequential Monte Carlo methods. Both subjective and objective results show the effectiveness of the method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Helander, Elina; Silen, Hanna; Gabbouji, Moncef] Tampere Univ Technol,
   Dept Signal Proc, FIN-33101 Tampere, Finland.
   <br>[Miguez, Joaquin] Univ Carlos III Madrid, Dept Teoria Senal
   Comunicaciones, Madrid, Spain.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Helander, E (reprint author), Tampere Univ Technol, Dept Signal Proc, FIN-33101 Tampere, Finland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>elina.helander@tut.fi; hanna.silen@tut.fi; joaquin.miguez@uc3m.es;
   moncef.gabbouj@tut.fi</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Gabbouj, Moncef</display_name>&nbsp;</font></td><td><font size="3">G-4293-2014&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Gabbouj, Moncef</display_name>&nbsp;</font></td><td><font size="3">0000-0002-9788-2323&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Helander, Elina</display_name>&nbsp;</font></td><td><font size="3">0000-0002-0076-0590&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">BP </td><td>1716</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000313086500041</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Lanchantin, P
   <br>Rodet, X</td>
</tr>

<tr>
<td valign="top">AF </td><td>Lanchantin, Pierre
   <br>Rodet, Xavier</td>
</tr>

<tr>
<td valign="top">GP </td><td>INST SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Dynamic Model Selection for Spectral Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 3 AND 4</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>11th Annual Conference of the
   International-Speech-Communication-Association 2010</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 26-30, 2010</td>
</tr>

<tr>
<td valign="top">CL </td><td>Makuhari, JAPAN</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; model selection</td>
</tr>

<tr>
<td valign="top">ID </td><td>TRANSFORMATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Statistical methods for voice conversion are usually based on a single model selected in order to represent a tradeoff between goodness of fit and complexity. In this paper we assume that the best model may change over time, depending on the source acoustic features. We present a new method for spectral voice conversion(1) called Dynamic Model Selection (DMS), in which a set of potential best models with increasing complexity - including a mixture of Gaussian and probabilistic principal component analyzers - are considered during the conversion of a source speech signal into a target speech signal. This set is built during the learning phase, according to the Bayes information criterion (BIC). During the conversion, the best model is dynamically selected among the models in the set, according to the acoustical features of each source frame. Subjective tests show that the method improves the conversion in terms of proximity to the target and quality.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Lanchantin, Pierre; Rodet, Xavier] Anal Synth Team, STMS, IRCAM,
   CNRS,UMR9912, F-75004 Paris, France.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Lanchantin, P (reprint author), Anal Synth Team, STMS, IRCAM, CNRS,UMR9912, 1 Pl Igor Stravinsky, F-75004 Paris, France.</td>
</tr>

<tr>
<td valign="top">EM </td><td>lanchantin@ircam.fr; rod@ircam.fr</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">BP </td><td>1720</td>
</tr>

<tr>
<td valign="top">EP </td><td>1723</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000313086500042</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nose, T
   <br>Kobayashi, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nose, Takashi
   <br>Kobayashi, Takao</td>
</tr>

<tr>
<td valign="top">GP </td><td>INST SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Speaker-independent HMM-based Voice Conversion Using Quantized
   Fundamental Frequency</td>
</tr>

<tr>
<td valign="top">SO </td><td>11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 3 AND 4</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>11th Annual Conference of the
   International-Speech-Communication-Association 2010</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 26-30, 2010</td>
</tr>

<tr>
<td valign="top">CL </td><td>Makuhari, JAPAN</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; segment-based mapping; HMM-based speech synthesis;
   speaker adaptation; average voice model</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH SYNTHESIS; ALGORITHM</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper proposes a segment-based voice conversion technique between arbitrary speakers with a small amount of training data. In the proposed technique, an input speech utterance of source speaker is decoded into phonetic and prosodic symbol sequences, and then the converted speech is generated from the pre-trained target speaker's HMM using the decoded information. To reduce the required amount of training data, we use speaker-independent model in the decoding of the input speech, and model adaptation for the training of the target speaker's model. Experimental results show that there is no need to prepare the source speaker's training data, and the proposed technique with only ten sentences of the target speaker's adaptation data outperforms the conventional GMM-based one using parallel data of 200 sentences.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nose, Takashi; Kobayashi, Takao] Tokyo Inst Technol, Interdisciplinary
   Grad Sch Sci &amp; Engn, Yokohama, Kanagawa 2268502, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nose, T (reprint author), Tokyo Inst Technol, Interdisciplinary Grad Sch Sci &amp; Engn, Yokohama, Kanagawa 2268502, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>takashi.nose@ip.titech.ac.jp; takao.kobayashi@ip.titech.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">BP </td><td>1724</td>
</tr>

<tr>
<td valign="top">EP </td><td>1727</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000313086500043</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Saito, D
   <br>Watanabe, S
   <br>Nakamura, A
   <br>Minematsu, N</td>
</tr>

<tr>
<td valign="top">AF </td><td>Saito, Daisuke
   <br>Watanabe, Shinji
   <br>Nakamura, Atsushi
   <br>Minematsu, Nobuaki</td>
</tr>

<tr>
<td valign="top">GP </td><td>INST SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Probabilistic Integration of Joint Density Model and Speaker Model for
   Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 3 AND 4</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>11th Annual Conference of the
   International-Speech-Communication-Association 2010</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 26-30, 2010</td>
</tr>

<tr>
<td valign="top">CL </td><td>Makuhari, JAPAN</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; joint density model; speaker model; probabilistic
   unification</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH RECOGNITION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper describes a novel approach to voice conversion using both a joint density model and a speaker model. In voice conversion studies, approaches based on Gaussian Mixture Model (GMM) with probabilistic densities of joint vectors of a source and a target speakers are widely used to estimate a transformation. However, for sufficient quality, they require a parallel corpus which contains plenty of utterances with the same linguistic content spoken by both the speakers. In addition, the joint density GMM methods often suffer from over-training effects when the amount of training data is small. To compensate for these problems, we propose a novel approach to integrate the speaker GMM of the target with the joint density model using probabilistic formulation. The proposed method trains the joint density model with a few parallel utterances, and the speaker model with non-parallel data of the target, independently. It eases the burden on the source speaker. Experiments demonstrate the effectiveness of the proposed method, especially when the amount of the parallel corpus is small.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Saito, Daisuke] Univ Tokyo, Grad Sch Engn, Tokyo 1138654, Japan.
   <br>[Watanabe, Shinji; Nakamura, Atsushi] NTT Corp, NTT Commun Sci Labs,
   Tokyo, Japan.
   <br>[Minematsu, Nobuaki] Univ Tokyo, Grad Sch Informat Sci &amp; Technol, Tokyo,
   Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Saito, D (reprint author), Univ Tokyo, Grad Sch Engn, Tokyo 1138654, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>dsk_saito@gavo.t.utokyo.ac.jp; watanabe@cslab.kecl.ntt.co.jp;
   ats@cslab.kecl.ntt.co.jp; mine@gavo.t.utokyo.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">BP </td><td>1728</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000313086500044</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wu, ZZ
   <br>Kinnunen, T
   <br>Chng, ES
   <br>Li, HZ</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wu, Zhi-Zheng
   <br>Kinnunen, Tomi
   <br>Chng, Eng Siong
   <br>Li, Haizhou</td>
</tr>

<tr>
<td valign="top">GP </td><td>INST SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Text-Independent F0 Transformation with Non-Parallel Data for Voice
   Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 3 AND 4</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>11th Annual Conference of the
   International-Speech-Communication-Association 2010</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 26-30, 2010</td>
</tr>

<tr>
<td valign="top">CL </td><td>Makuhari, JAPAN</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; F0 transformation; GMM; histogram equalization;
   text-independence</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPECTRUM</td>
</tr>

<tr>
<td valign="top">AB </td><td>In voice conversion, frame-level mean and variance normalization is typically used for fundamental frequency (F0) transformation, which is text-independent and requires no parallel training data. Some advanced methods transform pitch contours instead, but require either parallel training data or syllabic annotations. We propose a method which retains the simplicity and text-independence of the frame-level conversion while yielding high-quality conversion. We achieve these goals by (1) introducing a text-independent tri-frame alignment method, (2) including delta features of F0 into Gaussian mixture model (GMM) conversion and (3) reducing the well-known GMM oversmoothing effect by F0 histogram equalization. Our objective and subjective experiments on the CMU Arctic corpus indicate improvements over both the mean/variance normalization and the baseline GMM conversion.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Wu, Zhi-Zheng; Chng, Eng Siong; Li, Haizhou] Nanyang Technol Univ, Sch
   Comp Engn, Singapore 639798, Singapore.
   <br>[Kinnunen, Tomi; Li, Haizhou] Univ Eastern Finland UEF, Sch Comp,
   Joensuu, Finland.
   <br>[Chng, Eng Siong; Li, Haizhou] Inst Infocomm Res I2R, Human Language
   Technol Dept, Singapore, Singapore.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Wu, ZZ (reprint author), Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.</td>
</tr>

<tr>
<td valign="top">EM </td><td>wuzz@ntu.edu.sg; tomi.kinnunen@uef.fi; ASESChng@ntu.edu.sg;
   hli@i2r.a-star.edu.sg</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Li, Haizhou</display_name>&nbsp;</font></td><td><font size="3">Q-6438-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Li, Haizhou</display_name>&nbsp;</font></td><td><font size="3">0000-0001-9158-9401&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>7</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>9</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">BP </td><td>1732</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000313086500045</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Yanagisawa, K
   <br>Huckvale, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Yanagisawa, Kayoko
   <br>Huckvale, Mark</td>
</tr>

<tr>
<td valign="top">GP </td><td>INST SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Phonetic Alternative to Cross-language Voice Conversion in a
   Text-dependent Context: Evaluation of Speaker Identity</td>
</tr>

<tr>
<td valign="top">SO </td><td>11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 3 AND 4</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>11th Annual Conference of the
   International-Speech-Communication-Association 2010</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 26-30, 2010</td>
</tr>

<tr>
<td valign="top">CL </td><td>Makuhari, JAPAN</td>
</tr>

<tr>
<td valign="top">DE </td><td>accent morphing; voice conversion; spoken language conversion; speaker
   identity; cross-language</td>
</tr>

<tr>
<td valign="top">ID </td><td>AGREEMENT</td>
</tr>

<tr>
<td valign="top">AB </td><td>Spoken language conversion (SLC) aims to generate utterances in the voice of a speaker but in a language unknown to them, using speech synthesis systems and speech processing techniques. Previous approaches to SLC have been based on cross-language voice conversion (VC), which has underlying assumptions that ignore phonetic and phonological differences between languages, leading to a reduction in intelligibility of the output. Accent morphing (AM) was proposed as an alternative approach, and its intelligibility performance was investigated in a previous study. AM attempts to preserve the voice characteristics of the target speaker whilst modifying their accent, using phonetic knowledge obtained from a native speaker of the target language. This paper examines AM and VC in terms of how similar the output sounds like the target speaker. AM achieved similarity ratings at least equivalent to VC, but the study highlighted various difficulties in evaluating speaker identity in a SLC context.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Yanagisawa, Kayoko; Huckvale, Mark] UCL, Dept Speech Hearing &amp; Phonet
   Sci, London, England.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Yanagisawa, K (reprint author), UCL, Dept Speech Hearing &amp; Phonet Sci, London, England.</td>
</tr>

<tr>
<td valign="top">EM </td><td>kayoko.yanagisawa@uclmail.net; m.huckvale@ucl.ac.uk</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">BP </td><td>2150</td>
</tr>

<tr>
<td valign="top">EP </td><td>2153</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000313086500150</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ohta, K
   <br>Toda, T
   <br>Ohtani, Y
   <br>Saruwatari, H
   <br>Shikano, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ohta, Kumi
   <br>Toda, Tomoki
   <br>Ohtani, Yamato
   <br>Saruwatari, Hiroshi
   <br>Shikano, Kiyohiro</td>
</tr>

<tr>
<td valign="top">GP </td><td>INST SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Adaptive Voice-Quality Control Based on One-to-Many Eigenvoice
   Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 3 AND 4</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>11th Annual Conference of the
   International-Speech-Communication-Association 2010</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 26-30, 2010</td>
</tr>

<tr>
<td valign="top">CL </td><td>Makuhari, JAPAN</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice-quality control; voice conversion; eigenvoices; unsupervised
   adaptation</td>
</tr>

<tr>
<td valign="top">ID </td><td>ADAPTATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents adaptive voice-quality control methods based on one-to-many eigenvoice conversion. To intuitively control the converted voice quality by manipulating a small number of control parameters, a multiple regression Gaussian mixture model (MR-GMM) has been proposed. The MR-GMM also allows us to estimate the optimum control parameters if target speech samples are available. However, its adaptation performance is limited because the number of control parameters is too small to widely model voice quality of various target speakers. To improve the adaptation performance while keeping capability of voice-quality control, this paper proposes an extended MR-GMM (EMR-GMM) with additional adaptive parameters to extend a subspace modeling target voice quality. Experimental results demonstrate that the EMR-GMM yields significant improvements of the adaptation performance while allowing us to intuitively control the converted voice quality.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ohta, Kumi] Brother Ind Ltd, Nagoya, Aichi, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ohta, K (reprint author), Brother Ind Ltd, Nagoya, Aichi, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>tomoki@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>10</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>10</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">BP </td><td>2158</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000313086500152</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Villavicencio, F
   <br>Bonada, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Villavicencio, Fernando
   <br>Bonada, Jordi</td>
</tr>

<tr>
<td valign="top">GP </td><td>INST SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Applying Voice Conversion To Concatenative Singing-Voice Synthesis</td>
</tr>

<tr>
<td valign="top">SO </td><td>11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 3 AND 4</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>11th Annual Conference of the
   International-Speech-Communication-Association 2010</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 26-30, 2010</td>
</tr>

<tr>
<td valign="top">CL </td><td>Makuhari, JAPAN</td>
</tr>

<tr>
<td valign="top">DE </td><td>speech synthesis; speech analysis; singing synthesis; linear prediction;
   envelope extraction</td>
</tr>

<tr>
<td valign="top">AB </td><td>This work address the application of Voice Conversion to singing-voice. The GMM-based approach was applied to VOCALOID, a concatenative singing synthesizer, to perform singer timbre conversion. The conversion framework was applied to full-quality singing databases, achieving a satisfactory conversion effect on the synthesized utterances. We report in this paper the results of our experimentation focused to study the spectral conversion performance when applied to specific pitch-range data.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Villavicencio, Fernando] YAMAHA Corp, Corp Res &amp; Dev Ctr, Hamamatsu,
   Shizuoka, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Villavicencio, F (reprint author), YAMAHA Corp, Corp Res &amp; Dev Ctr, Hamamatsu, Shizuoka, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>villavicencio@beat.yamaha.co.jp; jordi.bonada@upf.edu</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Bonada, Jordi</display_name>&nbsp;</font></td><td><font size="3">D-5954-2014&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Bonada, Jordi</display_name>&nbsp;</font></td><td><font size="3">0000-0002-8671-0729&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>14</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>14</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">BP </td><td>2162</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000313086500153</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nambu, Y
   <br>Mikawa, M
   <br>Tanaka, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nambu, Yoshiki
   <br>Mikawa, Masahiko
   <br>Tanaka, Kazuyo</td>
</tr>

<tr>
<td valign="top">BE </td><td>Kleijn, B
   <br>Larsen, J</td>
</tr>

<tr>
<td valign="top">TI </td><td>FLEXIBLE VOICE MORPHING BASED ON LINEAR COMBINATION OF MULTI-SPEAKERS'
   VOCAL TRACT AREA FUNCTIONS</td>
</tr>

<tr>
<td valign="top">SO </td><td>18TH EUROPEAN SIGNAL PROCESSING CONFERENCE (EUSIPCO-2010)</td>
</tr>

<tr>
<td valign="top">SE </td><td>European Signal Processing Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>18th European Signal Processing Conference (EUSIPCO)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 23-27, 2010</td>
</tr>

<tr>
<td valign="top">CL </td><td>Aalborg, DENMARK</td>
</tr>

<tr>
<td valign="top">ID </td><td>CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a flexible voice morphing method based on conversion using a linear combination of multi-speakers' vocal tract area functions, in which phonological identity is maintained in terms of the overall interpolated area. In this system, the characteristic of vocal tract resonances is separated from that of glottal source waves using AR-HMM analysis of speech. The vocal tract resonances and glottal source wave characteristics are independently morphed. For the morphing of vocal tract resonances, log area vocal tract functions, which are derived from AR coefficients, are normalized and then processed by statistical mapping technique. For glottal source waves, statistical mapping is conducted in the cepstrum domain. Morphed speech is re-synthesized by an AR filter of converted glottal source waves which is resynthesized using a cepstrum domain conversion. With the proposed morphing system, the continuity of formants and perceptual differences between a conventional method and the proposed method are confirmed.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nambu, Yoshiki; Mikawa, Masahiko; Tanaka, Kazuyo] Univ Tsukuba, Grad
   Sch Lib Informat &amp; Media Studies, Tsukuba, Ibaraki 3058550, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nambu, Y (reprint author), Univ Tsukuba, Grad Sch Lib Informat &amp; Media Studies, 1-2 Kasuga, Tsukuba, Ibaraki 3058550, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ynambu@slis.tsukuba.ac.jp; mikawa@slis.tsukuba.ac.jp;
   ktanaka@slis.tsukuba.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">BP </td><td>790</td>
</tr>

<tr>
<td valign="top">EP </td><td>794</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000349999100160</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Yamamoto, K
   <br>Tsuchiya, M
   <br>Nakagawa, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Yamamoto, Kazumasa
   <br>Tsuchiya, Masatoshi
   <br>Nakagawa, Seiichi</td>
</tr>

<tr>
<td valign="top">BE </td><td>Fujii, Y</td>
</tr>

<tr>
<td valign="top">TI </td><td>Privacy protection for speech signals</td>
</tr>

<tr>
<td valign="top">SO </td><td>1ST INTERNATIONAL CONFERENCE ON SECURITY CAMERA NETWORK, PRIVACY
   PROTECTION AND COMMUNITY SAFETY 2009</td>
</tr>

<tr>
<td valign="top">SE </td><td>Procedia Social and Behavioral Sciences</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>1st International Conference of Social and Behavioral Sciences on
   Security Camera Network, Privacy Protection and Community Safety</td>
</tr>

<tr>
<td valign="top">CY </td><td>2009</td>
</tr>

<tr>
<td valign="top">CL </td><td>Kiryu, JAPAN</td>
</tr>

<tr>
<td valign="top">DE </td><td>Privacy protection; speech signal; personal information; signal
   processing; language processing</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we describe about some privacy protection techniques for speech signals captured by public sensors. Speech signal includes the privacy information, such as "voice characteristics" and "linguistic privacy information." We try to protect the privacy information by using "voice conversion" and "deletion of privacy linguistic information from speech recognition result." Additionally, "elimination of speech in noisy speech" technique is also considered. (C) 2009 Published by Elsevier Ltd.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Yamamoto, Kazumasa; Nakagawa, Seiichi] Toyohashi Univ Technol, Dept
   Informat &amp; Comp Sci, Toyohashi, Aichi 4418580, Japan.
   <br>[Tsuchiya, Masatoshi] Toyohashi Univ Technol, Informat &amp; Media Ctr,
   Toyohashi, Aichi 4418580, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Yamamoto, K (reprint author), Toyohashi Univ Technol, Dept Informat &amp; Comp Sci, Toyohashi, Aichi 4418580, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>kyama@tut.jp</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>nakagawa, seiichi</display_name>&nbsp;</font></td><td><font size="3">L-5543-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Tsuchiya, Masatoshi</display_name>&nbsp;</font></td><td><font size="3">0000-0003-1862-8149&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">VL </td><td>2</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>153</td>
</tr>

<tr>
<td valign="top">EP </td><td>160</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.sbspro.2010.01.029</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Criminology &amp; Penology; Imaging Science &amp; Photographic
   Technology; Social Sciences - Other Topics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000281956700025</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Tashiro, T
   <br>Kimura, H
   <br>Hadama, H</td>
</tr>

<tr>
<td valign="top">AF </td><td>Tashiro, T.
   <br>Kimura, H.
   <br>Hadama, H.</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>ADC Bit Resolution Requirements for Upstream Emergency Communication
   System in FTTH</td>
</tr>

<tr>
<td valign="top">SO </td><td>2010 15TH OPTOELECTRONICS AND COMMUNICATIONS CONFERENCE (OECC)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>15th Optoelectronics and Communications Conference (OECC)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUL 05-09, 2010</td>
</tr>

<tr>
<td valign="top">CL </td><td>Sapporo, JAPAN</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper discusses an upstream emergency optical access system that allows voice communication during power failure, and describes the ADC bit resolution requirements. Then we report the adequacy of previously reported up-conversion frequency based on this discussion.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Tashiro, T.; Kimura, H.; Hadama, H.] NTT Access Network Serv Syst Labs,
   Yokosuka, Kanagawa 2390847, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Tashiro, T (reprint author), NTT Access Network Serv Syst Labs, 1-1 Hikarinooka, Yokosuka, Kanagawa 2390847, Japan.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">BP </td><td>18</td>
</tr>

<tr>
<td valign="top">EP </td><td>19</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Optics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000301111700005</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Al-Shamma, SD
   <br>Fathi, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Al-Shamma, Saad D.
   <br>Fathi, Sami</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Arabic Braille Recognition and Transcription into Text and Voice</td>
</tr>

<tr>
<td valign="top">SO </td><td>2010 5TH CAIRO INTERNATIONAL BIOMEDICAL ENGINEERING CONFERENCE (CIBEC
   2010)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Cairo International Biomedical Engineering Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>5th Cairo International Biomedical Engineering Conference (CIBEC)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 16-18, 2010</td>
</tr>

<tr>
<td valign="top">CL </td><td>Cairo, EGYPT</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a system for a design and implementation of Optical Arabic Braille Recognition(OBR) with voice and text conversion. the implemented algorithm based on a comparison of Braille dot position extraction in each cell with the database generated for each Braille cell. Many digital image processing have been performed on the Braille scanned document like binary conversion, edge detection, holes filling and finally image filtering before dot extraction. The work in this paper also involved a unique decimal code generation for each Braille cell used as a base for word reconstruction with the corresponding voice and text conversion database. The implemented algorithm achieve expected result through letter and words recognition and transcription accuracy over 99% and average processing time around 32.6 sec per page using matlab environmemt</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Al-Shamma, Saad D.; Fathi, Sami] Sudan Univ Sci &amp; Technol, Coll Engn,
   Biomed Dept, Khartoum, Sudan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Al-Shamma, SD (reprint author), Sudan Univ Sci &amp; Technol, Coll Engn, Biomed Dept, Khartoum, Sudan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>saaddaoud2003@yahoo.com; samifathi@ymail.com</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Khalil, Sami</display_name>&nbsp;</font></td><td><font size="3">0000-0002-4835-4161&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>4</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>4</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">BP </td><td>227</td>
</tr>

<tr>
<td valign="top">EP </td><td>231</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/CIBEC.2010.5716095</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000395157100055</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Chen, Z
   <br>Zhang, LH</td>
</tr>

<tr>
<td valign="top">AF </td><td>Chen, Z.
   <br>Zhang, L. H.</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>A ANN BASED HIGH QUALITY METHOD FOR VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2010 6TH INTERNATIONAL CONFERENCE ON WIRELESS COMMUNICATIONS NETWORKING
   AND MOBILE COMPUTING (WICOM)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Wireless Communications Networking and
   Mobile Computing-WiCOM</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>6th International Conference on Wireless Communications, Networking and
   Mobile Computing (WICOM)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 23-25, 2010</td>
</tr>

<tr>
<td valign="top">CL </td><td>Chengdu, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; ANN; GMM; pitch conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>TRANSFORMATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we describe a novel conversion method for voice conversion (VC). Artificial Neural Network (ANN) model is employed for performing joint spectrum and pitch conversion between speakers. The conventional method converts spectral parameters and pitch independently. Those separate transformations lead to an unsatisfactory speech quality. The main reason maybe that F-0 sequences are usually converted by a simply linear function. To overcome this problem, we apply joint parameters for train and conversion. A comparative study of voice conversion with ANN and Gaussian Mixture Model (GMM) is conducted. Experimental results indicate that the performance of VC can be dramatically improved by the proposed method in view of both subjective evaluation and objective measurement.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Chen, Z.; Zhang, L. H.] Nanjing Univ Post &amp; Telecommun, Coll Telecommun
   &amp; Informat Engn, Nanjing, Jiangsu, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Chen, Z (reprint author), Nanjing Univ Post &amp; Telecommun, Coll Telecommun &amp; Informat Engn, Nanjing, Jiangsu, Peoples R China.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000305114901084</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Zeng, DJ
   <br>Yu, YB</td>
</tr>

<tr>
<td valign="top">AF </td><td>Zeng, Daojian
   <br>Yu, Yibiao</td>
</tr>

<tr>
<td valign="top">BE </td><td>Yuan, BZ
   <br>Ruan, QQ
   <br>Tang, XF</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Conversion Using Structrued Gaussian Mixture Model</td>
</tr>

<tr>
<td valign="top">SO </td><td>2010 IEEE 10TH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING PROCEEDINGS
   (ICSP2010), VOLS I-III</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE 10th International Conference on Signal Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 24-28, 2010</td>
</tr>

<tr>
<td valign="top">CL </td><td>Beijing, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; SGMM; AUS</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH</td>
</tr>

<tr>
<td valign="top">AB </td><td>Gaussian Mixture Model (GMM) is commonly used in voice conversion.. However, traditional GMM based voice conversion usually extracts a conversion function from parallel corpus, which greatly limits the application of the technology. In an attempt to overcome this drawback, structured Gaussian Mixture Model (SGMM) is applied to model the speaker's acoustic feature distribution. In particular, two speakers' isolated SGMMs are aligned based on Acoustic Universal Structure (AUS) theory. Then the conversion function is extracted from two aligned SGMMs in a manner similar to conventional method. The subjective listening tests indicate that the proposed method achieves equivalent: speech quality and speaker individuality compared with conventional method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Zeng, Daojian; Yu, Yibiao] Soochow Univ, Sch Elect &amp; Informat Engn,
   Suzhou, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Zeng, DJ (reprint author), Soochow Univ, Sch Elect &amp; Informat Engn, Suzhou, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>zengdj916@yahoo.cn; yuyb@suda.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>4</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>4</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">BP </td><td>541</td>
</tr>

<tr>
<td valign="top">EP </td><td>544</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000296986200135</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Doi, H
   <br>Nakamura, K
   <br>Toda, T
   <br>Saruwatari, H
   <br>Shikano, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Doi, Hironori
   <br>Nakamura, Keigo
   <br>Toda, Tomoki
   <br>Saruwatari, Hiroshi
   <br>Shikano, Kiyohiro</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>STATISTICAL APPROACH TO ENHANCING ESOPHAGEAL SPEECH BASED ON GAUSSIAN
   MIXTURE MODELS</td>
</tr>

<tr>
<td valign="top">SO </td><td>2010 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>2010 IEEE International Conference on Acoustics, Speech, and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 14-19, 2010</td>
</tr>

<tr>
<td valign="top">CL </td><td>Dallas, TX</td>
</tr>

<tr>
<td valign="top">DE </td><td>laryngectomees; esophageal speech; speech enhancement; voice conversion;
   eigenvoice conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a novel method of enhancing esophageal speech using statistical voice conversion. Esophageal speech is one of the alternative speaking methods for laryngectomees. Although it doesn't require any external devices, generated voices sound unnatural. To improve the intelligibility and naturalness of esophageal speech, we propose a voice conversion method from esophageal speech into normal speech. A spectral parameter and excitation parameters of target normal speech are separately estimated from a spectral parameter of the esophageal speech based on Gaussian mixture models. The experimental results demonstrate that the proposed method yields significant improvements in intelligibility and naturalness. We also apply one-to-many eigenvoice conversion to esophageal speech enhancement for flexibly controlling enhanced voice quality.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Doi, Hironori; Nakamura, Keigo; Toda, Tomoki; Saruwatari, Hiroshi;
   Shikano, Kiyohiro] Nara Inst Sci &amp; Technol, Grad Sch Informat Sci,
   Ikoma, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Doi, H (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Ikoma, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>hironori-d@is.naist.jp; kei-naka@is.naist.jp; tomoki@is.naist.jp;
   sawatari@is.naist.jp; shikano@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>4</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>4</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">BP </td><td>4250</td>
</tr>

<tr>
<td valign="top">EP </td><td>4253</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/ICASSP.2010.5495676</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000287096004045</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kawahara, H
   <br>Nisimura, R
   <br>Irino, T
   <br>Morise, M
   <br>Takahashi, T
   <br>Banno, H</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kawahara, H.
   <br>Nisimura, R.
   <br>Irino, T.
   <br>Morise, M.
   <br>Takahashi, T.
   <br>Banno, H.</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>HIGH-QUALITY AND LIGHT-WEIGHT VOICE TRANSFORMATION ENABLING
   EXTRAPOLATION WITHOUT PERCEPTUAL AND OBJECTIVE BREAKDOWN</td>
</tr>

<tr>
<td valign="top">SO </td><td>2010 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>2010 IEEE International Conference on Acoustics, Speech, and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 10-19, 2010</td>
</tr>

<tr>
<td valign="top">CL </td><td>Dallas, TX</td>
</tr>

<tr>
<td valign="top">DE </td><td>speech analysis; speech processing; speech synthesis; auditory system;
   computer music</td>
</tr>

<tr>
<td valign="top">ID </td><td>CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>A voice transformation method that only relies on vowel information is proposed. The method is based on empirical cumulative distributions of perceptually relevant spectral distances, which are used to design mapping functions from distance to proximity. A set of operators are optimized in the design phase to implement on-the-fly compilation of executable transformations used in the transformation phase. Proximity of the current input parameters to the speaker's own vowel templates is used in this compilation. The proposed method deforms the source speaker's parameter space using a set of monotonic and continuous mapping functions. This smooth and topology-preserving mapping yields high-quality modification of existing speech resources.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kawahara, H.; Nisimura, R.; Irino, T.] Wakayama Univ, Fac Syst Eng,
   Dept Informat Sci, 930 Sakaedani, Wakayama 6408510, Japan.
   <br>[Morise, M.] Ritsumeikan Univ, Tokyo, Japan.
   <br>[Takahashi, T.] Kyoto Univ, Grad Sch, Kyoto 6068501, Japan.
   <br>[Banno, H.] Meiji Univ, Tokyo 101, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kawahara, H (reprint author), Wakayama Univ, Fac Syst Eng, Dept Informat Sci, 930 Sakaedani, Wakayama 6408510, Japan.</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Kawahara, Hideki</display_name>&nbsp;</font></td><td><font size="3">0000-0001-9360-5700&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">BP </td><td>4818</td>
</tr>

<tr>
<td valign="top">EP </td><td>4821</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/ICASSP.2010.5495138</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000287096004183</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ohtani, Y
   <br>Toda, T
   <br>Saruwatari, H
   <br>Shikano, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ohtani, Yamato
   <br>Toda, Tomoki
   <br>Saruwatari, Hiroshi
   <br>Shikano, Kiyohiro</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>NON-PARALLEL TRAINING FOR MANY-TO-MANY EIGENVOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2010 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>2010 IEEE International Conference on Acoustics, Speech, and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 14-19, 2010</td>
</tr>

<tr>
<td valign="top">CL </td><td>Dallas, TX</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; Gaussian mixture model; eigenvoice; many-to-many;
   non-parallel training</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a novel training method of an eigenvoice Gaussian mixture model (EV-GMM) effectively using non-parallel data sets for many-to-many eigenvoice conversion, which is a technique for converting an arbitrary source speaker's voice into an arbitrary target speaker's voice. In the proposed method, an initial EV-GMM is trained with the conventional method using parallel data sets consisting of a single reference speaker and multiple pre-stored speakers. Then, the initial EV-GMM is further refined using non-parallel data sets including a larger number of pre-stored speakers while considering the reference speaker's voices as hidden variables. The experimental results demonstrate that the proposed method yields significant quality improvements in converted speech by enabling us to use data of a larger number of pre-stored speakers.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ohtani, Yamato; Toda, Tomoki; Saruwatari, Hiroshi; Shikano, Kiyohiro]
   Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Ikoma, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ohtani, Y (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Ikoma, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>yamato-o@is.naist.jp; tomoki@is.naist.jp; sawatari@is.naist.jp;
   shikano@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>10</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>10</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">BP </td><td>4822</td>
</tr>

<tr>
<td valign="top">EP </td><td>4825</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/ICASSP.2010.5495139</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000287096004184</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Qiao, Y
   <br>Saito, D
   <br>Minematsu, N</td>
</tr>

<tr>
<td valign="top">AF </td><td>Qiao, Yu
   <br>Saito, Daisuke
   <br>Minematsu, Nobuaki</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>HMM-BASED SEQUENCE-TO-FRAME MAPPING FOR VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2010 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>2010 IEEE International Conference on Acoustics, Speech, and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 14-19, 2010</td>
</tr>

<tr>
<td valign="top">CL </td><td>Dallas, TX</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; sequence-to-frame mapping; HMM; speech synthesis</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH SYNTHESIS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion can be reduced to a problem to find a transformation function between the corresponding speech sequences of two speakers. Perhaps the most voice conversions methods are GMM-based statistical mapping methods [1, 2]. However, the classical GMM-based mapping is frame-to-frame, and cannot take account of the contextual information existing over a speech sequence. It is well known that HMM yields an efficient method to model the density of a whole speech sequence and has found great successes in speech recognition and synthesis. Inspired by this fact, this paper studies how to use HMM for voice conversion. We derive an HMM-based sequence-to-frame mapping function with statistical analysis. Different from previous HMM-based voice conversion methods [3, 4, 5] that used forced alignment for segmentation and transform frames aligned to a state with its associated linear transformation, our method has a soft mapping function as a weighted summation of linear transformations. The weights are calculated as the HMM posterior probabilities of frames. We also propose and compare two methods to learn the parameters of our mapping functions, namely least square error estimation and maximum likelihood estimation. We carried out experiments to examine the proposed HMM-based method for voice conversion.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Qiao, Yu; Saito, Daisuke; Minematsu, Nobuaki] Univ Tokyo, Bunkyo Ku,
   Tokyo 1130033, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Qiao, Y (reprint author), Univ Tokyo, Bunkyo Ku, Tokyo 1130033, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>qiao@gavo.t.u-tokyo.ac.jp; dsk_saito@gavo.t.u-tokyo.ac.jp;
   mine@gavo.t.u-tokyo.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">BP </td><td>4830</td>
</tr>

<tr>
<td valign="top">EP </td><td>4833</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/ICASSP.2010.5495141</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000287096004186</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nwe, TL
   <br>Dong, MH
   <br>Chan, P
   <br>Wang, X
   <br>Ma, B
   <br>Li, HZ</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nwe, Tin Lay
   <br>Dong, Minghui
   <br>Chan, Paul
   <br>Wang, Xi
   <br>Ma, Bin
   <br>Li, Haizhou</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>VOICE CONVERSION: FROM SPOKEN VOWELS TO SINGING VOWELS</td>
</tr>

<tr>
<td valign="top">SO </td><td>2010 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME 2010)</td>
</tr>

<tr>
<td valign="top">SE </td><td>IEEE International Conference on Multimedia and Expo</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Multimedia and Expo</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUL 19-23, 2010</td>
</tr>

<tr>
<td valign="top">CL </td><td>Singapore, SINGAPORE</td>
</tr>

<tr>
<td valign="top">DE </td><td>Synthesis; music; speech; singing; voice conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>FREQUENCY</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, a voice conversion system that converts spoken vowels into singing vowels is proposed. Given the spoken vowels and their musical score, the system generates singing vowels. The system modifies the speech parameters of Fundamental frequency (F0), duration and spectral properties to produce singing voice. F0 contour is obtained using F0 fluctuation information from training singing voice and music score. Duration of each vowel of speech is stretched or shortened according to the length of the corresponding musical note. To transform speech spectrum to singing spectrum the following two approaches are employed. The first method employs spectral mean shifting and variance scaling method. And, the second approach uses weighted linear transformation method to transform speech to singing spectrum. The system is tested on the database including 75 speech and 30 singing voices sung using vowels. The results show that the proposed system is able to convert spoken vowels into singing vowels with a quality very close to the target singing voice.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nwe, Tin Lay; Dong, Minghui; Chan, Paul; Wang, Xi; Ma, Bin; Li,
   Haizhou] ASTAR, Inst Infocomm Res I2R, Singapore 138632, Singapore.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nwe, TL (reprint author), ASTAR, Inst Infocomm Res I2R, 1 Fusionopolis Way, Singapore 138632, Singapore.</td>
</tr>

<tr>
<td valign="top">EM </td><td>tlnma@i2r.a-star.edu.sg; mhdong@i2r.a-star.edu.sg;
   ychan@i2r.a-star.edu.sg; wangxi@i2r.a-star.edu.sg;
   mabin@i2r.a-star.edu.sg; hli@i2r.a-star.edu.sg</td>
</tr>

<tr xmlns:date="http://exslt.org/dates-and-times">
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Li, Haizhou</display_name>&nbsp;</font></td><td><font size="3">Q-6438-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Li, Haizhou</display_name>&nbsp;</font></td><td><font size="3">0000-0001-9158-9401&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">BP </td><td>1421</td>
</tr>

<tr>
<td valign="top">EP </td><td>1426</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000287977700251</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Xu, N
   <br>Yang, Z
   <br>Guo, HY</td>
</tr>

<tr>
<td valign="top">AF </td><td>Xu, Ning
   <br>Yang, Zhen
   <br>Guo, Haiyan</td>
</tr>

<tr>
<td valign="top">BE </td><td>Chen, W
   <br>Li, S</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Conversion with a Strategy for Separating Speaker Individuality
   Using State-Space Model</td>
</tr>

<tr>
<td valign="top">SO </td><td>2010 IEEE INTERNATIONAL CONFERENCE ON WIRELESS COMMUNICATIONS,
   NETWORKING AND INFORMATION SECURITY (WCNIS), VOL 1</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Wireless Communications, Networking and
   Information Security (WCNIS)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUN 25-27, 2010</td>
</tr>

<tr>
<td valign="top">CL </td><td>N China Elect Power Univ, Beijing, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">HO </td><td>N China Elect Power Univ</td>
</tr>

<tr>
<td valign="top">DE </td><td>Spectral envelope evolution; state-space model; voice conversion</td>
</tr>

<tr>
<td valign="top">AB </td><td>It is well known that the key to voice conversion (VC) is to transform the spectral parameters of the source speaker to match that of the target speaker, where Gaussian mixture model (GMM) based statistical transformations have been commonly studied. However, these methods are performed using a frame-by-frame procedure, disregarding spectral envelope evolution and resulting in the significantly degraded quality of the converted speech. In this paper, we propose a new voice conversion method using the state-space model (SSM) that can essentially describe the feature of dynamics between frames. Then, physical meaning of SSM for voice conversion has been examined, leading to the novel SSM-based training and transforming procedures. Experiments using both objective and subjective measurements show that the proposed SSM-based method significantly outperforms the traditional GMM-based technique.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Xu, Ning; Yang, Zhen; Guo, Haiyan] Nanjing Univ Posts &amp; Telecommun,
   Inst Signal Proc &amp; Transmiss, Nanjing, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Xu, N (reprint author), Nanjing Univ Posts &amp; Telecommun, Inst Signal Proc &amp; Transmiss, Nanjing, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>xuningdlts@gmail.com; yangz@njupt.edu.cn; D0718@njupt.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">BP </td><td>298</td>
</tr>

<tr>
<td valign="top">EP </td><td>301</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/WCINS.2010.5541787</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000287767500064</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Li, JL
   <br>Yang, HW
   <br>Zhang, WZ
   <br>Cai, LH</td>
</tr>

<tr>
<td valign="top">AF </td><td>Li, Jinlong
   <br>Yang, Hongwu
   <br>Zhang, Weizhao
   <br>Cai, Lianhong</td>
</tr>

<tr>
<td valign="top">BE </td><td>Zhang, Y
   <br>Tan, H</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Lyrics to Singing Voice Synthesis system with variable timbre</td>
</tr>

<tr>
<td valign="top">SO </td><td>2010 THE 3RD INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND
   INDUSTRIAL APPLICATION (PACIIA2010), VOL II</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>3rd International Conference on Computational Intelligence and
   Industrial Application (PACIIA2010)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 04-05, 2010</td>
</tr>

<tr>
<td valign="top">CL </td><td>Wuhan, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Singing voice synthesis; melody control model; timbre modification; GMM;
   MIDI</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we present a singing voice synthesis system, which can convert lyrics to singing voice. As the synthetic song's timbre is too monotonous, a new singing voice morphing algorithm based on GMM (Gaussian Mixture Model) was presented accordingly. The MOS test shows that the average MOS score of synthesized song is above 3.3 before timbre conversion. The professional singer's timbre can be added proportionally by changing the scale factor k in the system. The ABX test demonstrates that the accuracy can be up to 100% in the case of k=0 or k=1, and it can be higher than 64.5% in the case of 0&lt;k&lt;1. The experiments also show the mean of GMM has greater impact on a singer's timbre than weight ratio and covariance.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Li, Jinlong; Yang, Hongwu; Zhang, Weizhao] Northwest Normal Univ, Dept
   Phys &amp; Elect Engn, Lanzhou, Gansu, Peoples R China.
   <br>[Cai, Lianhong] Tsinghua Univ, Dept Comp Sci &amp; Technol, Beijing, Peoples
   R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Li, JL (reprint author), Northwest Normal Univ, Dept Phys &amp; Elect Engn, Lanzhou, Gansu, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>yang-hw03@mails.tsinghua.edu.cn; clh-dcs@tsinghua.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">BP </td><td>109</td>
</tr>

<tr>
<td valign="top">EP </td><td>112</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000398765400027</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Tao, Z
   <br>Tan, XD
   <br>Han, T
   <br>Gu, JH
   <br>Xu, YS
   <br>Zhao, HM</td>
</tr>

<tr>
<td valign="top">AF </td><td>Tao, Zhi
   <br>Tan, Xue-Dan
   <br>Han, Tao
   <br>Gu, Ji-Hua
   <br>Xu, Yi-Shen
   <br>Zhao, He-Ming</td>
</tr>

<tr>
<td valign="top">BE </td><td>Yu, F
   <br>Peng, X
   <br>Liu, H
   <br>Shu, J
   <br>Ng, R</td>
</tr>

<tr>
<td valign="top">TI </td><td>Reconstruction of Normal Speech from Whispered Speech based on RBF
   Neural Network</td>
</tr>

<tr>
<td valign="top">SO </td><td>2010 THIRD INTERNATIONAL SYMPOSIUM ON INTELLIGENT INFORMATION TECHNOLOGY
   AND SECURITY INFORMATICS (IITSI 2010)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>3rd International Symposium on Intelligent Information Technology and
   Security Informatics</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 02-04, 2010</td>
</tr>

<tr>
<td valign="top">CL </td><td>Jinggangshan, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>whispered speech; voice conversion; radial basis function neural network</td>
</tr>

<tr>
<td valign="top">AB </td><td>Restriction of normal speech from Chinese whispered speech based on radial basis function neural network (RBF NN) is proposed in this paper. Firstly, capture the nonlinear mapping of spectral envelope between whispered and normal speech by RBF NN; secondly, modify the spectral envelope of the whispered speech by adopting the trained neural network; finally, convert the whispered speech into normal speech by using the linear spectral pairs (LSP) synthesizer. Both subjective and objective assessments are conducted on the converted speech quality. Simulation results show that the score of the Mean Opinion Score (MOS) is 3.2; the distorted distance of bark spectrum is decreased. Both intelligibility and quality of the converted speech are satisfied.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Tao, Zhi; Tan, Xue-Dan; Han, Tao; Gu, Ji-Hua; Xu, Yi-Shen] Soochow
   Univ, Dept Phys Sci &amp; Tech, Suzhou, Peoples R China.
   <br>[Tao, Zhi; Zhao, He-Ming] Soochow Univ, Dept Electron, Suzhou, Peoples R
   China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Tao, Z (reprint author), Soochow Univ, Dept Phys Sci &amp; Tech, Suzhou, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>taoz@suda.edu.cn; hmzhao@suda.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">BP </td><td>374</td>
</tr>

<tr>
<td valign="top">EP </td><td>377</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/IITSI.2010.118</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000394796500083</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Li, YP
   <br>Zhang, LH
   <br>Ding, H</td>
</tr>

<tr>
<td valign="top">AF </td><td>Li, Yanping
   <br>Zhang, Linghua
   <br>Ding, Hui</td>
</tr>

<tr>
<td valign="top">BE </td><td>Wang, FL
   <br>Deng, H
   <br>Gao, Y
   <br>Lei, JS</td>
</tr>

<tr>
<td valign="top">TI </td><td>Text-Independent Voice Conversion Based on Kernel Eigenvoice</td>
</tr>

<tr>
<td valign="top">SO </td><td>ARTIFICIAL INTELLIGENCE AND COMPUTATIONAL INTELLIGENCE, PT I</td>
</tr>

<tr>
<td valign="top">SE </td><td>Lecture Notes in Artificial Intelligence</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Artificial Intelligence and Computational
   Intelligence (AICI)</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 23-24, 2010</td>
</tr>

<tr>
<td valign="top">CL </td><td>Sanya, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; Gaussian mixture model; text independent; kernel
   eigenvoice; adaptation</td>
</tr>

<tr>
<td valign="top">AB </td><td>Almost of the current spectral conversion methods required parallel corpus containing the same utterances from source and target speakers, which was often inconvenient and sometimes hard to fulfill. This paper proposed a novel algorithm for text-independent voice conversion, which can relax the parallel constraint. The proposed algorithm was based on speaker adaptation technique of kernel eigenvoice, adapting the conversion parameters derived for the pre-stored pairs of speakers to a desired pair, for which only a nonparallel corpus was available. Objective evaluation results demonstrated that the proposed kernel eigenvoice algorithm can effectively improve converted spectral similarity in a text-independent manner.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Li, Yanping; Zhang, Linghua] Nanjing Univ Posts &amp; Telecommun, Coll
   Telecommun &amp; Informat Engn, Nanjing, Peoples R China.
   <br>[Ding, Hui] Jiaxing Univ, Coll Math &amp; Informat Engn, Jiangxi, Peoples R
   China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Li, YP (reprint author), Nanjing Univ Posts &amp; Telecommun, Coll Telecommun &amp; Informat Engn, Nanjing, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>liyp@njupt.edu.cn; zhanglh@njupt.edu.cn; dh_jxxy@yahoo.com.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">VL </td><td>6319</td>
</tr>

<tr>
<td valign="top">BP </td><td>432</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000292891700051</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Li, BJ
   <br>Wu, DL
   <br>Jiang, H</td>
</tr>

<tr>
<td valign="top">AF </td><td>Li, Baojie
   <br>Wu, Dalei
   <br>Jiang, Hui</td>
</tr>

<tr>
<td valign="top">BE </td><td>Fred, A
   <br>Filipe, J
   <br>Gamboa, H</td>
</tr>

<tr>
<td valign="top">TI </td><td>MODEL-MAPPING BASED VOICE CONVERSION SYSTEM A Novel Approach to Improve
   Voice Similarity and Naturalness using Model-based Speech Synthesis
   Techniques</td>
</tr>

<tr>
<td valign="top">SO </td><td>BIOSIGNALS 2010: PROCEEDINGS OF THE THIRD INTERNATIONAL CONFERENCE ON
   BIO-INSPIRED SYSTEMS AND SIGNAL PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>3rd International Conference on Bio-Inspired Systems and Signal
   Processing (BIOSIGNALS 2010)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JAN 20-23, 2010</td>
</tr>

<tr>
<td valign="top">CL </td><td>Valencia, SPAIN</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; HMM-based speech synthesis; GMM; Model mapping</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper we present a novel voice conversion application in which no any knowledge of source speakers is available, but only sufficient utterances from a target speaker and a number of other speakers are in hand. Our approach consists in two separate stages. At the training stage, we estimate a speaker dependent (SD) Gaussian mixture model (GMM) for the target speaker and additionally, we also estimate a speaker independent (SI) GMM by using the data from a number of speakers other than the source speaker. A mapping correlation between the SD and the SI model is maintained during the training process in terms of each phone label. At the conversion stage, we use the SI GMM to recognize each input frame and find the closest Gaussian mixture for it. Next, according to a mapping list, the counterpart Gaussian of the SD GMM is obtained and then used to generate a parameter vector for each frame vector. Finally all the generated vectors are concatenated to synthesize speech of the target speaker. By using the Proposed model-mapping approach, we can not only avoid the over-fitting problem by keeping the number of mixtures of the SI GMM to a fixed value, but also simultaneously improve voice quality in terms of similarity and naturalness by increasing the number of mixtures of the SD GMM. Experiments showed the effectiveness of this method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Li, Baojie; Wu, Dalei; Jiang, Hui] York Univ, Dept Comp Sci &amp; Engn,
   4700 Keele St, Toronto, ON M3J 1P3, Canada.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Li, BJ (reprint author), York Univ, Dept Comp Sci &amp; Engn, 4700 Keele St, Toronto, ON M3J 1P3, Canada.</td>
</tr>

<tr>
<td valign="top">EM </td><td>lbjgavo@hotmail.com; daleiwu@cse.yorku.ca; hj@cse.yorku.ca</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">BP </td><td>442</td>
</tr>

<tr>
<td valign="top">EP </td><td>446</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Mathematical &amp; Computational Biology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000392907300074</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Pribil, J
   <br>Pribilova, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Pribil, Jiri
   <br>Pribilova, Anna</td>
</tr>

<tr>
<td valign="top">BE </td><td>Esposito, A
   <br>Campbell, N
   <br>Vogel, C
   <br>Hussain, A
   <br>Nijholt, A</td>
</tr>

<tr>
<td valign="top">TI </td><td>Microintonation Analysis of Emotional Speech</td>
</tr>

<tr>
<td valign="top">SO </td><td>DEVELOPMENT OF MULTIMODAL INTERFACES: ACTIVE LISTING AND SYNCHRONY</td>
</tr>

<tr>
<td valign="top">SE </td><td>Lecture Notes in Computer Science</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>2nd COST 2102 International Training School on Development of Multimodal
   Interfaces</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 23-27, 2009</td>
</tr>

<tr>
<td valign="top">CL </td><td>Dublin, IRELAND</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speech analysis; emotional speech; jitter; microintonation</td>
</tr>

<tr>
<td valign="top">ID </td><td>CEPSTRAL DESCRIPTION; VOICE QUALITY; TTS SYSTEM; COMMUNICATION;
   DECOMPOSITION; CONVERSION; JITTER</td>
</tr>

<tr>
<td valign="top">AB </td><td>The paper addresses reflection of microintonation in male and female acted emotional speech. Microintonation component of speech melody is analyzed regarding its spectral and statistical parameters. Achieved statistical results of microintonation analysis show good correlation comparing male and female voices for four emotional states (joy, sadness, anger, neutral state) portrayed by several professional actors.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Pribil, Jiri] Acad Sci Czech Republic, Inst Photon &amp; Elect, Vvi,
   Chaberska 57, CZ-18251 Prague 8, Czech Republic.
   <br>[Pribil, Jiri] Inst Measurement Sci, SAS, Bratislava SK-84104, Slovakia.
   <br>[Pribilova, Anna] Slovak Tech Univ, Fac Elect Engn &amp; Informat Technol,
   Dept Radio Elect, Bratislava SK-812 19, Slovakia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Pribil, J (reprint author), Acad Sci Czech Republic, Inst Photon &amp; Elect, Vvi, Chaberska 57, CZ-18251 Prague 8, Czech Republic.</td>
</tr>

<tr>
<td valign="top">EM </td><td>Jiri.Pribil@savba.sk; Anna.Pribilova@stuba.sk</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">VL </td><td>5967</td>
</tr>

<tr>
<td valign="top">BP </td><td>268</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000279734900022</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Pierre, JM</td>
</tr>

<tr>
<td valign="top">AF </td><td>Pierre, Joseph M.</td>
</tr>

<tr>
<td valign="top">TI </td><td>Hallucinations in Nonpsychotic Disorders: Toward a Differential
   Diagnosis of "Hearing Voices"</td>
</tr>

<tr>
<td valign="top">SO </td><td>HARVARD REVIEW OF PSYCHIATRY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>auditory hallucinations; conversion disorder; culture; diagnosis; DSM-V;
   pseudohallucination; psychosis; PTSD</td>
</tr>

<tr>
<td valign="top">ID </td><td>POSTTRAUMATIC-STRESS-DISORDER; AUDITORY VERBAL HALLUCINATIONS; CHILDHOOD
   SEXUAL-ABUSE; MUSICAL HALLUCINATIONS; PSYCHOTIC SYMPTOMS;
   GENERAL-POPULATION; CONVERSIVE HALLUCINATIONS; ACQUIRED DEAFNESS;
   PSEUDO-HALLUCINATIONS; TRUE HALLUCINATIONS</td>
</tr>

<tr>
<td valign="top">AB </td><td>While auditory hallucinations (AH) are prototypic psychotic symptoms whose clinical presence is often equated with a psychotic disorder, they are commonly found among those without mental illness as well as those with nonpsychotic disorders not typically associated with hallucinations in DSM-IV. This incongruity presents a significant challenge for clinical work and efforts to revise the next iteration of the DSM. Auditory hallucinations found among "normal" people suggest that either AH are not as pathologic as they are typically taken to be, or that less-than-hallucinatory experiences are routinely mischaracterized as AH. Such hallucinations in the context of conversion disorder, trauma, sensory deprivation, and certain cultural settings strengthen an association between AH and psychopathology but suggest limited diagnostic specificity and relevance. It may be useful to think of AH like coughs-common experiences that are often, but not always, symptoms of pathology associated with a larger illness. Although these issues have been known for many years, they are rarely discussed In American psychiatry and need to be addressed in future research and clinical work. (HARV REV PSYCHIATRY 2010;18:22-35.)</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Pierre, Joseph M.] Univ Calif Los Angeles, David Geffen Sch Med, Dept
   Psychiat &amp; Behav Sci, Los Angeles, CA 90095 USA.
   <br>[Pierre, Joseph M.] VA Greater Los Angeles Healthcare Syst, Dept
   Psychiat, Los Angeles, CA USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Pierre, JM (reprint author), VA Greater Los Angeles Healthcare Syst Psychiat, 11301 Wilshire Blvd,Bldg 210,Room 15, Los Angeles, CA 90073 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>joseph.pierre2@va.gov</td>
</tr>

<tr>
<td valign="top">TC </td><td>38</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>38</td>
</tr>

<tr>
<td valign="top">PD </td><td>JAN-FEB</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">VL </td><td>18</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>22</td>
</tr>

<tr>
<td valign="top">EP </td><td>35</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.3109/10673220903523706</td>
</tr>

<tr>
<td valign="top">SC </td><td>Psychiatry</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000275057900002</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Huang, YP
   <br>Chiu, HW
   <br>Chuan, WP
   <br>Sandnes, FE</td>
</tr>

<tr>
<td valign="top">AF </td><td>Huang, Yo-Ping
   <br>Chiu, Hong-Wen
   <br>Chuan, Wei-Po
   <br>Sandnes, Frode Eika</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Discovering Fuzzy Association Rules from Patient's Daily Text Messages
   to Diagnose Melancholia</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN AND CYBERNETICS (SMC 2010)</td>
</tr>

<tr>
<td valign="top">SE </td><td>IEEE International Conference on Systems Man and Cybernetics Conference
   Proceedings</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Systems, Man and Cybernetics</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 10-13, 2010</td>
</tr>

<tr>
<td valign="top">CL </td><td>Istanbul, TURKEY</td>
</tr>

<tr>
<td valign="top">DE </td><td>Data Mining; association rules; fuzzy model; word segmentation</td>
</tr>

<tr>
<td valign="top">ID </td><td>SALINITY</td>
</tr>

<tr>
<td valign="top">AB </td><td>With the constant stress from work load and daily life people may show symptoms of melancholia. However, most people are reluctant to describe it or may not know that they already have it. In this paper a novel system is proposed to discover clues from patient's interaction with psychologist or from self-recorded voice or text messages. A user friendly interface is provided for patients to input text messages or record a voice file by mobile phones or other input devices. A speech-to-text conversion software is used to convert voice mails to simple text files in advance. Based on the text files, a data mining model is used to discover frequent keywords mentioned in the text or speech files. The association rules can be used to help psychologists diagnose patients' degree of melancholia. Experimental results show that the proposed system can effectively discover melancholia keywords.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Huang, Yo-Ping; Chiu, Hong-Wen; Chuan, Wei-Po] Natl Taipei Univ
   Technol, Dept Elect Engn, Taipei 10608, Taiwan.
   <br>[Sandnes, Frode Eika] Univ Oslo Coll, Fac Engn, Oslo, Norway.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Huang, YP (reprint author), Natl Taipei Univ Technol, Dept Elect Engn, Taipei 10608, Taiwan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>yphuang@ntut.edu.tw; frodes@hio.no</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Sandnes, Frode Eika</display_name>&nbsp;</font></td><td><font size="3">0000-0001-7781-748X&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">BP </td><td>3523</td>
</tr>

<tr>
<td valign="top">EP </td><td>3528</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000295015303067</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Pawig, M
   <br>Enzner, G
   <br>Vary, P</td>
</tr>

<tr>
<td valign="top">AF </td><td>Pawig, Matthias
   <br>Enzner, Gerald
   <br>Vary, Peter</td>
</tr>

<tr>
<td valign="top">TI </td><td>Adaptive Sampling Rate Correction for Acoustic Echo Control in
   Voice-Over-IP</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE TRANSACTIONS ON SIGNAL PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Acoustic signal processing; adaptive filters; echo suppression;
   interpolation; least mean squares methods; resampling; teleconferencing</td>
</tr>

<tr>
<td valign="top">ID </td><td>FILTERS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Hands-free terminals for speech communication employ adaptive filters to reduce echoes resulting from the acoustic coupling between loudspeaker and microphone. When using a personal computer with commercial audio hardware for teleconferencing, a sampling frequency offset between the loudspeaker output D/A converter and the microphone input A/D converter often occurs. In this case, state-of-the-art echo cancellation algorithms fail to track the correct room impulse response. In this paper, we present a novel least mean square (LMS-type) adaptive algorithm to estimate the frequency offset and resynchronize the signals using arbitrary sampling rate conversion. In conjunction with a normalized LMS-type adaptive filter for room impulse response tracking, the proposed system widely removes the deteriorating effects of a frequency offset up to several Hz and restores the functionality of echo cancellation.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Pawig, Matthias; Vary, Peter] Rhein Westfal TH Aachen, Inst Commun Syst
   &amp; Data Proc, D-52065 Aachen, Germany.
   <br>[Enzner, Gerald] Ruhr Univ Bochum, Inst Commun Acoust, D-44780 Bochum,
   Germany.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Pawig, M (reprint author), Rhein Westfal TH Aachen, Inst Commun Syst &amp; Data Proc, D-52065 Aachen, Germany.</td>
</tr>

<tr>
<td valign="top">EM </td><td>pawig@ind.rwth-aachen.de; gerald.enzner@rub.de; vary@ind.rwth-aachen.de</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Enzner, Gerald</display_name>&nbsp;</font></td><td><font size="3">0000-0002-3096-1342&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>15</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>15</td>
</tr>

<tr>
<td valign="top">PD </td><td>JAN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">VL </td><td>58</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>189</td>
</tr>

<tr>
<td valign="top">EP </td><td>199</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TSP.2009.2028187</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000272843900015</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Pflug, B
   <br>Kumarapeli, P
   <br>Van Vlymen, J
   <br>Ammenwerth, E
   <br>De Lusignan, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Pflug, Bernhar
   <br>Kumarapeli, Pushpa
   <br>van Vlymen, Jeremy
   <br>Ammenwerth, Elske
   <br>de Lusignan, Simon</td>
</tr>

<tr>
<td valign="top">TI </td><td>Measuring the impact of the computer on the consultation: An open source
   application to combine multiple observational outputs</td>
</tr>

<tr>
<td valign="top">SO </td><td>INFORMATICS FOR HEALTH &amp; SOCIAL CARE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Video recordings; process assessment; computer; general practice;
   medical records system; computerised; family practice; clinical
   consultation; software; open source</td>
</tr>

<tr>
<td valign="top">ID </td><td>PATIENT; FRAMEWORK; QUALITY; SYSTEMS</td>
</tr>

<tr>
<td valign="top">AB </td><td>A diverse range of tools and techniques can be used to observe the clinical consultation and the use of information technology. These technologies range from transcripts; to video observation with one or more cameras; to voice and pattern recognition applications. Currently, these have to be observed separately and there is limited capacity to combine them. Consequently, when multiple methods are used to analyse the consultation a significant proportion of time is spent linking events in one log file (e. g. mouse movements and keyboard use when prescribing alerts appear) with what was happening in the consultation at that time. The objective of this study was to develop an application capable of combining and comparing activity log-files and with facilities to view simultaneously all data relating to any time point or activity. Interviews, observations and design prototypes were used to develop a specification. Class diagram of the application design was used to make further development decisions. The application development used object-orientated design principles. We used open source tools; Java as the programming language and JDeveloper (TM) as the development environment. The final output is log file aggregation (LFA) tool which forms part of the wider aggregation of log files for analysis (ALFA) open source toolkit (www.biomedicalinformatics.info/alfa/). Testing was done using sample log files and reviewed the application's utility for analysis of the consultation activities. Separation of the presentation and functionality in the design stage enabled us to develop a modular and extensible application. The application is capable of converting and aggregating several log files of different formats and displays them in different presentation layouts. We used the Java Media Framework to aggregate video channels. Java extensible mark-up language (XML) package facilitated the conversion of aggregated output into XML format. Analysts can now move easily between observation tools and find all the data related to an activity. The LFA application makes new analysis tasks feasible and established tasks much more efficient. Researchers can now store multiple log file data as a single file isolate and investigate different doctor-computer-patient interaction.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kumarapeli, Pushpa; van Vlymen, Jeremy; de Lusignan, Simon] St Georges
   Univ London, Div Community Hlth Sci, Dept Primary Care Informat, London
   SW17 0RE, England.
   <br>[Pflug, Bernhar; Ammenwerth, Elske] Univ Hlth Sci, Dept Med Informat &amp;
   Technol, Tyrol, Austria.</td>
</tr>

<tr>
<td valign="top">RP </td><td>De Lusignan, S (reprint author), St Georges Univ London, Div Community Hlth Sci, Dept Primary Care Informat, London SW17 0RE, England.</td>
</tr>

<tr>
<td valign="top">EM </td><td>slusigna@sgul.ac.uk</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>de Lusignan, Simon</display_name>&nbsp;</font></td><td><font size="3">A-4125-2009&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>de Lusignan, Simon</display_name>&nbsp;</font></td><td><font size="3">A-6609-2011&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>de Lusignan, Simon</display_name>&nbsp;</font></td><td><font size="3">0000-0002-8553-2641&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>5</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>5</td>
</tr>

<tr>
<td valign="top">PD </td><td>JAN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">VL </td><td>35</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>10</td>
</tr>

<tr>
<td valign="top">EP </td><td>24</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.3109/17538150903358693</td>
</tr>

<tr>
<td valign="top">SC </td><td>Health Care Sciences &amp; Services; Medical Informatics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000285004600002</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Yamamoto, K
   <br>Nakagawa, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Yamamoto, Kazumasa
   <br>Nakagawa, Seiichi</td>
</tr>

<tr>
<td valign="top">BE </td><td>Hullermeier, E
   <br>Kruse, R
   <br>Hoffmann, F</td>
</tr>

<tr>
<td valign="top">TI </td><td>Evaluation of Privacy Protection Techniques for Speech Signals</td>
</tr>

<tr>
<td valign="top">SO </td><td>INFORMATION PROCESSING AND MANAGEMENT OF UNCERTAINTY IN KNOWLEDGE-BASED
   SYSTEMS: APPLICATIONS, PT II</td>
</tr>

<tr>
<td valign="top">SE </td><td>Communications in Computer and Information Science</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>13th International Conference on Information Processing and Management
   of Uncertainty in Knowledge-Based Systems</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUN 28-JUL 02, 2010</td>
</tr>

<tr>
<td valign="top">CL </td><td>Dortmund, GERMANY</td>
</tr>

<tr>
<td valign="top">DE </td><td>Privacy protection; Speech signal; Personal information; Speech
   elimination; Voice conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>A ubiquitous networked society, in which all electronic equipment including "sensors" are connected to a network and are able to communicate with one another to share information, will shortly become a reality. Although sensor information is most important in such a network, it does include a large amount of privacy information and therefore it is preferable not to send raw information across the network. In this paper, we focus on privacy protection for speech, where privacy information in speech is defined as the "speaker's characteristics" and "linguistic privacy information." We set out to protect privacy information by using "voice conversion" and "deletion of privacy linguistic information from the results of speech recognition." However, since speech recognition technology is not robust enough in real environments, "speech elimination" technique is also considered. In this paper, we focus mainly on the evaluation of speech elimination and voice conversion.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Yamamoto, Kazumasa; Nakagawa, Seiichi] Toyohashi Univ Technol, Dept
   Comp Sci &amp; Engn, Aichi 4418580, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Yamamoto, K (reprint author), Toyohashi Univ Technol, Dept Comp Sci &amp; Engn, 1-1 Hibarigaoka,Tenpaku Cho, Aichi 4418580, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>kyama@slp.cs.tut.ac.jp; nakagawa@slp.cs.tut.ac.jp</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>nakagawa, seiichi</display_name>&nbsp;</font></td><td><font size="3">L-5543-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">VL </td><td>81</td>
</tr>

<tr>
<td valign="top">BP </td><td>653</td>
</tr>

<tr>
<td valign="top">EP </td><td>662</td>
</tr>

<tr>
<td valign="top">PN </td><td>II</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000290642600067</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Farrus, M
   <br>Wagner, M
   <br>Erro, D
   <br>Hernando, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Farrus, Mireia
   <br>Wagner, Michael
   <br>Erro, Daniel
   <br>Hernando, Javier</td>
</tr>

<tr>
<td valign="top">TI </td><td>Automatic speaker recognition as a measurement of voice imitation and
   conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERNATIONAL JOURNAL OF SPEECH LANGUAGE AND THE LAW</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>IMITATION; VOICE CONVERSION; PROSODY; JITTER; SHIMMER; SPEAKER
   RECOGNITION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voices can be deliberately disguised by means of human imitation or voice conversion. The question arises to what extent they can be modified by using either method. In the current paper, a set of speaker identification experiments are conducted; first, analysing some prosodic features extracted from voices of professional impersonators attempting to mimic a target voice and, second, using both intragender and crossgender converted voices in a spectral-based speaker recognition system. The results obtained in the current experiments show that the identification error rate increases when testing with imitated voices, as well as when using converted voices, especially the crossgender conversions.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Farrus, Mireia; Erro, Daniel; Hernando, Javier] Univ Politecn Cataluna,
   E-08028 Barcelona, Spain.
   <br>[Wagner, Michael] Univ Canberra, Canberra, ACT 2601, Australia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Farrus, M (reprint author), Univ Politecn Cataluna, E-08028 Barcelona, Spain.</td>
</tr>

<tr>
<td valign="top">EM </td><td>mfarrus@gps.tsc.upc.edu</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Erro, Daniel</display_name>&nbsp;</font></td><td><font size="3">H-7043-2015&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Farrus, Mireia</display_name>&nbsp;</font></td><td><font size="3">K-5525-2017&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Hernando, Javier</display_name>&nbsp;</font></td><td><font size="3">G-1863-2014&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Farrus, Mireia</display_name>&nbsp;</font></td><td><font size="3">O-1402-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Erro, Daniel</display_name>&nbsp;</font></td><td><font size="3">0000-0003-0954-6942&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Farrus, Mireia</display_name>&nbsp;</font></td><td><font size="3">0000-0002-7160-9513&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Farrus, Mireia</display_name>&nbsp;</font></td><td><font size="3">0000-0002-7160-9513&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>6</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>6</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">VL </td><td>17</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>119</td>
</tr>

<tr>
<td valign="top">EP </td><td>142</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1558/ijsll.v17i1.119</td>
</tr>

<tr>
<td valign="top">SC </td><td>Criminology &amp; Penology; Linguistics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000279892500006</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Mousa, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Mousa, Allam</td>
</tr>

<tr>
<td valign="top">TI </td><td>VOICE CONVERSION USING PITCH SHIFTING ALGORITHM BY TIME STRETCHING WITH
   PSOLA AND RE-SAMPLING</td>
</tr>

<tr>
<td valign="top">SO </td><td>JOURNAL OF ELECTRICAL ENGINEERING-ELEKTROTECHNICKY CASOPIS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice changing; PSOLA; pitch shifting; resampling; SIFT</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice changing has many applications in the industry and commercial filed. This paper emphasizes voice conversion using a pitch shifting method which depends on detecting the pitch of the signal (fundamental frequency) Using simplified Inverse Filter Tracking (SIFT) and changing it according to the target pitch period using time stretching with Pitch Synchronous Over Lap Add Algorithm (PSOLA), then resampling the signal in order to have the same play rate. The same study was performed to see the effect of voice conversion when some Arabic speech signal is considered. Treatment of certain Arabic voiced vowels and the conversion between male and female speech has, shown some expansion or compression in the resulting speech. Comparison in terms of pitch shifting is presented here. Analysis was performed for a single frame and a full segmentation of speech.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Najah Univ, Dept Elect Engn, Nablus, Palestinian Aut, Israel.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Mousa, A (reprint author), Najah Univ, Dept Elect Engn, POB 7, Nablus, Palestinian Aut, Israel.</td>
</tr>

<tr>
<td valign="top">EM </td><td>allam@najah.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>13</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>14</td>
</tr>

<tr>
<td valign="top">PD </td><td>JAN-FEB</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">VL </td><td>61</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>57</td>
</tr>

<tr>
<td valign="top">EP </td><td>61</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.2478/v10187-010-0008-5</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000275629400008</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wang, W
   <br>Yang, Z</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wang, Wei
   <br>Yang, Zhen</td>
</tr>

<tr>
<td valign="top">BE </td><td>Jusoff, K
   <br>Xie, Y</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Conversion Using Dynamic Features for High Quality Transformation</td>
</tr>

<tr>
<td valign="top">SO </td><td>SECOND INTERNATIONAL CONFERENCE ON DIGITAL IMAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">SE </td><td>Proceedings of SPIE-The International Society for Optical Engineering</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>2nd International Conference on Digital Image Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>FEB 26-28, 2010</td>
</tr>

<tr>
<td valign="top">CL </td><td>Singapore, SINGAPORE</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; dynamic features; modified GMM</td>
</tr>

<tr>
<td valign="top">AB </td><td>A novel voice morphing method is proposed to make the speech of the source speaker sound like the voice uttered by a target speaker. This method is based on the Gaussian Mixture Model (GMM). However, the traditional GMM has the over-smoothed phenomenon and may get discontinuity of the converted speech due to the inaccuracy of the extracted feature information. In order to overcome it, we consider the dynamic spectral features between frames. The conversion function is also modified to deal with the discontinuities. The Speech Transformation and Representation using Adaptive Interpolation of weiGHTed spectrogram (STRAIGHT) algorithm is adopted for the analysis and synthesis process. Objective and perceptual experiments show that the quality of the speech converted by our proposed method is significantly improved compared with the traditional GMM method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Wang, Wei; Yang, Zhen] Nanjing Univ Posts &amp; Telecommun, Inst Signal
   Proc &amp; Transmiss, Nanjing, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Wang, W (reprint author), Nanjing Univ Posts &amp; Telecommun, Inst Signal Proc &amp; Transmiss, Nanjing, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>Y070803@njupt.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">VL </td><td>7546</td>
</tr>

<tr>
<td valign="top">AR </td><td>75463Q</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1117/12.855168</td>
</tr>

<tr>
<td valign="top">SC </td><td>Optics; Imaging Science &amp; Photographic Technology; Radiology, Nuclear
   Medicine &amp; Medical Imaging</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000285574500134</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Azarov, E
   <br>Petrovsky, A
   <br>Zubrycki, P</td>
</tr>

<tr>
<td valign="top">AF </td><td>Azarov, Elias
   <br>Petrovsky, Alexander
   <br>Zubrycki, Piotr</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE, Poland Sect</td>
</tr>

<tr>
<td valign="top">TI </td><td>MULTI VOICE TEXT TO SPEECH SYNTHESIS BASED ON THE INSTANTANEOUS
   PARAMETRIC VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">SO </td><td>SPA 2010: SIGNAL PROCESSING ALGORITHMS, ARCHITECTURES, ARRANGEMENTS, AND
   APPLICATIONS CONFERENCE PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>14th IEEE Conference on Signal Processing: Algorithms, Architectures,
   Arrangements, and Applications (SPA)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 23-25, 2010</td>
</tr>

<tr>
<td valign="top">CL </td><td>Poznan, POLAND</td>
</tr>

<tr>
<td valign="top">ID </td><td>SIGNAL</td>
</tr>

<tr>
<td valign="top">AB </td><td>The paper describes an approach to text-to-speech synthesis based on processing in harmonic domain. A special harmonic analysis technique is presented that provides accurate estimation of instantaneous harmonic parameters. The technique is based on narrow band filtering aligned to the fundamental frequency, which improves estimation accuracy of higher-order harmonics with rapid frequency changes. The advanced analysis ensures natural-sounding amplitude, pitch and phase matching because of the fine deterministic/stochastic separation. Speech synthesis is carried out using parametric representation that allows applying voice conversion techniques in order to get a multi voice synthesis system with a single voice acoustic database.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">BP </td><td>78</td>
</tr>

<tr>
<td valign="top">EP </td><td>82</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000316506700016</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Toth, B
   <br>Fegyo, T
   <br>Nemeth, G</td>
</tr>

<tr>
<td valign="top">AF </td><td>Toth, Balint
   <br>Fegyo, Tibor
   <br>Nemeth, Geza</td>
</tr>

<tr>
<td valign="top">BE </td><td>Sojka, P
   <br>Horak, A
   <br>Kopecek, I
   <br>Pala, K</td>
</tr>

<tr>
<td valign="top">TI </td><td>Some Aspects of ASR Transcription Based Unsupervised Speaker Adaptation
   for HMM Speech Synthesis</td>
</tr>

<tr>
<td valign="top">SO </td><td>TEXT, SPEECH AND DIALOGUE</td>
</tr>

<tr>
<td valign="top">SE </td><td>Lecture Notes in Artificial Intelligence</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>13th International Conference on Text, Speech and Dialogue</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 06-10, 2010</td>
</tr>

<tr>
<td valign="top">CL </td><td>Brno, CZECH REPUBLIC</td>
</tr>

<tr>
<td valign="top">DE </td><td>HMM-based speech synthesis; unsupervised adaptation; automatic speech
   recognition</td>
</tr>

<tr>
<td valign="top">ID </td><td>INTERPOLATION; CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Statistical parametric synthesis offers numerous techniques to create new voices. Speaker adaptation is one of the most exciting ones. However, it still requires high quality audio data with low signal to noise ration and precise labeling. This paper presents an automatic speech recognition based unsupervised adaptation method for Hidden Markov Model (HMM) speech synthesis and its quality evaluation. The adaptation technique automatically controls the number of phone mismatches. The evaluation involves eight different HMM voices, including supervised and unsupervised speaker adaptation. The effects of segmentation and linguistic labeling errors in adaptation data are also investigated. The results show that unsupervised adaptation can contribute to speeding up the creation of new HMM voices with comparable quality to supervised adaptation.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Toth, Balint; Fegyo, Tibor; Nemeth, Geza] Budapest Univ Technol &amp; Econ,
   Dept Telecommun &amp; Media Informat, Budapest, Hungary.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Toth, B (reprint author), Budapest Univ Technol &amp; Econ, Dept Telecommun &amp; Media Informat, Budapest, Hungary.</td>
</tr>

<tr>
<td valign="top">EM </td><td>toth.b@tmit.bme.hu; fegyo@tmit.bme.hu; nemeth@tmit.bme.hu</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">VL </td><td>6231</td>
</tr>

<tr>
<td valign="top">BP </td><td>408</td>
</tr>

<tr>
<td valign="top">EP </td><td>415</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000288619400052</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Vogel, AP
   <br>Morgan, AT</td>
</tr>

<tr>
<td valign="top">AF </td><td>Vogel, Adam P.
   <br>Morgan, Angela T.</td>
</tr>

<tr>
<td valign="top">TI </td><td>Factors affecting the quality of sound recording for speech and voice
   analysis</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERNATIONAL JOURNAL OF SPEECH-LANGUAGE PATHOLOGY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speech signal acquisition; acoustics; Evidence Based Practice (EBP)</td>
</tr>

<tr>
<td valign="top">ID </td><td>PERTURBATION MEASUREMENTS; ENVIRONMENTAL NOISE; ACOUSTIC ANALYSIS;
   MICROPHONE TYPE; PITCH; TECHNOLOGY; ACCURACY; SYSTEM</td>
</tr>

<tr>
<td valign="top">AB </td><td>The importance and utility of objective evidence-based measurement of the voice is well documented. Therefore, greater consideration needs to be given to the factors that influence the quality of voice and speech recordings. This manuscript aims to bring together the many features that affect acoustically acquired voice and speech. Specifically, the paper considers the practical requirements of individual speech acquisition configurations through examining issues relating to hardware, software and microphone selection, the impact of environmental noise, analogue to digital conversion and file format as well as the acoustic measures resulting from varying levels of signal integrity. The type of recording environment required by a user is often dictated by a variety of clinical and experimental needs, including: the acoustic measures being investigated; portability of equipment; an individual's budget; and the expertise of the user. As the quality of recorded signals is influenced by many factors, awareness of these issues is essential. This paper aims to highlight the importance of these methodological considerations to those previously uninitiated with voice and speech acoustics. With current technology, the highest quality recording would be made using a stand-alone hard disc recorder, an independent mixer to attenuate the incoming signal, and insulated wiring combined with a high quality microphone in an anechoic chamber or sound treated room.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Vogel, Adam P.; Morgan, Angela T.] Univ Melbourne, Melbourne, Vic 3010,
   Australia.
   <br>[Vogel, Adam P.; Morgan, Angela T.] Murdoch Childrens Res Inst, Murdoch,
   WA, Australia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Vogel, AP (reprint author), Level 7-21 Victoria St, Melbourne, Vic 3000, Australia.</td>
</tr>

<tr>
<td valign="top">EM </td><td>avogel@cogstate.com</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Morgan, Angela</display_name>&nbsp;</font></td><td><font size="3">J-5235-2017&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Vogel, Adam</display_name>&nbsp;</font></td><td><font size="3">A-2004-2014&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Morgan, Angela</display_name>&nbsp;</font></td><td><font size="3">0000-0003-1147-7405&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Vogel, Adam</display_name>&nbsp;</font></td><td><font size="3">0000-0002-3505-2631&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>17</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>17</td>
</tr>

<tr>
<td valign="top">PD </td><td>DEC</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">VL </td><td>11</td>
</tr>

<tr>
<td valign="top">IS </td><td>6</td>
</tr>

<tr>
<td valign="top">BP </td><td>431</td>
</tr>

<tr>
<td valign="top">EP </td><td>437</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.3109/17549500902822189</td>
</tr>

<tr>
<td valign="top">SC </td><td>Audiology &amp; Speech-Language Pathology; Linguistics; Rehabilitation</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000273241500001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Rao, RS
   <br>Duncan, TD</td>
</tr>

<tr>
<td valign="top">AF </td><td>Rao, Ravi S.
   <br>Duncan, Titus D.</td>
</tr>

<tr>
<td valign="top">TI </td><td>Endoscopic Total Thyroidectomy</td>
</tr>

<tr>
<td valign="top">SO </td><td>JSLS-JOURNAL OF THE SOCIETY OF LAPAROENDOSCOPIC SURGEONS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Minimal; Invasive; Endoscopic; Thyroidectomy; Transaxillary</td>
</tr>

<tr>
<td valign="top">ID </td><td>SUBTOTAL PARATHYROIDECTOMY; BREAST APPROACH; SURGERY; NECK; SCARLESS;
   DISEASE</td>
</tr>

<tr>
<td valign="top">AB </td><td>Background and Objective: Endoscopic neck surgery for the thyroid and parathyroid is being tested as an alternative to open thyroidectomy. The aim of this study was to determine the safety and feasibility of endoscopic transaxillary total thyroidectomy (ETTT).
   <br>Methods and Results: Twenty-two consecutive patients from January 2006 to September 2008 underwent ETTT. No conversions to open were necessary. Mean age was 49.3+/-12.9 years. 20 were female, and 2 were male. Mean operating time was 238 minutes +/- 72.7. Mean blood loss was 40mL +/- 28.3mL. Mean weight of the gland was 137.05g +/- 129.21g, The recurrent laryngeal nerve was identified with no permanet injury. Six patients developed hoarsences of the voice for a mean of 15.1 +/- 8.01 days. No patient developed tetany or hypocalcemia requiring treatment. Six patients experienced transient numbness in the anterior chest wall lasting 2 weeks in 5 patients and 2 months in one. All patients were discharged within 24 hours of admission.
   <br>Conclusion: ETTT requires additional operative time compared with the open approach, but is cosmetically favorable. Visualization of the nerve and parathyroid much better. Allthough the learning curve is steep, with experience the operative time will decrease. ETTT is different but safe and feasible.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Duncan, Titus D.] Atlanta Med Ctr Georgia, Atlanta, GA USA.
   <br>[Rao, Ravi S.] Royal Australasian Coll Surg, Melbourne, Vic, Australia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Rao, RS (reprint author), 315 Blvd NE,Ste 224, Atlanta, GA 30312 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ravi_rao_s@hotmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>5</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>6</td>
</tr>

<tr>
<td valign="top">PD </td><td>OCT-DEC</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">VL </td><td>13</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">BP </td><td>522</td>
</tr>

<tr>
<td valign="top">EP </td><td>527</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.4293/108680809X12589998404209</td>
</tr>

<tr>
<td valign="top">SC </td><td>Surgery</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000274747900010</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Felps, D
   <br>Bortfeld, H
   <br>Gutierrez-Osuna, R</td>
</tr>

<tr>
<td valign="top">AF </td><td>Felps, Daniel
   <br>Bortfeld, Heather
   <br>Gutierrez-Osuna, Ricardo</td>
</tr>

<tr>
<td valign="top">TI </td><td>Foreign accent conversion in computer assisted pronunciation training</td>
</tr>

<tr>
<td valign="top">SO </td><td>SPEECH COMMUNICATION</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; Foreign accent; Speaker identity; Computer assisted
   pronunciation training; Implicit feedback</td>
</tr>

<tr>
<td valign="top">ID </td><td>PROCESSING TECHNIQUES; VOICE CONVERSION; SPEECH; ENGLISH; SPEAKER;
   IDENTIFICATION; RECOGNITION; FRAMEWORK; FEATURES; VOWELS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Learners of a second language practice their pronunciation by listening to and imitating utterances from native speakers. Recent research has shown that choosing a well-matched native speaker to imitate can have a positive impact on pronunciation training. Here we propose a voice-transformation technique that can be used to generate the (arguably) ideal voice to imitate: the own voice of the learner with a native accent. Our work extends previous research, which suggests that providing learners with prosodically corrected versions of their utterances can be a suitable form of feedback in computer assisted pronunciation training. Our technique provides a conversion of both prosodic and segmental characteristics by means of a pitch-synchronous decomposition of speech into glottal excitation and spectral envelope. We apply the technique to a corpus containing parallel recordings of foreign-accented and native-accented utterances, and validate the resulting accent conversions through a series of perceptual experiments. Our results indicate that the technique can reduce foreign accentedness without significantly altering the voice quality properties of the foreign speaker. Finally, we propose a pedagogical strategy for integrating accent conversion as a form of behavioral shaping in computer assisted pronunciation training. (C) 2008 Elsevier B.V. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Felps, Daniel; Gutierrez-Osuna, Ricardo] Texas A&amp;M Univ, Dept Comp Sci,
   College Stn, TX 77843 USA.
   <br>[Bortfeld, Heather] Texas A&amp;M Univ, Dept Psychol, College Stn, TX 77843
   USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Gutierrez-Osuna, R (reprint author), Texas A&amp;M Univ, Dept Comp Sci, 3112 TAMU, College Stn, TX 77843 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>dlfelps@cs.tamu.edu; bortfeld@psyc.ta-mu.edu; rgutier@cs.tamu.edu</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Bortfeld, Heather</display_name>&nbsp;</font></td><td><font size="3">0000-0002-3545-5449&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Gutierrez-Osuna, Ricardo</display_name>&nbsp;</font></td><td><font size="3">0000-0003-2817-2085&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>36</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>37</td>
</tr>

<tr>
<td valign="top">PD </td><td>OCT</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">VL </td><td>51</td>
</tr>

<tr>
<td valign="top">IS </td><td>10</td>
</tr>

<tr>
<td valign="top">SI </td><td>SI</td>
</tr>

<tr>
<td valign="top">BP </td><td>920</td>
</tr>

<tr>
<td valign="top">EP </td><td>932</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.specom.2008.11.004</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000269092500010</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Yamagishi, J
   <br>Nose, T
   <br>Zen, H
   <br>Ling, ZH
   <br>Toda, T
   <br>Tokuda, K
   <br>King, S
   <br>Renals, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Yamagishi, Junichi
   <br>Nose, Takashi
   <br>Zen, Heiga
   <br>Ling, Zhen-Hua
   <br>Toda, Tomoki
   <br>Tokuda, Keiichi
   <br>King, Simon
   <br>Renals, Steve</td>
</tr>

<tr>
<td valign="top">TI </td><td>Robust Speaker-Adaptive HMM-Based Text-to-Speech Synthesis</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Average voice; HMM-based speech synthesis; HMM Speech Synthesis System;
   HTS; speaker adaptation; speech synthesis; voice conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>HIDDEN MARKOV-MODELS; SYNTHESIS SYSTEM; MAXIMUM-LIKELIHOOD;
   INSTANTANEOUS-FREQUENCY; ADAPTATION; ALGORITHM; GENERATION; VOCODER;
   HSMM</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper describes a speaker-adaptive HMM-based speech synthesis system. The new system, called "HTS-2007," employs speaker adaptation (CSMAPLR+MAP), feature-space adaptive training, mixed-gender modeling, and full-covariance modeling using CSMAPLR transforms, in addition to several other techniques that have proved effective in our previous systems. Subjective evaluation results show that the new system generates significantly better quality synthetic speech than speaker-dependent approaches with realistic amounts of speech data, and that it bears comparison with speaker-dependent approaches even when large amounts of speech data are available. In addition, a comparison study with several speech synthesis techniques shows the new system is very robust: It is able to build voices from less-than-ideal speech data and synthesize good-quality speech even for out-of-domain sentences.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Yamagishi, Junichi; King, Simon; Renals, Steve] Univ Edinburgh, CSTR,
   Edinburgh EH8 9AB, Midlothian, Scotland.
   <br>[Nose, Takashi] Tokyo Inst Technol, Grad Sch Sci &amp; Engn, Yokohama,
   Kanagawa 2268502, Japan.
   <br>[Zen, Heiga; Tokuda, Keiichi] Nagoya Inst Technol, Dept Comp Sci &amp; Engn,
   Nagoya, Aichi 4668555, Japan.
   <br>[Ling, Zhen-Hua] Univ Sci &amp; Technol China, iFlytek Speech Lab, Hefei
   230027, Anhui, Peoples R China.
   <br>[Toda, Tomoki] Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara
   6300192, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Yamagishi, J (reprint author), Univ Edinburgh, CSTR, Edinburgh EH8 9AB, Midlothian, Scotland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jyamagis@inf.ed.ac.uk; takashi.nose@ip.titech.ac.jp;
   zen@sp.nitech.ac.jp; zhling@ustc.edu; tomoki@is.naist.jp;
   tokuda@nitech.ac.jp; simon.king@ed.ac.uk; s.renals@ed.ac.uk</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Renals, Steve</display_name>&nbsp;</font></td><td><font size="3">L-7175-2014&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>King, Simon</display_name>&nbsp;</font></td><td><font size="3">0000-0002-2694-2843&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Renals, Steve</display_name>&nbsp;</font></td><td><font size="3">0000-0002-8790-3389&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>89</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>89</td>
</tr>

<tr>
<td valign="top">PD </td><td>AUG</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">VL </td><td>17</td>
</tr>

<tr>
<td valign="top">IS </td><td>6</td>
</tr>

<tr>
<td valign="top">BP </td><td>1208</td>
</tr>

<tr>
<td valign="top">EP </td><td>1230</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASL.2009.2016394</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000268039700013</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Xu, N
   <br>Yang, Z
   <br>Zhang, LH
   <br>Zhu, WP
   <br>Bao, JY</td>
</tr>

<tr>
<td valign="top">AF </td><td>Xu, N.
   <br>Yang, Z.
   <br>Zhang, L. H.
   <br>Zhu, W. P.
   <br>Bao, J. Y.</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice conversion based on state-space model for modelling spectral
   trajectory</td>
</tr>

<tr>
<td valign="top">SO </td><td>ELECTRONICS LETTERS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">AB </td><td>A novel voice conversion (VC) method using a state-space model (SSM) is presented. The SSM, which has never been shown before in the context of VC, has the advantage of explicitly modelling spectral parameter trajectory. Thus, it will be superior to the conventional Gaussian mixture model (GMM)-based method, where the conversion algorithm is performed on a frame-by-frame procedure, ignoring the correlation between adjacent frames. Experiments using both objective and subjective measurements show that the proposed SSM-based method significantly outperforms the traditional GMM-based technique in the view of both speech quality and conversion accuracy for speaker individuality.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Xu, N.; Yang, Z.; Zhang, L. H.; Zhu, W. P.] Nanjing Univ Posts &amp;
   Telecommun, Inst Signal Proc &amp; Transmiss, Nanjing 210003, Peoples R
   China.
   <br>[Bao, J. Y.] Changzhou Inst Technol, Sch Elect Informat &amp; Elect Engn,
   Changzhou 213002, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Xu, N (reprint author), Nanjing Univ Posts &amp; Telecommun, Inst Signal Proc &amp; Transmiss, Nanjing 210003, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>D0705@njupt.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>5</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>9</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUL 2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">VL </td><td>45</td>
</tr>

<tr>
<td valign="top">IS </td><td>14</td>
</tr>

<tr>
<td valign="top">BP </td><td>763</td>
</tr>

<tr>
<td valign="top">EP </td><td>U73</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1049/el.2009.0904</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000268004600033</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Katib, I
   <br>Medhi, D</td>
</tr>

<tr>
<td valign="top">AF </td><td>Katib, Iyad
   <br>Medhi, Deep</td>
</tr>

<tr>
<td valign="top">TI </td><td>Adaptive alternate routing in WDM networks and its performance tradeoffs
   in the presence of wavelength converters</td>
</tr>

<tr>
<td valign="top">SO </td><td>OPTICAL SWITCHING AND NETWORKING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article; Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>8th International Workshop on Optical Networking Technologies</td>
</tr>

<tr>
<td valign="top">CY </td><td>NOV, 2007</td>
</tr>

<tr>
<td valign="top">CL </td><td>Washington, DC</td>
</tr>

<tr>
<td valign="top">DE </td><td>Routing in WDM networks; Wavelength converters; Adaptive alternate
   routing</td>
</tr>

<tr>
<td valign="top">ID </td><td>ALL-OPTICAL NETWORKS; CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Routing in wavelength-routed all-optical WDM networks has received much attention in the past decade, for which fixed and dynamic routing methods have been proposed. Taking into account the observation that wavelength-routed all-optical WDM networks are similar to circuit-switched voice networks, except with regard to wavelength conversion, we propose an adaptive alternate routing (AAR) scheme for wavelength-routed all-optical WDM networks. A major benefit of AAR is that it can operate and adapt without requiring an exchange of network status, i.e., it is an information-less adaptive routing scheme, The scope of this work is to understand this scheme in its own right since no other dynamic routing schemes are known to have the information-less property. In this paper, we conduct a systematic study of AAR with regard to factors such as the number of converters, load conditions, traffic patterns, network topologies, and the number of alternate paths considered. We observe that the routing scheme with multiple alternate routes provides more gain at a lower load instead of requiring any nodes to be equipped with wavelength converters. On the other hand, the availability of wavelength converters at some nodes, along with adaptive routing, is beneficial at a moderate to high load without requiring all nodes to be equipped with wavelength converters. We also observed that a small number of alternate routes considered in a network without wavelength converters gives a much better performance than a network with full wavelength converters and fewer alternate routes. Throughout this study, we observed that the proposed adaptive alternate routing scheme adapts well to the network traffic condition. (c) 2009 Elsevier B.V. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Katib, Iyad; Medhi, Deep] Univ Missouri, Dept Comp Sci &amp; Elect Engn,
   Kansas City, MO 64110 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Medhi, D (reprint author), Univ Missouri, Dept Comp Sci &amp; Elect Engn, Kansas City, MO 64110 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>IyadKatib@umkc.edu; DMedhi@umkc.edu</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Katib, Iyad</display_name>&nbsp;</font></td><td><font size="3">H-9728-2012&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Katib, Iyad</display_name>&nbsp;</font></td><td><font size="3">0000-0003-3376-3218&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>7</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>7</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUL</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">VL </td><td>6</td>
</tr>

<tr>
<td valign="top">IS </td><td>3</td>
</tr>

<tr>
<td valign="top">SI </td><td>SI</td>
</tr>

<tr>
<td valign="top">BP </td><td>181</td>
</tr>

<tr>
<td valign="top">EP </td><td>193</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.osn.2009.03.001</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Optics; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000269279000006</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Inanoglu, Z
   <br>Young, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Inanoglu, Zeynep
   <br>Young, Steve</td>
</tr>

<tr>
<td valign="top">TI </td><td>Data-driven emotion conversion in spoken English</td>
</tr>

<tr>
<td valign="top">SO </td><td>SPEECH COMMUNICATION</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Emotion conversion; Expressive speech synthesis; Prosody modeling</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION; SPEECH SYNTHESIS</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper describes an emotion conversion system that combines independent parameter transformation techniques to endow a neutral utterance with a desired target emotion. A set of prosody conversion methods have been developed which utilise a small amount of expressive training data (similar to 15 min) and which have been evaluated for three target emotions: anger, surprise and sadness. The system performs F0 conversion at the syllable level while duration conversion takes place at the phone level using a set of linguistic regression trees. Two alternative methods are presented as a means to predict F0 contours for unseen utterances. Firstly, an HMM-based approach uses syllables as linguistic building blocks to model and generate F0 contours. Secondly, an F0 segment selection approach expresses F0 conversion as a search problem, where syllable-based F0 contour segments from a target speech corpus are spliced together under contextual constraints. To complement the prosody modules, a GMM-based spectral conversion function is used to transform the voice quality. Each independent module and the combined emotion conversion framework were evaluated through a perceptual study. Preference tests demonstrated that each module contributes a measurable improvement in the perception of the target emotion. Furthermore, an emotion classification test showed that converted utterances with either F0 generation technique were able to convey the desired emotion above chance level. However, F0 segment selection outperforms the HMM-based F0 generation method both in terms of emotion recognition rates as well as intonation quality scores, particularly in the case of anger and surprise. Using segment selection, the emotion recognition rates for the converted neutral utterances were comparable to the same utterances spoken directly in the target emotion. (c) 2008 Elsevier B.V. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Inanoglu, Zeynep; Young, Steve] Univ Cambridge, Dept Engn, Cambridge
   CB2 1PZ, England.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Young, S (reprint author), Univ Cambridge, Dept Engn, Trumpington St, Cambridge CB2 1PZ, England.</td>
</tr>

<tr>
<td valign="top">EM </td><td>zeynep@gatesscholar.org; sjy@eng.cam.ac.uk</td>
</tr>

<tr>
<td valign="top">TC </td><td>28</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>28</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">VL </td><td>51</td>
</tr>

<tr>
<td valign="top">IS </td><td>3</td>
</tr>

<tr>
<td valign="top">BP </td><td>268</td>
</tr>

<tr>
<td valign="top">EP </td><td>283</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.specom.2008.09.006</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000263203900007</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Feldhoffer, G
   <br>Oroszi, B</td>
</tr>

<tr>
<td valign="top">AF </td><td>Feldhoffer, Gergely
   <br>Oroszi, Balazs</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>An Efficient Voice Driven Face Animation Method For Cyber Telepresence
   Applications</td>
</tr>

<tr>
<td valign="top">SO </td><td>2009 2ND INTERNATIONAL SYMPOSIUM ON APPLIED SCIENCES IN BIOMEDICAL AND
   COMMUNICATION TECHNOLOGIES (ISABEL 2009)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>2nd International Symposium on Applied Sciences in Biomedical and
   Communication Technologies</td>
</tr>

<tr>
<td valign="top">CY </td><td>NOV 24-27, 2009</td>
</tr>

<tr>
<td valign="top">CL </td><td>Bratislava, SLOVAKIA</td>
</tr>

<tr>
<td valign="top">AB </td><td>In on-line cyber spaces there are artificial bodies which imitate realistic behavior controlled by remote users. An important aspect is the realistic facial motion of human like characters according to the actual speech sounds. This paper describes a memory and CPU efficient method for visual speech synthesis for on-line applications using voice connection over network. The method is real-time, can be activated on the receiver client without server support. It is needed only to send coded speech signal and the visual speech synthesis is the task of the receiving client. This way deaf or hard of hearing people can lipread the transmitted audio speech. The animation rendering is supported by graphical accelerator devices, so CPU load of the conversion is insignificant.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Feldhoffer, Gergely; Oroszi, Balazs] Pazmany Peter Catholic Univ,
   H-1083 Budapest, Hungary.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Feldhoffer, G (reprint author), Pazmany Peter Catholic Univ, Prater U 50-A, H-1083 Budapest, Hungary.</td>
</tr>

<tr>
<td valign="top">EM </td><td>flugi@itk.ppke.hu; oroba@digitus.itk.ppke.hu</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">BP </td><td>195</td>
</tr>

<tr>
<td valign="top">EP </td><td>200</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000277624100039</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Jian, ZH</td>
</tr>

<tr>
<td valign="top">AF </td><td>Jian, Zhihua</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>An Approach to Voice Conversion Based on Non-Linear Canonical
   Correlation Analysis</td>
</tr>

<tr>
<td valign="top">SO </td><td>2009 5TH INTERNATIONAL CONFERENCE ON WIRELESS COMMUNICATIONS, NETWORKING
   AND MOBILE COMPUTING, VOLS 1-8</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>5th International Conference on Wireless Communications, Networking and
   Mobile Computing</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 24-26, 2009</td>
</tr>

<tr>
<td valign="top">CL </td><td>Beijing, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speech processing; Voice conversion; NLCCA; GMM</td>
</tr>

<tr>
<td valign="top">ID </td><td>NEURAL NETWORKS; TRANSFORMATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion algorithm aims to provide high level of similarity to the target voice with an acceptable level of quality. The main object of this paper was to build a nonlinear relationship between the parameters for the acoustical features of source and target speaker using non-linear canonical correlation analysis (NLCCA) based on jointed Gaussian mixture model. Speaker individuality transformation was achieved mainly by altering vocal tract characteristics represented by line spectral frequencies (LSF). To obtain the transformed speech sounded more like the target voices, prosody modification is involved through residual prediction. Both objective and subjective evaluations were conducted. The experimental results demonstrated that our proposed algorithm was effective and outperformed the conventional conversion method utilized by the minimum mean square error (MMSE) estimation.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>HangZhou DianZi Univ, Sch Commun Engn, Hangzhou, Zhejiang, Peoples R
   China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Jian, ZH (reprint author), HangZhou DianZi Univ, Sch Commun Engn, Hangzhou, Zhejiang, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jianzh@hdu.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">BP </td><td>1982</td>
</tr>

<tr>
<td valign="top">EP </td><td>1985</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000275789400481</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Stylianou, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Stylianou, Yannis</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>VOICE TRANSFORMATION: A SURVEY</td>
</tr>

<tr>
<td valign="top">SO </td><td>2009 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING, VOLS 1- 8, PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech and Signal Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 19-24, 2009</td>
</tr>

<tr>
<td valign="top">CL </td><td>Taipei, TAIWAN</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice Transformation; Speech production; speech perception; voice
   quality; speaking style</td>
</tr>

<tr>
<td valign="top">ID </td><td>CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice transformation refers to the various modifications one may apply to the sound produced by a person, speaking or singing. Voice Transformation is usually seen as an add-on or an external system in speech synthesis systems since it may create virtual voices in a simple and flexible way. In this paper we review the state-of-the-art Voice Transformation methodology showing its limitations in producing good speech quality and its current challenges. Addressing quality issues of current voice transformation algorithms in conjunction with properties of the speech production and speech perception systems we try to pave the way for more natural Voice Transformation algorithms in the future. Facing the challenges, will allow Voice Transformation systems to be applied in important and versatile areas of speech technology; applications that are far beyond speech synthesis.</td>
</tr>

<tr>
<td valign="top">EM </td><td>yannis@csd.uoc.gr</td>
</tr>

<tr>
<td valign="top">TC </td><td>51</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>55</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">BP </td><td>3585</td>
</tr>

<tr>
<td valign="top">EP </td><td>3588</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/ICASSP.2009.4960401</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000268919201436</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Lee, CH
   <br>Hsia, CC
   <br>Wu, CH
   <br>Lin, MC</td>
</tr>

<tr>
<td valign="top">AF </td><td>Lee, Chung-Han
   <br>Hsia, Chi-Chun
   <br>Wu, Chung-Hsien
   <br>Lin, Mai-Chun</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Regression-based Clustering for Hierarchical Pitch Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>2009 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING, VOLS 1- 8, PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech and Signal Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 19-24, 2009</td>
</tr>

<tr>
<td valign="top">CL </td><td>Taipei, TAIWAN</td>
</tr>

<tr>
<td valign="top">DE </td><td>Regression-based; hierarchical; pitch conversion; clustering</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This study presents a hierarchical pitch conversion method using regress ion-based clustering for conversion function modeling. The pitch contour of a speech utterance is first extracted and decomposed into sentence-, word- and sub-syllable-level features in a top-down mechanism. The pair-wise source and target pitch feature vectors at each level are then clustered to generate the pitch conversion function. Regression-based clustering, which clusters the feature vectors to achieve a minimum conversion error between the predicted and the real feature vectors is proposed for conversion function generation. A classification and regression tree (CART), incorporating linguistic, phonetic and source prosodic features, is adopted to select the most suitable function for pitch conversion. Several objective and subjective evaluations were conducted and the comparison results to the GMM-based methods for pitch conversion confirm the performance of the proposed regression-based clustering approach.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Lee, Chung-Han; Wu, Chung-Hsien; Lin, Mai-Chun] Natl Cheng Kung Univ,
   Dept Comp Sci &amp; Informat Engn, Tainan 70101, Taiwan.
   <br>[Hsia, Chi-Chun] Ind Technol Res Inst, Enabled Healthcare Inst, Hsinchu,
   Taiwan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Lee, CH (reprint author), Natl Cheng Kung Univ, Dept Comp Sci &amp; Informat Engn, Tainan 70101, Taiwan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>chlee@csie.ncku.edu.tw; shiacj@itri.org.tw; chwu@csie.ncku.edu.tw;
   chun@csie.ncku.edu.tw</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">BP </td><td>3593</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000268919201438</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Turk, O
   <br>Buyuk, O
   <br>Haznedaroglu, A
   <br>Arslan, LM</td>
</tr>

<tr>
<td valign="top">AF </td><td>Tuerk, Oytun
   <br>Bueyuek, Osman
   <br>Haznedaroglu, Ali
   <br>Arslan, Levent M.</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Application of Voice Conversion for Cross-Language Rap Singing
   Transformation</td>
</tr>

<tr>
<td valign="top">SO </td><td>2009 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING, VOLS 1- 8, PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech and Signal Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 19-24, 2009</td>
</tr>

<tr>
<td valign="top">CL </td><td>Taipei, TAIWAN</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; singing voice transformation; weighted codebook
   mapping</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion enables generation of a desired speaker's voice from audio recordings of another speaker. In this paper, we focus on a music application and describe the first steps towards generating voices of music celebrities using conventional voice conversion techniques. Specifically, rap singing transformations from English to Spanish are performed using parallel training material in English. Weighted codebook mapping based voice conversion with two different alignment methods and temporal smoothing of the transformation filter are employed. The first aligner uses a HMM trained for each source recording to force-align the corresponding target recording. The second aligner employs speaker-independent HMMs trained from a large number of speakers. Additionally, a smoothing step is devised to reduce discontinuities and to improve performance. The results of subjective evaluations indicate that both aligners perform equivalenty well. The proposed smoothing technique improves both similarity to target singer and quality significantly regardless of the alignment method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Tuerk, Oytun] DFKI GmbH Language Technol Lab, Speech Grp, Berlin,
   Germany.
   <br>[Bueyuek, Osman; Haznedaroglu, Ali; Arslan, Levent M.] ITU Ayazaga
   Kampusu, Sestek Inc, Istanbul, Turkey.
   <br>[Bueyuek, Osman; Haznedaroglu, Ali; Arslan, Levent M.] Bogazici Univ,
   Dept Elect Engn &amp; Elect, TR-80815 Bebek, Turkey.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Turk, O (reprint author), DFKI GmbH Language Technol Lab, Speech Grp, Berlin, Germany.</td>
</tr>

<tr>
<td valign="top">EM </td><td>oytun.turk@dfki.de; osman.buyuk@sestek.com.tr;
   ali.haznedaroglu@sestek.com.tr; levent.arslan@sestek.com.tr</td>
</tr>

<tr xmlns:date="http://exslt.org/dates-and-times">
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Arslan, Levent</display_name>&nbsp;</font></td><td><font size="3">D-6377-2015&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Arslan, Levent</display_name>&nbsp;</font></td><td><font size="3">0000-0002-6086-8018&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>5</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>5</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">BP </td><td>3597</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/ICASSP.2009.4960404</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000268919201439</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Toda, T
   <br>Nakamura, K
   <br>Sekimoto, H
   <br>Shikano, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Toda, Tomoki
   <br>Nakamura, Keigo
   <br>Sekimoto, Hidehiko
   <br>Shikano, Kiyohiro</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>VOICE CONVERSION FOR VARIOUS TYPES OF BODY TRANSMITTED SPEECH</td>
</tr>

<tr>
<td valign="top">SO </td><td>2009 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING, VOLS 1- 8, PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech and Signal Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 19-24, 2009</td>
</tr>

<tr>
<td valign="top">CL </td><td>Taipei, TAIWAN</td>
</tr>

<tr>
<td valign="top">DE </td><td>body transmitted speech; voice conversion; noise robust speech
   communication; silent speech communication; speaking aid</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we review our proposed statistical voice conversion approaches to enhancing various types of body transmitted speech captured with non-audible murmur (NAM) microphone. Body transmitted speech conversion is a potential technique to bring a new paradigm to human-to-human speech communication. In addition to our previously proposed methods of enhancing body transmitted unvoiced speech for silent speech communication and of enhancing body transmitted artificial speech for speaking aid, we further propose conversion methods of enhancing body transmitted voiced speech for noise robust speech communication. An experimental result demonstrates that the proposed methods yield significant improvements in quality of body transmitted voiced speech.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Toda, Tomoki; Nakamura, Keigo; Sekimoto, Hidehiko; Shikano, Kiyohiro]
   Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, NAIST, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Toda, T (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, NAIST, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>tomoki@is.naist.jp; kei-naka@is.naist.jp; shikano@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>13</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>13</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">BP </td><td>3601</td>
</tr>

<tr>
<td valign="top">EP </td><td>3604</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/ICASSP.2009.4960405</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000268919201440</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Desai, S
   <br>Raghavendra, EV
   <br>Yegnanarayana, B
   <br>Black, AW
   <br>Prahallad, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Desai, Srinivas
   <br>Raghavendra, E. Veera
   <br>Yegnanarayana, B.
   <br>Black, Alan W.
   <br>Prahallad, Kishore</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>VOICE CONVERSION USING ARTIFICIAL NEURAL NETWORKS</td>
</tr>

<tr>
<td valign="top">SO </td><td>2009 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING, VOLS 1- 8, PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech and Signal Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 19-24, 2009</td>
</tr>

<tr>
<td valign="top">CL </td><td>Taipei, TAIWAN</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; Artificial Neural Networks; Gaussian Mixture Model</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we propose to use Artificial Neural Networks (ANN) for voice conversion. We have exploited the mapping abilities of ANN to perform mapping of spectral features of a source speaker to that of a target speaker. A comparative study of voice conversion using ANN and the state-of-the-art Gaussian Mixture Model (GMM) is conducted. The results of voice conversion evaluated using subjective and objective measures confirm that ANNs perform better transformation than GMMs and the quality of the transformed speech is intelligible and has the characteristics of the target speaker.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Desai, Srinivas; Raghavendra, E. Veera; Yegnanarayana, B.; Prahallad,
   Kishore] Int Inst Informat Technol, Hyderabad, Andhra Pradesh, India.
   <br>[Black, Alan W.; Prahallad, Kishore] Carnegie Mellon Univ, Language
   Technol Inst, Pittsburgh, PA 15213 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Desai, S (reprint author), Int Inst Informat Technol, Hyderabad, Andhra Pradesh, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>srinivasdesai@research.iiit.ac.in; raghavendra@iiit.ac.in;
   yegna@iiit.ac.in; awb@cs.cmu.edu; skishore@cs.cmu.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>59</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>61</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">BP </td><td>3893</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/ICASSP.2009.4960478</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000268919202048</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Yutani, K
   <br>Uto, Y
   <br>Nankaku, Y
   <br>Lee, A
   <br>Tokuda, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Yutani, Kaori
   <br>Uto, Yosuke
   <br>Nankaku, Yoshihiko
   <br>Lee, Akinobu
   <br>Tokuda, Keiichi</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>VOICE CONVERSION BASED ON SIMULTANEOUS MODELING OF SPECTRUM AND F0</td>
</tr>

<tr>
<td valign="top">SO </td><td>2009 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING, VOLS 1- 8, PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech and Signal Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 19-24, 2009</td>
</tr>

<tr>
<td valign="top">CL </td><td>Taipei, TAIWAN</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; F(0) conversion; MSD-GMM; MSD-HMM</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper proposes a simultaneous modeling of spectrum and F(0) for voice conversion based on MSD (Multi-Space Probability Distribution) models. As a conventional technique, a spectral conversion based on GMM (Gaussian Mixture Model) has been proposed. Although this technique converts spectral feature sequences nonlinearly based on GMM, F(0) sequences are usually converted by a simple linear function. This is because F(0) is undefined in unvoiced segments. To overcome this problem, we apply MSD models. The MSD-GMM allows to model continuous F(0) values in voiced frames and a discrete symbol representing unvoiced frames within an unified framework. Furthermore, the MSD-HMM is adopted to model long term correlations in F(0) sequences.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Yutani, Kaori; Uto, Yosuke; Nankaku, Yoshihiko; Lee, Akinobu; Tokuda,
   Keiichi] Nagoya Inst Technol, Dept Comp Sci &amp; Engn, Nagoya, Aichi, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Yutani, K (reprint author), Nagoya Inst Technol, Dept Comp Sci &amp; Engn, Nagoya, Aichi, Japan.</td>
</tr>

<tr>
<td valign="top">TC </td><td>4</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>4</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">BP </td><td>3897</td>
</tr>

<tr>
<td valign="top">EP </td><td>3900</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/ICASSP.2009.4960479</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000268919202049</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Miyamoto, D
   <br>Nakamura, K
   <br>Toda, T
   <br>Saruwatari, H
   <br>Shikano, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Miyamoto, Daisuke
   <br>Nakamura, Keigo
   <br>Toda, Tomoki
   <br>Saruwatari, Hiroshi
   <br>Shikano, Kiyohiro</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>ACOUSTIC COMPENSATION METHODS FOR BODY TRANSMITTED SPEECH CONVERSION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2009 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING, VOLS 1- 8, PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech and Signal Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 19-24, 2009</td>
</tr>

<tr>
<td valign="top">CL </td><td>Taipei, TAIWAN</td>
</tr>

<tr>
<td valign="top">DE </td><td>Acoustic Compensation; CSMAPLR; CMLLR; CMS; Body Transmitted Voice
   Conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>RECOGNITION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Statistical voice conversion is very effective for enhancing body transmitted speech recorded with Non-Audible Murmur (NAM) microphone. In this method, a probabilistic model to convert body transmitted speech into natural speech is trained previously. Because acoustic characteristics of body transmitted speech is sensitive to recording conditions such as a location of NAM microphone, significant degradation of the conversion performance is often caused in practical situations by acoustic mismatches between training and conversion processes. To alleviate this problem, we propose unsupervised acoustic compensation methods for body transmitted voice conversion. Experimental results demonstrate that the proposed methods significantly reduce the quality degradation of converted speech caused by the acoustic mismatches.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Miyamoto, Daisuke; Nakamura, Keigo; Toda, Tomoki; Saruwatari, Hiroshi;
   Shikano, Kiyohiro] Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara,
   Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Miyamoto, D (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>daisuke-m@is.naist.jp; kei-naka@is.naist.jp; tomoki@is.naist.jp;
   sawatari@is.naist.jp; shikano@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">BP </td><td>3901</td>
</tr>

<tr>
<td valign="top">EP </td><td>3904</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/ICASSP.2009.4960480</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000268919202050</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Qiao, Y
   <br>Minematsu, N</td>
</tr>

<tr>
<td valign="top">AF </td><td>Qiao, Yu
   <br>Minematsu, Nobuaki</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>MIXTURE OF PROBABILISTIC LINEAR REGRESSIONS: A UNIFIED VIEW OF GMM-BASED
   MAPPING TECHNIQUES</td>
</tr>

<tr>
<td valign="top">SO </td><td>2009 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING, VOLS 1- 8, PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech and Signal Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 19-24, 2009</td>
</tr>

<tr>
<td valign="top">CL </td><td>Taipei, TAIWAN</td>
</tr>

<tr>
<td valign="top">DE </td><td>Space mapping; mixture model; linear regression; non-linear transform;
   voice conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>MAXIMUM-LIKELIHOOD; VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper introduces a model of Mixture of Probabilistic Linear Regressions (MPLR) to learn a mapping function between two feature spaces. The MPLR consists of weighted combination of several probabilistic linear regressions, whose parameters are estimated by using matrix calculation. The mixture nature of MPLR allows it to model nonlinear transformation. The formulation of MPLR is general and independent of the types of the density models used. Two well-known GMM-based mapping methods for voice conversion [1, 2] can be regarded as special cases of MPLR. This unified view not only provides insights to the GMM-based mapping techniques, but also indicates methods to improve them. Compared to [1), our formulation of MPLR avoids solving complex linear equations and yields a faster estimation of the transform parameters. As for [2], the MPLR estimation provides a modified mapping function which overcomes an implicit problem in [2]'s mapping function. We carried out experiments to compare the MPLR-based methods with the traditional GMM-based methods [1, 2] on a voice conversion task. The experimental results show that the MPLR-based methods always have better performance in various parameter setups.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Qiao, Yu; Minematsu, Nobuaki] Univ Tokyo, Grad Sch Engn, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Qiao, Y (reprint author), Univ Tokyo, Grad Sch Engn, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>qiao@gavo.t.u-tokyo.ac.jp; mine@gavo.t.u-tokyo.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>11</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>11</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">BP </td><td>3913</td>
</tr>

<tr>
<td valign="top">EP </td><td>3916</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/ICASSP.2009.4960483</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000268919202053</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Zhang, M
   <br>Tao, JH
   <br>Nurminen, J
   <br>Tian, JL
   <br>Wang, X</td>
</tr>

<tr>
<td valign="top">AF </td><td>Zhang, Meng
   <br>Tao, Jiaohua
   <br>Nurminen, Jani
   <br>Tian, Jilei
   <br>Wang, Xia</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>PHONEME CLUSTER BASED STATE MAPPING FOR TEXT-INDEPENDENT VOICE
   CONVERSION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2009 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING, VOLS 1- 8, PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech and Signal Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 19-24, 2009</td>
</tr>

<tr>
<td valign="top">CL </td><td>Taipei, TAIWAN</td>
</tr>

<tr>
<td valign="top">DE </td><td>text-independent voice conversion; Hidden Markov Model; state mapping</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper takes phonetic information into account for data alignment in text-independent voice conversion. Hidden Markov Models are used for representing the phonetic structure of training speech. States belonging to same phoneme are grouped together to form a phoneme cluster. A state mapped codebook based transformation is established using information on the corresponding phoneme clusters from source and targets speech and weighted linear transform. For each source vector, several nearest clusters are considered simultaneously while mapping in order to generate a continuous and stable transform. Experimental results indicate that the proposed use of phonetic information increases the similarity between converted speech and target speech. The proposed technique is applicable to both intra-lingual and cross-lingual voice conversion.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Zhang, Meng; Tao, Jiaohua] Chinese Acad Sci, Natl Lab Pattern Recognit,
   Inst Automat, Beijing, Peoples R China.
   <br>[Nurminen, Jani] Nokia, Devices R&amp;D, Tampere, Finland.
   <br>[Tian, Jilei] Nokia Res Ctr, Tampere, Finland.
   <br>[Wang, Xia] Nokia Res Ctr, Beijing, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Zhang, M (reprint author), Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>mzhang@nlpr.ia.ac.cn; jhtao@nlpr.ia.ac.cn; jani.k.nurminen@nokia.com;
   jilei.tian@nokia.com; xia.s.wang@nokia.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">BP </td><td>4281</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/ICASSP.2009.4960575</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000268919202145</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Villavicencio, F
   <br>Robel, A
   <br>Rodet, X</td>
</tr>

<tr>
<td valign="top">AF </td><td>Villavicencio, Fernando
   <br>Roebel, Axel
   <br>Rodet, Xavier</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>APPLYING IMPROVED SPECTRAL MODELING FOR HIGH QUALITY VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2009 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING, VOLS 1- 8, PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech and Signal Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 19-24, 2009</td>
</tr>

<tr>
<td valign="top">CL </td><td>Taipei, TAIWAN</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speech synthesis; speech analysis; cepstral analysis; spectral analysis;
   linear predictive coding</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this work, accurate spectral envelope estimation is applied to Voice Conversion in order to achieve High-Quality timbre conversion. True-Envelope based estimators allow model order selection leading to an adaptation of the spectral features to the characteristics of the speaker. Optimal residual signals can also be computed following a local adaptation of the model order in terms of the F-0. A new perceptual criteria is proposed to measure the impact of the spectral conversion error. The proposed envelope models show improved spectral conversion performance as well as increased converted-speech quality when compared to Linear Prediction.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Villavicencio, Fernando] Univ Pompeu Fabra, Music Technol Grp, Ocata 1,
   Barcelona 08003, Spain.
   <br>[Roebel, Axel; Rodet, Xavier] CNRS, IRCAM, STMS, Anal Synthesis Team,
   F-75004 Paris, France.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Villavicencio, F (reprint author), Univ Pompeu Fabra, Music Technol Grp, Ocata 1, Barcelona 08003, Spain.</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">BP </td><td>4285</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/ICASSP.2009.4960576</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000268919202146</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Lv, G
   <br>Zhao, H</td>
</tr>

<tr>
<td valign="top">AF </td><td>Lv, Gang
   <br>Zhao, Heming</td>
</tr>

<tr>
<td valign="top">BE </td><td>Loach, K</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Modified Adaptive Algorithm for Formant Bandwidth in Whisper
   Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>2009 INTERNATIONAL ASIA CONFERENCE ON INFORMATICS IN CONTROL,
   AUTOMATION, AND ROBOTICS, PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Asia Conference on Informatics in Control, Automation and
   Robotics</td>
</tr>

<tr>
<td valign="top">CY </td><td>FEB 01-02, 2009</td>
</tr>

<tr>
<td valign="top">CL </td><td>Bangkok, THAILAND</td>
</tr>

<tr>
<td valign="top">DE </td><td>whisper speech; voice conversion; formant combination; bandwidth</td>
</tr>

<tr>
<td valign="top">AB </td><td>The whisper conversion technology is to transform undistinguished whispers with lower SNR into clear normal speech, and it has important application prospect in mobile communication. Because the whisper speech is stirred by the yawp source, its formant position shifts and its bandwidth increases, which induces the problem of formant combination occurs in the whisper conversion. By analyzing the power spectrum, in this article, we proposed a modified adaptive algorithm for formant bandwidth. Based on the rule that the pole power does not change, the algorithm has resolved the problem of formant combination by modifying the formant bandwidth of whisper before implementing formant conversion. The experimental results with six Chinese mandarin monophthong phoneme conversions proved the validity of the algorithm.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Lv, Gang; Zhao, Heming] Soochow Univ, Sch Elect Informat, Suzhou
   215021, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Lv, G (reprint author), Soochow Univ, Sch Elect Informat, Suzhou 215021, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>lvgang@suda.edu.cn; hmzhao@suda.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">BP </td><td>368</td>
</tr>

<tr>
<td valign="top">EP </td><td>371</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/CAR.2009.33</td>
</tr>

<tr>
<td valign="top">SC </td><td>Automation &amp; Control Systems; Computer Science; Engineering; Robotics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000265094200079</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Li, Z
   <br>Gao, WL
   <br>Wang, Q
   <br>Zhang, GH
   <br>Yu, L
   <br>Du, JH
   <br>Li, JR</td>
</tr>

<tr>
<td valign="top">AF </td><td>Li, Zhen
   <br>Gao, Wanlin
   <br>Wang, Qing
   <br>Zhang, Ganghong
   <br>Yu, Lina
   <br>Du, Jianhui
   <br>Li, Jieru</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Design of Rural Voice Service System</td>
</tr>

<tr>
<td valign="top">SO </td><td>2009 INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY AND COMPUTER
   SCIENCE, VOL 1, PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Information Technology and Computer Science</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUL 25-26, 2009</td>
</tr>

<tr>
<td valign="top">CL </td><td>Kiev, UKRAINE</td>
</tr>

<tr>
<td valign="top">DE </td><td>rural voice service system; self-organization inquiry process; voice
   service</td>
</tr>

<tr>
<td valign="top">AB </td><td>In existed rural communication conditions, voice service system would solve the problem which the rural users cannot obtain useful and real-time information. This paper gives a design of rural voice service system, and discusses the system structure and layer division. This system realizes functions of self-organization inquiry process and real-time conversion from text to speech by means of navigating table and conversion module. Its implementation provides an effective approach for agricultural information transmission in China.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Li, Zhen; Gao, Wanlin; Wang, Qing; Zhang, Ganghong; Yu, Lina] China Agr
   Univ, Coll Informat &amp; Elect Engn, Beijing 100094, Peoples R China.
   <br>[Du, Jianhui] Yunnan Agr Dept, Yunnan, Peoples R China.
   <br>[Li, Jieru] BEIJING POWER SMART TECHNOL CO LTD, Beijing, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Li, Z (reprint author), China Agr Univ, Coll Informat &amp; Elect Engn, Beijing 100094, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>gaowlin@cau.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">BP </td><td>501</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/ITCS.2009.109</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000270803600119</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Karam, W
   <br>Mokbel, C
   <br>Greige, H
   <br>Chollet, G</td>
</tr>

<tr>
<td valign="top">AF </td><td>Karam, Walid
   <br>Mokbel, Chafic
   <br>Greige, Hanna
   <br>Chollet, Gerard</td>
</tr>

<tr>
<td valign="top">BE </td><td>Tistarelli, M
   <br>Nixon, MS</td>
</tr>

<tr>
<td valign="top">TI </td><td>Audio-Visual Identity Verification and Robustness to Imposture</td>
</tr>

<tr>
<td valign="top">SO </td><td>ADVANCES IN BIOMETRICS</td>
</tr>

<tr>
<td valign="top">SE </td><td>Lecture Notes in Computer Science</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>3rd IAPR/IEEE International Conference on Advances in Biometrics</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUN 02-05, 2009</td>
</tr>

<tr>
<td valign="top">CL </td><td>Comp Vis Lab, Alghero, ITALY</td>
</tr>

<tr>
<td valign="top">HO </td><td>Comp Vis Lab</td>
</tr>

<tr>
<td valign="top">DE </td><td>Identity verification; audio-visual forgery; talking-face imposture;
   voice conversion; face animation; biometric verification robustness</td>
</tr>

<tr>
<td valign="top">ID </td><td>FACE</td>
</tr>

<tr>
<td valign="top">AB </td><td>The robustness of talking-face identity verification (IV) systems is best evaluated by monitoring their behavior under impostor attacks. We propose a scenario where the impostor uses a still face picture and a sample of speech of the genuine client to transform his/her speech and visual appearance into that of the target client. We propose MixTrans, an original text-independent technique for voice transformation in the cepstral domain, which allows it transformed audio signal to be estimated and reconstructed in the temporal domain. We also propose a face transformation technique that allows a frontal face image of a client to be animated, using principal warps to deform defined MPEG-4 facial feature points based oil determined facial animation parameters. The robustness of the talking-face IV system is evaluated Under these attacks. Results Oil the BANCA talking-face database clearly show that such attacks represent a serious challenge and it security threat to IV systems.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Karam, Walid; Mokbel, Chafic; Greige, Hanna] Univ Balamand, Deir El
   Balamand, Al Kurah, Lebanon.
   <br>[Karam, Walid; Chollet, Gerard] Telecom Paris Tech, CNRS LTCI, F-75634
   Paris, France.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Karam, W (reprint author), Univ Balamand, Deir El Balamand, Al Kurah, Lebanon.</td>
</tr>

<tr>
<td valign="top">EM </td><td>Walid.Karam@balamand.edu.lb; Chafic.Mokbel@balamand.edu.lb;
   Hanna.Greige@balamand.edu.lb; chollet@telecom-paristech.fr</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Chollet, Gerard</display_name>&nbsp;</font></td><td><font size="3">0000-0003-4245-146X&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">VL </td><td>5558</td>
</tr>

<tr>
<td valign="top">BP </td><td>796</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Mathematical &amp; Computational Biology; Imaging Science
   &amp; Photographic Technology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000267377100081</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Tao, JH
   <br>Li, AJ</td>
</tr>

<tr>
<td valign="top">AF </td><td>Tao, Jianhua
   <br>Li, Aijun</td>
</tr>

<tr>
<td valign="top">BE </td><td>Tao, J
   <br>Tan, T</td>
</tr>

<tr>
<td valign="top">TI </td><td>Emotional Speech Generation by Using Statistic Prosody Conversion
   Methods</td>
</tr>

<tr>
<td valign="top">SO </td><td>AFFECTIVE INFORMATION PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>1st International Conference on Affective Computing and Intelligent
   Interaction</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 22-24, 2005</td>
</tr>

<tr>
<td valign="top">CL </td><td>Beijing, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>The chapter introduces prosody conversion models for emotional speech generation by using a Gaussian Mixture Model (GMM), and a Classification And Regression Tree (CART) model. Unlike the rule-based or linear modification method, the GMM and CART models try to map the subtle prosody distributions between neutral and emotional speech. A pitch target model that is optimized to describe Mandarin F0 contours is also introduced. For all conversion methods, a Deviation of Perceived Expressiveness (DPE) measure is created to evaluate the expressiveness of the output speech. The results show that the GMM method is more suitable for a small training set, whereas the CART method gives the better emotional speech output if trained with a large context-balanced corpus. The methods discussed in the chapter indicate ways to generate emotional speech in speech synthesis. The objective and Subjective evaluation processes are also analyzed. These results support the use of a neutral semantic content text in databases for emotional speech synthesis.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Li, Aijun] Chinese Acad Social Sci, Inst Linguist, Beijing, Peoples R
   China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Tao, JH (reprint author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jhtao@nlpr.ia.ac.cn; liaj@cass.org.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">BP </td><td>127</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/978-1-84800-306-4_8</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000264082400008</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Laskar, RH
   <br>Talukdar, FA
   <br>Bhattacharjee, R
   <br>Das, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Laskar, Rabul Hussain
   <br>Talukdar, Fazal Ahmed
   <br>Bhattacharjee, Rajib
   <br>Das, Saugat</td>
</tr>

<tr>
<td valign="top">BE </td><td>Mehnen, J
   <br>Koppen, M
   <br>Saad, A
   <br>Tiwari, A</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Conversion by Mapping the Spectral and Prosodic Features Using
   Support Vector Machine</td>
</tr>

<tr>
<td valign="top">SO </td><td>APPLICATIONS OF SOFT COMPUTING: FROM THEORY TO PRAXIS</td>
</tr>

<tr>
<td valign="top">SE </td><td>Advances in Intelligent and Soft Computing</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>13th World Conference on Soft Computing in Industrial Application</td>
</tr>

<tr>
<td valign="top">CY </td><td>2008</td>
</tr>

<tr>
<td valign="top">CL </td><td>ELECTR NETWORK</td>
</tr>

<tr>
<td valign="top">ID </td><td>TRANSFORMATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents an alternative voice conversion technique using support vector machine (SVM)-regression as a tool which converts a source speaker's voice to specific standard target speaker. The main objective of the work is to capture a nonlinear mapping function between the parameters for the acoustic features of the two speakers. Line spectral frequencies (LSFs) have been used as features to represent the vocal tract characteristics. We use kernel induced feature space with radial basis function network (RBFN) type SVM that uses gaussian kernel. The intonation characteristics (pitch contour) is modified using the baseline technique, i.e. gaussian normalization. The transformed LSFs along with the modified pitch contour are used to synthesize the speech signal for the desired target speaker. The target speaker's speech signal is synthesized and evaluated using both the subjective and the listening tests. The results signify that the proposed model improves the voice conversion performance in terms of capturing the speaker's identity as compared to our previous approach. In the previous approach we used feed forward neural network (FFNN) based model for vocal tract modification and codebook based method for pitch contour modification. However, the performance of the proposed system can further be improved by suitably modifying various user-defined parameters used in regression analysis and using more training LSF vectors in the training stage.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Laskar, Rabul Hussain; Talukdar, Fazal Ahmed; Bhattacharjee, Rajib;
   Das, Saugat] NIT Silchar 10, Silchar, Assam, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Laskar, RH (reprint author), NIT Silchar 10, Silchar, Assam, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>laskar_r@nits.ac.in; fazal@nits.ac.in; rajib13_nits@yahoo.co.in;
   saugat_nits@yahoo.co.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">VL </td><td>58</td>
</tr>

<tr>
<td valign="top">BP </td><td>519</td>
</tr>

<tr>
<td valign="top">EP </td><td>528</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/978-3-540-89619-7_51</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000269657800051</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Pribilova, A
   <br>Pribil, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Pribilova, Anna
   <br>Pribil, Jiri</td>
</tr>

<tr>
<td valign="top">BE </td><td>Fierrez, J
   <br>OrtegaGarcia, J
   <br>Esposito, A
   <br>Drygajlo, A
   <br>Faundez-Zanuy, M</td>
</tr>

<tr>
<td valign="top">TI </td><td>Harmonic Model for Female Voice Emotional Synthesis</td>
</tr>

<tr>
<td valign="top">SO </td><td>BIOMETRIC ID MANAGEMENT AND MULTIMODAL COMMUNICATION, PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">SE </td><td>Lecture Notes in Computer Science</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Joint International Conference on Biometric ID Management and Multimodal
   Communication (BioID-MultiComm)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 16-18, 2009</td>
</tr>

<tr>
<td valign="top">CL </td><td>Biometr Recognit Grp, Madrid, SPAIN</td>
</tr>

<tr>
<td valign="top">HO </td><td>Biometr Recognit Grp</td>
</tr>

<tr>
<td valign="top">DE </td><td>emotional speech; spectral envelope; harmonic speech model; emotional
   voice conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH SYNTHESIS; RECOGNITION; FEATURES; TTS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Spectral and prosodic modifications for emotional speech synthesis using harmonic modelling are described. Autoregressive parameterization of inverse Fourier transformed log spectral envelope is used. Spectral flatness determines the voicing transition frequency dividing spectrum of synthesized speech into minimum phases and random phases of the harmonic model. Female emotional voice conversion is evaluated by a listening test.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Pribilova, Anna] Slovak Univ Technol Bratislava, Dept Radio Elect,
   Ikovicova 3, SK-81219 Bratislava, Slovakia.
   <br>[Pribil, Jiri] Acad Sci Czech Republic, Inst Photon &amp; Elect, CZ-18251
   Prague, Czech Republic.
   <br>[Pribil, Jiri] Slovak Acad Sci, Inst Measurement Sci, Bratislava
   SK-84104, Slovakia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Pribilova, A (reprint author), Slovak Univ Technol Bratislava, Dept Radio Elect, Ikovicova 3, SK-81219 Bratislava, Slovakia.</td>
</tr>

<tr>
<td valign="top">EM </td><td>Anna.Pribilova@stuba.sk; Jiri.Pribil@savba.sk</td>
</tr>

<tr>
<td valign="top">TC </td><td>4</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>4</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">VL </td><td>5707</td>
</tr>

<tr>
<td valign="top">BP </td><td>41</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000274242800006</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Yamamoto, K
   <br>Nakagawa, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Yamamoto, Kazumasa
   <br>Nakagawa, Setichi</td>
</tr>

<tr>
<td valign="top">BE </td><td>Pan, JS
   <br>Guo, BL
   <br>Abraham, A</td>
</tr>

<tr>
<td valign="top">TI </td><td>Privacy Protection for Speech Information</td>
</tr>

<tr>
<td valign="top">SO </td><td>FIFTH INTERNATIONAL CONFERENCE ON INFORMATION ASSURANCE AND SECURITY,
   VOL 1, PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>5th International Conference on Information Assurance and Security</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 18-20, 2009</td>
</tr>

<tr>
<td valign="top">CL </td><td>Xidian Univ, Xian, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">HO </td><td>Xidian Univ</td>
</tr>

<tr>
<td valign="top">DE </td><td>privacy protection; speech information; personal information; voice
   conversion; speech recognition; speech elimination</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Ubiquitous network society will be achieved soon. In the society all electronic equipments including "sensors" are connected to the network and communicate each other to share information. Sensor information is very important for the network, especially for virtual reality systems which give feeling of begin there. Since the sensor information, however, includes a lot of privacy information, it is not preferred to send raw privacy information through the network. In this paper, we describe about privacy protection for speech information. We think that the privacy information in speech is "voice characteristics" and "linguistic privacy information." We try to protect these privacy information by using "voice conversion" and "deletion of privacy linguistic information for speech recognition result." Since, however, speech recognition technology is not robust in real environment still now, "elimination of only speech in noisy speech" technique is also considered.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Yamamoto, Kazumasa; Nakagawa, Setichi] Toyohashi Univ Technol, Dept
   Informat &amp; Comp Sci, Toyohashi, Aichi 440, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Yamamoto, K (reprint author), Toyohashi Univ Technol, Dept Informat &amp; Comp Sci, Toyohashi, Aichi 440, Japan.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">BP </td><td>717</td>
</tr>

<tr>
<td valign="top">EP </td><td>720</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/IAS.2009.321</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000275852000163</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Perrot, P
   <br>Morel, M
   <br>Razik, J
   <br>Chollet, G</td>
</tr>

<tr>
<td valign="top">AF </td><td>Perrot, Patrick
   <br>Morel, Mathieu
   <br>Razik, Joseph
   <br>Chollet, Gerard</td>
</tr>

<tr>
<td valign="top">BE </td><td>Sorell, M</td>
</tr>

<tr>
<td valign="top">TI </td><td>Vocal Forgery in Forensic Sciences</td>
</tr>

<tr>
<td valign="top">SO </td><td>FORENSICS IN TELECOMMUNICATIONS, INFORMATION AND MULTIMEDIA</td>
</tr>

<tr>
<td valign="top">SE </td><td>Lecture Notes of the Institute for Computer Sciences Social Informatics
   and Telecommunications Engineering</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>2nd International Conference on Forensic Applications and Techniques in
   Telecommunications, Information and Multimedia</td>
</tr>

<tr>
<td valign="top">CY </td><td>JAN 19-21, 2009</td>
</tr>

<tr>
<td valign="top">CL </td><td>Univ Adelaide, Australian Natl Wine Ctr, Adelaide, AUSTRALIA</td>
</tr>

<tr>
<td valign="top">HO </td><td>Univ Adelaide, Australian Natl Wine Ctr</td>
</tr>

<tr>
<td valign="top">DE </td><td>disguised voices; voice conversion; SVM classifier; identification</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEAKER IDENTIFICATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This article describes techniques of vocal forgery able to affect automatic speaker recognition system in a forensic context. Vocal forgery covers two main aspects: voice transformation and voice conversion. Concerning voice transformation, this article proposes an automatic analysis of four specific disguised voices in order to detect the forgery and, for voice conversion, different ways to automatically imitate a target voice. Vocal forgery appears as a real and relevant question for forensic expertise. In most cases, criminals who make a terrorist claim or a miscellaneous call, disguise their voices to hide their identity or to take the identity of another person. Disguise is considered in this paper as a deliberate action of the speaker who wants to conceal or falsify his identity. Different techniques exist to transform one's own voice. Some are sophisticated as software manipulation, some others are simpler as using an handkerchief over the mouth. In voice transformation, the presented work is dedicated to the study of disguise used in the most common cases. In voice conversion, different techniques will be presented, compared, and applied on an original example of the French President voice.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Perrot, Patrick] Inst Rech Criminelle Gendarmerie Natl, F-93110 Rosny
   Sous Bois, France.
   <br>[Perrot, Patrick; Morel, Mathieu; Razik, Joseph; Chollet, Gerard] CNRS
   LTCI Telecom ParisTech, F-75014 Paris, France.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Perrot, P (reprint author), Inst Rech Criminelle Gendarmerie Natl, F-93110 Rosny Sous Bois, France.</td>
</tr>

<tr>
<td valign="top">EM </td><td>patrick.perrot@gendarmerie.defense.gouv.fr;
   chollet@telecom-paristech.fr; razik@telecom-paristech.fr;
   morel@telecom-paristech.fr</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Chollet, Gerard</display_name>&nbsp;</font></td><td><font size="3">0000-0003-4245-146X&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">VL </td><td>8</td>
</tr>

<tr>
<td valign="top">BP </td><td>179</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000302329300021</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Yamagishi, J
   <br>Kobayashi, T
   <br>Nakano, Y
   <br>Ogata, K
   <br>Isogai, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Yamagishi, Junichi
   <br>Kobayashi, Takao
   <br>Nakano, Yuji
   <br>Ogata, Katsumi
   <br>Isogai, Juri</td>
</tr>

<tr>
<td valign="top">TI </td><td>Analysis of Speaker Adaptation Algorithms for HMM-Based Speech Synthesis
   and a Constrained SMAPLR Adaptation Algorithm</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Average voice; hidden Markov model (HMM)-based speech synthesis; speaker
   adaptation; speech synthesis; voice conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>HIDDEN MARKOV-MODELS; MAXIMUM-LIKELIHOOD; SYNTHESIS SYSTEM; RECOGNITION;
   GENERATION; VARIANCE; HSMM</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we analyze the effects of several factors and configuration choices encountered during training and model construction when we want to obtain better and more stable adaptation in HMM-based speech synthesis. We then propose a new adaptation algorithm called constrained structural maximum a posteriori linear regression (CSMAPLR) whose derivation is based on the knowledge obtained in this analysis and on the results of comparing several conventional adaptation algorithms. Here, we investigate six major aspects of the speaker adaptation: initial models; the amount of the training data for the initial models; the transform functions, estimation criteria, and sensitivity of several linear regression adaptation algorithms; and combination algorithms. Analyzing the effect of the initial model, we compare speaker-dependent models, gender-independent models, and the simultaneous use of the gender-dependent models to single use of the gender-dependent models. Analyzing the effect of the transform functions, we compare the transform function for only mean vectors with that for mean vectors and covariance matrices. Analyzing the effect of the estimation criteria, we compare the ML criterion with a robust estimation criterion called structural MAP. We evaluate the sensitivity of several thresholds for the piecewise linear regression algorithms and take up methods combining MAP adaptation with the linear regression algorithms. We incorporate these adaptation algorithms into our speech synthesis system and present several subjective and objective evaluation results showing the utility and effectiveness of these algorithms in speaker adaptation for HMM-based speech synthesis.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Yamagishi, Junichi] Univ Edinburgh, Ctr Speech Technol Res CSTR,
   Edinburgh EH8 9LW, Midlothian, Scotland.
   <br>[Kobayashi, Takao; Nakano, Yuji; Ogata, Katsumi; Isogai, Juri] Tokyo
   Inst Technol, Interdisciplinary Grad Sch Sci &amp; Engn, Yokohama, Kanagawa
   2268502, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Yamagishi, J (reprint author), Univ Edinburgh, Ctr Speech Technol Res CSTR, Edinburgh EH8 9LW, Midlothian, Scotland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jyamagis@inf.ed.ac.uk; takao.kobayashi@ip.titech.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>182</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>190</td>
</tr>

<tr>
<td valign="top">PD </td><td>JAN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">VL </td><td>17</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>66</td>
</tr>

<tr>
<td valign="top">EP </td><td>83</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASL.2008.2006647</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000262327000007</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wu, CH
   <br>Lee, CH
   <br>Liang, CH</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wu, Chung-Hsien
   <br>Lee, Chung-Han
   <br>Liang, Chung-Hau</td>
</tr>

<tr>
<td valign="top">TI </td><td>Idiolect Extraction and Generation for Personalized Speaking Style
   Modeling</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Idiolect extraction; idiolect generation; speaking style; superfluous
   idiolect</td>
</tr>

<tr>
<td valign="top">ID </td><td>LATENT SEMANTIC ANALYSIS</td>
</tr>

<tr>
<td valign="top">AB </td><td>A person's speaking style, consisting of such attributes as voice, choice of vocabulary, and the physical motions employed, not only expresses the speaker's identity but also emphasizes the content of an utterance. Speech combining these aspects of speaking style becomes more vivid and expressive to listeners. Recent research on speaking style modeling has paid more attention to speech signal processing. This approach focuses on text processing for idiolect extraction and generation to model a specific person's speaking style for the application of text-to-speech (TTS) conversion. The first stage of this study adopts a statistical method to automatically detect the candidate idiolects from a personalized, transcribed speech corpus. Based on the categorization of the detected candidate idiolects, superfluous idiolects are extracted using the fluency measure while the remaining candidates are regarded as the nonsuperfluous idiolects. In idiolect generation, the input text is converted into a target text with a particular speaker's speaking style via the insertion of superfluous idiolect or synonym substitution of nonsuperfluous idiolect. To evaluate the performance of the proposed methods, experiments were conducted on a Chinese corpus collected and transcribed from the speech files of three Taiwanese politicians. The results show that the proposed method can effectively convert a source text into a target text with a personalized speaking style.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Wu, Chung-Hsien; Lee, Chung-Han; Liang, Chung-Hau] Natl Cheng Kung
   Univ, Dept Comp Sci &amp; Informat Engn, Tainan 70101, Taiwan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Wu, CH (reprint author), Natl Cheng Kung Univ, Dept Comp Sci &amp; Informat Engn, Tainan 70101, Taiwan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>chwu@csie.ncku.edu.tw; chlee@csie.ncku.edu.tw; mcse.mcsd@msa.hinet.net</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Wu, Chung-Hsien</display_name>&nbsp;</font></td><td><font size="3">E-7970-2013&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PD </td><td>JAN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">VL </td><td>17</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>127</td>
</tr>

<tr>
<td valign="top">EP </td><td>137</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASL.2008.2006578</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000262327000012</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Toda, T
   <br>Nakamura, K
   <br>Nagai, T
   <br>Kaino, T
   <br>Nakajima, Y
   <br>Shikano, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Toda, Tomoki
   <br>Nakamura, Keigo
   <br>Nagai, Takayuki
   <br>Kaino, Tomomi
   <br>Nakajima, Yoshitaka
   <br>Shikano, Kiyohiro</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INST SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Technologies for Processing Body-Conducted Speech Detected with
   Non-Audible Murmur Microphone</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2009: 10TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH
   COMMUNICATION ASSOCIATION 2009, VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>10th INTERSPEECH 2009 Conference</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 06-10, 2009</td>
</tr>

<tr>
<td valign="top">CL </td><td>Brighton, ENGLAND</td>
</tr>

<tr>
<td valign="top">DE </td><td>silent speech; Non-Audible Murmur; body-conducted speech; voice
   conversion; automatic speech recognition</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION; RECOGNITION</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we review our recent research on technologies for processing body-conducted speech detected with Non-Audible Murmur (NAM) microphone. NAM microphone enables its to detect various types of body-conducted speech such as extremely soft whisper, normal speech, and so on. Moreover, it is robust against external noise due to its noise-proof structure. To make speech communication more universal by effectively using these properties of NAM microphone, we have so far developed two main technologies: one is body-conducted speech conversion for human-to-human speech communication; and the other is body-conducted speech recognition for man-machine speech communication. This paper gives an overview of these technologies and presents our new attempts to investigate the effectiveness of body-conducted speech recognition.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Toda, Tomoki; Nakamura, Keigo; Nagai, Takayuki; Kaino, Tomomi;
   Nakajima, Yoshitaka; Shikano, Kiyohiro] Nara Inst Sci &amp; Technol, Grad
   Sch Informat Sci, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Toda, T (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>tomoki@is.naist.jp; kei-naka@is.naist.jp; shikano@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">BP </td><td>636</td>
</tr>

<tr>
<td valign="top">EP </td><td>639</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000276842800157</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Tran, VA
   <br>Bailly, G
   <br>Loevenbruck, H
   <br>Toda, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Tran, Viet-Anh
   <br>Bailly, Gerard
   <br>Loevenbruck, Helene
   <br>Toda, Tomoki</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INST SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Multimodal HMM-based NAM-to-speech conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2009: 10TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH
   COMMUNICATION ASSOCIATION 2009, VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>10th INTERSPEECH 2009 Conference</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 06-10, 2009</td>
</tr>

<tr>
<td valign="top">CL </td><td>Brighton, ENGLAND</td>
</tr>

<tr>
<td valign="top">DE </td><td>audiovisual voice conversion; non-audible murmur; whispered speech;
   silent speech interface; HMM-based conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>EXTRACTION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Although the segmental intelligibility of converted speech from silent speech using direct signal-to-signal mapping proposed by Toda et al. [1] is quite acceptable, listeners have sometimes difficulty in chunking the speech continuum into meaningful words due to incomplete phonetic cues provided by output signals. This paper studies another approach consisting in combining HMM-based statistical speech recognition and synthesis techniques, as well as training on aligned corpora, to convert silent speech to audible voice. By introducing phonological constraints, such systems are expected to improve the phonetic consistency of output signals. Facial movements are used in order to improve the performance of both recognition and synthesis procedures. The results show that including these movements improves the recognition rate by 6.2% and a final improvement of the spectral distortion by 2.7% is observed. The comparison between direct signal-to-signal and phonetic-based mappings is finally commented in this paper.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Tran, Viet-Anh; Bailly, Gerard; Loevenbruck, Helene] CNRS, Dept Parole
   &amp; Cognit, GIPSA Lab, UMR 5216,INPG UJF U Stendhal, F-75700 Paris, France.
   <br>[Toda, Tomoki] Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara,
   Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Tran, VA (reprint author), CNRS, Dept Parole &amp; Cognit, GIPSA Lab, UMR 5216,INPG UJF U Stendhal, F-75700 Paris, France.</td>
</tr>

<tr>
<td valign="top">EM </td><td>viet-anh.tran@gipsa-lab.inpg.fr; gerard.bailly@gipsa-lab.inpg.fr;
   helene.loevenbruck@gipsa-lab.inpg.fr; tomoki@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">BP </td><td>648</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000276842800160</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nakamura, K
   <br>Toda, T
   <br>Saruwatari, H
   <br>Shikano, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nakamura, Keigo
   <br>Toda, Tomoki
   <br>Saruwatari, Hiroshi
   <br>Shikano, Kiyohiro</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INST SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Electrolaryngeal Speech Enhancement Based on Statistical Voice
   Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2009: 10TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH
   COMMUNICATION ASSOCIATION 2009, VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>10th INTERSPEECH 2009 Conference</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 06-10, 2009</td>
</tr>

<tr>
<td valign="top">CL </td><td>Brighton, ENGLAND</td>
</tr>

<tr>
<td valign="top">DE </td><td>Electrolarynx; Laryngectomee; Voice conversion; Speaking-aid</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper proposes a speaking-aid system for laryngectomees using GMM-based voice conversion that converts electrolaryngeal speech (EL speech) to normal speech. Because valid Po information cannot be obtained from the EL speech, we have so far converted the EL speech to whispering. This paper conducts the EL speech conversion to normal speech using Fo counters estimated from the spectral information of the EL speech. In this paper, we experimentally evaluate these two types of output speech of our speaking-aid system from several points of view. The experimental results demonstrate that the converted normal speech is preferred to the converted whisper.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nakamura, Keigo; Toda, Tomoki; Saruwatari, Hiroshi; Shikano, Kiyohiro]
   Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nakamura, K (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>kei-naka@is.naist.jp; tomoki@is.naist.jp; sawatari@is.naist.jp;
   shikano@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">BP </td><td>1443</td>
</tr>

<tr>
<td valign="top">EP </td><td>1446</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000276842800359</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ohtani, Y
   <br>Toda, T
   <br>Saruwatari, H
   <br>Shikano, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ohtani, Yamato
   <br>Toda, Tomoki
   <br>Saruwatari, Hiroshi
   <br>Shikano, Kiyohiro</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INST SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Many-to-many eigenvoice conversion with reference voice</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2009: 10TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH
   COMMUNICATION ASSOCIATION 2009, VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>10th INTERSPEECH 2009 Conference</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 06-10, 2009</td>
</tr>

<tr>
<td valign="top">CL </td><td>Brighton, ENGLAND</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speech synthesis; voice conversion; Gaussian mixture model; eigenvoice;
   many-to-many</td>
</tr>

<tr>
<td valign="top">ID </td><td>ADAPTATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we propose many-to-many voice conversion (VC) techniques to convert an arbitrary source speaker's voice into an arbitrary target speaker's voice. We have proposed one-to-many eigenvoice conversion (EVC) and many-to-one EVC. In the EVC, an eigenvoice Gaussian mixture model (EV-GMM) is trained in advance using multiple parallel data sets of a reference speaker and many pre-stored speakers. The EV-GMM is flexibly adapted to an arbitrary speaker using a small amount of adaptation data without any linguistic constraints. In this paper, we achieve many-to-many VC by sequentially performing many-to-one EVC and one-to-many EVC through the reference speaker using the same EV-GMM. Experimental results demonstrate the effectiveness of the proposed many-to-many VC.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ohtani, Yamato; Toda, Tomoki; Saruwatari, Hiroshi; Shikano, Kiyohiro]
   Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ohtani, Y (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>yamato-o@is.naist.jp; tomoki@is.naist.jp; sawatari@is.naist.jp;
   shikano@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">BP </td><td>1591</td>
</tr>

<tr>
<td valign="top">EP </td><td>1594</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000276842800396</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Godoy, E
   <br>Rosec, O
   <br>Chonavel, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Godoy, Elizabeth
   <br>Rosec, Olivier
   <br>Chonavel, Thierry</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INST SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Alleviating the One-to-Many Mapping Problem in Voice Conversion with
   Context-Dependent Modeling</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2009: 10TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH
   COMMUNICATION ASSOCIATION 2009, VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>10th INTERSPEECH 2009 Conference</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 06-10, 2009</td>
</tr>

<tr>
<td valign="top">CL </td><td>Brighton, ENGLAND</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; GMM; Spectral mapping</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper addresses the "one-to-many" mapping problem in Voice Conversion (VC) by exploring source-to-target mappings in GMM-based spectral transformation. Specifically, we examine differences using source-only versus joint source/target information in the classification stage of transformation, effectively illustrating a "one-to-many effect" in the traditional acoustically-based GMM. We propose combating this effect by using phonetic information in the GMM learning and classification. We then show the success of our proposed context-dependent modeling with transformation results using an objective error criterion. Finally, we discuss implications of our work in adapting current approaches to VC.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Godoy, Elizabeth; Rosec, Olivier] Orange Labs, Lannion, France.
   <br>[Chonavel, Thierry] Telecom Bretagne, Signal &amp; Commun, Brest, France.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Godoy, E (reprint author), Orange Labs, Lannion, France.</td>
</tr>

<tr>
<td valign="top">EM </td><td>elizabeth.godoy@orange-ftgroup.com; olivier.rosec@orange-ftgroup.com;
   thierry.chonavel@enst-bretagne.fr</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">BP </td><td>1595</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000276842800397</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nguyen, BP
   <br>Akagi, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nguyen, Binh Phu
   <br>Akagi, Masato</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INST SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Efficient Modeling of Temporal Structure of Speech For Applications in
   Voice Transformation</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2009: 10TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH
   COMMUNICATION ASSOCIATION 2009, VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>10th INTERSPEECH 2009 Conference</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 06-10, 2009</td>
</tr>

<tr>
<td valign="top">CL </td><td>Brighton, ENGLAND</td>
</tr>

<tr>
<td valign="top">DE </td><td>spectral modification; voice transformation; temporal decomposition</td>
</tr>

<tr>
<td valign="top">ID </td><td>CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Aims of voice transformation are to change styles of given utterances. Most voice transformation methods process speech signals in a time-frequency domain. In the time domain, when processing spectral information, conventional methods do not consider relations between neighboring frames. If unexpected modifications happen, there are discontinuities between frames, which lead to the degradation of the transformed speech quality. This paper proposes a new modeling of temporal structure of speech to ensure the smoothness of the transformed speech for improving the quality of transformed speech in the voice transformation. In our work, we propose an improvement of the temporal decomposition (TD) technique, which decomposes a speech signal into event targets and event functions, to model the temporal structure of speech. The TD is used to control the spectral dynamics and to ensure the smoothness of transformed speech. We investigate the TD in two applications, concatenative speech synthesis and spectral voice conversion. Experimental results confirm the effectiveness of TO in terms of improving the quality of the transformed speech.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nguyen, Binh Phu; Akagi, Masato] Japan Adv Inst Sci &amp; Technol, Sch
   Informat Sci, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nguyen, BP (reprint author), Japan Adv Inst Sci &amp; Technol, Sch Informat Sci, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>npbinh@jaist.ac.jp; akagi@jaist.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">BP </td><td>1599</td>
</tr>

<tr>
<td valign="top">EP </td><td>1602</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000276842800398</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Charlier, M
   <br>Ohtani, Y
   <br>Toda, T
   <br>Moinet, A
   <br>Dutoit, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Charlier, Malorie
   <br>Ohtani, Yamato
   <br>Toda, Tomoki
   <br>Moinet, Alexis
   <br>Dutoit, Thierry</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INST SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Cross-Language Voice Conversion Based on Eigenvoices</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2009: 10TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH
   COMMUNICATION ASSOCIATION 2009, VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>10th INTERSPEECH 2009 Conference</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 06-10, 2009</td>
</tr>

<tr>
<td valign="top">CL </td><td>Brighton, ENGLAND</td>
</tr>

<tr>
<td valign="top">DE </td><td>speech synthesis; voice conversion; cross-language; eigenvoice
   conversion; unsupervised adaptation</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a novel cross-language voice conversion (VC) method based on eigenvoice conversion (EVC). Cross-language VC is a technique for converting voice quality between two speakers uttering different languages each other. In general, parallel data consisting of utterance pairs of those two speakers are not available. To deal with this problem, we apply EVC to cross-language VC. First, we train an eigenvoice GMM (EV-GMM) using many parallel data sets by a source speaker and many pre-stored other speakers who can utter the same language as the source speaker. And then, the conversion model between the source speaker and a target speaker who cannot utter the source speaker's language is developed by adapting the EV-GMM using a few arbitrary sentences uttered by the target speaker in a different language. The experimental results demonstrate that the proposed method yields significant performance improvements in both speech quality and conversion accuracy for speaker individuality compared with a conventional cross-language VC method based on frame selection.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Charlier, Malorie; Ohtani, Yamato; Toda, Tomoki] Nara Inst Sci &amp;
   Technol, Grad Sch Informat Sci, Nara, Japan.
   <br>[Charlier, Malorie; Moinet, Alexis; Dutoit, Thierry] Fac Polytech Mons,
   Mons, Belgium.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Charlier, M (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>Malorie.CHARLIER@student.fpms.ac.be; yamato-o@is.naist.jp;
   tomoki@is.naist.jp; Alexis.Moinet@fpms.ac.be; Thierry.Dutoit@fpms.ac.be</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">BP </td><td>1603</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000276842800399</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Uriz, AJ
   <br>Aguero, PD
   <br>Bonafonte, A
   <br>Tulli, JC</td>
</tr>

<tr>
<td valign="top">AF </td><td>Jose Uriz, Alejandro
   <br>Daniel Agueero, Pablo
   <br>Bonafonte, Antonio
   <br>Carlos Tulli, Juan</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INST SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Conversion using K-Histograms and Frame Selection</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2009: 10TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH
   COMMUNICATION ASSOCIATION 2009, VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>10th INTERSPEECH 2009 Conference</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 06-10, 2009</td>
</tr>

<tr>
<td valign="top">CL </td><td>Brighton, ENGLAND</td>
</tr>

<tr>
<td valign="top">DE </td><td>speech synthesis; voice conversion; frame selection; non-gaussian
   transformation</td>
</tr>

<tr>
<td valign="top">AB </td><td>The goal of voice conversion systems is to modify the voice of a source speaker to be perceived as if it had been uttered by another specific speaker. Many approaches found in the literature work based on statistical models and introduce an oversmoothing in the target features. Our proposal is a new model that combines several techniques used in unit selection for text-to-speech and a non-gaussian transformation mathematical model. Subjective results support the proposed approach.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Jose Uriz, Alejandro; Daniel Agueero, Pablo; Carlos Tulli, Juan] Univ
   Nacl Mar del Plata, Fac Ingn, Mar Del Plata, Buenos Aires, Argentina.
   <br>[Bonafonte, Antonio] Univ Politecn Cataluna, Barcelona, Spain.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Uriz, AJ (reprint author), Univ Nacl Mar del Plata, Fac Ingn, Mar Del Plata, Buenos Aires, Argentina.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ajuriz@fi.mdp.edu.ar; pdaguero@fi.mdp.edu.ar</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">BP </td><td>1607</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000276842800400</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wu, DL
   <br>Li, BJ
   <br>Jiang, H
   <br>Fu, QJ</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wu, Dalei
   <br>Li, Baojie
   <br>Jiang, Hui
   <br>Fu, Qian-Jie</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INST SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Online Model Adaptation for Voice Conversion using Model-based Speech
   Synthesis Techniques</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2009: 10TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH
   COMMUNICATION ASSOCIATION 2009, VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>10th INTERSPEECH 2009 Conference</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 06-10, 2009</td>
</tr>

<tr>
<td valign="top">CL </td><td>Brighton, ENGLAND</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; HMM-based speech synthesis; GMM; model adaptation</td>
</tr>

<tr>
<td valign="top">ID </td><td>TRANSFORMATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper. we present a novel voice conversion method using model-based speech synthesis that can be used for some applications where prior knowledge or training data is not available from the source speaker. In the proposed method, training data front a target speaker is used to build a GMM-based speech model and voice conversion is then performed for each utterance from the source speaker according to the pre-trained target speaker model. To reduce the mismatch between source and target speakers, online model adaptation is proposed to improve model selection accuracy. based on maximum likelihood linear regression (MLLR). Objective and subjective evaluations suggest that the proposed methods are quite effective in generating acceptable voice quality for voice conversion even without training data from source speakers.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Wu, Dalei; Li, Baojie; Jiang, Hui] York Univ, Dept Comp Sci &amp; Engn,
   4700 Keele St, Toronto, ON M3J 1P3, Canada.
   <br>[Fu, Qian-Jie] House Ear Res Inst, Los Angeles, CA 90057 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Wu, DL (reprint author), York Univ, Dept Comp Sci &amp; Engn, 4700 Keele St, Toronto, ON M3J 1P3, Canada.</td>
</tr>

<tr>
<td valign="top">EM </td><td>daleiwu@cse.yorku.ca; lbj@cse.yorku.ca; hj@cse.yorku.ca; qfu@hei.org</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">BP </td><td>1611</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000276842800401</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Watts, O
   <br>Yamagishi, J
   <br>King, S
   <br>Berkling, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Watts, Oliver
   <br>Yamagishi, Junichi
   <br>King, Simon
   <br>Berkling, Kay</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INST SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>HMM adaptation and voice conversion for the synthesis of child speech: a
   comparison</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2009: 10TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH
   COMMUNICATION ASSOCIATION 2009, VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>10th INTERSPEECH 2009 Conference</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 06-10, 2009</td>
</tr>

<tr>
<td valign="top">CL </td><td>Brighton, ENGLAND</td>
</tr>

<tr>
<td valign="top">DE </td><td>child speech; statistical parametric speech synthesis; HMM-based speech
   synthesis; voice conversion; HTS; Average Voice Models; Festival</td>
</tr>

<tr>
<td valign="top">AB </td><td>This study compares two different methodologies for producing data-driven synthesis of child speech from existing systems that have been trained on the speech of adults. On one hand, an existing statistical parametric synthesiser is transformed using model adaptation techniques, informed by linguistic and prosodic knowledge, to the speaker characteristics of a child speaker. This is compared with the application of voice conversion techniques to convert the output of an existing waveform concatenation synthesiser with no explicit linguistic or prosodic knowledge. In a subjective evaluation of the similarity of synthetic speech to natural speech from the target speaker, the HMM-based systems evaluated are generally preferred, although this is at least in part due to the higher dimensional acoustic features supported by these techniques.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Watts, Oliver; Yamagishi, Junichi; King, Simon] Univ Edinburgh, Ctr
   Speech Technol Res, Edinburgh EH8 9YL, Midlothian, Scotland.
   <br>[Berkling, Kay] Inline Internet Online Dienste GmbH, Karlsruhe, Germany.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Watts, O (reprint author), Univ Edinburgh, Ctr Speech Technol Res, Edinburgh EH8 9YL, Midlothian, Scotland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>O.S.Watts@sms.ed.ac.uk; jyamagis@inf.ed.ac.uk; Simon.King@ed.ac.uk;
   kay@berkling.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">BP </td><td>2595</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000276842801190</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nambu, Y
   <br>Mikawa, M
   <br>Tanaka, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nambu, Yoshiki
   <br>Mikawa, Masahiko
   <br>Tanaka, Kazuyo</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INST SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Morphing based on Interpolation of Vocal Tract Area Functions
   Using AR-HMM Analysis of Speech</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2009: 10TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH
   COMMUNICATION ASSOCIATION 2009, VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>10th INTERSPEECH 2009 Conference</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 06-10, 2009</td>
</tr>

<tr>
<td valign="top">CL </td><td>Brighton, ENGLAND</td>
</tr>

<tr>
<td valign="top">DE </td><td>morphing voice; phonological continuity; speech synthesis; inter- and
   extra-polation</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a new voice morphing method which focuses on the continuity of phonological identity overall inter- and extra-polated regions. Main features of the method are 1) to separate the characteristic of vocal tract area resonances from that of vocal cord waves by using AR-HMM analysis of speech, 2) interpolation in a log vocal tract area function domain and 3) independent morphing for the vocal tract resonances and vocal cord wave characteristics. By the morphing system constructed on a statistical conversion method, the continuity of formants and perceptual difference between a conventional method and the proposed method are confirmed.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nambu, Yoshiki; Mikawa, Masahiko; Tanaka, Kazuyo] Univ Tsukuba, Grad
   Sch Lib Informat &amp; Media Studies, Tsukuba, Ibaraki 305, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nambu, Y (reprint author), Univ Tsukuba, Grad Sch Lib Informat &amp; Media Studies, Tsukuba, Ibaraki 305, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ynambu@slis.tsukuba.ac.jp; mikawa@slis.tsukuba.ac.jp;
   ktanaka@slis.tsukuba.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">BP </td><td>2607</td>
</tr>

<tr>
<td valign="top">EP </td><td>2610</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000276842801193</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hwang, HT
   <br>Chiang, CY
   <br>Sung, PY
   <br>Chen, SH</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hwang, Hsin-Te
   <br>Chiang, Chen-Yu
   <br>Sung, Po-Yi
   <br>Chen, Sin-Horng</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INST SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Novel Model-based Pitch Conversion Method for Mandarin Speech</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2009: 10TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH
   COMMUNICATION ASSOCIATION 2009, VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>10th INTERSPEECH 2009 Conference</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 06-10, 2009</td>
</tr>

<tr>
<td valign="top">CL </td><td>Brighton, ENGLAND</td>
</tr>

<tr>
<td valign="top">DE </td><td>prosodic model; pitch conversion; prosody conversion; voice conversion</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, a novel model-based pitch conversion method for Mandarin is presented and compared with other two conventional conversion methods, i.e. the mean/variance transformation approach and the GMM-based mapping approach. Syllable pitch contour is first quantized by 3(rd) order orthogonal expansion coefficients; then, the source and target speakers' prosodic models are constructed, respectively. Two mapping methods based on the prosodic model are presented. Objective tests confirmed that one of the proposed methods are superior the conventional methods. Some findings in informal listening tests and objective tests are worthwhile to further investigate.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Hwang, Hsin-Te; Chiang, Chen-Yu; Sung, Po-Yi; Chen, Sin-Horng] Natl
   Chiao Tung Univ, Dept Commun Engn, Hsinchu, Taiwan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hwang, HT (reprint author), Natl Chiao Tung Univ, Dept Commun Engn, Hsinchu, Taiwan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>max0219.cm94g@nctu.edu.tw; gene.cm91g@nctu.edu.tw;
   notaya.cm96g@nctu.edu.tw; schen@mail.nctu.edu.tw</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">BP </td><td>2611</td>
</tr>

<tr>
<td valign="top">EP </td><td>2614</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000276842801194</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kawahara, H
   <br>Morise, M
   <br>Takahashi, T
   <br>Banno, H
   <br>Nisimura, R
   <br>Irino, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kawahara, Hideki
   <br>Morise, Masanori
   <br>Takahashi, Toru
   <br>Banno, Hideki
   <br>Nisimura, Ryuichi
   <br>Irino, Toshio</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INST SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Observation of empirical cumulative distribution of vowel spectral
   distances and its application to vowel based voice conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2009: 10TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH
   COMMUNICATION ASSOCIATION 2009, VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>10th INTERSPEECH 2009 Conference</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 06-10, 2009</td>
</tr>

<tr>
<td valign="top">CL </td><td>Brighton, ENGLAND</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; STRAIGHT; vowel structure; line spectral pair; vocal
   tract length</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH; SOUNDS</td>
</tr>

<tr>
<td valign="top">AB </td><td>A simple and fast voice conversion method based only on vowel information is proposed. The proposed method relies on empirical distribution of perceptual spectral distances between representative examples of each vowel segment extracted using TANDEM-STRAIGHT spectral envelope estimation procedure [1]. Mapping functions of vowel spectra are designed to preserve vowel space structure defined by the observed empirical distribution while transforming position and orientation of the structure in an abstract vowel spectral space. By introducing physiological constraints in vocal tract shapes and vocal tract length normalization, difficulties in careful frequency alignment between vowel template spectra of the source and the target speakers can be alleviated without significant degradations in converted speech. The proposed method is a frame-based instantaneous method and is relevant for real-time processing. Applications of the proposed method in-cross language voice conversion are also discussed.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kawahara, Hideki; Nisimura, Ryuichi; Irino, Toshio] Wakayama Univ, Dept
   Design Informat Sci, Wakayama, Japan.
   <br>[Morise, Masanori] Ritsumeikan Univ, Dept Media Technol, Kyoto, Japan.
   <br>[Takahashi, Toru] Kyoto Univ, Grad Sch Informat, Kyoto, Japan.
   <br>[Banno, Hideki] Meijo Univ, Dept Informat Engn, Nagoya, Aichi 4648601,
   Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kawahara, H (reprint author), Wakayama Univ, Dept Design Informat Sci, Wakayama, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>kawahara@sys.wakayama-u.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">BP </td><td>2615</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000276842801195</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Tachibana, R
   <br>Shuang, ZW
   <br>Nishimura, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Tachibana, Ryuki
   <br>Shuang, Zhiwei
   <br>Nishimura, Masafumi</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INST SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Japanese Pitch Conversion for Voice Morphing Based on Differential
   Modeling</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2009: 10TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH
   COMMUNICATION ASSOCIATION 2009, VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>10th INTERSPEECH 2009 Conference</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 06-10, 2009</td>
</tr>

<tr>
<td valign="top">CL </td><td>Brighton, ENGLAND</td>
</tr>

<tr>
<td valign="top">DE </td><td>pitch conversion; voice conversion; voice morphing; speech synthesis;
   differential modeling</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we convert the pitch contours predicted by a TTS system that models a source speaker to resemble the pitch contours of a target speaker. When the speaking styles of the speakers are very different, complex conversions such as adding or deleting pitch peaks may be required. Our method does the conversions by modeling the direct pitch features and differential pitch features at the same time based on linguistic features. The differential pitch features are calculated from matched pairs of source and target pitch values. We show experimental results in which the target speaker's characteristics are successfully modeled based on a very limited training corpus. The proposed pitch conversion method stretches the possibilities of TTS customization for various speaking styles.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Tachibana, Ryuki; Nishimura, Masafumi] IBM Res Corp, Tokyo Res Lab,
   Kanagawa, Japan.
   <br>[Shuang, Zhiwei] IBM Res Corp, China Res Lab, Beijing, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Tachibana, R (reprint author), IBM Res Corp, Tokyo Res Lab, Kanagawa, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ryuki@jp.ibm.com; shuangzw@cn.ibm.com; nisimura@jp.ibm.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">BP </td><td>2619</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000276842801196</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Popa, V
   <br>Nurminen, J
   <br>Gabbouj, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Popa, Victor
   <br>Nurminen, Jani
   <br>Gabbouj, Moncef</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INST SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Novel Technique for Voice Conversion Based on Style and Content
   Decomposition with Bilinear Models</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2009: 10TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH
   COMMUNICATION ASSOCIATION 2009, VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>10th INTERSPEECH 2009 Conference</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 06-10, 2009</td>
</tr>

<tr>
<td valign="top">CL </td><td>Brighton, ENGLAND</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a novel technique for voice conversion by solving a two-factor task using bilinear models. The spectral content of the speech represented as line spectral frequencies is separated into so-called style and content parameterizations using a framework proposed in [1]. This formulation of the voice conversion problem in terms of style and content offers a flexible representation of factor interactions and facilitates the use of efficient training algorithms based on singular value decomposition and expectation maximization. Promising results in a comparison with the traditional Gaussian mixture model based method indicate increased robustness with small training sets.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Popa, Victor; Gabbouj, Moncef] Tampere Univ Technol, Dept Signal Proc,
   FIN-33101 Tampere, Finland.
   <br>[Nurminen, Jani] Nokia, Devices, Tampere, Finland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Popa, V (reprint author), Tampere Univ Technol, Dept Signal Proc, FIN-33101 Tampere, Finland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>victor.popa@tut.fi; jani.k.nurminen@nokia.com; moncef.gabbouj@tut.fi</td>
</tr>

<tr xmlns:date="http://exslt.org/dates-and-times">
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Gabbouj, Moncef</display_name>&nbsp;</font></td><td><font size="3">G-4293-2014&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Gabbouj, Moncef</display_name>&nbsp;</font></td><td><font size="3">0000-0002-9788-2323&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">BP </td><td>2623</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000276842801197</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Turk, O
   <br>Arslan, LM</td>
</tr>

<tr>
<td valign="top">AF </td><td>Turk, Oytun
   <br>Arslan, Levent M.</td>
</tr>

<tr>
<td valign="top">TI </td><td>Automatic source speaker selection for voice conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>JOURNAL OF THE ACOUSTICAL SOCIETY OF AMERICA</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>hearing; learning (artificial intelligence); neural nets; regression
   analysis; speaker recognition; speech coding</td>
</tr>

<tr>
<td valign="top">ID </td><td>PROCESSING TECHNIQUES; SPEECH; TRANSFORMATION; QUALITY</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper focuses on the importance of source speaker selection for a weighted codebook mapping based voice conversion algorithm. First, the dependency on source speakers is evaluated in a subjective listening test using 180 different source-target pairs from a database of 20 speakers. Subjective scores for similarity to target speaker's voice and quality are obtained. Statistical analysis of scores confirms the dependence of performance on source speakers for both male-to-male and female-to-female transformations. A source speaker selection algorithm is devised given a target speaker and a set of source speaker candidates. For this purpose, an artificial neural network (ANN) is trained that learns the regression between a set of acoustical distance measures and the subjective scores. The estimated scores are used in source speaker ranking. The average cross-correlation coefficient between rankings obtained from median subjective scores and rankings estimated by the algorithm is 0.84 for similarity and 0.78 for quality in male-to-male transformations. The results for female-to-female transformations were less reliable with a cross-correlation value of 0.58 for both similarity and quality.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Turk, Oytun] Bogazici Univ, Dept Elect &amp; Elect Engn, TR-34342 Istanbul,
   Turkey.
   <br>[Arslan, Levent M.] Sestek Inc, R&amp;D Dept, TR-34342 Istanbul, Turkey.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Turk, O (reprint author), DFKI Language Technol Lab, Speech Grp, Berlin, Germany.</td>
</tr>

<tr>
<td valign="top">EM </td><td>oytun.turk@sestek.com.tr; arslanle@boun.edu.tr</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Arslan, Levent</display_name>&nbsp;</font></td><td><font size="3">D-6377-2015&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Arslan, Levent</display_name>&nbsp;</font></td><td><font size="3">0000-0002-6086-8018&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>JAN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">VL </td><td>125</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>480</td>
</tr>

<tr>
<td valign="top">EP </td><td>491</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1121/1.3027445</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Audiology &amp; Speech-Language Pathology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000262672600057</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Pribilova, A
   <br>Pribil, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Pribilova, Anna
   <br>Pribil, Jiri</td>
</tr>

<tr>
<td valign="top">BE </td><td>Esposito, A
   <br>Hussain, A
   <br>Marinaro, M
   <br>Martone, R</td>
</tr>

<tr>
<td valign="top">TI </td><td>Spectrum Modification for Emotional Speech Synthesis</td>
</tr>

<tr>
<td valign="top">SO </td><td>MULTIMODAL SIGNAL: COGNITIVE AND ALGORITHMIC ISSUES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Lecture Notes in Artificial Intelligence</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>euCognition International Training School on Multimodal Signals -
   Cognitive and Algorithmic Issues</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 21-26, 2008</td>
</tr>

<tr>
<td valign="top">CL </td><td>Vietri sul Mare, ITALY</td>
</tr>

<tr>
<td valign="top">DE </td><td>emotional speech; spectral envelope; speech synthesis; emotional voice
   conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>CEPSTRAL DESCRIPTION; RECOGNITION; CONVERSION; FEATURES; MACHINE;
   CORPORA; SYSTEM; TTS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Emotional stale of a speaker is accompanied by physiological changes affecting respiration, phonation. and articulation. These changes are manifested mainly in prosodic patterns of F0, energy, and duration, but also in segmental parameters of speech spectrum. Therefore, our new emotion speech synthesis method is supplemented with spectrum modification. It comprises non-linear frequency scale transformation of speech spectral envelope. filtering for emphasizing low or high frequency range, and controlling of spectral noise by spectral flatness measure according to knowledge of psychological and phonetic research. The proposed spectral modification is combined with linear modification of F0 mean. F0 range, energy, and duration. Speech resynthesis with applied modification that should represent joy. anger and sadness is evaluated by a listening test.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Pribilova, Anna] Slovak Tech Univ, Dept Radio Elect, Ilkovicova 3,
   SK-81219 Bratislava, Slovakia.
   <br>[Pribil, Jiri] Acad Sci Czech Republic, Inst Photon &amp; Elect, CZ-18251
   Prague, Czech Republic.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Pribilova, A (reprint author), Slovak Tech Univ, Dept Radio Elect, Ilkovicova 3, SK-81219 Bratislava, Slovakia.</td>
</tr>

<tr>
<td valign="top">EM </td><td>Anna.Pribilova@stuba.sk; Jiri.Pribil@savba.sk</td>
</tr>

<tr>
<td valign="top">TC </td><td>11</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>11</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">VL </td><td>5398</td>
</tr>

<tr>
<td valign="top">BP </td><td>232</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Imaging Science &amp; Photographic Technology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000265464200023</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Papuzinski, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Papuzinski, Andrzej</td>
</tr>

<tr>
<td valign="top">TI </td><td>The Idea of Philosophy vs. Eco-Philosophy</td>
</tr>

<tr>
<td valign="top">SO </td><td>Problemy Ekorozwoju</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>environmental philosophy; contemporary philosophy; ecophilosophy;
   Skolimowski; idea of philosophy; rationality; cultural rationality</td>
</tr>

<tr>
<td valign="top">AB </td><td>This article on environmental philosophy was inspired by a discussion on the latter's compatibility with the standards in place in academic philosophy. It seeks to consider how environmental philosophy relates to the idea of philosophy as such, electing to do so by reference to one of the best-known systems of environmental philosophy: the concept Henryk Skolimowski dubbed ecophilosophy (eko-filozofia). It is the author's contention that the latter is a contemporary philosophy in which the core idea of philosophy is being renewed. This main thesis is developed in the article's three parts, of which the first deals with the ongoing relationship between the idea and subject of philosophy. Here it is noted that the critical attitude is the one constant element underpinning the idea of philosophy. This attitude entails reservations as to - or simply a lack of full confidence in - the certainties and truths deemed to make up each and every one of history's concrete forms of rationality. The lack of confidence manifests itself in a refusal to engage in a priori acceptance of any cultural tradition as exhausting the layers of human rationality (in terms of values, cognitive stereotypes or standards determining actions).
   <br>The second part of the article addresses with the crisis in contemporary philosophy taken to reflect the marginalization of philosophical thought in today's Euro-Atlantic culture. A theory as to why this is now being noted is presented and developed, the assertion - in brief - being that such a state of affairs has arisen because the main currents in contemporary philosophy have separated from the idea of philosophy, the currents in question tending to seek philosophy's conversion into yet another scientific discipline.
   <br>The third part of the article then focuses on ecophilosophy as a system aiming to re-supply philosophy with the idea of philosophy it has otherwise contrived to lose. Two issues raised here are thus the characterisation of the remaining currents to contemporary philosophy - as a voice of protest against the main direction philosophy adopted in the 19th and 20th centuries, as well as the characterisation of ecophilosophy as heir to that other style of philosophical reflection wherein the aforementioned criticism remains a powerful feature, at least in relation to today's most troublesome civilisational and social problems.
   <br>The methodological background to the article comprises a popular typology of contemporary philosophy identifying the scientistic, the anti-scientistic and the ideological as the three fundamental groups of theories present; as well as a detailing of the sources of philosophical issues that points to those spheres of public life in which universally-accepted solutions are ceasing to be self-evident.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Kazimierz Wielki Univ Bydgoszcz, Bydgoszcz, Poland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Papuzinski, A (reprint author), Kazimierz Wielki Univ Bydgoszcz, Bydgoszcz, Poland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>papuzin@ukw.edu.pl</td>
</tr>

<tr>
<td valign="top">TC </td><td>5</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>5</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">VL </td><td>4</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>51</td>
</tr>

<tr>
<td valign="top">EP </td><td>59</td>
</tr>

<tr>
<td valign="top">SC </td><td>Environmental Sciences &amp; Ecology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000262566400005</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Gao, YY
   <br>Zhu, WB</td>
</tr>

<tr>
<td valign="top">AF </td><td>Gao Yingying
   <br>Zhu Weibin</td>
</tr>

<tr>
<td valign="top">BE </td><td>Yang, YX</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Conversion Based on STRAIGHT and UBM-GMM</td>
</tr>

<tr>
<td valign="top">SO </td><td>PROCEEDINGS OF 2009 CONFERENCE ON COMMUNICATION FACULTY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Conference on Communication Faculty</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 18-20, 2009</td>
</tr>

<tr>
<td valign="top">CL </td><td>Nanning, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; STRAIGHT; UBM-GMM</td>
</tr>

<tr>
<td valign="top">ID </td><td>TRANSFORMATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>A novel approach for voice conversion is proposed which uses STRAIGHT analysis/synthesis arithmetic to analyze and synthesize speech. UBM-GMM is adopted and the speaker characters are described by the parameters of GMM. Consequently the amount of the target speaker training dataset is reduced. The transformation function based on GMM is also modified and the parallel corpus is successfully avoided then, along with an improvement in the flexibility of the function to the character parameters.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Gao Yingying; Zhu Weibin] Beijing Jiaotong Univ, Inst Informat Sci,
   Beijing, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Gao, YY (reprint author), Beijing Jiaotong Univ, Inst Informat Sci, Beijing, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>wbzhu@bjtu.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">BP </td><td>342</td>
</tr>

<tr>
<td valign="top">EP </td><td>345</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000274174800074</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Mesbahi, L
   <br>Barreaud, V
   <br>Boeffard, O</td>
</tr>

<tr>
<td valign="top">AF </td><td>Mesbahi, Larbi
   <br>Barreaud, Vincent
   <br>Boeffard, Olivier</td>
</tr>

<tr>
<td valign="top">BE </td><td>Rudas, I
   <br>Demiralp, M
   <br>Mastorakis, N</td>
</tr>

<tr>
<td valign="top">TI </td><td>Comparing linear and non-linear transformation of speech</td>
</tr>

<tr>
<td valign="top">SO </td><td>PROCEEDINGS OF THE 9TH WSEAS INTERNATIONAL CONFERENCE ON SIGNALS, SPEECH
   AND IMAGE PROCESSING/9TH WSEAS INTERNATIONAL CONFERENCE ON MULTIMEDIA,
   INTERNET &amp; VIDEO TECHNOLOGIES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Recent Advances in Computer Engineering</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th WSEAS International Conference on Signal, Speech and Image
   Processing/9th WSEAS International Conference on Multimedia, Internet
   and Video Technologies</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 03-05, 2009</td>
</tr>

<tr>
<td valign="top">CL </td><td>Budapest, HUNGARY</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; Gaussian Mixture Model; Radial Basis Function</td>
</tr>

<tr>
<td valign="top">ID </td><td>BASIS FUNCTION NETWORKS; CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper aims to study voice conversion using linear and non-linear transform systems, based on respectively the Gaussian Mixture Models, GMM, and the Radial Basis function. RBF. We compare on an identical speech database both proposed approaches. We insist in particular on the objective measures of the transformation, in the case that we have not enough data recorded for the target speaker. We show for databases containing only one and two speech sentences, that the non-linear transform (RBF) gives weaker distortion scores than the GMM.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Mesbahi, Larbi; Barreaud, Vincent; Boeffard, Olivier] Univ Rennes 1,
   IRISA ENSSAT, Lannion, France.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Mesbahi, L (reprint author), Univ Rennes 1, IRISA ENSSAT, 6 Rue Kerampont, Lannion, France.</td>
</tr>

<tr>
<td valign="top">EM </td><td>lmesbahi@irisa.fr; vincent.barreaud@irisa.fr; olivier.boeffard@irisa.fr</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">BP </td><td>68</td>
</tr>

<tr>
<td valign="top">EP </td><td>73</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000276614500011</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hanzlicek, Z
   <br>Matousek, J
   <br>Tihelka, D</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hanzlicek, Zdenek
   <br>Matousek, Jindrich
   <br>Tihelka, Daniel</td>
</tr>

<tr>
<td valign="top">BE </td><td>Matousek, V
   <br>Mautner, P</td>
</tr>

<tr>
<td valign="top">TI </td><td>First Experiments on Text-to-Speech System Personification</td>
</tr>

<tr>
<td valign="top">SO </td><td>TEXT, SPEECH AND DIALOGUE, PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">SE </td><td>Lecture Notes in Artificial Intelligence</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>12th International Conference on Text, Speech and Dialogue</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 13-17, 2009</td>
</tr>

<tr>
<td valign="top">CL </td><td>Pilsen, CZECH REPUBLIC</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>In the present paper, several experiments on text-to-speech system personification are described. The personification enables TTS system to produce new voices by employing voice conversion methods. The baseline speech synthetizer is a concatenative corpus-based TTS system which utilizes the unit selection method. The voice identity change is performed by the transformation of spectral envelope, spectral detail and pitch. Two different personification approaches are compared in this paper. The former is based on the transformation of the original speech corpus, the latter transforms the output of the synthesizer. Specific advantages and disadvantages of both approaches are discussed and their performance is compared in listening tests.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Hanzlicek, Zdenek; Matousek, Jindrich; Tihelka, Daniel] Univ W Bohemia,
   Fac Sci Appl, Dept Cybernet, Plzen 30614, Czech Republic.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hanzlicek, Z (reprint author), Univ W Bohemia, Fac Sci Appl, Dept Cybernet, Univ 8, Plzen 30614, Czech Republic.</td>
</tr>

<tr>
<td valign="top">EM </td><td>zhanzlic@kky.zcu.cz; jmatouse@kky.zcu.cz; dtihelka@kky.zcu.cz</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Matousek, Jindrich</display_name>&nbsp;</font></td><td><font size="3">C-2146-2011&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Tihelka, Daniel</display_name>&nbsp;</font></td><td><font size="3">A-4318-2012&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Matousek, Jindrich</display_name>&nbsp;</font></td><td><font size="3">0000-0002-7408-7730&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Tihelka, Daniel</display_name>&nbsp;</font></td><td><font size="3">0000-0002-3149-2330&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>4</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>4</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">VL </td><td>5729</td>
</tr>

<tr>
<td valign="top">BP </td><td>186</td>
</tr>

<tr>
<td valign="top">EP </td><td>193</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000270445700026</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nordstrom, KI
   <br>Tzanetakis, G
   <br>Driessen, PF</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nordstrom, Karl I.
   <br>Tzanetakis, George
   <br>Driessen, Peter F.</td>
</tr>

<tr>
<td valign="top">TI </td><td>Transforming perceived vocal effort and breathiness using adaptive
   pre-emphasis linear prediction</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>adaptive pre-emphasis; breathiness; linear prediction (LP);
   pre-emphasis; spectral slope; vocal effort; voice quality</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH; RECOGNITION; PERCEPTION; VOWELS</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a technique to transform high-effort voices into breathy voices using adaptive pre-emphasis linear prediction (APLP). The primary benefit of this technique is that it estimates a spectral emphasis filter that can be used to manipulate the perceived vocal effort. The other benefit of APLP is that it estimates a formant filter that is more consistent across varying voice qualities. This paper describes how constant pre-emphasis linear prediction (LP) estimates a voice source with a constant spectral envelope even though the spectral envelope of the true voice source varies over time. A listening experiment demonstrates how differences in vocal effort and breathiness are audible in the formant filter estimated by constant pre-emphasis LP. APLP is presented as a technique to estimate a spectral emphasis filter that captures the combined influence of the glottal source and the vocal tract upon the spectral envelope of the voice. A final listening experiment demonstrates how APLP can be used to effectively transform high-effort voices into breathy voices. The techniques presented here are relevant to researchers in voice conversion, voice quality, singing, and emotion.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nordstrom, Karl I.; Driessen, Peter F.] Univ Victoria, Dept Elect &amp;
   Comp Engn, Victoria, BC V8W 3P6, Canada.
   <br>[Tzanetakis, George] Univ Victoria, Dept Comp Sci, Victoria, BC V8W 3P6,
   Canada.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nordstrom, KI (reprint author), Univ Victoria, Dept Elect &amp; Comp Engn, Victoria, BC V8W 3P6, Canada.</td>
</tr>

<tr>
<td valign="top">EM </td><td>knordstr@uvic.ca; gtzan@cs.uvic.ca; peter@ece.uvic.ca</td>
</tr>

<tr>
<td valign="top">TC </td><td>7</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>7</td>
</tr>

<tr>
<td valign="top">PD </td><td>AUG</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">VL </td><td>16</td>
</tr>

<tr>
<td valign="top">IS </td><td>6</td>
</tr>

<tr>
<td valign="top">BP </td><td>1087</td>
</tr>

<tr>
<td valign="top">EP </td><td>1096</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASL.2008.2001105</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000258286800002</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ippolito, G
   <br>Palazzo, FF
   <br>Sebag, F
   <br>Thakur, A
   <br>Cherenko, M
   <br>Henry, JF</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ippolito, Giuseppe
   <br>Palazzo, Fausto F.
   <br>Sebag, Frederic
   <br>Thakur, Abhijit
   <br>Cherenko, Mariya
   <br>Henry, Jean-Francois</td>
</tr>

<tr>
<td valign="top">TI </td><td>Safety of laparoscopic adrenalectomy in patients with large
   pheochromocytomas: A single institution review</td>
</tr>

<tr>
<td valign="top">SO </td><td>WORLD JOURNAL OF SURGERY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">ID </td><td>CONSECUTIVE PROCEDURES; BENIGN; CATECHOLAMINES; RESECTION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Background Laparoscopic adrenalectomy is the procedure of choice for small adrenal tumors, but some concerns have been voiced when this approach is adopted for larger tumors and pheochromocytomas. The aim of this study was to examine the results of the laparoscopic resection of large pheochromocytomas.
   <br>Methods A retrospective review of adrenalectomies performed for adrenal pheochromocytomas &gt; 6 cm in diameter. We compiled and analyzed the early operative complications, histologic findings, and cure rates with a minimum of 1 year of follow-up after surgery.
   <br>Results From 1996 to 2005, a total of 445 laparoscopic adrenalectomies were performed in our institution using the anterolateral transperitoneal approach. From this series we identified 18 procedures for pheochromocytomas with an average diameter on imaging of 78.2 mm (range 60-130 mm). All patients were rendered safe with a standard departmental protocol involving calcium-channel blockade initiated at least 2 weeks prior to surgery. The average peak intraoperative blood pressure was 187 mmHg. Capsular disruption occurred in two cases. One patient required an intraoperative blood transfusion due to intraoperative blood loss. No immediate conversions to an open procedure were required, but one patient underwent a delayed laparotomy for hematoma formation. Histologically, four of the adrenal tumors displayed evidence of vascular invasion. Biochemical cure was achieved in all patients after a median follow-up of 58 months (16-122 months).
   <br>Conclusions Laparoscopic adrenalectomy appears to be a safe and effective approach for large pheochromocytomas when no preoperative or intraoperative evidence of local invasion is present.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ippolito, Giuseppe; Palazzo, Fausto F.; Thakur, Abhijit; Cherenko,
   Mariya; Henry, Jean-Francois] La Timone Hosp, Dept Endocrine Surg,
   F-13385 Marseille, France.
   <br>[Palazzo, Fausto F.] Hammersmith Hosp, Dept Surg, London W12 0HS,
   England.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ippolito, G (reprint author), La Timone Hosp, Dept Endocrine Surg, 264 Rue St Pierre, F-13385 Marseille, France.</td>
</tr>

<tr>
<td valign="top">EM </td><td>giuseppe_ippolito@yahoo.it</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Cherenko, Mariya</display_name>&nbsp;</font></td><td><font size="3">0000-0003-1731-2994&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>23</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>23</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAY</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">VL </td><td>32</td>
</tr>

<tr>
<td valign="top">IS </td><td>5</td>
</tr>

<tr>
<td valign="top">BP </td><td>840</td>
</tr>

<tr>
<td valign="top">EP </td><td>844</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/s00268-007-9327-5</td>
</tr>

<tr>
<td valign="top">SC </td><td>Surgery</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000255096200029</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Husein, OF
   <br>Husein, TN
   <br>Gardner, R
   <br>Chiang, T
   <br>Larson, DG
   <br>Obert, K
   <br>Thompson, J
   <br>Trudeau, MD
   <br>Dell, DM
   <br>Forrest, LA</td>
</tr>

<tr>
<td valign="top">AF </td><td>Husein, Omar F.
   <br>Husein, Tiffany N.
   <br>Gardner, Ricky
   <br>Chiang, Tendy
   <br>Larson, Douglas G.
   <br>Obert, Keri
   <br>Thompson, Jennifer
   <br>Trudeau, Micheal D.
   <br>Dell, Don M.
   <br>Forrest, L. Arick</td>
</tr>

<tr>
<td valign="top">TI </td><td>Formal Psychological Testing in Patients With Paradoxical Vocal Fold
   Dysfunction</td>
</tr>

<tr>
<td valign="top">SO </td><td>LARYNGOSCOPE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>PVCD; paradoxical vocal cord dysfunction; vocal cord dysfunction; airway
   obstruction; voice; conversion disorder; somatoform disorder;
   depression; anxiety; stress; abuse; asthma; gastroesophageal reflux
   disease; reflux laryngitis</td>
</tr>

<tr>
<td valign="top">ID </td><td>PAROXYSMAL LARYNGOSPASM; MOTION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Objective: The etiology of paradoxical vocal fold dysfunction (PVFD) has been unclear, but it has long been hypothesized that there is a significant psychological component. The purpose of this study was to elucidate the psychological profiles of patients newly diagnosed with PVFD using psychometrically-sound psychological assessment instruments.
   <br>Study Design: Prospective cohort study of 45 adults newly diagnosed with PVFD at a tertiary university referral center.
   <br>Methods: The Minnesota Multiphasic Personality Inventory (MMPI-2) was administered to test for psychopathology. The Life Experiences Survey (LES) was administered to investigate levels of stress. Demographic, medical, and social histories were reviewed. MMPI-2 and LES scores for the PVFD cohort were compared with scores previously established for normative populations.
   <br>Results: The study population included 81% female and 60% who were age 50 or older. Compared to the normative population for the MMPI-2, significant differences were noted for both male and female PVFD patients; on average, scores were highly elevated on the hypochondriasis scale and hysteria scale and less elevated on the depression scale. This pattern was consistent with conversion disorder (P &lt; .01). In MMPI-2 subset analysis, 18 patients had a classic conversion profile while 13 others had elevated scores in the three scales of interest, but not in the classic conversion disorder pattern. Also, 11 patients had normal scores, suggesting no psychopathology. PVFD patients with a psychological history scored significantly higher on the depression and anxiety scales than PVFD patients without a psychological history. Patients with a history of asthma or gastroesophageal reflux disease (GERD) achieved significantly higher scores on the hypochondriasis scale than those without that medical history. On the LES assessment, female PVFD patients had significantly lower levels of positive stress and higher levels of negative stress than the general population; total levels of stress were not significantly different, however. Male PVFD patients had significantly lower levels of positive, negative, and total stress. For the entire cohort, asthma (65%), GERD (51%), and a history of abuse (38%) were common comorbidities.
   <br>Conclusions: On average, in both male and female adults, PVFD is associated with conversion disorder, representing a physical manifestation of underlying psychological difficulty. There also appears to be a subset of PVFD that is not associated with psychopathology. PVFD patients with a previous psychological history are prone to more depressive and anxious symptomatology. Patients with PVFD and a history of asthma or GERD are more likely to excessively complain about physical symptoms. Overall levels of stress are not higher in PVFD patients compared to a general population. However, females report more negative stress, and both males and females may have trouble coping with the amount of stress that they do have. PVFD is more common among women, more prevalent among older individuals, and can be comorbid with asthma, GERD, and previous abuse. These results have implications for treatment-psychotherapy directed for somatoform and conversion disorders may be added to traditional speech therapy for increased efficacy.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Husein, Omar F.; Gardner, Ricky; Chiang, Tendy; Larson, Douglas G.;
   Obert, Keri; Thompson, Jennifer; Trudeau, Micheal D.; Forrest, L. Arick]
   Ohio State Univ, Dept Otolaryngol Head &amp; Neck Surg, Columbus, OH 43210
   USA.
   <br>[Husein, Tiffany N.; Dell, Don M.] Ohio State Univ, Dept Psychol,
   Columbus, OH 43210 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Husein, OF (reprint author), UC Irvine Dept Otolaryngol, 101 City Dr S,Bldg 56,Room 500, Orange, CA 92868 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>tiffhusein@yahoo.com</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Forrest, Lowell</display_name>&nbsp;</font></td><td><font size="3">E-3074-2011&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>44</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>44</td>
</tr>

<tr>
<td valign="top">PD </td><td>APR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">VL </td><td>118</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">BP </td><td>740</td>
</tr>

<tr>
<td valign="top">EP </td><td>747</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1097/MLG.0b013e31815ed13a</td>
</tr>

<tr>
<td valign="top">SC </td><td>Research &amp; Experimental Medicine; Otorhinolaryngology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000260662000029</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Reagan-Shaw, S
   <br>Nihal, M
   <br>Ahmad, N</td>
</tr>

<tr>
<td valign="top">AF </td><td>Reagan-Shaw, Shannon
   <br>Nihal, Minakshi
   <br>Ahmad, Nihal</td>
</tr>

<tr>
<td valign="top">TI </td><td>Dose translation from animal to human studies revisited</td>
</tr>

<tr>
<td valign="top">SO </td><td>FASEB JOURNAL</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>drug dose conversion; body surface area</td>
</tr>

<tr>
<td valign="top">ID </td><td>BODY-SURFACE AREA; CHEMOTHERAPY; RESVERATROL; FORMULA; WEIGHT; ADULTS</td>
</tr>

<tr>
<td valign="top">AB </td><td>As new drugs are developed, it is essential to appropriately translate the drug dosage from one animal species to another. A misunderstanding appears to exist regarding the appropriate method for allometric dose translations, especially when starting new animal or clinical studies. The need for education regarding appropriate translation is evident from the media response regarding some recent studies where authors have shown that resveratrol, a compound found in grapes and red wine, improves the health and life span of mice. Immediately after the online publication of these papers, the scientific community and popular press voiced concerns regarding the relevance of the dose of resveratrol used by the authors. The animal dose should not be extrapolated to a human equivalent dose (HED) by a simple conversion based on body weight, as was reported. For the more appropriate conversion of drug doses from animal studies to human studies, we suggest using the body surface area (BSA) normalization method. BSA correlates well across several mammalian species with several parameters of biology, including oxygen utilization, caloric expenditure, basal metabolism, blood volume, circulating plasma proteins, and renal function. We advocate the use of BSA as a factor when converting a dose for translation from animals to humans, especially for phase I and phase II clinical trials.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Reagan-Shaw, Shannon; Nihal, Minakshi; Ahmad, Nihal] Univ Wisconsin,
   Dept Dermatol, Madison, WI 53706 USA.
   <br>[Nihal, Minakshi; Ahmad, Nihal] Univ Wisconsin, Paul P Carbone
   Comprehens Canc Ctr, Madison, WI USA.
   <br>[Ahmad, Nihal] Univ Wisconsin, Mol Environm Toxicol Ctr, Madison, WI USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ahmad, N (reprint author), Univ Wisconsin, Dept Dermatol, B-25 Med Sci Ctr,1300 Univ Ave, Madison, WI 53706 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>nahmad@wisc.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>2603</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2655</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">VL </td><td>22</td>
</tr>

<tr>
<td valign="top">IS </td><td>3</td>
</tr>

<tr>
<td valign="top">BP </td><td>659</td>
</tr>

<tr>
<td valign="top">EP </td><td>661</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1096/fj.07-9574LSF</td>
</tr>

<tr>
<td valign="top">SC </td><td>Biochemistry &amp; Molecular Biology; Life Sciences &amp; Biomedicine - Other
   Topics; Cell Biology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000254143700004</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Toda, T
   <br>Black, AW
   <br>Tokuda, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Toda, Tomoki
   <br>Black, Alan W.
   <br>Tokuda, Keiichi</td>
</tr>

<tr>
<td valign="top">TI </td><td>Statistical mapping between articulatory movements and acoustic spectrum
   using a Gaussian mixture model</td>
</tr>

<tr>
<td valign="top">SO </td><td>SPEECH COMMUNICATION</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>articulatory-to-acoustic mapping; acoustic-to-articulatory inversion
   mapping; GMM; MMSE; dynamic features</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH PRODUCTION-MODEL; VOCAL-TRACT; HMM; INVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we describe a statistical approach to both an articulatory-to-acoustic mapping and an acoustic-to-articulatory inversion mapping without using phonetic information. The joint probability density of an articulatory parameter and an acoustic parameter is modeled using a Gaussian mixture model (GMM) based on a parallel acoustic-articulatory speech database. We apply the GMM-based mapping using the minimum mean-square error (MMSE) criterion, which has been proposed for voice conversion, to the two mappings. Moreover, to improve the mapping performance, we apply maximum likelihood estimation (MLE) to the GMM-based mapping method. The determination of a target parameter trajectory having appropriate static and dynamic properties is obtained by imposing an explicit relationship between static and dynamic features in the MLE-based mapping. Experimental results demonstrate that the MLE-based mapping with dynamic features can significantly improve the mapping performance compared with the MMSE-based mapping in both the articulatory-to-acoustic mapping and the inversion mapping. (c) 2007 Elsevier B.V. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Toda, Tomoki] Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Ikoma,
   Nara 6030192, Japan.
   <br>[Black, Alan W.] Carnegie Mellon Univ, Language Technol Inst,
   Pittsburgh, PA 15213 USA.
   <br>[Tokuda, Keiichi] Nagoya Inst Technol, Grad Sch Engn, Showa Ku, Nagoya,
   Aichi 4668555, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Toda, T (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, 8916-5 Takayama, Ikoma, Nara 6030192, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>tomoki@is.naist.jp; awb@es.cmu.edu; tokuda@ics.nitech.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>133</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>138</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">VL </td><td>50</td>
</tr>

<tr>
<td valign="top">IS </td><td>3</td>
</tr>

<tr>
<td valign="top">BP </td><td>215</td>
</tr>

<tr>
<td valign="top">EP </td><td>227</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.specom.2007.09.001</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000254239900005</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ahmadi, F
   <br>McLoughlin, IV
   <br>Sharifzadeh, HR</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ahmadi, Farzaneh
   <br>McLoughlin, Ian Vince
   <br>Sharifzadeh, Hamid Reza</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Analysis-by-Synthesis Method for Whisper-Speech Reconstruction</td>
</tr>

<tr>
<td valign="top">SO </td><td>2008 IEEE ASIA PACIFIC CONFERENCE ON CIRCUITS AND SYSTEMS (APCCAS 2008),
   VOLS 1-4</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE Asia Pacific Conference on Circuits and Systems (APCCAS 2008)</td>
</tr>

<tr>
<td valign="top">CY </td><td>NOV 30-DEC 03, 2008</td>
</tr>

<tr>
<td valign="top">CL </td><td>Macao, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">AB </td><td>In the following paper, a method for the real-time conversion of whispers to normal phonated speech through a code excited linear prediction analysis-by-synthesis codec is discussed. This approach uses a template of a speaker's normal phonated speech for extraction of excitation parameters such as pitch and gain, and then injects these estimated excitations into whispered signal to synthesize normal-sounding speech through the CELP codec. Furthermore, since restoring pitch to whispered speech requires some considerations of quality and accuracy, spectral enhancements are required in terms of formant shifting (LSPs modification) and pitch injection based on voiced/unvoiced decision. Spectral shifting is accomplished through line-spectral pair adjustment. Implementing such methods by using the popular CELP codec allows integration of the technique with any modern speech applications and devices. Subjective testing results are presented to determine the effectiveness of the technique.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ahmadi, Farzaneh; McLoughlin, Ian Vince; Sharifzadeh, Hamid Reza]
   Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ahmadi, F (reprint author), Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ahmadi@ntu.edu.sg; mcloughlin@ntu.edu.sg; hami0003@ntu.edu.sg</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>McLoughlin, Ian</display_name>&nbsp;</font></td><td><font size="3">A-3674-2011&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Sharifzadeh, Hamid Reza</display_name>&nbsp;</font></td><td><font size="3">D-8829-2012&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>McLoughlin, Ian</display_name>&nbsp;</font></td><td><font size="3">0000-0001-7111-2008&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Ahmadi, Farzaneh</display_name>&nbsp;</font></td><td><font size="3">0000-0002-0076-0304&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>7</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>7</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">BP </td><td>1280</td>
</tr>

<tr>
<td valign="top">EP </td><td>1283</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/APCCAS.2008.4746261</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000268007100316</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Villavicencio, F
   <br>Robel, A
   <br>Rodet, X</td>
</tr>

<tr>
<td valign="top">AF </td><td>Villavicencio, Fernando
   <br>Roebel, Axel
   <br>Rodet, Xavier</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Extending efficient spectral envelope modeling to mel-frequency based
   representation</td>
</tr>

<tr>
<td valign="top">SO </td><td>2008 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING, VOLS 1-12</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   (ICASSP)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>33rd IEEE International Conference on Acoustics, Speech and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 30-APR 04, 2008</td>
</tr>

<tr>
<td valign="top">CL </td><td>Las Vegas, NV</td>
</tr>

<tr>
<td valign="top">DE </td><td>speech synthesis; speech analysis; envelope detection; speech codecs;
   speech enhancement</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this work we consider the problem of spectral envelope estimation using spectra with perceptually warped frequency axis. The goal of this work is the reduction of the order of the spectral envelope model which will facilitate the use of these envelopes for training of voice conversion systems. We adapt the True-Envelope estimator to Mel-frequency representations and adapt a recently proposed cepstral model order selection criterion taking into account the distortion of the frequency axis. We evaluate the modified order selection procedure using a perceptual framework for the evaluation of envelope estimation errors. The experimental evaluation carried out with real speech confirms our modifications. The results demonstrate that the mel frequency based true envelope estimator achieves superior envelope estimation with significantly reduced model order.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Villavicencio, Fernando; Roebel, Axel; Rodet, Xavier] IRCAM CNRS STMS,
   Anal Synth Team, F-75004 Paris, France.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Villavicencio, F (reprint author), IRCAM CNRS STMS, Anal Synth Team, Pl Igor Stravinsky, F-75004 Paris, France.</td>
</tr>

<tr>
<td valign="top">TC </td><td>5</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>7</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">BP </td><td>1625</td>
</tr>

<tr>
<td valign="top">EP </td><td>1628</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/ICASSP.2008.4517937</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science; Engineering; Mathematical &amp; Computational
   Biology; Imaging Science &amp; Photographic Technology; Radiology, Nuclear
   Medicine &amp; Medical Imaging; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000257456701068</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Zhang, M
   <br>Tao, JH
   <br>Tian, JL
   <br>Wang, X</td>
</tr>

<tr>
<td valign="top">AF </td><td>Zhang, Meng
   <br>Tao, Jianhua
   <br>Tian, Jilei
   <br>Wang, Xia</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Text-independent voice conversion based on state mapped codebook</td>
</tr>

<tr>
<td valign="top">SO </td><td>2008 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING, VOLS 1-12</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>33rd IEEE International Conference on Acoustics, Speech and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 30-APR 04, 2008</td>
</tr>

<tr>
<td valign="top">CL </td><td>Las Vegas, NV</td>
</tr>

<tr>
<td valign="top">DE </td><td>text-independent; voice conversion; hidden Markov model; state mapped
   codebook</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion has become more and more important in speech technology, but most of current works have to use parallel utterances of both source and target speaker as the training corpus, which limits the application of the technology. In the paper, we propose a new method of text-independent voice conversion which uses non-parallel corpus for the training. The Hidden Markov Model (HMM) is used to represent the phonetic structure of training speech and to generate the training pairs of source and target speakers by mapping the HMM states between source and target speeches. Then, HMM state mapped codebooks are generated to create the mapping function for the text-independent voice conversion. The subjective experiments based on ABX tests and MOS tests show that the method proposed in the paper gets the similar conversion performance and better speech quality compared to the conventional voice conversion systems.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Zhang, Meng; Tao, Jianhua] Chinese Acad Sci, Inst Automat, Natl Lab
   Pattern Recognit, Beijing, Peoples R China.
   <br>[Tian, Jilei] Nokia Res Ctr, Interact Core Technol Ctr, Tampere, Finland.
   <br>[Wang, Xia] Nokia Res Ctr, Tampere, Finland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Zhang, M (reprint author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>mzhang@nlpr.ia.ac.cn; jhtao@nlpr.ia.ac.cn; jilei.tian@nokia.com;
   xia.s.wang@nokia.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>12</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>12</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">BP </td><td>4605</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science; Engineering; Mathematical &amp; Computational
   Biology; Imaging Science &amp; Photographic Technology; Radiology, Nuclear
   Medicine &amp; Medical Imaging; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000257456703135</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Shuang, ZW
   <br>Meng, FP
   <br>Qin, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Shuang, Zhiwei
   <br>Meng, Fanping
   <br>Qin, Yong</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice conversion by combining frequency warping with unit selection</td>
</tr>

<tr>
<td valign="top">SO </td><td>2008 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING, VOLS 1-12</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   (ICASSP)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>33rd IEEE International Conference on Acoustics, Speech and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 30-APR 04, 2008</td>
</tr>

<tr>
<td valign="top">CL </td><td>Las Vegas, NV</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; warping; selection</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we propose a novel voice conversion method by combining frequency warping and. unit selection to improve the similarity to target speaker. We use frequency warping to get the warped source spectrum, which will be used as estimated target for later unit selection of the target speaker's spectrum. Such estimated target can preserve the natural transition of human's speech. Then, part of the warped source spectrum is replaced by the selected target speaker's real spectrum before reconstructing the converted speech to reduce the difference in detailed spectrum. TC-STAR 2007 voice conversion evaluation results show that the proposed method can achieve about 20% improvement in similarity score compared to only frequency warping.</td>
</tr>

<tr>
<td valign="top">TC </td><td>8</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>8</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">BP </td><td>4661</td>
</tr>

<tr>
<td valign="top">EP </td><td>4664</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science; Engineering; Mathematical &amp; Computational
   Biology; Imaging Science &amp; Photographic Technology; Radiology, Nuclear
   Medicine &amp; Medical Imaging; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000257456703149</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Erro, D
   <br>Polyakova, T
   <br>Moreno, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Erro, Daniel
   <br>Polyakova, Tatyana
   <br>Moreno, Asuncion</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>On combining statistical methods and frequency warping for high-quality
   voice conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>2008 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING, VOLS 1-12</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   (ICASSP)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>33rd IEEE International Conference on Acoustics, Speech and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 30-APR 04, 2008</td>
</tr>

<tr>
<td valign="top">CL </td><td>Las Vegas, NV</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; speech synthesis; gaussian mixture model; weighted
   frequency warping</td>
</tr>

<tr>
<td valign="top">AB </td><td>In current voice conversion systems, obtaining a high similarity between converted and target voices requires a high degree of signal manipulation, which implies important quality degradation, up to the point that in some cases the quality scores are unacceptable for real-life applications. Indeed, a tradeoff can be observed between the similarity scores and the quality scores achieved by a given voice conversion system. In our previous works we proved that statistical methods and frequency warping transformations could be combined to yield a better similarity-quality balance than conventional systems, due to significant quality improvements. In this paper, two different ways of combining these two approaches are compared through perceptual tests in order to determine the best strategy for high-quality voice conversion. The comparison is made under the same training conditions, using the same speech model and vector dimensions. The results indicate that the Weighted Frequency Warping method is preferred by listeners.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Erro, Daniel; Polyakova, Tatyana; Moreno, Asuncion] Univ Politecn
   Cataluna, TALP Res Ctr, E-08028 Barcelona, Spain.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Erro, D (reprint author), Univ Politecn Cataluna, TALP Res Ctr, E-08028 Barcelona, Spain.</td>
</tr>

<tr xmlns:date="http://exslt.org/dates-and-times">
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Erro, Daniel</display_name>&nbsp;</font></td><td><font size="3">H-7043-2015&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Moreno, Asuncion</display_name>&nbsp;</font></td><td><font size="3">H-2315-2017&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Erro, Daniel</display_name>&nbsp;</font></td><td><font size="3">0000-0003-0954-6942&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Moreno, Asuncion</display_name>&nbsp;</font></td><td><font size="3">0000-0002-1823-5970&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">BP </td><td>4665</td>
</tr>

<tr>
<td valign="top">EP </td><td>4668</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/ICASSP.2008.4518697</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science; Engineering; Mathematical &amp; Computational
   Biology; Imaging Science &amp; Photographic Technology; Radiology, Nuclear
   Medicine &amp; Medical Imaging; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000257456703150</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Helander, E
   <br>Nurminen, J
   <br>Gabbouj, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Helander, Elina
   <br>Nurminen, Jani
   <br>Gabbouj, Moncef</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>LSF mapping for voice conversion with very small training sets</td>
</tr>

<tr>
<td valign="top">SO </td><td>2008 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING, VOLS 1-12</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>33rd IEEE International Conference on Acoustics, Speech and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 30-APR 04, 2008</td>
</tr>

<tr>
<td valign="top">CL </td><td>Las Vegas, NV</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; line spectral frequencies</td>
</tr>

<tr>
<td valign="top">AB </td><td>To make voice conversion usable in practical applications, the number of training sentences should be minimized. With traditional Gaussian mixture model (GMM) based techniques small training sets lead to over-fitting and estimation problems. We propose a new approach for mapping line spectral frequencies (LSFs) representing the vocal tract. The idea is based on inherent intra-frame correlations of LSFs. For each target LSF, a separate GMM is used and only the source and target LSF elements best correlating with the current LSF are used in training. The proposed method is evaluated both objectively and in listening tests, and it is shown that the method outperform the conventional GMM approach especially with very small training sets.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Helander, Elina; Gabbouj, Moncef] Tampere Univ Technol, Inst Signal
   Proc, FIN-33101 Tampere, Finland.
   <br>[Nurminen, Jani] Nokia Technol Platforms, Tampere, Finland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Helander, E (reprint author), Tampere Univ Technol, Inst Signal Proc, FIN-33101 Tampere, Finland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>elina.helander@tut.fi; jani.k.nurminen@nokia.com; moncef.gabbouj@tut.fi</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Gabbouj, Moncef</display_name>&nbsp;</font></td><td><font size="3">G-4293-2014&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Gabbouj, Moncef</display_name>&nbsp;</font></td><td><font size="3">0000-0002-9788-2323&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Helander, Elina</display_name>&nbsp;</font></td><td><font size="3">0000-0002-0076-0590&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>5</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>5</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">BP </td><td>4669</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/ICASSP.2008.4518698</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science; Engineering; Mathematical &amp; Computational
   Biology; Imaging Science &amp; Photographic Technology; Radiology, Nuclear
   Medicine &amp; Medical Imaging; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000257456703151</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Percybrooks, WS
   <br>Moore, E</td>
</tr>

<tr>
<td valign="top">AF </td><td>Percybrooks, Winston S.
   <br>Moore, Elliot, II</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice conversion with linear prediction residual estimaton</td>
</tr>

<tr>
<td valign="top">SO </td><td>2008 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING, VOLS 1-12</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>33rd IEEE International Conference on Acoustics, Speech and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 30-APR 04, 2008</td>
</tr>

<tr>
<td valign="top">CL </td><td>Las Vegas, NV</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; LP residual estimation; GMM; linear spectral
   frequencies</td>
</tr>

<tr>
<td valign="top">ID </td><td>TRANSFORMATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>The work presented here shows a comparison between a voice conversion system based on converting only the vocal tract representation of the source speaker and an augmented system that adds an algorithm for estimating the target excitation signal. The estimation algorithm uses a stochastic model for relating the excitation signal to the vocal tract features. The two systems were subjected to objective and subjective tests for assessing the effectiveness of the perceived identity conversion and the overall quality of the synthesized speech. Male-to-male and female-to-female conversion cases were tested. The main objective of this work is to improve the recognizability of the converted speech while maintaining a high synthesis quality.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Percybrooks, Winston S.; Moore, Elliot, II] Georgia Inst Technol, Dept
   Elect &amp; Comp Engn, Savannah, GA 31407 USA.
   <br>[Percybrooks, Winston S.] Univ Norte, Dept Elect &amp; Elect Engn,
   Barranquilla, Colombia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Percybrooks, WS (reprint author), Georgia Inst Technol, Dept Elect &amp; Comp Engn, Savannah, GA 31407 USA.</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Percybrooks, Winston</display_name>&nbsp;</font></td><td><font size="3">X-2913-2018&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Percybrooks, Winston</display_name>&nbsp;</font></td><td><font size="3">0000-0002-0169-7562&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">BP </td><td>4673</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/ICASSP.2008.4518699</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science; Engineering; Mathematical &amp; Computational
   Biology; Imaging Science &amp; Photographic Technology; Radiology, Nuclear
   Medicine &amp; Medical Imaging; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000257456703152</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Adachi, Y
   <br>Kawamoto, S
   <br>Morishima, S
   <br>Nakamura, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Adachi, Yoshihiro
   <br>Kawamoto, Shinichi
   <br>Morishima, Shigeo
   <br>Nakamura, Satoshi</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Perceptual similarity measurement of speech by combination of acoustic
   features</td>
</tr>

<tr>
<td valign="top">SO </td><td>2008 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING, VOLS 1-12</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>33rd IEEE International Conference on Acoustics, Speech and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 30-APR 04, 2008</td>
</tr>

<tr>
<td valign="top">CL </td><td>Las Vegas, NV</td>
</tr>

<tr>
<td valign="top">DE </td><td>acoustic correlators; speaker recognition; speech analysis</td>
</tr>

<tr>
<td valign="top">ID </td><td>RECOGNITION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Future cast system is a new entertainment system where participant's face is captured and rendered into the movie as an instant Computer Graphics (CG) movie star, which had been first exhibited at the 2005 World Exposition in Aichi Japan. We are working to add new functionality which enables mapping not only faces but also speech individualities to the cast. Our approach is to find a speaker with the closest speech individuality and apply voice conversion. This paper inves tigates acoustic features to estimate perceptual similarity of speech individuality. We propose a method linearly combined eight acoustic features related to the perception of speech individualities. The proposed method optimizes weights for the acoustic features considering perceptual similarities. We have evaluated performance of our method with Spearman's rank correlation coefficients to perceptual similarities. As the results, the experiments evidenced that the proposed method achieves a correlation coefficient of 0.66.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Adachi, Yoshihiro; Kawamoto, Shinichi; Nakamura, Satoshi] ATR, Spoken
   Language Commun Res Labs, 2-2-2 Keihanna, Kyoto 6190288, Japan.
   <br>[Adachi, Yoshihiro; Morishima, Shigeo] Waseda Univ, Sci &amp; Engn, Shinjuku
   Ku, Tokyo 1698555, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Adachi, Y (reprint author), ATR, Spoken Language Commun Res Labs, 2-2-2 Keihanna, Kyoto 6190288, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>xyadachi@toki.waseda.jp; shinichi.kawamoto@atr.jp; shigeo@waseda.jp;
   satoshi.nakamura@atr.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>6</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>6</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">BP </td><td>4861</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science; Engineering; Mathematical &amp; Computational
   Biology; Imaging Science &amp; Photographic Technology; Radiology, Nuclear
   Medicine &amp; Medical Imaging; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000257456703199</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Heidari, AR</td>
</tr>

<tr>
<td valign="top">AF </td><td>Heidari, A. Ryan</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Trends &amp; technical challenges in conversational voice services</td>
</tr>

<tr>
<td valign="top">SO </td><td>2008 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING, VOLS 1-12</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   (ICASSP)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>33rd IEEE International Conference on Acoustics, Speech and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 30-APR 04, 2008</td>
</tr>

<tr>
<td valign="top">CL </td><td>Las Vegas, NV</td>
</tr>

<tr>
<td valign="top">DE </td><td>speech coding; telephony; wireless voice services; multimedia
   communication; land mobile radio data communication; internetworking;
   voice over IP</td>
</tr>

<tr>
<td valign="top">AB </td><td>Technologies continuously evolve to meet the challenging needs of wireless communication. The ever growing demand for wireless communication requires greater capacity and higher network efficiencies. Spectral efficiency is a key driver of the economics of voice and data services as we approach the rollout of 3G network. In this paper we discuss the technology trends and challenges associated with conversational voice services in the context of fixed mobile conversion. We discuss evolution in cellular modem technology, switching technology, and speech coding technology. The combination of voice over internet protocol and wireless communication promises to revolutionize the whole telecommunication market.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Qualcomm Inc, San Diego, CA 92121 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Heidari, AR (reprint author), Qualcomm Inc, 5775 Morehouse Dr, San Diego, CA 92121 USA.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">BP </td><td>5348</td>
</tr>

<tr>
<td valign="top">EP </td><td>5351</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/ICASSP.2008.4518868</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science; Engineering; Mathematical &amp; Computational
   Biology; Imaging Science &amp; Photographic Technology; Radiology, Nuclear
   Medicine &amp; Medical Imaging; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000257456703321</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Makki, B
   <br>Hosseini, MN
   <br>Seyyedsalehi, SA</td>
</tr>

<tr>
<td valign="top">AF </td><td>Makki, B.
   <br>Hosseini, M. Noori
   <br>Seyyedsalehi, S. A.</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Unsupervised Extraction of Meaningful Nonlinear Principal Components
   Applied for Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>2008 IEEE INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS, VOLS 1-8</td>
</tr>

<tr>
<td valign="top">SE </td><td>IEEE International Joint Conference on Neural Networks (IJCNN)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Joint Conference on Neural Networks</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUN 01-08, 2008</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hong Kong, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">ID </td><td>NEURAL-NETWORKS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Nonlinear Principal Component Analysis (NLPCA) is one of the most progressive computational tools developed during the last two decades. However, in spite of its proper performance in feature extraction and dimension reduction, it is considered as a blind processor which can not extract physical or meaningful factors from dataset. This paper presents a new distributed model of autoassociative neural network which increases meaningfulness degree of the extracted parameters. The model is implemented to perform Voice Conversion (VC) and, as it will be seen through comparisons, results in proper conversion quality.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Makki, B.; Hosseini, M. Noori; Seyyedsalehi, S. A.] Amir Kabir Univ
   Technol, Dept Biomed Engn, Tehran, Iran.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Makki, B (reprint author), Amir Kabir Univ Technol, Dept Biomed Engn, POB 15875-4413, Tehran, Iran.</td>
</tr>

<tr>
<td valign="top">EM </td><td>behrooz.makki@gmail.com; monanoori@gmail.com; ssalehi@aut.ac.ir</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">BP </td><td>1370</td>
</tr>

<tr>
<td valign="top">EP </td><td>1373</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/IJCNN.2008.4633976</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000263827200219</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Laskar, RH
   <br>Talukdar, FA
   <br>Bhattacharjee, R
   <br>Das, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Laskar, R. H.
   <br>Talukdar, F. A.
   <br>Bhattacharjee, Rajib
   <br>Das, Saugat</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Conversion System using SVM for Vocal Tract Modification and
   Codebook based Model for Pitch Contour Modification</td>
</tr>

<tr>
<td valign="top">SO </td><td>2008 IEEE REGION 10 CONFERENCE: TENCON 2008, VOLS 1-4</td>
</tr>

<tr>
<td valign="top">SE </td><td>TENCON IEEE Region 10 Conference Proceedings</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE Region 10 Conference (TENCON 2008)</td>
</tr>

<tr>
<td valign="top">CY </td><td>NOV 19-21, 2008</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hyderabad, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Support Vector Machine; Vector Quantization; Radial Basis Function
   Network; Regression Analysis; Intonation pattern; Pitch Contour;
   Codebook</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH; TRANSFORMATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>The basic idea of this paper is to design an alternative voice conversion technique using support vector machine (SVM) as a regression tool that, converts the voice of a source speaker to specific standard target speaker. A nonlinear mapping function between the parameters for the acoustic features of the two speakers has been captured in our work. The vocal tract characteristics have been represented by the line spectral frequencies (LSFs). The kernel induced feature space using radial basis function network type SVM with Gaussian basis function have been used in our work. The codebook based technique has been used to modify the intonation characteristic (pitch contour). Mapping of the pitch contour has been achieved at the word level by associating the codebooks derived from the pitch contours of the source and the target speakers. The speech signals for the desired target speaker have been synthesized using the transformed LSFs along with the modified pitch contour and evaluated using both the subjective and the listening tests. The results signify that the proposed model improves the voice conversion performance in terms of capturing the speaker's identity. However, the performance can further be improved by suitably modifying various user defined parameters used in regression analysis and using more training LSF vectors in the training stage.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Laskar, R. H.; Talukdar, F. A.; Bhattacharjee, Rajib; Das, Saugat] Natl
   Inst Technol, Dept Elect &amp; Telecommun Engn, Silchar, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Laskar, RH (reprint author), Natl Inst Technol, Dept Elect &amp; Telecommun Engn, Silchar, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>laskar_r@nits.ac.in; fazal@nits.ac.in; rajib13_nits@yahoo.co.in;
   saugat_nits@yahoo.co.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">BP </td><td>2205</td>
</tr>

<tr>
<td valign="top">EP </td><td>2210</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000266545101162</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Raghavendra, EV
   <br>Desai, S
   <br>Yegnanarayana, B
   <br>Black, AW
   <br>Prahallad, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Raghavendra, E. Veera
   <br>Desai, Srinivas
   <br>Yegnanarayana, B.
   <br>Black, Alan W.
   <br>Prahallad, Kishore</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>GLOBAL SYLLABLE SET FOR BUILDING SPEECH SYNTHESIS IN INDIAN LANGUAGES</td>
</tr>

<tr>
<td valign="top">SO </td><td>2008 IEEE WORKSHOP ON SPOKEN LANGUAGE TECHNOLOGY: SLT 2008, PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE Workshop on Spoken Language Technology</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 15-19, 2008</td>
</tr>

<tr>
<td valign="top">CL </td><td>Goa, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speech synthesis; polyglot synthesis; global syllable set</td>
</tr>

<tr>
<td valign="top">AB </td><td>Indian languages are syllabic in nature where many syllables are found common across its languages. This motivates us to build a global syllable set by combining multiple language syllables to build a synthesizer which can borrow units from a different language when the required syllable is not found. Such synthesizer make use of speech database in different languages spoken by different speakers, whose output is likely to pick units from multiple languages and hence the synthesized utterance contains units spoken by multiple speakers which would annoy the user. We intend to use a cross lingual Voice Conversion framework using Artificial Neural Networks (ANN) to transform such an utterance to a single target speaker.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Raghavendra, E. Veera; Desai, Srinivas; Yegnanarayana, B.; Prahallad,
   Kishore] Int Inst Informat Technol, Hyderabad, Andhra Pradesh 500032,
   India.
   <br>[Black, Alan W.; Prahallad, Kishore] Carnegie Mellon Univ, Language
   Technol Inst, Pittsburgh, PA 15213 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Raghavendra, EV (reprint author), Int Inst Informat Technol, Hyderabad, Andhra Pradesh 500032, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>raghavendra@iiit.ac.in; srinivasdesai@research.iiit.ac.in;
   yegna@iiit.ac.in; awb@cs.cmu.edu; skishore@cs.cmu.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">BP </td><td>49</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000266764600013</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Farhid, M
   <br>Tinati, MA</td>
</tr>

<tr>
<td valign="top">AF </td><td>Farhid, M.
   <br>Tinati, M. A.</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Robust Voice conversion systems using MFDWC</td>
</tr>

<tr>
<td valign="top">SO </td><td>2008 INTERNATIONAL SYMPOSIUM ON TELECOMMUNICATIONS, VOLS 1 AND 2</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Symposium on Telecommunications</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 27-28, 2008</td>
</tr>

<tr>
<td valign="top">CL </td><td>Tehran, IRAN</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion is a method used to transform one speaker's voice into another speaker's voice. New modification approach for voice conversion is proposed in this paper. We take Mel-frequency Discrete Wavelet coefficients (MFDWC) as the basic feature. This feature copes well with small training sets of high dimension, which is a problem often encountered in voice conversion. The proposed voice conversion system assume parallel training data from source and target speakers and use: the theory of wavelets in order to extract speaker feature information. The satisfactory performance of the voice conversion system can be confirmed through ABX listening test and MOS grade.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Farhid, M.; Tinati, M. A.] Univ Tabriz, Tabriz, Iran.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Farhid, M (reprint author), Univ Tabriz, Tabriz, Iran.</td>
</tr>

<tr>
<td valign="top">EM </td><td>morfid@gmail.com; tinati@tabrizu.ac.ir</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">BP </td><td>778</td>
</tr>

<tr>
<td valign="top">EP </td><td>781</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/ISTEL.2008.4651405</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000262259200143</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nguyen, BP
   <br>Akagi, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nguyen, Binh Phu
   <br>Akagi, Masato</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Phoneme-based spectral voice conversion using temporal decomposition and
   Gaussian mixture model</td>
</tr>

<tr>
<td valign="top">SO </td><td>2008 SECOND INTERNATIONAL CONFERENCE ON COMMUNICATIONS AND ELECTRONICS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>2nd International Conference on Communications and Electronics</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUN 04-06, 2008</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hoi An, VIETNAM</td>
</tr>

<tr>
<td valign="top">DE </td><td>spectral voice conversion; temporal decomposition; Gaussian mixture
   model (GMM)</td>
</tr>

<tr>
<td valign="top">AB </td><td>In state-of-the-art voice conversion systems, GMM-based voice conversion methods are regarded as some of the best systems. However, the quality of converted speech is still far from natural. There are three main reasons for the degradation of the quality of converted speech: (i) modeling the distribution of acoustic features in voice conversion often uses unstable frames, which degrades the precision of GMM parameters (ii) the transformation function may generate discontinuous features if frames are processed independently (iii) over-smooth effect occurs in each converted frame. This paper presents a new spectral voice conversion method to deal with the two first drawbacks of standard spectral modification methods, insufficient precision of GMM parameters and insufficient smoothness of the converted spectra between frames. A speech analysis technique called temporal decomposition (TD), which decomposes speech into event targets and event functions, is used to effectively model the spectral evolution. For improvement of estimation of GMM parameters, we use phoneme-based features of event targets as spectral vectors in training procedure to take into account relations between spectral parameters in each phoneme, and to avoid using spectral parameters in transition parts. For enhancement of the continuity of speech spectra, we only need to convert event targets, instead of converting source features to target features frame by frame, and the smoothness of converted speech is ensured by the shape of the event functions. Experimental results show that our proposed spectral voice conversion method improves both the speech quality and the speaker individuality of converted speech.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nguyen, Binh Phu; Akagi, Masato] Japan Adv Inst Sci &amp; Technol, Sch
   Informat Sci, Nomi, Ishikawa 9231292, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nguyen, BP (reprint author), Japan Adv Inst Sci &amp; Technol, Sch Informat Sci, 1-1 Asahidai, Nomi, Ishikawa 9231292, Japan.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">BP </td><td>222</td>
</tr>

<tr>
<td valign="top">EP </td><td>227</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000259024400040</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Farhid, M
   <br>Tinati, MA</td>
</tr>

<tr>
<td valign="top">AF </td><td>Farhid, M.
   <br>Tinati, M. A.</td>
</tr>

<tr>
<td valign="top">BE </td><td>Sarbazi-Azad, H
   <br>Parhami, B
   <br>Miremadi, SG
   <br>Hessabi, S</td>
</tr>

<tr>
<td valign="top">TI </td><td>Improving Quality of Voice Conversion Systems</td>
</tr>

<tr>
<td valign="top">SO </td><td>ADVANCES IN COMPUTER SCIENCE AND ENGINEERING</td>
</tr>

<tr>
<td valign="top">SE </td><td>Communications in Computer and Information Science</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>13th International-Computer-Society-of-Iran-Computer Conference</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 09-11, 2008</td>
</tr>

<tr>
<td valign="top">CL </td><td>Kish Isl, IRAN</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; HFCC; Vector quantization</td>
</tr>

<tr>
<td valign="top">AB </td><td>New improvement scheme for voice conversion are proposed in this paper. We take Human factor cepstral coefficients (HFCC), a modification of MFCC that uses the known relationship between center frequency and critical bandwidth from human psychoacoustics to decouple filter bandwidth from filter spacing, as the basic feature. We propose UN (Unvoiced/Voiced) decision rule such that two sets of codebooks are used to capture the difference between unvoiced and voiced segments of' the source speaker. Moreover, we apply three schemes to refine the synthesized voice, including pitch refinernent, energy equalization, and frarne concatenation. The acceptable performance of the voice conversion system can be verified through ABX listening test and MOS gad.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Farhid, M.; Tinati, M. A.] Tabriz Univ, Tabriz, Iran.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Farhid, M (reprint author), Tabriz Univ, Tabriz, Iran.</td>
</tr>

<tr>
<td valign="top">EM </td><td>morfid@gmail.com; tinati@tabrizu.ac.ir</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">VL </td><td>6</td>
</tr>

<tr>
<td valign="top">BP </td><td>880</td>
</tr>

<tr>
<td valign="top">EP </td><td>883</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000264100100124</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>D'Haeze, L
   <br>Sevenhans, J
   <br>Casier, H
   <br>Macq, D
   <br>van Roeyen, S
   <br>Servaes, S
   <br>De Pril, G
   <br>Geirnaert, K
   <br>Hakim, H</td>
</tr>

<tr>
<td valign="top">AF </td><td>D'Haeze, Luc
   <br>Sevenhans, Jan
   <br>Casier, Herman
   <br>Macq, Damien
   <br>van Roeyen, Stefan
   <br>Servaes, Stef
   <br>De Pril, Geert
   <br>Geirnaert, Koen
   <br>Hakim, Hedi</td>
</tr>

<tr>
<td valign="top">BE </td><td>Casier, H
   <br>Steyaert, M
   <br>VanRoermund, AHM</td>
</tr>

<tr>
<td valign="top">TI </td><td>VoIP SLIC open platform: The wideband subscriber line interface circuit
   for voice over IP (VoIP) applications</td>
</tr>

<tr>
<td valign="top">SO </td><td>ANALOG CIRCUIT DESIGN</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>16th Workshop on Advances in Analog Circuit Design</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 27-29, 2007</td>
</tr>

<tr>
<td valign="top">CL </td><td>Oostende, BELGIUM</td>
</tr>

<tr>
<td valign="top">ID </td><td>TECHNOLOGY</td>
</tr>

<tr>
<td valign="top">AB </td><td>The large scale deployment of the voice over IP, or voice over packet technology and the migration of this technology towards the edge of the network stimulates a revival of SLIC and CODEC design. The basics of this existing technology are refreshed and the architecture of a specific implementation is described in this paper.
   <br>New smart power technologies with single chip low voltage CMOS and high voltage DMOS and bipolar devices allow for full single chip SLIC &amp; CODEC integration. Another option is to put the low voltage CODEC in a deep sub micron system on chip with a mini-SLIC, integrating only the high voltage drivers.
   <br>In the field new voice circuits get deployed now to bring voice to the phone over the wideband network and do the conversion from wideband IP coded voice to the analogue real world speech signals as close to the phone as possible via an Analogue Telephone Adaptor [ATA]. The conversion can be done in the phone as a real IP-phone, in a PC (Skype) via the audio of the PC or via an ATA, driving a regular phone. The conversion can also be done in the house via a VoIP to POTS converter next to the wideband modem, in your garage e.g. to drive your existing home phone cabling. The conversion can also be done with an ATA in the street cabinets driving only the last kilometer to your home phone.
   <br>In this paper a Short Haul Line Integrated Circuit [SHLIC] is described, used to transmit Voice over Internet Protocol, also called VoIP, IP Telephony, Internet telephony, wideband telephony, wideband phone, and the routing of voice conversations over the Internet or through any other IP-based network. The SHLIC is a pure analogue circuit processed in a high voltage technology. Also the CODEC, performing the wideband 16Ksamples/s or A/mu law narrow band signal processing, is briefly described.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[D'Haeze, Luc; Sevenhans, Jan; Casier, Herman; Macq, Damien; van Roeyen,
   Stefan; Servaes, Stef; De Pril, Geert; Geirnaert, Koen; Hakim, Hedi] AMI
   Semicond, Louvain, Belgium.</td>
</tr>

<tr>
<td valign="top">RP </td><td>D'Haeze, L (reprint author), AMI Semicond, Louvain, Belgium.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">BP </td><td>205</td>
</tr>

<tr>
<td valign="top">EP </td><td>233</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/978-1-4020-8263-4_11</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Remote Sensing; Transportation</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000255182200011</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Terris, DJ
   <br>Angelos, P
   <br>Steward, DL
   <br>Simental, AA</td>
</tr>

<tr>
<td valign="top">AF </td><td>Terris, David J.
   <br>Angelos, Peter
   <br>Steward, David L.
   <br>Simental, Alfred A.</td>
</tr>

<tr>
<td valign="top">TI </td><td>Minimally invasive video-assisted thyroidectomy - A multi-institutional
   North American experience</td>
</tr>

<tr>
<td valign="top">SO </td><td>ARCHIVES OF OTOLARYNGOLOGY-HEAD &amp; NECK SURGERY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">ID </td><td>ENDOSCOPIC THYROIDECTOMY; CONVENTIONAL THYROIDECTOMY; SURGERY; LOBECTOMY</td>
</tr>

<tr>
<td valign="top">AB </td><td>Objective: To report the results of a multi-institutional experience with the minimally invasive videoassisted thyroidectomy, which was conceived in Europe and Asia and has only recently been embraced in the United States.
   <br>Design: Prospective, nonrandomized analysis.
   <br>Setting: Four academic thyroid surgical practices.
   <br>Patients: Consecutive series of 228 patients who required thyroid surgery and were deemed at surgeon discretion to be eligible for a minimal access surgery.
   <br>Interventions: Minimally invasive video-assisted thyroidectomy was performed in 216 patients.
   <br>Main Outcome Measures: The data, which were recorded prospectively, included age, sex, indication for surgery, incision. length, and complications of surgery.
   <br>Results: Because conversion to an open approach was required in 12 of the 228 patients, the study group comprised 216 patients (25 men and 191 women; mean [SD] age, 44.5 [14.1] years). There were no hematomas and no cases of permanent hypoparathyroidism or permanent vocal cord paralysis. Nine patients had a transient vocal cord paresis (3.2% of nerves at risk); 5 patients experienced temporary hypocalcemia (8.1% of total thyroidectomies);1 patient reported a change in voice pitch; and 1 patient required a scar revision.
   <br>Conclusions: Use of the minimally invasive video-assisted thyroidectomy technique has been adopted cautiously in the United States. The safety of the procedure represented by the data from this multi-institutional experience would support its expanded adoption by high-volume thyroid surgeons.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Terris, David J.] Med Coll Georgia, Dept Otolaryngol Head &amp; Neck Surg,
   Augusta, GA 30912 USA.
   <br>[Angelos, Peter] Northwestern Univ, Dept Surg, Div Endocrine Surg,
   Chicago, IL 60611 USA.
   <br>[Steward, David L.] Univ Cincinnati, Dept Otolaryngol Head &amp; Neck Surg,
   Cincinnati, OH USA.
   <br>[Simental, Alfred A.] Loma Linda Univ, Dept Surg, Div Otolaryngol Head &amp;
   Neck Surg, Loma Linda, CA 92350 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Terris, DJ (reprint author), Med Coll Georgia, Dept Otolaryngol Head &amp; Neck Surg, 1120 15th St, Augusta, GA 30912 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>dterris@mcg.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>58</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>59</td>
</tr>

<tr>
<td valign="top">PD </td><td>JAN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">VL </td><td>134</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>81</td>
</tr>

<tr>
<td valign="top">EP </td><td>84</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1001/archoto.2007.22</td>
</tr>

<tr>
<td valign="top">SC </td><td>Otorhinolaryngology; Surgery</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000252313300015</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Yue, ZJ
   <br>Zou, X
   <br>Jia, YX
   <br>Wang, H</td>
</tr>

<tr>
<td valign="top">AF </td><td>Yue Zhenjun
   <br>Zou Xiang
   <br>Jia Yongxing
   <br>Wang Hao</td>
</tr>

<tr>
<td valign="top">BE </td><td>Li, D
   <br>Deng, G</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice conversion using HMM combined with GMM</td>
</tr>

<tr>
<td valign="top">SO </td><td>CISP 2008: FIRST INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING,
   VOL 5, PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>1st International Congress on Image and Signal Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 27-30, 2008</td>
</tr>

<tr>
<td valign="top">CL </td><td>Sanya, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">AB </td><td>According to HMM's strong representation capability of speech signal and GMM's better transformation effect, a method for LSF conversion using HMM combined with GMM is proposed The theoretical derivation and flow diagram of this algorithm are offered, and gauss model is introduced to achieve the prosodic features transformation. The experiment is applied on two segment speech, and the result reveals that the converted speech has good naturalness. The ABX test indicates that converted speech is 90.2% similar to the target speaker's.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Yue Zhenjun; Zou Xiang; Jia Yongxing; Wang Hao] PLA Univ Sci &amp; Tech,
   Inst Sci, Nanjing, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Yue, ZJ (reprint author), PLA Univ Sci &amp; Tech, Inst Sci, Nanjing, Peoples R China.</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">BP </td><td>366</td>
</tr>

<tr>
<td valign="top">EP </td><td>370</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Imaging Science &amp; Photographic Technology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000258873900077</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Cheng, YM
   <br>Ma, CX
   <br>Melnar, L</td>
</tr>

<tr>
<td valign="top">AF </td><td>Cheng, Yan Ming
   <br>Ma, Changxue
   <br>Melnar, Lynette</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice-to-Phoneme Conversion Algorithms for Voice-Tag Applications in
   Embedded Platforms</td>
</tr>

<tr>
<td valign="top">SO </td><td>EURASIP JOURNAL ON AUDIO SPEECH AND MUSIC PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">AB </td><td>We describe two voice-to-phoneme conversion algorithms for speaker-independent voice-tag creation specifically targeted at applications on embedded platforms. These algorithms (batch mode and sequential) are compared in speech recognition experiments where they are first applied in a same-language context in which both acoustic model training and voice-tag creation and application are performed on the same language. Then, their performance is tested in a cross-language setting where the acoustic models are trained on a particular source language while the voice-tags are created and applied on a different target language. In the same-language environment, both algorithms either perform comparably to or significantly better than the baseline where utterances are manually transcribed by a phonetician. In the cross-language context, the voice-tag performances vary depending on the source-target language pair, with the variation reflecting predicted phonological similarity between the source and target languages. Among the most similar languages, performance nears that of the native-trained models and surpasses the native reference baseline. Copyright (c) 2008 Yan Ming Cheng et al.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Cheng, Yan Ming; Ma, Changxue; Melnar, Lynette] Motorola Labs, Human
   Interact Res, Schaumburg, IL 60196 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Melnar, L (reprint author), Motorola Labs, Human Interact Res, 1925 Algonquin Rd, Schaumburg, IL 60196 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>melnar@labs.mot.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">AR </td><td>568737</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1155/2008/568737</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000207770100001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Chou, HJ
   <br>Kao, YW
   <br>Hsieh, SL
   <br>Yuan, SM</td>
</tr>

<tr>
<td valign="top">AF </td><td>Chou, Hung-Jen
   <br>Kao, Yung-Wei
   <br>Hsieh, Sheau-Ling
   <br>Yuan, Shyan-Ming</td>
</tr>

<tr>
<td valign="top">BE </td><td>Lee, G
   <br>Ahn, TN
   <br>Howard, D
   <br>Slezak, D</td>
</tr>

<tr>
<td valign="top">TI </td><td>Visualized Voice Service System by VSXML</td>
</tr>

<tr>
<td valign="top">SO </td><td>ICHIT 2008: INTERNATIONAL CONFERENCE ON CONVERGENCE AND HYBRID
   INFORMATION TECHNOLOGY, PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Convergence and Hybrid Information
   Technology</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 28-29, 2008</td>
</tr>

<tr>
<td valign="top">CL </td><td>Daejeon, SOUTH KOREA</td>
</tr>

<tr>
<td valign="top">AB </td><td>The paper presents Voice Service XML, (VSXML) technologies to transform the traditional voice services into the visualized ones for mobile phone users. The technologies contain VSXML formatted files, VSXML Designer, and VSXML Renderer. The VSXML E file encapsulates the interactive Question Units plus the communication protocols during the voice services conversation. The VSXML Designer is a toolkit can provide visualizable and handy applications for voice service designers. The VSXML Renderer generates the corresponding controls of the voice service conversion according to the VSXML file. The techniques have been designed and implemented to Motorola and Nokia mobile phones. We did the preliminary investigation regarding the VSXML Designer toolkit usage as well. The paper concludes the technologies are flexible and easy to adapt. In general, the voice services can be visualizable completely if there is no database accessing. We also addressed the future work of the technologies.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Chou, Hung-Jen; Kao, Yung-Wei; Hsieh, Sheau-Ling; Yuan, Shyan-Ming]
   Natl Chiao Tung Univ, Dept Comp Sci &amp; Engn, Hsinchu 300, Taiwan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Chou, HJ (reprint author), Natl Chiao Tung Univ, Dept Comp Sci &amp; Engn, 1001 Ta Hsueh Rd, Hsinchu 300, Taiwan.</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Yuan, Shyan-Ming</display_name>&nbsp;</font></td><td><font size="3">O-1809-2013&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>chen, jenny</display_name>&nbsp;</font></td><td><font size="3">G-6723-2015&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Yuan, Shyan-Ming</display_name>&nbsp;</font></td><td><font size="3">0000-0002-3621-9528&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>chen, jenny</display_name>&nbsp;</font></td><td><font size="3">0000-0003-3501-9046&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Liu, HH</display_name>&nbsp;</font></td><td><font size="3">0000-0002-5661-2247&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">BP </td><td>633</td>
</tr>

<tr>
<td valign="top">EP </td><td>638</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering; Robotics; Remote Sensing</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000260412900107</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Zhang, B
   <br>Yu, YB</td>
</tr>

<tr>
<td valign="top">AF </td><td>Zhang Bing
   <br>Yu Yibiao</td>
</tr>

<tr>
<td valign="top">BE </td><td>Yuan, BZ
   <br>Ruan, QQ
   <br>Tang, XF</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Conversion Based on Improved GMM and Spectrum with Synchronous
   Prosody</td>
</tr>

<tr>
<td valign="top">SO </td><td>ICSP: 2008 9TH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING, VOLS 1-5,
   PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th International Conference on Signal Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 26-29, 2008</td>
</tr>

<tr>
<td valign="top">CL </td><td>Beijing, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; Improved GMM; Spectrum with prosody; LSF</td>
</tr>

<tr>
<td valign="top">AB </td><td>A new voice conversion approach is proposed based on improved GMM speaker model and short-time spectrum with synchronous prosody. Improved GMM speaker model which is trained by feature vector of original and target speaker can overcome over-smooth phenomenon. The short-lime spectrum with prosody, is composed of LSF parameter and pitch parameter It can describe speaker's vocal tract characteristics and exciting characteristics more accurately, comparing with normal methods which the pitch usually set as constant. Experimental results show this method can describe personality and transformation relationship of the source speaker and target speaker effectively. In addition, transformed speech has good quality, while speaker's individuality transformed well.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Zhang Bing; Yu Yibiao] Soochow Univ, Sch Elect &amp; Informat Engn, Suzhou
   215021, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Zhang, B (reprint author), Soochow Univ, Sch Elect &amp; Informat Engn, Suzhou 215021, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>zhangbing1212@gmail.com; yuyb@suda.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">BP </td><td>659</td>
</tr>

<tr>
<td valign="top">EP </td><td>662</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/ICOSP.2008.4697217</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000270665400160</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Xu, N
   <br>Yang, Z</td>
</tr>

<tr>
<td valign="top">AF </td><td>Xu, Ning
   <br>Yang, Zhen</td>
</tr>

<tr>
<td valign="top">BE </td><td>Yuan, BZ
   <br>Ruan, QQ
   <br>Tang, XF</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Precise Estimation of Vocal Tract Parameters for High Quality Voice
   Morphing</td>
</tr>

<tr>
<td valign="top">SO </td><td>ICSP: 2008 9TH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING, VOLS 1-5,
   PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th International Conference on Signal Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 26-29, 2008</td>
</tr>

<tr>
<td valign="top">CL </td><td>Beijing, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">AB </td><td>One of the most recent models for voice conversion is the classical LPC analysis-synthesis model combined with GMM, which aims to separate information from excitation and vocal tract and to learn the transformation rules with statistical methods. However it does not work well as it is supposed to be due to the inaccuracy of the extracted feature information as well as the overly-smoothed spectral converted by traditional GMM. In this paper we propose a novel method to solve the problem which is based on the technique of the separation of glottal waveforms and the prediction of the excitations. The final result shows that not only are the transformed vocal tract parameters matching the target one better, but also is the high quality of the synthesized speech preserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Xu, Ning; Yang, Zhen] Nanjing Univ Post &amp; Telecommun, Inst Signal Proc
   &amp; Transmiss, Nanjing, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Xu, N (reprint author), Nanjing Univ Post &amp; Telecommun, Inst Signal Proc &amp; Transmiss, Nanjing, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>xuningdlts@gmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">BP </td><td>684</td>
</tr>

<tr>
<td valign="top">EP </td><td>687</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/ICOSP.2008.4697223</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000270665400166</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Zhang, M
   <br>Tao, JH
   <br>Nurminen, J
   <br>Tian, JL
   <br>Wang, X</td>
</tr>

<tr>
<td valign="top">AF </td><td>Zhang, Meng
   <br>Tao, Jiaohua
   <br>Nurminen, Jani
   <br>Tian, Jilei
   <br>Wang, Xia</td>
</tr>

<tr>
<td valign="top">BE </td><td>Yuan, BZ
   <br>Ruan, QQ
   <br>Tang, XF</td>
</tr>

<tr>
<td valign="top">TI </td><td>PHONETIC ANCHOR BASED STATE MAPPING FOR TEXTINDEPENDENT VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">SO </td><td>ICSP: 2008 9TH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING, VOLS 1-5,
   PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th International Conference on Signal Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 26-29, 2008</td>
</tr>

<tr>
<td valign="top">CL </td><td>Beijing, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper describes a novel method for text-independent voice conversion using improved state mapping. HMM is used for representing the phonetic structure of training speech. Centroids of the common phonemes between source and target speech are utilized as phonetic anchors while establishing a mapping between acoustic spaces of source and target speakers. These phonetic anchors and weighted linear transform are used for creating a continuous parametric mapping from source to target speech parameters. The proposed technique is applicable to both intra-lingual and cross-lingual voice conversion. Experimental results show that state mapping is improved using proposed technique.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Zhang, Meng; Tao, Jiaohua] Chinese Acad Sci, Inst Automat, Natl Lab
   Pattern Recognit, Beijing, Peoples R China.
   <br>[Nurminen, Jani] Nokia Devices R&amp;D, Tampere, Finland.
   <br>[Tian, Jilei] Nokia Res Ctr, Tampere, Finland.
   <br>[Wang, Xia] Nokia Res Ctr, Beijing, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Zhang, M (reprint author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>mzhang@nlpr.ia.ac.cn; jhtao@nlpr.ia.ac.cn; jani.k.nurminen@nokia.com;
   jilei.tian@nokia.com; xia.s.wang@nokia.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">BP </td><td>723</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000270665400175</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hanzlicek, Z
   <br>Matousek, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hanzlicek, Zdenek
   <br>Matousek, Jindrich</td>
</tr>

<tr>
<td valign="top">BE </td><td>Yuan, BZ
   <br>Ruan, QQ
   <br>Tang, XF</td>
</tr>

<tr>
<td valign="top">TI </td><td>On Using Warping Function for LSFs Transformation in a Voice Conversion
   System</td>
</tr>

<tr>
<td valign="top">SO </td><td>ICSP: 2008 9TH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING, VOLS 1-5,
   PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th International Conference on Signal Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 26-29, 2008</td>
</tr>

<tr>
<td valign="top">CL </td><td>Beijing, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, a new approach to line spectral frequencies transformation is introduced and employed in the voice conversion framework. This approach steins from the fact that LSFs are some specific points on the frequency axis and their positions determine the shape of the spectral envelope. Thus they could be transformed directly by frequency axis warping. Two warping functions were designed specially for LSFs and compared with the traditional GMM-based conversion function. Listening tests and mathematical evaluation revealed that speech transformed by using proposed warping functions is of higher quality and does not suffer from oversmoothing which is common for GMM-based transformation. On the other hand, the speaker identity is slightly better transformed by GMM-based conversion. However, it is possible to combine these two approaches to obtain a compromise between quality and speaker identity.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Hanzlicek, Zdenek; Matousek, Jindrich] Univ W Bohemia, Fac Sci Appl,
   Dept Cybernet, Plzen 30614, Czech Republic.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hanzlicek, Z (reprint author), Univ W Bohemia, Fac Sci Appl, Dept Cybernet, Univ 8, Plzen 30614, Czech Republic.</td>
</tr>

<tr>
<td valign="top">EM </td><td>zhanzlic@kky.zcu.cz; jmatouse@kky.zcu.cz</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Matousek, Jindrich</display_name>&nbsp;</font></td><td><font size="3">C-2146-2011&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Matousek, Jindrich</display_name>&nbsp;</font></td><td><font size="3">0000-0002-7408-7730&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">BP </td><td>2722</td>
</tr>

<tr>
<td valign="top">EP </td><td>2725</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000270665401228</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Yang, J
   <br>Zhou, F
   <br>Liu, XN
   <br>Shi, LX</td>
</tr>

<tr>
<td valign="top">AF </td><td>Yang, Jun
   <br>Zhou, Fan
   <br>Liu, Xinning
   <br>Shi, Longxin</td>
</tr>

<tr>
<td valign="top">TI </td><td>A novel method for the float-point to fixed-point conversion from
   statistical perspective</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERNATIONAL JOURNAL OF ELECTRONICS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>fixed-point; float-point; SNR; statistical</td>
</tr>

<tr>
<td valign="top">AB </td><td>Float-point to fixed-point conversion is required by many communication, voice and image applications, in order to achieve acceptable levels of performance and cost. This is particularly crucial in real-time applications that have strict timeline requirement on processing throughput and latency. Therefore, fixed-point conversion from float-point computation brings great challenges with the improvement at the same time: How to evaluate the accuracy of the fixed-point system quickly? This paper proposes a novel method for the float-point to fixed-point conversion and evaluates the precision of complex signal systems from statistical perspective. The proposed approach greatly reduces the complexity of the evaluation, and achieves optimum fixed-point implementation in a short time. The experiments in MP3 and the AAC decoding algorithm demonstrate that the accuracy of the fixed-point system is greatly improved by this method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Yang, Jun; Zhou, Fan; Liu, Xinning; Shi, Longxin] SE Univ, Natl ASIC
   Syst Engn Res Ctr, Nanjing 210096, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Yang, J (reprint author), SE Univ, Natl ASIC Syst Engn Res Ctr, Nanjing 210096, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>dragon@seu.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">VL </td><td>95</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">BP </td><td>319</td>
</tr>

<tr>
<td valign="top">EP </td><td>332</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1080/00207210801976453</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000254949500003</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Yanagisawa, K
   <br>Huckvale, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Yanagisawa, Kayoko
   <br>Huckvale, Mark</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INST SPEECH COMMUNICATION ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Phonetic Assessment of Cross-Language Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2008: 9TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH
   COMMUNICATION ASSOCIATION 2008, VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2008)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 22-26, 2008</td>
</tr>

<tr>
<td valign="top">CL </td><td>Brisbane, AUSTRALIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; spoken language conversion; intelligibility;
   cross-language</td>
</tr>

<tr>
<td valign="top">AB </td><td>Cross-language voice conversion maps the speech of speaker SI in language L1 to the voice of speaker S2 using knowledge only of how S2 speaks a different language L2. This mapping is usually performed using speech material from SI and S2 that has been deemed "equivalent" in either acoustic or phonetic terms. This study investigates the issue of equivalence in more detail, and contrasts the performance of a voice conversion system operating in both mono-lingual and cross-lingual modes using Japanese and English. We show that voice conversion impacts the intelligibility of the converted speech, but to a significantly greater degree for cross-language conversion. A phonetic comparison of the monolingual and cross-language converted speech suggests that consonantal information is degraded in both conditions, but vowel information is degraded more in the cross-language condition.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Yanagisawa, Kayoko; Huckvale, Mark] UCL, Dept Speech Hearing &amp; Phonet
   Sci, London, England.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Yanagisawa, K (reprint author), UCL, Dept Speech Hearing &amp; Phonet Sci, London, England.</td>
</tr>

<tr>
<td valign="top">EM </td><td>k.yanagisawa@ucl.ac.uk; m.huckvale@ucl.ac.uk</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">BP </td><td>593</td>
</tr>

<tr>
<td valign="top">EP </td><td>596</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000277026100159</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kawahara, H
   <br>Morise, M
   <br>Banno, H
   <br>Takahashi, T
   <br>Nisimura, R
   <br>Irino, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kawahara, Hideki
   <br>Morise, Masanori
   <br>Banno, Hideki
   <br>Takahashi, Toru
   <br>Nisimura, Ryuichi
   <br>Irino, Toshio</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INST SPEECH COMMUNICATION ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Spectral Envelope Recovery beyond the Nyquist Limit for High-Quality
   Manipulation of Speech Sounds</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2008: 9TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH
   COMMUNICATION ASSOCIATION 2008, VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2008)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 22-26, 2008</td>
</tr>

<tr>
<td valign="top">CL </td><td>Brisbane, AUSTRALIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>speech analysis; sampling theory; speech modification</td>
</tr>

<tr>
<td valign="top">AB </td><td>A simple new method to recover details in a spectral envelope is proposed based on a recently introduced speech analysis, modification and resynthesis framework called TANDEM-STRAIGHT. Spectral envelope recovery of voiced sounds is a discrete-to-analog conversion in the frequency domain. However, there is a fundamental problem because the spatial frequency contents of vocal tract functions generally exceed the Nyquist limit of the equivalent sampling rate determined by the fundamental frequency. TANDEM-STRAIGHT yields a method to recover a spectral envelope based on the consistent sampling theory and provides base information for exceeding this limit. At the final stage, the AR spectral envelope estimated from the TANDEM-STRAIGHT spectrum is divided by the F0 adaptively smoothed version of itself to supply the missing high-spatial-frequency details of the envelope. The underlying principle of the proposed method can also be applied to other speech synthesis frameworks.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kawahara, Hideki; Nisimura, Ryuichi; Irino, Toshio] Wakayama Univ, Dept
   Design Informat Sci, Wakayama, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kawahara, H (reprint author), Wakayama Univ, Dept Design Informat Sci, Wakayama, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>kawahara@sys.wakayamau.ac.jp</td>
</tr>

<tr xmlns:date="http://exslt.org/dates-and-times">
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Kawahara, Hideki</display_name>&nbsp;</font></td><td><font size="3">0000-0001-9360-5700&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">BP </td><td>650</td>
</tr>

<tr>
<td valign="top">EP </td><td>653</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000277026100177</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Zen, H
   <br>Nankaku, Y
   <br>Tokuda, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Zen, Heiga
   <br>Nankaku, Yoshihiko
   <br>Tokuda, Keiichi</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INST SPEECH COMMUNICATION ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Probabilistic Feature Mapping Based on Trajectory HMMs</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2008: 9TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH
   COMMUNICATION ASSOCIATION 2008, VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2008)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 22-26, 2008</td>
</tr>

<tr>
<td valign="top">CL </td><td>Brisbane, AUSTRALIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>trajectory HMM; voice conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION; MODEL</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper proposes a feature mapping algorithm based on the trajectory GMM or trajectory HMM. Although the GMM or HMM-based feature mapping algorithm works effectively, its conversion quality sometimes degrades due to the inappropriate dynamic characteristics caused by the frame-by-frame conversion. While the use of dynamic features can alleviate this problem, it also introduces an inconsistency between training and mapping. The proposed algorithm can solve this inconsistency while keeping the benefits of the use of dynamic features, and offers an entire sequence-level transformation rather than the frame-by-frame conversion. Experimental results in voice conversion show that the proposed algorithm outperforms the conventional one both in objective and subjective tests.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Zen, Heiga; Nankaku, Yoshihiko; Tokuda, Keiichi] Nagoya Inst Technol,
   Dept Comp Sci &amp; Engn, Showa Ku, Nagoya, Aichi 4668555, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Zen, H (reprint author), Nagoya Inst Technol, Dept Comp Sci &amp; Engn, Showa Ku, Gokiso Cho, Nagoya, Aichi 4668555, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>zen@sp.nitech.ac.jp; nankaku@sp.nitech.ac.jp; tokuda@sp.nitech.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>5</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>5</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">BP </td><td>1068</td>
</tr>

<tr>
<td valign="top">EP </td><td>1071</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000277026100286</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Yutani, K
   <br>Uto, Y
   <br>Nankaku, Y
   <br>Toda, T
   <br>Tokuda, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Yutani, Kaori
   <br>Uto, Yosuke
   <br>Nankaku, Yoshihiko
   <br>Toda, Tomoki
   <br>Tokuda, Keiichi</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INST SPEECH COMMUNICATION ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Simultaneous Conversion of Duration and Spectrum Based on Statistical
   Models Including Time-Sequence Matching</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2008: 9TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH
   COMMUNICATION ASSOCIATION 2008, VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2008)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 22-26, 2008</td>
</tr>

<tr>
<td valign="top">CL </td><td>Brisbane, AUSTRALIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; GMM; duration conversion</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper describes a simultaneous conversion technique of duration and spectrum based on a statistical model including time-sequence matching. Conventional GMM-based approaches cannot perform spectral conversion taking account of speaking rate because it assumes one to one frame matching between source and target features. However, speaker characteristics may appear in speaking rates. In order to perform duration conversion, we attach duration models to statistical models including time-sequence matching (DPGMM). Since DPGMM can represent two different length sequences directly, the conversion of spectrum and duration can be performed within an integrated framework. In the proposed technique, each mixture component of DPGMM has different duration transformation functions, therefore durations are converted nonlinearly and dependently on spectral information. In the subjective DMOS test, the proposed method is superior to the conventional method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Yutani, Kaori; Uto, Yosuke; Nankaku, Yoshihiko; Tokuda, Keiichi] Nagoya
   Inst Technol, Showa Ku, Gokiso Cho, Nagoya, Aichi 4668555, Japan.
   <br>[Toda, Tomoki] Nara Inst Sci &amp; Technol, Nara 6300101, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Yutani, K (reprint author), Nagoya Inst Technol, Showa Ku, Gokiso Cho, Nagoya, Aichi 4668555, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>yutani@sp.nitech.ac.jp; uto@sp.nitech.ac.jp; nankaku@sp.nitech.ac.jp;
   tomoki@is.naist.jp; tokuda@sp.nitech.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>4</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>4</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">BP </td><td>1072</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000277026100287</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Muramatsu, T
   <br>Ohtani, Y
   <br>Toda, T
   <br>Saruwatari, H
   <br>Shikano, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Muramatsu, Takashi
   <br>Ohtani, Yamato
   <br>Toda, Tomoki
   <br>Saruwatari, Hiroshi
   <br>Shikano, Kiyohiro</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INST SPEECH COMMUNICATION ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Low-Delay Voice Conversion based on Maximum Likelihood Estimation of
   Spectral Parameter Trajectory</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2008: 9TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH
   COMMUNICATION ASSOCIATION 2008, VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2008)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 22-26, 2008</td>
</tr>

<tr>
<td valign="top">CL </td><td>Brisbane, AUSTRALIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>speech synthesis; voice conversion; Gaussian mixture model; maximum
   likelihood estimation; time-recursive algorithm</td>
</tr>

<tr>
<td valign="top">AB </td><td>As typical voice conversion methods, two spectral conversion processes have been proposed: 1) the frame-based conversion that converts spectral parameters frame by frame and 2) the trajectory-based conversion that converts all spectral parameters over an utterance simultaneously. The former process is capable of real-time conversion but it sometimes causes inappropriate spectral movements. On the other hand, the latter process provides the converted spectral parameters exhibiting proper dynamic characteristics but a batch process is inevitable. To achieve the real-time conversion process considering spectral dynamic characteristics, we propose a time-recursive conversion algorithm based on maximum likelihood estimation of spectral parameter trajectory. Experimental results show that the proposed method achieves the low-delay conversion process, e.g., only one frame delay, while keeping the conversion performance comparably high to that of the conventional trajectory-based conversion.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Muramatsu, Takashi; Ohtani, Yamato; Toda, Tomoki; Saruwatari, Hiroshi;
   Shikano, Kiyohiro] Nara Inst Sci &amp; Technol, Grad Sch Informat Sci,
   Ikoma, Nara 6300192, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Muramatsu, T (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, 8916-5 Takayama Cho, Ikoma, Nara 6300192, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>takashi-mu@is.naist.jp; yamato-o@is.naist.jp; tomoki@is.naist.jp;
   sawatari@is.naist.jp; shikano@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>21</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>21</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">BP </td><td>1076</td>
</tr>

<tr>
<td valign="top">EP </td><td>1079</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000277026100288</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ohtani, Y
   <br>Toda, T
   <br>Saruwatari, H
   <br>Shikano, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ohtani, Yamato
   <br>Toda, Tomoki
   <br>Saruwatari, Hiroshi
   <br>Shikano, Kiyohiro</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INST SPEECH COMMUNICATION ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>An Improved One-to-Many Eigenvoice Conversion System</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2008: 9TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH
   COMMUNICATION ASSOCIATION 2008, VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2008)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 22-26, 2008</td>
</tr>

<tr>
<td valign="top">CL </td><td>Brisbane, AUSTRALIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speech synthesis; eigenvoice conversion; speaker adaptive training;
   mixed excitation; global variance</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>We have previously developed a one-to-many eigenvoice conversion (EVC) system enabling the conversion from a specific source speaker's voice into an arbitrary target speaker's voice. In this system, eigenvoice Gaussian mixture model (EV-GMM) is trained in advance with multiple parallel data sets composed of utterance pairs of the source and many pre-stored target speakers. The EV-GMM is effectively adapted to an arbitrary target speaker using a small amount of adaptation data. Although this system achieves the very flexible training of the conversion model, the quality of the converted speech is still not high enough. In order to alleviate this problem, we simultaneously apply the following promising techniques to the one-to-many EVC system: 1) STRAIGHT mixed excitation, 2) the conversion algorithm considering global variance, and 3) speaker adaptive training of the EV-GMM. Experimental results demonstrate that the proposed system causes remarkable improvements in the performance of EVC.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ohtani, Yamato; Toda, Tomoki; Saruwatari, Hiroshi; Shikano, Kiyohiro]
   Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ohtani, Y (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>yamato-o@is.naist.jp; tomoki@is.naist.jp; sawatari@is.naist.jp;
   shikano@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">BP </td><td>1080</td>
</tr>

<tr>
<td valign="top">EP </td><td>1083</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000277026100289</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Helander, E
   <br>Schwarz, J
   <br>Nurminen, J
   <br>Silen, H
   <br>Gabbouj, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Helander, Elina
   <br>Schwarz, Jan
   <br>Nurminen, Jani
   <br>Silen, Hanna
   <br>Gabbouj, Moncef</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INST SPEECH COMMUNICATION ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>On the impact of alignment on voice conversion performance</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2008: 9TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH
   COMMUNICATION ASSOCIATION 2008, VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2008)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 22-26, 2008</td>
</tr>

<tr>
<td valign="top">CL </td><td>Brisbane, AUSTRALIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; alignment; DTW</td>
</tr>

<tr>
<td valign="top">AB </td><td>Most of the current voice conversion systems model the joint density of source and target features using a Gaussian mixture model. An inherent property of this approach is that the source and target features have to be properly aligned for the training. It is intuitively clear that the accuracy of the alignment has some effect on the conversion quality but this issue has not been thoroughly studied in the literature. Examples of alignment techniques include the usage of a speech recognizer with forced alignment or dynamic time warping (DTW). In this paper, we study the effect of alignment on voice conversion quality through extensive experiments and discuss issues that should be considered. The main outcome of the study is that alignment clearly matters but with simple voice activity detection, DTW and some constraints we can achieve the same quality as with hand-marked labels.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Helander, Elina; Silen, Hanna; Gabbouj, Moncef] Tampere Univ Technol,
   Dept Signal Proc, FIN-33101 Tampere, Finland.
   <br>[Schwarz, Jan] Univ Kiel, Inst Circuit &amp; Syst Theory, Kiel, Germany.
   <br>[Nurminen, Jani] Nokia Devices R&amp;D, Tampere, Finland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Helander, E (reprint author), Tampere Univ Technol, Dept Signal Proc, FIN-33101 Tampere, Finland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>elina.helander@tut.fi; js@tf.uni-kiel.de; jani.k.nurminen@nokia.com;
   hanna.silen@tut.fi</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Gabbouj, Moncef</display_name>&nbsp;</font></td><td><font size="3">G-4293-2014&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Gabbouj, Moncef</display_name>&nbsp;</font></td><td><font size="3">0000-0002-9788-2323&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Helander, Elina</display_name>&nbsp;</font></td><td><font size="3">0000-0002-0076-0590&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>6</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>6</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">BP </td><td>1453</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000277026100390</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>del Pozo, A
   <br>Young, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>del Pozo, Arantza
   <br>Young, Steve</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INST SPEECH COMMUNICATION ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>The Linear Transformation of LF Glottal Waveforms for Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2008: 9TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH
   COMMUNICATION ASSOCIATION 2008, VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2008)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 22-26, 2008</td>
</tr>

<tr>
<td valign="top">CL </td><td>Brisbane, AUSTRALIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>LF waveform; deconvolution; voice conversion</td>
</tr>

<tr>
<td valign="top">AB </td><td>Most Voice Conversion (VC) systems exploit source-filter decomposition based on linear prediction (LP) to transform spectral envelopes, incurring as a result various issues related to the oversimplification of the LP voice source model. Whilst residual prediction methods can mitigate this problem, they cannot be used to modify voice source quality. In this paper, a system which employs linear transformations to convert both the spectral envelope and the LF glottal waveform is presented. Its performance is shown to be comparable to that of a state-of-the-art VC implementation in terms of speaker identity conversion but its output has better quality. In addition, it is also capable of transforming the quality of the voice source.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[del Pozo, Arantza; Young, Steve] Univ Cambridge, Dept Engn, Cambridge
   CB2 1PZ, England.</td>
</tr>

<tr>
<td valign="top">RP </td><td>del Pozo, A (reprint author), Univ Cambridge, Dept Engn, Trumpington St, Cambridge CB2 1PZ, England.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ad371@eng.cam.ac.uk; sjy@eng.cam.ac.uk</td>
</tr>

<tr>
<td valign="top">TC </td><td>4</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>4</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">BP </td><td>1457</td>
</tr>

<tr>
<td valign="top">EP </td><td>1460</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000277026100391</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Tani, D
   <br>Toda, T
   <br>Ohtani, Y
   <br>Saruwatari, H
   <br>Shikano, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Tani, Daisuke
   <br>Toda, Tomoki
   <br>Ohtani, Yamato
   <br>Saruwatari, Hiroshi
   <br>Shikano, Kiyohiro</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INST SPEECH COMMUNICATION ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Maximum A Posteriori Adaptation for Many-to-One Eigenvoice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2008: 9TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH
   COMMUNICATION ASSOCIATION 2008, VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2008)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 22-26, 2008</td>
</tr>

<tr>
<td valign="top">CL </td><td>Brisbane, AUSTRALIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>speech synthesis; voice conversion; GMM; eigenvoice; MAP</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Many-to-one eigenvoice conversion (EVC) allows the conversion from an arbitrary speaker's voice into the pre-determined target speaker's voice. In this method, a canonical eigenvoice Gaussian mixture model is effectively adapted to any source speaker using only a few utterances as the adaptation data. In this paper, we propose a many-to-one EVC based on maximum a posteriori (MAP) adaptation for further improving the robustness of the adaptation process to the amount of adaptation data. Results of objective and subjective evaluations demonstrate that the proposed method is the most effective among the other conventional many-to-one VC methods when using any amount of adaptation data (e.g., from 300 ms to 16 utterances).</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Tani, Daisuke; Toda, Tomoki; Ohtani, Yamato; Saruwatari, Hiroshi;
   Shikano, Kiyohiro] Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara,
   Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Tani, D (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>tomoki@is.naist.jp; yamato-o@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">BP </td><td>1461</td>
</tr>

<tr>
<td valign="top">EP </td><td>1464</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000277026100392</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Tran, VA
   <br>Bailly, G
   <br>Laevenbruck, H
   <br>Jutten, C</td>
</tr>

<tr>
<td valign="top">AF </td><td>Tran, Viet-Anh
   <br>Bailly, Gerard
   <br>Laevenbruck, Helene
   <br>Jutten, Christian</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INST SPEECH COMMUNICATION ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Improvement to a NAM captured whisper-to-speech system</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2008: 9TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH
   COMMUNICATION ASSOCIATION 2008, VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2008)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 22-26, 2008</td>
</tr>

<tr>
<td valign="top">CL </td><td>Brisbane, AUSTRALIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>audiovisual voice conversion; non-audible murmur; whispered speech</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, new techniques to improve whisper-to-speech conversion are investigated, in the framework of silent speech telephone communication. A preliminary conversion method from Non-Audible Murmur (NAM) to modal speech, based on statistical mapping trained using aligned corpora has been proposed. Although it is a very promising technique, its performance is still insufficient due to the difficulties in estimating F-0 from unvoiced speech. In this paper, two distinct modifications are proposed, in order to improve the naturalness of the synthesized speech. In the first modification, LDA (Linear Discriminant Analysis) is used instead of PCA (Principal Component Analysis) to reduce the dimensionality of the input spectral vectors. In addition, the influence of long-term variation of spectral information on pitch estimation is examined. The second modification is an attempt to integrate visual information as a complementary input to improve spectral estimation, F-0 estimation and voicing decision.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Tran, Viet-Anh; Bailly, Gerard; Laevenbruck, Helene] UMR 5216 CNRS INPG
   UJF U Stendhal, Dept Parole &amp; Cognit, GIPSA Lab, Grenoble, France.
   <br>[Jutten, Christian] UMR 5216 CNRS INPG UJF U Stendhal, Dept Imaging &amp;
   Signal, GIPSA Lab, Grenoble, France.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Tran, VA (reprint author), UMR 5216 CNRS INPG UJF U Stendhal, Dept Parole &amp; Cognit, GIPSA Lab, Grenoble, France.</td>
</tr>

<tr>
<td valign="top">EM </td><td>viet-anh.tran@gipsa-lab.inpg.fr; gerard.bailly@gipsa-lab.inpg.fr;
   helene.loevenbruck@gipsa-lab.inpg.fr; christian.jutten@gipsa-lab.inpg.fr</td>
</tr>

<tr>
<td valign="top">TC </td><td>8</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>8</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">BP </td><td>1465</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000277026100393</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Inanoglu, Z
   <br>Young, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Inanoglu, Zeynep
   <br>Young, Steve</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INST SPEECH COMMUNICATION ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Emotion Conversion using F0 Segment Selection</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2008: 9TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH
   COMMUNICATION ASSOCIATION 2008, VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2008)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 22-26, 2008</td>
</tr>

<tr>
<td valign="top">CL </td><td>Brisbane, AUSTRALIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>emotion conversion; expressive prosody</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION; SPEECH</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper describes F0 segment selection, a novel syllable-based F0 conversion method, which provides a concatenative framework to search for F0 segments in a modest corpus of emotional speech (similar to 15 minutes of data). The method is compared with our earlier work on F0 generation using context-sensitive syllable HMMs. Both methods are complemented with a duration conversion module as well as GMM-based spectral conversion to form a unified emotion conversion framework in English. The system was evaluated using three target styles: surprise, anger and sadness. The results of an extensive perceptual test show that segment selection significantly outperforms the HMM-based method in terms of both emotion recognition rates and intonation quality ratings for surprise and anger. For conveying sadness both methods were effective.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Inanoglu, Zeynep; Young, Steve] Univ Cambridge, Dept Engn, Cambridge
   CB2 1PZ, England.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Inanoglu, Z (reprint author), Univ Cambridge, Dept Engn, Cambridge CB2 1PZ, England.</td>
</tr>

<tr>
<td valign="top">EM </td><td>zeynepinan@post.harvard.edu; sjy@eng.cam.ac.uk</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">BP </td><td>2122</td>
</tr>

<tr>
<td valign="top">EP </td><td>2125</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000277026101105</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Turk, O
   <br>Schroder, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Tuerk, Oytun
   <br>Schroeder, Marc</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INST SPEECH COMMUNICATION ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Comparison of Voice Conversion Methods for Transforming Voice Quality
   in Emotional Speech Synthesis</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2008: 9TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH
   COMMUNICATION ASSOCIATION 2008, VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2008)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 22-26, 2008</td>
</tr>

<tr>
<td valign="top">CL </td><td>Brisbane, AUSTRALIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice quality transformation; voice conversion; emotional speech
   synthesis</td>
</tr>

<tr>
<td valign="top">ID </td><td>PROCESSING TECHNIQUES</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a comparison of methods for transforming voice quality in neutral synthetic speech to match cheerful, aggressive, and depressed expressive styles. Neutral speech is generated using the unit selection system in the MARY TTS platform and a large neutral database in German. The output is modified using voice conversion techniques to match the target expressive styles, the focus being on spectral envelope conversion for transforming the overall voice quality. Various improvements over the state-of-the-art weighted codebook mapping and GMM based voice conversion frameworks are employed resulting in three algorithms. Objective evaluation results show that all three methods result in comparable reduction in objective distance to target expressive ITS outputs whereas weighted frame mapping and GMM based transformations were perceived slightly better than the weighted codebook mapping outputs in generating the target expressive style in a listening test.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Tuerk, Oytun; Schroeder, Marc] DFKI GmbH, Language Technol Lab,
   Saarbrucken, Germany.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Turk, O (reprint author), DFKI GmbH, Language Technol Lab, Saarbrucken, Germany.</td>
</tr>

<tr>
<td valign="top">EM </td><td>oytun.tuerk@dfki.de; marc.schroeder@dfki.de</td>
</tr>

<tr>
<td valign="top">TC </td><td>6</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>6</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">BP </td><td>2282</td>
</tr>

<tr>
<td valign="top">EP </td><td>2285</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000277026101146</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Song, P
   <br>Zhao, L</td>
</tr>

<tr>
<td valign="top">AF </td><td>Song, Peng
   <br>Zhao, Li</td>
</tr>

<tr>
<td valign="top">BE </td><td>Zhang, Y
   <br>Tan, H
   <br>Luo, Q</td>
</tr>

<tr>
<td valign="top">TI </td><td>Improving the Performance of GMM Based Voice Conversion Method</td>
</tr>

<tr>
<td valign="top">SO </td><td>PACIIA: 2008 PACIFIC-ASIA WORKSHOP ON COMPUTATIONAL INTELLIGENCE AND
   INDUSTRIAL APPLICATION, VOLS 1-3, PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Pacific/Asia Workshop on Computational Intelligence and Industrial
   Application</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 19-20, 2008</td>
</tr>

<tr>
<td valign="top">CL </td><td>Wuhan, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion systems modify a speaker's voice to be perceived as another speaker uttered, in most state-of-art conversion systems, the quality of converted speech is still unsatisfactory. In this paper, a novel improved Guassian Mixture Model (GMM) based voice conversion method is presented. To avoid the over-smoothed phenomena of traditional GMM conversion method, we adopt GMM method considering posterior threshold of each frame, we can make a balance between GMM and Linear Multivariate Regression (LMR) method, which is defined as GMM+LMR method. Since the conversion performs frame-by frame using only spectral information, GMM conversion method may get discontinuity of converted speech, in order to overcome it, we consider the dynamic spectral features between frames, while dynamic pitch information is also considered, objective and perceptual experiments showed that our new extended GMM method achieved better performance than traditional GMM methods.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Song, Peng; Zhao, Li] Southeast Univ, Sch Informat Sci &amp; Engn, Nanjing
   210096, Jiangsu, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Song, P (reprint author), Southeast Univ, Sch Informat Sci &amp; Engn, Nanjing 210096, Jiangsu, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>pengsongseu@gmail.com; zhaoli@seu.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">BP </td><td>436</td>
</tr>

<tr>
<td valign="top">EP </td><td>440</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000270675200089</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Naiel, MA
   <br>Khamis, SA
   <br>Nasr, ME</td>
</tr>

<tr>
<td valign="top">AF </td><td>Naiel, Mohamed A.
   <br>Khamis, Salah A.
   <br>Nasr, Mohamed E.</td>
</tr>

<tr>
<td valign="top">BE </td><td>Nasr, ME
   <br>Ragheb, AM
   <br>Bedeer, EE
   <br>ElKhouly, EA</td>
</tr>

<tr>
<td valign="top">TI </td><td>Flexible realization scheme for streaming voice signals over IP networks</td>
</tr>

<tr>
<td valign="top">SO </td><td>PROCEEDINGS OF THE 25TH NATIONAL RADIO SCIENCE CONFERENCE: NRSC 2008</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>25th National Radio Science Conference</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 18-20, 2008</td>
</tr>

<tr>
<td valign="top">CL </td><td>Tanta, EGYPT</td>
</tr>

<tr>
<td valign="top">AB </td><td>Streaming of Voice over Internet Protocol (SVoIP) means transmission of real.-time voice, signals and associated call control information over an IP-based network. This paper introduces a microcontroller based system that can be used for making a SVoIP gateway (SVoIP GW) that uses TCP/IP Stack. Using this technique one can transmit the voice of the POTS (Plain Old Telephone Service) over the internet by making conversion of the analogue voice into IP-packet. Further an OPNET program is used to simulate a system to accommodate a voice over IP networks.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Naiel, Mohamed A.; Khamis, Salah A.; Nasr, Mohamed E.] Tanta Univ, Dept
   Elect &amp; Commun Engn, Fac Engn, Tanta, Egypt.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Naiel, MA (reprint author), Tanta Univ, Dept Elect &amp; Commun Engn, Fac Engn, Tanta, Egypt.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">BP </td><td>U563</td>
</tr>

<tr>
<td valign="top">EP </td><td>U570</td>
</tr>

<tr>
<td valign="top">SC </td><td>Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000257003900054</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Blin, L
   <br>Boeffard, O
   <br>Barreaud, V</td>
</tr>

<tr>
<td valign="top">AF </td><td>Blin, Laurent
   <br>Boeffard, Olivier
   <br>Barreaud, Vincent</td>
</tr>

<tr>
<td valign="top">GP </td><td>European Language Resources Association</td>
</tr>

<tr>
<td valign="top">TI </td><td>WEB-based listening test system for speech synthesis and speech
   conversion evaluation</td>
</tr>

<tr>
<td valign="top">SO </td><td>SIXTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION,
   LREC 2008</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>6th International Conference on Language Resources and Evaluation (LREC)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 28-30, 2008</td>
</tr>

<tr>
<td valign="top">CL </td><td>Marrakech, MOROCCO</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this article, we propose a web based listening test system that can be used with a large range of listeners. Our main goals were to make the configuration of the tests as simple and flexible as possible, to simplify the recruiting of the testees and, of course, to keep track of the results using a relational database. This first version of our system can perform the most widely used listening tests in the speech processing community (AB-BA, ABX and MOS tests). It can also easily evolve and propose other tests implemented by the tester by means of a module interface. This scenario is explored in this article which proposes an implementation of a module for Comparison Mean Opinion Score (CMOS) tests and conduct of such an experiment. This test allowed us to extract from the BREF120 corpus a couple of voices of distinct supra-segmental characteristics. This system is offered to the speech synthesis and speech conversion community under free license.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Blin, Laurent; Boeffard, Olivier; Barreaud, Vincent] Univ Rennes 1,
   IRISA, Lannion, France.</td>
</tr>

<tr>
<td valign="top">EM </td><td>Laurent.Blin@univ-rennes1.fr; Olivier.Boeffard@univ-rennes1.fr;
   Vincent.Barreaud@univ-rennes1.fr</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">BP </td><td>2270</td>
</tr>

<tr>
<td valign="top">EP </td><td>2274</td>
</tr>

<tr>
<td valign="top">SC </td><td>Linguistics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000324028902059</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ganchev, T
   <br>Lazaridis, A
   <br>Mporas, I
   <br>Fakotakis, N</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ganchev, Todor
   <br>Lazaridis, Alexandros
   <br>Mporas, Iosif
   <br>Fakotakis, Nikos</td>
</tr>

<tr>
<td valign="top">BE </td><td>Sojka, P
   <br>Horak, A
   <br>Kopecek, I
   <br>Pala, K</td>
</tr>

<tr>
<td valign="top">TI </td><td>Performance Evaluation for Voice Conversion Systems</td>
</tr>

<tr>
<td valign="top">SO </td><td>TEXT, SPEECH AND DIALOGUE, PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">SE </td><td>LECTURE NOTES IN ARTIFICIAL INTELLIGENCE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>11th International Conference on Text, Speech and Dialogue</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 08-12, 2008</td>
</tr>

<tr>
<td valign="top">CL </td><td>Brno, CZECH REPUBLIC</td>
</tr>

<tr>
<td valign="top">DE </td><td>Performance evaluation; voice conversion; speaker identification</td>
</tr>

<tr>
<td valign="top">AB </td><td>In the present work, we introduce a new performance evaluation measure for assessing the capacity of voice conversion systems to modify the speech of one speaker (Source) so that it Sounds as if it was uttered by another speaker (target). This measure relics on it GMM-UBM-based likelihood estimator that estimates (lie degree of proximity between an utterance of the converted voice and the predefined models of the Source and target voices. The proposed approach allows the formulation of an objective criterion, which is applicable Cor both evaluation of the virtue of a single system and for direct comparison (benchmarking) among different voice conversion systems. To illustrate the functionality and the practical usefulness of the proposed measure. we contrast it with four well-known objective evaluation criteria.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ganchev, Todor; Lazaridis, Alexandros; Mporas, Iosif; Fakotakis, Nikos]
   Univ Patras, Dept Elect &amp; Comp Engn, Wire Commun Lab, Rion 26500, Greece.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ganchev, T (reprint author), Univ Patras, Dept Elect &amp; Comp Engn, Wire Commun Lab, Rion 26500, Greece.</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Ganchev, Todor</display_name>&nbsp;</font></td><td><font size="3">A-1915-2017&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Ganchev, Todor</display_name>&nbsp;</font></td><td><font size="3">0000-0003-0384-4033&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Lazaridis, Alexandros</display_name>&nbsp;</font></td><td><font size="3">0000-0003-1045-3508&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">VL </td><td>5246</td>
</tr>

<tr>
<td valign="top">BP </td><td>317</td>
</tr>

<tr>
<td valign="top">EP </td><td>324</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000259634700041</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Vich, R
   <br>Nouza, J
   <br>Vondra, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Vich, Robert
   <br>Nouza, Jan
   <br>Vondra, Martin</td>
</tr>

<tr>
<td valign="top">BE </td><td>Esposito, A
   <br>Bourbakis, NG
   <br>Avouris, N
   <br>Hatzilygeroudis, I</td>
</tr>

<tr>
<td valign="top">TI </td><td>Automatic Speech Recognition Used for Intelligibility Assessment of
   Text-to-Speech Systems</td>
</tr>

<tr>
<td valign="top">SO </td><td>VERBAL AND NONVERBAL FEATURES OF HUMAN-HUMAN AND HUMAN-MACHINE
   INTERACTIONS</td>
</tr>

<tr>
<td valign="top">SE </td><td>Lecture Notes in Artificial Intelligence</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Verbal and Nonverbal Features of Human and
   Human-Machine Interaction</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 29-31, 2007</td>
</tr>

<tr>
<td valign="top">CL </td><td>Patras, GREECE</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speech recognition; speech synthesis; speech assessment</td>
</tr>

<tr>
<td valign="top">ID </td><td>CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Speech intelligibility is the most important parameter in evaluation of speech quality. In the contribution, a new objective intelligibility assessment of general speech processing algorithms is proposed. It is based on automatic recognition methods developed for discrete and fluent speech processing. The idea is illustrated on two case studies: a) comparison of listening evaluation of Czech rhyme tests with automatic discrete speech recognition and b) automatic continuous speech recognition of general topic Czech texts read by professional and nonprofessional speakers vs. the same texts generated by several Czech Text-to-Speech systems. The aim of the proposed approach is fast and objective intelligibility assessment of Czech Text-to-Speech systems, which include male and female voices and a voice conversion module.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Vich, Robert; Vondra, Martin] Acad Sci Czech Republic, Inst Photon &amp;
   Elect, Chaberska 57, CZ-18251 Prague 8, Czech Republic.
   <br>[Nouza, Jan] Techn Univ Liberec, Inst Informat Technol &amp; Elect, CZ-46117
   Libechov, Czech Republic.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Vich, R (reprint author), Acad Sci Czech Republic, Inst Photon &amp; Elect, Chaberska 57, CZ-18251 Prague 8, Czech Republic.</td>
</tr>

<tr>
<td valign="top">EM </td><td>vich@ufe.cz; jan.nouza@tul.cz; vondra@ufe.cz</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Nouza, Jan</display_name>&nbsp;</font></td><td><font size="3">E-9914-2011&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>7</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>7</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">VL </td><td>5042</td>
</tr>

<tr>
<td valign="top">BP </td><td>136</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000262977300010</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Pribil, J
   <br>Pribilova, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Pribil, Jiri
   <br>Pribilova, Anna</td>
</tr>

<tr>
<td valign="top">BE </td><td>Esposito, A
   <br>Bourbakis, NG
   <br>Avouris, N
   <br>Hatzilygeroudis, I</td>
</tr>

<tr>
<td valign="top">TI </td><td>Application of Expressive Speech in TTS System with Cepstral Description</td>
</tr>

<tr>
<td valign="top">SO </td><td>VERBAL AND NONVERBAL FEATURES OF HUMAN-HUMAN AND HUMAN-MACHINE
   INTERACTIONS</td>
</tr>

<tr>
<td valign="top">SE </td><td>Lecture Notes in Artificial Intelligence</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Verbal and Nonverbal Features of Human and
   Human-Machine Interaction</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 29-31, 2007</td>
</tr>

<tr>
<td valign="top">CL </td><td>Patras, GREECE</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speech analysis and synthesis; expressive speech; text-to-speech system</td>
</tr>

<tr>
<td valign="top">ID </td><td>CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Expressive speech synthesis representing different human emotions has been in the interests of researchers for a longer time. Recently, some experiments with storytelling speaking style have been performed. This particular speaking style is suitable for applications aimed at children as well as special applications aimed at blind people. Analyzing human storytellers' speech, we designed a set of prosodic parameters prototypes for converting speech produced by the text-to-speech (TTS) system into storytelling speech. In addition to suprasegmental characteristics (pitch, intensity, and duration) included in these speech prototypes, also information about significant frequencies of spectral envelope and spectral flatness determining degree of voicing was used.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Pribil, Jiri] Acad Sci Czech Republic, Inst Photon &amp; Elect, Chaberska
   57, CZ-18251 Prague 8, Czech Republic.
   <br>[Pribilova, Anna] Slovak Tech Univ, Fac lect Engn, InformationTecnol
   Dept Radio Elect, SK-81219 Bratislava, Slovakia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Pribil, J (reprint author), Acad Sci Czech Republic, Inst Photon &amp; Elect, Chaberska 57, CZ-18251 Prague 8, Czech Republic.</td>
</tr>

<tr>
<td valign="top">EM </td><td>Jiri.Pribil@savba.sk; Anna.Pribilova@stuba.sk</td>
</tr>

<tr>
<td valign="top">TC </td><td>7</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>7</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">VL </td><td>5042</td>
</tr>

<tr>
<td valign="top">BP </td><td>200</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000262977300015</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Guido, RC
   <br>Vieira, LS
   <br>Barbon, S
   <br>Sanchez, FL
   <br>Maciel, CD
   <br>Fonseca, ES
   <br>Pereira, JC</td>
</tr>

<tr>
<td valign="top">AF </td><td>Guido, Rodrigo Capobianco
   <br>Vieira, Lucimar Sasso
   <br>Barbon Junior, Sylvio
   <br>Sanchez, Fabricio Lopes
   <br>Maciel, Carlos Dias
   <br>Fonseca, Everthon Silva
   <br>Pereira, Jose Carlos</td>
</tr>

<tr>
<td valign="top">TI </td><td>A neural-wavelet architecture for voice conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>NEUROCOMPUTING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>RBF neural networks; wavelet transforms; voice conversion</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this letter we propose a new architecture for voice conversion that is based on a joint neural-wavelet approach. We also examine the characteristics of many wavelet families and determine the one that best matches the requirements of the proposed system. The conclusions presented in theory are confirmed in practice with utterances extracted from TIMIT speech corpus. (c) 2007 Elsevier B.V. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Univ Sao Paulo, Inst Phys Sao Carlos, Dept Phys &amp; Informat, Speech Lab
   FFI IFSC, BR-13560970 Sao Carlos, SP, Brazil.
   <br>Univ Sao Paulo, Sch Engn Sao Carlos, Dept Elect Engn, SEL EESC,
   BR-13560970 Sao Carlos, SP, Brazil.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Guido, RC (reprint author), Univ Sao Paulo, Inst Phys Sao Carlos, Dept Phys &amp; Informat, Speech Lab FFI IFSC, Ave Trabalhador Sao Carlense 400, BR-13560970 Sao Carlos, SP, Brazil.</td>
</tr>

<tr>
<td valign="top">EM </td><td>guido@ifsc.usp.br</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Sao Carlos Institute of Physics, IFSC/USP</display_name>&nbsp;</font></td><td><font size="3">M-2664-2016&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Maciel, Carlos Dias</display_name>&nbsp;</font></td><td><font size="3">A-2680-2008&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Barbon, Sylvio</display_name>&nbsp;</font></td><td><font size="3">L-6137-2013&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>GUIDO, RODRIGO CAPOBIANCO C</display_name>&nbsp;</font></td><td><font size="3">L-6236-2018&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Maciel, Carlos Dias</display_name>&nbsp;</font></td><td><font size="3">0000-0003-0137-6678&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Barbon, Sylvio</display_name>&nbsp;</font></td><td><font size="3">0000-0002-4988-0702&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>GUIDO, RODRIGO CAPOBIANCO C</display_name>&nbsp;</font></td><td><font size="3">0000-0002-0924-8024&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>14</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>15</td>
</tr>

<tr>
<td valign="top">PD </td><td>DEC</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">VL </td><td>71</td>
</tr>

<tr>
<td valign="top">IS </td><td>1-3</td>
</tr>

<tr>
<td valign="top">BP </td><td>174</td>
</tr>

<tr>
<td valign="top">EP </td><td>180</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.neucom.2007.08.010</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000251500600018</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Toda, T
   <br>Black, AW
   <br>Tokuda, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Toda, Tomoki
   <br>Black, Alan W.
   <br>Tokuda, Keiichi</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice conversion based on maximum-likelihood estimation of spectral
   parameter trajectory</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>dynamic feature; global variance; Gaussian mixture model (GMM);
   maximum-likelihood estimation (MLE); voice conversion (VC)</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH SYNTHESIS; HMM; TRANSFORMATION; NETWORKS</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we describe a novel spectral conversion method for voice conversion (VC). A Gaussian mixture model (GMM) of the joint probability density of source and target features is employed for performing spectral conversion between speakers. The conventional method converts spectral parameters frame by frame based on the minimum mean square error. Although it is reasonably effective, the deterioration of speech quality is caused by some problems: 1) appropriate spectral movements are not always caused by the frame-based conversion process, and 2) the converted spectra are excessively smoothed by statistical modeling. In order to address those problems, we propose a conversion method based on the maximum-likelihood estimation of a spectral parameter trajectory. Not only static but also dynamic feature statistics are used for realizing the appropriate converted spectrum-sequence. Moreover, the oversmoothing effect is alleviated by considering a global variance feature of the converted spectra. Experimental results indicate that the performance of VC can be dramatically improved by the proposed method in view of both speech quality and conversion accuracy for speaker individuality.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara 6300192, Japan.
   <br>Carnegie Mellon Univ, Language Technol Inst, Pittsburgh, PA 15213 USA.
   <br>Nagoya Inst Technol, Grad Sch Engn, Nagoya, Aichi 4668555, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Toda, T (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara 6300192, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>tomoki@is.naist.jp; awb@cs.cmu.edu; tokuda@ics.nitech.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>441</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>456</td>
</tr>

<tr>
<td valign="top">PD </td><td>NOV</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">VL </td><td>15</td>
</tr>

<tr>
<td valign="top">IS </td><td>8</td>
</tr>

<tr>
<td valign="top">BP </td><td>2222</td>
</tr>

<tr>
<td valign="top">EP </td><td>2235</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASL.2007.907344</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000250282800005</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Uchino, E
   <br>Yano, K
   <br>Azetsu, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Uchino, Eiji
   <br>Yano, Kazuaki
   <br>Azetsu, Tadahiro</td>
</tr>

<tr>
<td valign="top">TI </td><td>A self-organizing map with twin units capable of describing a nonlinear
   input-output relation applied to speech code vector mapping</td>
</tr>

<tr>
<td valign="top">SO </td><td>INFORMATION SCIENCES</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>twin units; self-organizing map; voice conversion; bone conduction
   voice; quantization distortion</td>
</tr>

<tr>
<td valign="top">ID </td><td>ARCHITECTURE</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper describes a new type of self-organizing map (SOM) with twin units as opposed to the single unit type conventional SOM proposed by Kohonen. The present self-organizing map with twin units (TW-SOM) can describe a nonlinear input-output relation with high accuracy. It is applied to voice conversion problem from bone conduction voice to air conduction voice (nonlinear code vector mapping), and its superiority over the conventional method using Linde-Buzo-Gray (LBG) algorithm is discussed. The tone quality of the converted voice is examined not only from the quantization distortion viewpoint, but also from the auditory sensation viewpoint through actual listening tests. The enhancement of the tone quality was experimentally confirmed. (c) 2007 Elsevier Inc. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Yamaguchi Univ, Grad Sch Engn Sci, Div Nat Sci &amp; Math, Yamaguchi, Japan.
   <br>Yamaguchi Univ, Grad Sch Engn Sci, Div Nat Sci &amp; Symbios, Yamaguchi,
   Japan.
   <br>Yamaguchi Prefectural Univ, Fac Human Life Sci, Dept Environm Design,
   Yamaguchi, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Uchino, E (reprint author), Yamaguchi Univ, Grad Sch Engn Sci, Div Nat Sci &amp; Math, Yamaguchi, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>uchino@sci.yamaguchi-u.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>13</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>13</td>
</tr>

<tr>
<td valign="top">PD </td><td>NOV 1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">VL </td><td>177</td>
</tr>

<tr>
<td valign="top">IS </td><td>21</td>
</tr>

<tr>
<td valign="top">BP </td><td>4634</td>
</tr>

<tr>
<td valign="top">EP </td><td>4644</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.ins.2007.05.028</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000249714300005</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Portone, CR
   <br>Hapner, ER
   <br>McGregor, L
   <br>Otto, K
   <br>Johns, MM</td>
</tr>

<tr>
<td valign="top">AF </td><td>Portone, Carissa R.
   <br>Hapner, Edie R.
   <br>McGregor, Laura
   <br>Otto, Kristen
   <br>Johns, Michael M., III</td>
</tr>

<tr>
<td valign="top">TI </td><td>Correlation of the voice handicap index (VHI) and the voice-related
   quality of life measure (V-RQOL)</td>
</tr>

<tr>
<td valign="top">SO </td><td>JOURNAL OF VOICE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article; Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>34th Annual Symposium on Care of the Professional Voice</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUN 02-06, 2005</td>
</tr>

<tr>
<td valign="top">CL </td><td>Philadelphia, PA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice; outcomes; quality of life</td>
</tr>

<tr>
<td valign="top">ID </td><td>VALIDATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>The purpose of this study was to investigate the correlation between the Voice Handicap Index (VHI) and the Voice-Related Quality of Life Measure (V-RQOL), and to test conversion of scores between the two instruments. Understanding the relationship between instruments will facilitate comparison of voice outcome studies using different measures. A retrospective medical chart review of 140 consecutive patients with a chief complaint related to their voice presenting for speech pathology voice evaluation following laryngology evaluation and diagnosis was adopted. Each patient who filled out the VHI and V-RQOL within a 2-week period with no intervening treatment was included in the study. Correlation analysis for total scores was performed for the patients meeting inclusion criteria (n = 132). Correlations were also performed as a function of diagnosis. Calculated VHI score based on measured V-RQOL score was compared to measured VHI score. Pearson correlation between scores on the VHI and V-RQOL was -0.82. There was no significant difference between the mean measured and mean calculated VHI scores. For individual scores, however, regression analysis did reveal a significant difference between calculated and measured VHI. The VHI and V-RQOL are highly correlated; however, this study suggests that the two instruments are not interchangeable for individuals.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Emory Univ, Emory Voice Ctr, Dept Otolaryngol, Atlanta, GA 30322 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Johns, MM (reprint author), Emory Univ, Dept Otolaryngol, 550 Peachtree St NE,9th Flr,Suite 4400, Atlanta, GA 30308 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>michael.johns2@emory.edu</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Hapner, Edie</display_name>&nbsp;</font></td><td><font size="3">0000-0001-9666-911X&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>42</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>45</td>
</tr>

<tr>
<td valign="top">PD </td><td>NOV</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">VL </td><td>21</td>
</tr>

<tr>
<td valign="top">IS </td><td>6</td>
</tr>

<tr>
<td valign="top">BP </td><td>723</td>
</tr>

<tr>
<td valign="top">EP </td><td>727</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.jvoice.2006.06.001</td>
</tr>

<tr>
<td valign="top">SC </td><td>Audiology &amp; Speech-Language Pathology; Otorhinolaryngology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000251218500009</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Gugatschka, M
   <br>Rechenmacher, J
   <br>Chibidziura, J
   <br>Friedrich, G</td>
</tr>

<tr>
<td valign="top">AF </td><td>Gugatschka, M.
   <br>Rechenmacher, J.
   <br>Chibidziura, J.
   <br>Friedrich, G.</td>
</tr>

<tr>
<td valign="top">TI </td><td>Comparability and conversion of stimmstoerungsindex (SS) and Voice
   Handicap Index (VHI)</td>
</tr>

<tr>
<td valign="top">SO </td><td>LARYNGO-RHINO-OTOLOGIE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice assessment; voice related quality of life; Voice Handicap Index
   (VHI); Stimmstorungsindex (SSI); dysphonia</td>
</tr>

<tr>
<td valign="top">ID </td><td>VALIDATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Background: The Voice Handicap Index (VHI) has been established internationally as a reliable tool for describing voice-dependent quality of life. Nawka developed a new score by using 12 questions out of the VHI, called SSI (Stimmstorungsindex) [ 1 ]. The SSI has proven its role in daily clinical routine, main problem remains a lack of comparability of the SSI (range: 0-48) to the internationally established VHI (Voice Handicap Index) (range: 0-120). Aim of our study was to demonstrate that by a simple multiplication of SSI scores a statistically significant correlation with scores of the VHI can be achieved.
   <br>Methods: 210 consecutive patients of the ENT University Hospital Graz were evaluated by the German version of VHI. We calculated total score of VHI as well as the score of the SSI. By multiplication of the SSI scores by 2.5 we calculated a new score, called,,VHI-korr". This score was compared with the VHI.
   <br>Results: We can demonstrate that by simple multiplication of the SSI scores by 2.5 a statistically significant correlation to the VHI can be achieved (r = 0.98; p &lt; 0.01).
   <br>Conclusion: The SSI-based,VHI-korr" provides good international comparability of voice disorders with VHI and allows to maintain the common use of SSI in daily clinical routine.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Med Univ Graz, Klin Abt Phoniatrie, HNO Univ Klin Graz, A-8036 Graz,
   Austria.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Gugatschka, M (reprint author), Med Univ Graz, Klin Abt Phoniatrie, HNO Univ Klin Graz, Auenbruggerpl 26-28, A-8036 Graz, Austria.</td>
</tr>

<tr>
<td valign="top">EM </td><td>markus.gugatschka@klinikum-graz.at</td>
</tr>

<tr>
<td valign="top">TC </td><td>8</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>8</td>
</tr>

<tr>
<td valign="top">PD </td><td>NOV</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">VL </td><td>86</td>
</tr>

<tr>
<td valign="top">IS </td><td>11</td>
</tr>

<tr>
<td valign="top">BP </td><td>785</td>
</tr>

<tr>
<td valign="top">EP </td><td>788</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1055/s-2007-966686</td>
</tr>

<tr>
<td valign="top">SC </td><td>Otorhinolaryngology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000250836300011</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Seo, KY
   <br>Oh, SW
   <br>Suh, SH
   <br>Park, GK</td>
</tr>

<tr>
<td valign="top">AF </td><td>Seo, Ki-Yeol
   <br>Oh, Se-Woong
   <br>Suh, Sang-Hyun
   <br>Park, Gyei-Kark</td>
</tr>

<tr>
<td valign="top">TI </td><td>Intelligent steering control system based on voice instructions</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERNATIONAL JOURNAL OF CONTROL AUTOMATION AND SYSTEMS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>fuzzy inference; intelligent steering control system; LIBL; VIBL</td>
</tr>

<tr>
<td valign="top">AB </td><td>The important field of research in ship operation is related to the high efficiency of transportation, the convenience of maneuvering ships and the safety of navigation. For these purposes, many intelligent technologies for ship automation have been required and studied. In this paper, we propose an intelligent voice instruction-based learning (VIBL) method and discuss the building of a ship's steering control system based on this method. The VIBL system concretely consists of two functions: a text conversion function where an instructor's inputted voice is recognized and converted to text, and a linguistic instruction based learning function where the text instruction is understood through a searching process of given meaning elements. As a study method, the fuzzy theory is adopted to build maneuvering models of steersmen and then the existing LIBL is improved and combined with the voice recognition technology to propose the VIBL. The ship steering control system combined with VIBL is tested in a ship maneuvering simulator and its validity is shown.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Korea Ocean Res &amp; Dev Inst, Ocean Explorat Syst Res Div, Taejon 305343,
   South Korea.
   <br>Mokpo Natl Maritime Univ, Dept Maritime Transportat Syst, Mokpo 530729,
   South Korea.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Seo, KY (reprint author), Korea Ocean Res &amp; Dev Inst, Ocean Explorat Syst Res Div, 171 Jang Dong, Taejon 305343, South Korea.</td>
</tr>

<tr>
<td valign="top">EM </td><td>fvito@moeri.re.kr; osw@moeri.re.kr; shsuh@moeri.re.kr; gkpark@mmu.ac.kr</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PD </td><td>OCT</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">VL </td><td>5</td>
</tr>

<tr>
<td valign="top">IS </td><td>5</td>
</tr>

<tr>
<td valign="top">BP </td><td>539</td>
</tr>

<tr>
<td valign="top">EP </td><td>546</td>
</tr>

<tr>
<td valign="top">SC </td><td>Automation &amp; Control Systems</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000249818100007</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hsia, CC
   <br>Wu, CH
   <br>Wu, JQ</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hsia, Chi-Chun
   <br>Wu, Chung-Hsien
   <br>Wu, Jian-Qi</td>
</tr>

<tr>
<td valign="top">TI </td><td>Conversion function clustering and selection using linguistic and
   spectral information for emotional voice conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE TRANSACTIONS ON COMPUTERS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>emotional text-to-speech synthesis; emotional voice conversion;
   linguistic feature; function clustering and selection; Gaussian mixture
   bigram model</td>
</tr>

<tr>
<td valign="top">ID </td><td>PROSODIC INFORMATION; SPEECH SYNTHESIS</td>
</tr>

<tr>
<td valign="top">AB </td><td>In emotional speech synthesis, a large speech database is required for high-quality speech output. Voice conversion needs only a compact-sized speech database for each emotion. This study designs and accumulates a set of phonetically balanced small-sized emotional parallel speech databases to construct conversion functions. The Gaussian mixture bigram model (GMBM) is adopted as the conversion function to characterize the temporal and spectral evolution of the speech signal. The conversion function is initially constructed for each instance of parallel subsyllable pairs in the collected speech database. To reduce the total number of conversion functions and select an appropriate conversion function, this study presents a framework by incorporating linguistic and spectral information for conversion function clustering and selection. Subjective and objective evaluations with statistical hypothesis testing are conducted to evaluate the quality of the converted speech. The proposed method compares favorably with previous methods in conversion-based emotional speech synthesis.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Natl Cheng Kung Univ, Dept Comp Sci &amp; Informat Engn, Tainan 70101,
   Taiwan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hsia, CC (reprint author), Natl Cheng Kung Univ, Dept Comp Sci &amp; Informat Engn, 1 Ta Hsueh Rd, Tainan 70101, Taiwan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>shiacj@csie.ncku.edu.tw; chwu@csie.ncku.edu.tw; glinewu@csie.ncku.edu.tw</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Wu, Chung-Hsien</display_name>&nbsp;</font></td><td><font size="3">E-7970-2013&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>12</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>15</td>
</tr>

<tr>
<td valign="top">PD </td><td>SEP</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">VL </td><td>56</td>
</tr>

<tr>
<td valign="top">IS </td><td>9</td>
</tr>

<tr>
<td valign="top">BP </td><td>1245</td>
</tr>

<tr>
<td valign="top">EP </td><td>1254</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TC.2007.1079</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000248208300010</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kalteis, M
   <br>Pistrich, R
   <br>Schimetta, WF
   <br>Polz, W</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kalteis, Manfred
   <br>Pistrich, Renate
   <br>Schimetta, Wo Fgang
   <br>Poelz, Werner</td>
</tr>

<tr>
<td valign="top">TI </td><td>Laparoscopic cholecystectomy as solo surgery with the aid of a robotic
   camera holder - A case-control study</td>
</tr>

<tr>
<td valign="top">SO </td><td>SURGICAL LAPAROSCOPY ENDOSCOPY &amp; PERCUTANEOUS TECHNIQUES</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article; Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>47th Annual Meeting of the Austrian-Society-of-Surgery</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUN, 2006</td>
</tr>

<tr>
<td valign="top">CL </td><td>Vienna, AUSTRIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>robotics; AESOP robot system; laparoscopic cholecystectomy; solo surgery</td>
</tr>

<tr>
<td valign="top">ID </td><td>AESOP ROBOT; ACUTE CHOLECYSTITIS; VOICE; ENDOASSIST; EXPERIENCE;
   EFFICIENCY; ASSISTANT; SYSTEM; TRIAL; ARM</td>
</tr>

<tr>
<td valign="top">AB </td><td>Background: By using robotic camera holders, a laparoscopic cholecystectomy (LC) is possible as a solo-surgeon operation. The purpose of this paper is to examine the safeness and efficiency of solo-surgeon LCs. Methods: A series of 72 solo-surgeon LCs was retrospectively compared with a control cohort (matched pairs). Efficiency and safety parameters were compared by means of equivalence tests (scope = 10%). Results: Nearly identical incision-suture times (means: 69.6 vs. 70.7-min) were recorded. An equivalence was also found in the cohorts for the total time in the operating room (means: 117.4 vs. 117.2min). In terms of the rate of complications, the perioperative difference in hemoglobin, and the conversion rate, the robot cohort proved to be at least equal to the control cohort. The postoperative hospital stay was shorter for the robot cohort. Conclusion: Solo-surgeon LC with a robotic camera holder is an efficient and safe method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>St Franziskus Hosp, Dept Surg, Grieskirchen, Austria.
   <br>Johannes Kepler Univ Linz, Int Inst Appl Syst Res &amp; Stat, A-4040 Linz,
   Austria.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kalteis, M (reprint author), Krankenhaus Grieskirchen, Wagnleithnerst 27, A-4710 Grieskirchen, Austria.</td>
</tr>

<tr>
<td valign="top">EM </td><td>manfred.kalteis@aon.at</td>
</tr>

<tr>
<td valign="top">TC </td><td>10</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>10</td>
</tr>

<tr>
<td valign="top">PD </td><td>AUG</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">VL </td><td>17</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">BP </td><td>277</td>
</tr>

<tr>
<td valign="top">EP </td><td>282</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1097/SLE.0b013e31806030ae</td>
</tr>

<tr>
<td valign="top">SC </td><td>Surgery</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000248999500007</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>McInnes, RJ
   <br>Wright, C
   <br>Haq, S
   <br>McGranachan, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>McInnes, Rhona J.
   <br>Wright, Charlotte
   <br>Haq, Shogufta
   <br>McGranachan, Margaret</td>
</tr>

<tr>
<td valign="top">TI </td><td>Who's keeping the code? Compliance with the international code for the
   marketing of breast-milk substitutes in Greater Glasgow</td>
</tr>

<tr>
<td valign="top">SO </td><td>PUBLIC HEALTH NUTRITION</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>infant feeding; breast-milk substitutes; WHO Code; policy; primary care</td>
</tr>

<tr>
<td valign="top">ID </td><td>PHARMACEUTICAL-INDUSTRY; PHYSICIANS; PREVALENCE; COMPANIES</td>
</tr>

<tr>
<td valign="top">AB </td><td>Objective: To evaluate compliance with the World Health Organization's International Code of Marketing of Breast-milk Substitutes in primary care, after the introduction of strict local infant feeding guidelines.
   <br>Design. An audit form was sent to all community-based health professionals with an infant feeding remit. Walking tours were conducted in a random sample of community care facilities.
   <br>Setting: Greater Glasgow, Primary Care Division.
   <br>Subjects: (1) Primary-care staff with an infant feeding remit; (2) community health-care facilities.
   <br>Main outcome measures: Contact with manufacturers of breast-milk substitutes (BMS) and BMS company personnel, free samples or incentives, and advertising of BMS.
   <br>Results: Contact with company personnel was minimal, usually unsolicited and was mainly to provide product information. Free samples of BMS or feeding equipment were rare but childcare or parenting literature was more prevalent. Staff voiced concerns about the lack of relevant information for bottle-feeding mothers and the need to Support the mother's feeding choice. one-third of facilities were still displaying materials non-compliant with the Code, with the most common materials being. weight conversion charts and posters.
   <br>Conclusions: Contact between personnel from primary care and BMS companies was minimal and generally unsolicited. The presence of materials from BMS companies in health-care premises was more common. Due to the high level of bottle-feeding in Glasgow, primary-care staff stated a need for information about BMS.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Univ Stirling, Nursing Midwifery &amp; Allied Hlth Profess Res Unit,
   Stirling FK9 4LA, Scotland.
   <br>Univ Glasgow, Royal Hosp Sick Children, Paediat Epidemiol &amp; Child Hlth
   Unit, Dept Child Hlth, Glasgow G3 8SJ, Lanark, Scotland.
   <br>Stobhill Gen Hosp, Child &amp; Youth Hlth Promot Team, N Community Hlth &amp;
   Care Partnership, Glasgow G21 3UR, Lanark, Scotland.
   <br>NHS Greater Glasgow &amp; Clyde, Publ Hlth Resource Unit, Glasgow G3 8YY,
   Lanark, Scotland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>McInnes, RJ (reprint author), Univ Stirling, Nursing Midwifery &amp; Allied Hlth Profess Res Unit, Iris Murdoch Bldg, Stirling FK9 4LA, Scotland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>rjm2@stir.ac.uk</td>
</tr>

<tr xmlns:date="http://exslt.org/dates-and-times">
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>McInnes, Rhona</display_name>&nbsp;</font></td><td><font size="3">0000-0003-0938-2861&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>5</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>5</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUL</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">VL </td><td>10</td>
</tr>

<tr>
<td valign="top">IS </td><td>7</td>
</tr>

<tr>
<td valign="top">BP </td><td>719</td>
</tr>

<tr>
<td valign="top">EP </td><td>725</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1017/S1368980007441453</td>
</tr>

<tr>
<td valign="top">SC </td><td>Public, Environmental &amp; Occupational Health; Nutrition &amp; Dietetics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000247779600011</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Mouchtaris, A
   <br>Van der Spiegel, J
   <br>Mueller, P
   <br>Tsakalides, P</td>
</tr>

<tr>
<td valign="top">AF </td><td>Mouchtaris, Athanasios
   <br>Van der Spiegel, Jan
   <br>Mueller, Paul
   <br>Tsakalides, Panagiotis</td>
</tr>

<tr>
<td valign="top">TI </td><td>A spectral conversion approach to single-channel speech enhancement</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Gaussian mixture model (GMM); parameter adaptation; spectral conversion;
   speech enhancement</td>
</tr>

<tr>
<td valign="top">ID </td><td>HIDDEN MARKOV-MODELS; AMPLITUDE ESTIMATOR; VOICE CONVERSION; NOISE;
   RECOGNITION; SUPPRESSION; ADAPTATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, a novel method for single-channel speech enhancement is proposed, which is based on a spectral conversion feature denoising approach. Spectral conversion has been applied previously in the context of voice conversion, and has been shown to successfully transform spectral features with particular statistical properties into spectral features that best fit (with the constraint of a piecewise linear transformation) different target statistics. This spectral transformation is applied as an initialization step to two well-known single channel enhancement methods, namely the iterative Wiener filter (IWF) and a particular iterative implementation of the Kalman filter. In both cases, spectral conversion is shown here to provide a significant improvement as opposed to initializations using the spectral features directly from the noisy speech. In essence, the proposed approach allows for applying these two algorithms in a user-centric manner, when "clean" speech training data are available from a particular speaker. The extra step of spectral conversion is shown to offer significant advantages regarding output signal-to-noise ratio (SNR) improvement over the conventional initializations, which can reach 2 dB for the IWF and 6 dB for the Kalman filtering algorithm, for low input SNRs and for white and colored noise, respectively.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Univ Crete, Dept Comp Sci, GR-71409 Iraklion, Crete, Greece.
   <br>Fdn Res &amp; Technol Hellas, FORTH, ICS, GR-71110 Iraklion, Crete, Greece.
   <br>Univ Penn, Dept Elect &amp; Syst Engn, Philadelphia, PA 19104 USA.
   <br>Corticon Inc, King Of Prussia, PA 19406 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Mouchtaris, A (reprint author), Univ Crete, Dept Comp Sci, GR-71409 Iraklion, Crete, Greece.</td>
</tr>

<tr>
<td valign="top">EM </td><td>mouchtar@ieee.org; jan@seas.upenn.edu; cortion@aol.com;
   tsakalid@ics.forth.gr</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Tsakalides, Panagiotis</display_name>&nbsp;</font></td><td><font size="3">O-2063-2015&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Tsakalides, Panagiotis</display_name>&nbsp;</font></td><td><font size="3">0000-0003-4918-603X&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>10</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>11</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAY</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">VL </td><td>15</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">BP </td><td>1180</td>
</tr>

<tr>
<td valign="top">EP </td><td>1193</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASL.2007.894511</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000245909800006</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Lee, KS</td>
</tr>

<tr>
<td valign="top">AF </td><td>Lee, Ki-Seung</td>
</tr>

<tr>
<td valign="top">TI </td><td>Statistical approach for voice personality transformation</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>maximum likelihood (ML) estimation; prosody modification; voice
   conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH ENHANCEMENT; CONVERSION; ALGORITHM; NETWORKS</td>
</tr>

<tr>
<td valign="top">AB </td><td>A voice transformation method which changes the source speaker's utterances so as to sound similar to those of a target speaker is described. Speaker individuality transformation is achieved by altering the LPC cepstrum, average pitch period and average speaking rate. The main objective of the work involves building a nonlinear relationship between the parameters for the acoustical features of two speakers, based on a probabilistic model. The conversion rules involve the probabilistic classification and a cross correlation probability between the acoustic features of the two speakers. The parameters of the conversion rules are estimated by estimating the maximum likelihood of the training data. To obtain transformed speech signals which are perceptually closer to the target speaker's voice, prosody modification is also involved. Prosody modification is achieved by scaling excitation spectrum and time scale modification with appropriate modification factors. An evaluation by objective tests and informal listening tests clearly indicated the effectiveness of the proposed transformation method. We also confirmed that the proposed method leads to smoothly evolving spectral contours over time, which, from a perceptual standpoint, produced results that were superior to conventional vector quantization (VQ)-based methods.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Konkuk Univ, Dept Elect Engn, Seoul 143701, South Korea.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Lee, KS (reprint author), Konkuk Univ, Dept Elect Engn, Seoul 143701, South Korea.</td>
</tr>

<tr>
<td valign="top">EM </td><td>kseung@konkuk.ac.kr</td>
</tr>

<tr>
<td valign="top">TC </td><td>25</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>28</td>
</tr>

<tr>
<td valign="top">PD </td><td>FEB</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">VL </td><td>15</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>641</td>
</tr>

<tr>
<td valign="top">EP </td><td>651</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASL.2006.876760</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000243914800025</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Yan, Q
   <br>Vaseghi, S
   <br>Rentzos, D
   <br>Ho, CH</td>
</tr>

<tr>
<td valign="top">AF </td><td>Yan, Qin
   <br>Vaseghi, Saeed
   <br>Rentzos, Dimitrios
   <br>Ho, Ching-Hsiang</td>
</tr>

<tr>
<td valign="top">TI </td><td>Analysis and synthesis of formant spaces, of British, Australian, and
   American accents</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>formant; hidden Markov model (HMM); linear prediction (LP)</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION; ENGLISH; CLASSIFICATION; TRANSFORMATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, the probability distribution functions (pdf's) of the formant spaces of three major accents of the English language, namely, British Received Pronunciation. (RP), General American, and Broad Australian, are modeled and compared. The statistical differences across the formant spaces of these accents are employed for accent conversion. An improved formant tracking method, based on linear prediction (LP) feature analysis and a two-dimensional hidden Markov model (2-D-HMM) of format trajectories, is used for estimation of the formant trajectories of vowels and diphthongs of each accent. Comparative analysis of the formant spaces of the three accents indicates that these accents are partly conveyed by the differences of the formants of vowels. The estimates of the probability distributions of the formants for each accent are used in a speech synthesis system for accent conversion. Accent synthesis, through modification of the acoustic parameters of speech, provides a means of assessing the perceptual contribution of each formant parameter on conveying an accent. The results of perceptual evaluations of accent conversion illustrate that formants play an important role in conveying accents.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Chinese Acad Sci, Inst Acoust, Beijing 100080, Peoples R China.
   <br>Brunel Univ, Dept Elect &amp; Comp Engn, Uxbridge UB8 3PH, Middx, England.
   <br>IBM Germany Res &amp; Dev, Voice Technol Dev Lab, D-71032 Bblingen, Germany.
   <br>Fortune Inst Technol, Kaohsiung 831, Taiwan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Yan, Q (reprint author), Chinese Acad Sci, Inst Acoust, Beijing 100080, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>yanqin@ieee.org; saeed.vaseghi@brunel.ac.uk; drentzos@de.ibm.com;
   ch.ho@center.fjtc.edu.tw</td>
</tr>

<tr>
<td valign="top">TC </td><td>15</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>15</td>
</tr>

<tr>
<td valign="top">PD </td><td>FEB</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">VL </td><td>15</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>676</td>
</tr>

<tr>
<td valign="top">EP </td><td>689</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASL.2006.885923</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000243914800028</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Yamagishi, J
   <br>Kobayashi, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Yamagishi, Junichi
   <br>Kobayashi, Takao</td>
</tr>

<tr>
<td valign="top">TI </td><td>Average-voice-based speech synthesis using HSMM-based speaker adaptation
   and adaptive training</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEICE TRANSACTIONS ON INFORMATION AND SYSTEMS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>HMM-based speech synthesis; speaker adaptation; speaker adaptive
   training (SAT); hidden semi-Markov model (HSMM); maximum likelihood
   linear regression (MLLR); voice conversion</td>
</tr>

<tr>
<td valign="top">AB </td><td>In speaker adaptation for speech synthesis, it is desirable to convert both voice characteristics and prosodic features such as F0 and phone duration. For simultaneous adaptation of spectrum, F0 and phone duration within the HMM framework, we need to transform not only the state output distributions corresponding to spectrum and F0 but also the duration distributions corresponding to phone duration. However, it is not straightforward to adapt the state duration because the original HMM does not have explicit duration distributions. Therefore, we utilize the framework of the hidden semi-Markov model (HSMM), which is an HMM having explicit state duration distributions, and we apply an HSMM-based model adaptation algorithm to simultaneously transform both the state output and state duration distributions. Furthermore, we propose an HSMM-based adaptive training algorithm to simultaneously normalize the state output and state duration distributions of the average voice model. We incorporate these techniques into our HSMM-based speech synthesis system, and show their effectiveness from the results of subjective and objective evaluation tests.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Tokyo Inst Technol, Interdisciplinary Grad Sch Sci &amp; Engn, Yokohama,
   Kanagawa 2268502, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Yamagishi, J (reprint author), Tokyo Inst Technol, Interdisciplinary Grad Sch Sci &amp; Engn, Yokohama, Kanagawa 2268502, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>junichi.yamagishi@ip.titech.ac.jp; takao.kobayashi@ip.titech.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>109</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>110</td>
</tr>

<tr>
<td valign="top">PD </td><td>FEB</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">VL </td><td>E90D</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>533</td>
</tr>

<tr>
<td valign="top">EP </td><td>543</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1093/ietisy/e90-d.2.533</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000244546400019</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hoshino, O</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hoshino, Osamu</td>
</tr>

<tr>
<td valign="top">TI </td><td>Spatiotemporal conversion of auditory information for cochleotopic
   mapping</td>
</tr>

<tr>
<td valign="top">SO </td><td>NEURAL COMPUTATION</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">ID </td><td>INFERIOR COLLICULUS; REPRESENTATION; CORTEX; PERCEPTION; WHISTLE;
   MONKEYS; SPEECH; VOICE</td>
</tr>

<tr>
<td valign="top">AB </td><td>Auditory communication signals such as monkey calls are complex FM vocal sounds and in general induce action potentials in different timing in the primary auditory cortex. Delay line scheme is one of the effective ways for detecting such neuronal timing. However, the scheme is not straightforwardly applicable if the time intervals of signals are beyond the latency time of delay lines. In fact, monkey calls are often expressed in longer time intervals (hundreds of milliseconds to seconds) and are beyond the latency times observed in the brain (less than several hundreds of milliseconds). Here, we propose a cochleotopic map similar to that in vision known as a retinotopic map. We show that information about monkey calls could be mapped on a cochleotopic cortical network as spatiotemporal firing patterns of neurons, which can then be decomposed into simple (linearly sweeping) FM components and integrated into unified percepts by higher cortical networks. We suggest that the spatiotemporal conversion of auditory information may be essential for developing the cochleotopic map, which could serve as the foundation for later processing, or monkey call identification by higher cortical areas.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Ibaraki Univ, Dept Intelligent Syst Engn, Hitachi, Ibaraki 3168511,
   Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hoshino, O (reprint author), Ibaraki Univ, Dept Intelligent Syst Engn, Hitachi, Ibaraki 3168511, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>hoshino@mx.ibaraki.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>8</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>8</td>
</tr>

<tr>
<td valign="top">PD </td><td>FEB</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">VL </td><td>19</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>351</td>
</tr>

<tr>
<td valign="top">EP </td><td>370</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1162/neco.2007.19.2.351</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Neurosciences &amp; Neurology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000243524000003</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Mouchtaris, A
   <br>Agiomyrgiannakis, Y
   <br>Stylianou, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Mouchtaris, A.
   <br>Agiomyrgiannakis, Y.
   <br>Stylianou, Y.</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Conditional vector quantization for voice conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>2007 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING, VOL IV, PTS 1-3</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>32nd IEEE International Conference on Acoustics, Speech and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 15-20, 2007</td>
</tr>

<tr>
<td valign="top">CL </td><td>Honolulu, HI</td>
</tr>

<tr>
<td valign="top">DE </td><td>spectral analysis; speech synthesis; vector quantization; voice
   conversion</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion methods have the objective of transforming speech spoken by a particular source speaker, so that it sounds as if spoken by a different target speaker. The majority of voice conversion methods is based on transforming the short-time spectral envelope of the source speaker, based on derived correspondences between the source and target vectors using training speech data from both speakers. These correspondences are usually obtained by segmenting the spectral vectors of one or both speakers into clusters, using soft (GMM-based) or hard (VQ-based) clustering. Here, we propose that voice conversion performance can be improved by taking advantage of the fact that often the relationship between the source and target vectors is one-to-many. In order to illustrate this, we propose that a VQ approach namely constrained vector quantization (CVQ), can be used for voice conversion. Results indicate that indeed such a relationship between the source and target data exists and can be exploited by following a CVQ-based function for voice conversion.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Mouchtaris, A.; Agiomyrgiannakis, Y.; Stylianou, Y.] Univ Crete, Dept
   Comp Sci, Multimedia Informat Lab, Iraklion, Crete, Greece.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Mouchtaris, A (reprint author), Univ Crete, Dept Comp Sci, Multimedia Informat Lab, Iraklion, Crete, Greece.</td>
</tr>

<tr>
<td valign="top">EM </td><td>mouchtar@csd.uoc.gr; jagiom@csd.uoc.gr; yannis@csd.uoc.gr</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">BP </td><td>505</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000248909200127</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Helander, EE
   <br>Nurminen, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Helander, Elina E.
   <br>Nurminen, Jani</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>A novel method for prosody prediction in voice conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>2007 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING, VOL IV, PTS 1-3</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>32nd IEEE International Conference on Acoustics, Speech and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 15-20, 2007</td>
</tr>

<tr>
<td valign="top">CL </td><td>Honolulu, HI</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; prosody conversion; prosodic codebook</td>
</tr>

<tr>
<td valign="top">AB </td><td>Most of the published voice conversion schemes do not consider detailed prosody modeling but only control the F0 level and range. However, the detailed prosody can also carry a significant amount of speaker identity related information. This paper introduces a new method for converting the prosody in voice conversion. A syllable-based prosodic codebook is used to predict the converted F0 using not only the source contour but also linguistic information and segmental durations. The selection of the most suitable target contour is carried out using a trained classification and regression tree. The F0 contours in the codebook are represented in a transformed domain which allows compression and fast comparison. The performance of the prosodic conversion is evaluated in a real voice conversion system. The results indicate a significant improvement in speaker identity and naturalness when compared to GMM (Gaussian mixture model) based pitch prediction approach.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Helander, Elina E.] Tampere Univ Technol, Inst Signal Proc, FIN-33101
   Tampere, Finland.
   <br>[Nurminen, Jani] Nokia Res Ctr, Multimedia Technol Lab, Cambridge,
   England.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Helander, EE (reprint author), Tampere Univ Technol, Inst Signal Proc, FIN-33101 Tampere, Finland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>elina.helander@tut.fi; jani.k.nurminen@nokia.com</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Helander, Elina</display_name>&nbsp;</font></td><td><font size="3">0000-0002-0076-0590&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>13</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>13</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">BP </td><td>509</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000248909200128</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Dutoit, T
   <br>Holzapfel, A
   <br>Jottrand, M
   <br>Moinet, A
   <br>Perez, J
   <br>Stylianou, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Dutoit, T.
   <br>Holzapfel, A.
   <br>Jottrand, M.
   <br>Moinet, A.
   <br>Perez, J.
   <br>Stylianou, Y.</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Towards a voice conversion system based on frame selection</td>
</tr>

<tr>
<td valign="top">SO </td><td>2007 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING, VOL IV, PTS 1-3</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>32nd IEEE International Conference on Acoustics, Speech and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 15-20, 2007</td>
</tr>

<tr>
<td valign="top">CL </td><td>Honolulu, HI</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; frame selection; voice mapping</td>
</tr>

<tr>
<td valign="top">AB </td><td>The subject of this paper is the conversion of a given speaker's voice (the source speaker) into another identified voice (the target one). We assume we have at our disposal a large amount of speech samples from source and target voice with at least a part of them being parallel. The proposed system is built on a mapping function between source and target spectral envelopes followed by a frame selection algorithm to produce final spectral envelopes. Converted speech is produced by a basic LP analysis of the source and LP synthesis using the converted spectral envelopes. We compared three types of conversion: without mapping, with mapping and using the excitation of the source speaker and finally with mapping using the excitation of the target. Results show that the combination of mapping and frame selection provide the best results, and underline the interest to work on methods to convert the LP excitation.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Dutoit, T.; Jottrand, M.; Moinet, A.] Fac Polytech Mons, Mons, Belgium.
   <br>[Holzapfel, A.; Stylianou, Y.] Univ Crete, Iraklion, Greece.
   <br>[Perez, J.] Univ Politecn Cataluna, E-08028 Barcelona, Spain.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Dutoit, T (reprint author), Fac Polytech Mons, Mons, Belgium.</td>
</tr>

<tr>
<td valign="top">TC </td><td>14</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>14</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">BP </td><td>513</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000248909200129</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hsia, CC
   <br>Wu, CH
   <br>Wu, JQ</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hsia, Chi-Chun
   <br>Wu, Chung-Hsien
   <br>Wu, Jian-Qi</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Conversion function clustering and selection for expressive voice
   conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>2007 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING, VOL IV, PTS 1-3</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>32nd IEEE International Conference on Acoustics, Speech and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 15-20, 2007</td>
</tr>

<tr>
<td valign="top">CL </td><td>Honolulu, HI</td>
</tr>

<tr>
<td valign="top">DE </td><td>speech synthesis; voice conversion; Gaussian mixture bi-gram model;
   linguistic information; expression</td>
</tr>

<tr>
<td valign="top">ID </td><td>PROSODIC INFORMATION; SPEECH</td>
</tr>

<tr>
<td valign="top">AB </td><td>in this study, a conversion function clustering and selection approach to conversion-based expressive speech synthesis is proposed. First, a set of small-sized emotional parallel speech databases is designed and collected to train the conversion functions. Gaussian mixture bi-gram model (GMBM) is adopted as the conversion function to model the temporal and spectral evolution of speech. Conversion functions initially constructed from the parallel sub-syllable pairs in the speech database are clustered based on linguistic and spectral information. Subjective and objective evaluations with statistical hypothesis testing were conducted to evaluate the quality of the converted speech. The results show that the proposed method exhibits encouraging potential in conversion-based expressive speech synthesis.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Hsia, Chi-Chun; Wu, Chung-Hsien; Wu, Jian-Qi] Natl Cheng Kung Univ,
   Dept Comp Sci &amp; Informat Engn, Tainan 70101, Taiwan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hsia, CC (reprint author), Natl Cheng Kung Univ, Dept Comp Sci &amp; Informat Engn, Tainan 70101, Taiwan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>shiacj@csie.ncku.edu.tw; chwu@csie.ncku.edu.tw; glinwu@csie.ncku.edu.tw</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">BP </td><td>689</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000248909200173</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Masuda, T
   <br>Shozakai, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Masuda, Tsuyoshi
   <br>Shozakai, Makoto</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Cost reduction of training mapping function based on multistep voice
   conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>2007 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING, VOL IV, PTS 1-3</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>32nd IEEE International Conference on Acoustics, Speech and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 15-20, 2007</td>
</tr>

<tr>
<td valign="top">CL </td><td>Honolulu, HI</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; speech synthesis; training cost; multistep voice
   conversion</td>
</tr>

<tr>
<td valign="top">AB </td><td>Several approaches based on a statistical method for voice conversion from one speaker to another have been developed. In a statistical spectral mapping method which is a typical one in these approaches, a mapping function which represents a correlation between different speakers is determined using spectral features. This technique has the problem that it is necessary to train the mapping function for each speaker pair. The training cost must become a serious issue in case that the number of speakers increases significantly.
   <br>This paper describes a novel voice conversion method for reducing the training cost. This technique is easily implemented and can use conventional techniques directly. Experimental results demonstrate that the converted speech is almost maintaining the conventional quality despite the significant training cost reduction by the proposed method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Masuda, Tsuyoshi; Shozakai, Makoto] Asahi Kasei Corp, Speech Solut, New
   Business Dev, Atsugi AXT Maintower 22F,3050 Okata, Kanagawa 2430021,
   Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Masuda, T (reprint author), Asahi Kasei Corp, Speech Solut, New Business Dev, Atsugi AXT Maintower 22F,3050 Okata, Kanagawa 2430021, Japan.</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">BP </td><td>693</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000248909200174</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ni, JF
   <br>Nakamura, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ni, Jinfu
   <br>Nakamura, Satoshi</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Use of poisson processes to generate fundamental frequency contours</td>
</tr>

<tr>
<td valign="top">SO </td><td>2007 IEEE International Conference on Acoustics, Speech, and Signal
   Processing, Vol IV, Pts 1-3</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   (ICASSP)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>32nd IEEE International Conference on Acoustics, Speech and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 15-20, 2007</td>
</tr>

<tr>
<td valign="top">CL </td><td>Honolulu, HI</td>
</tr>

<tr>
<td valign="top">DE </td><td>prosody modeling; Poisson distributions; voice conversion; speech
   synthesis; speech processing</td>
</tr>

<tr>
<td valign="top">ID </td><td>INTONATION; MANDARIN</td>
</tr>

<tr>
<td valign="top">AB </td><td>The prosodic contributions to voice fundamental frequency (F-0) contours can be analyzed into a series of sparser tonal targets (F-0 peaks and valleys). The transitions through these targets are interpolated by spline or filtering functions to predict the shape of F-0 contours. A functional model was proposed in the previous work for this purpose. This paper presents an enhanced version of this model achieved by replacing its decay filter with a Poisson-process-induced filter. It is enhanced because the former is a special case of the latter. The new filter manages to delay the decaying process while interpolations are being uttered. A target point can thus act as target levels, if necessary. The algorithms for estimating parameters, which were implemented on computers, are also presented. Experiments conducted on thousands of observed F-0 contours, including Mandarin, Japanese, and English, indicate that the enhanced version significantly facilitates their automatic parameterization.</td>
</tr>

<tr>
<td valign="top">TC </td><td>5</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>5</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">BP </td><td>825</td>
</tr>

<tr>
<td valign="top">EP </td><td>828</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000248909200207</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Yamagishi, J
   <br>Kobayashi, T
   <br>Tachibana, M
   <br>Ogata, K
   <br>Nakano, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Yamagishi, Junichi
   <br>Kobayashi, Takao
   <br>Tachibana, Makoto
   <br>Ogata, Katsumi
   <br>Nakano, Yuji</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Model adaptation approach to speech synthesis with diverse voices and
   styles</td>
</tr>

<tr>
<td valign="top">SO </td><td>2007 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING, VOL IV, PTS 1-3</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>32nd IEEE International Conference on Acoustics, Speech and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 15-20, 2007</td>
</tr>

<tr>
<td valign="top">CL </td><td>Honolulu, HI</td>
</tr>

<tr>
<td valign="top">DE </td><td>HMM; speech synthesis; average voice; speaker adaptation; voice
   conversion</td>
</tr>

<tr>
<td valign="top">AB </td><td>In human computer interaction and dialogue systems, it is often desirable for text-to-speech synthesis to be able to generate natural sounding speech with an arbitrary speaker's voice and with varying speaking styles and/or emotional expressions. We have developed an average-voice-based speech synthesis method using statistical average voice models and model adaptation techniques for this purpose. In this paper, we describe an overview of the speech synthesis system and show the current performance with several experimental results.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Yamagishi, Junichi; Kobayashi, Takao; Tachibana, Makoto; Ogata,
   Katsumi; Nakano, Yuji] Tokyo Inst Technol, Interdisciplinary Grad Sch
   Sci &amp; Engn, Yokohama, Kanagawa 2268502, Japan.
   <br>[Yamagishi, Junichi] Univ Edinburgh, Cent Speech Technol Res, Edinburgh
   EH8 9YL, Midlothian, Scotland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Yamagishi, J (reprint author), Tokyo Inst Technol, Interdisciplinary Grad Sch Sci &amp; Engn, Yokohama, Kanagawa 2268502, Japan.</td>
</tr>

<tr>
<td valign="top">TC </td><td>7</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>7</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">BP </td><td>1233</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000248909200309</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Toda, T
   <br>Ohtani, Y
   <br>Shikano, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Toda, Tomoki
   <br>Ohtani, Yamato
   <br>Shikano, Kiyohiro</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>One-to-many and many-to-one voice conversion based on eigenvoices</td>
</tr>

<tr>
<td valign="top">SO </td><td>2007 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING, VOL IV, PTS 1-3</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>32nd IEEE International Conference on Acoustics, Speech and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 15-20, 2007</td>
</tr>

<tr>
<td valign="top">CL </td><td>Honolulu, HI</td>
</tr>

<tr>
<td valign="top">DE </td><td>speech synthesis; voice conversion; eigenvoice; one-to-many; many-to-one</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper describes two flexible frameworks of voice conversion (VC), i.e., one-to-many VC and many-to-one VC. One-to-many VC realizes the conversion from a user's voice as a source to arbitrary target speakers' ones and many-to-one VC realizes the conversion vice versa. We apply eigenvoice conversion (EVC) to both VC frameworks. Using multiple parallel data sets consisting of utterance-pairs of the user and multiple pre-stored speakers, an eigenvoice Gaussian mixture model (EV-GMM) is trained in advance. Unsupervised adaptation of the EV-GMM is available to construct the conversion model for arbitrary target speakers in one-to-many VC or arbitrary source speakers in many-to-one VC using only a small amount of their speech data. Results of various experimental evaluations demonstrate the effectiveness of the proposed VC frameworks.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Toda, Tomoki; Ohtani, Yamato; Shikano, Kiyohiro] Nara Inst Sci &amp;
   Technol, Grad Sch Informat Sci, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Toda, T (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>tomoki@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>25</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>25</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">BP </td><td>1249</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000248909200313</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Makki, B
   <br>Seyedsalehi, SA
   <br>Sadati, N
   <br>Hosseini, MN</td>
</tr>

<tr>
<td valign="top">AF </td><td>Makki, B.
   <br>Seyedsalehi, S. A.
   <br>Sadati, N.
   <br>Hosseini, M. Noori</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice conversion using nonlinear principal component analysis</td>
</tr>

<tr>
<td valign="top">SO </td><td>2007 IEEE SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE IN IMAGE AND SIGNAL
   PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE Symposium on Computational Intelligence in Image and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 01-05, 2007</td>
</tr>

<tr>
<td valign="top">CL </td><td>Honolulu, HI</td>
</tr>

<tr>
<td valign="top">AB </td><td>In the last decades, much attention has been paid to the design of multi-speaker voice conversion. In this work, a new method for voice conversion (VC) using nonlinear principal component analysis (NLPCA) is presented. The principal components are extracted and transformed by a feed-forward neural network which is trained by combination of Genetic Algorithm (GA) and Back-Propagation (BP). Common pre- and post-processing approaches are applied to increase the quality of the synthesized speech. The results indicate that the proposed method can be considered as a step towards multi-speaker voice conversion.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Makki, B.; Seyedsalehi, S. A.; Hosseini, M. Noori] Amirkabir Univ
   Technol, Dept Biomed Engn, POB 15875-4413, Tehran, Iran.
   <br>[Sadati, N.] Sharif Univ Technol, Dept Elect Engn, Tchran, Iran.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Makki, B (reprint author), Amirkabir Univ Technol, Dept Biomed Engn, POB 15875-4413, Tehran, Iran.</td>
</tr>

<tr>
<td valign="top">EM </td><td>behrooz.makki@yahoo.com; ssalehi@cic.aut.ac.ir; sadai@sina.sharif.edu;
   monanoori@gmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">BP </td><td>336</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/CIISP.2007.369191</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Imaging Science &amp; Photographic Technology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000252299800057</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Li, X
   <br>Gunawardana, A
   <br>Acero, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Li, Xiao
   <br>Gunawardana, Asela
   <br>Acero, Alex</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Adapting grapheme-to-phoneme conversion for name recognition</td>
</tr>

<tr>
<td valign="top">SO </td><td>2007 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING,
   VOLS 1 AND 2</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE Workshop on Automatic Speech Recognition and Understanding</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 09-13, 2007</td>
</tr>

<tr>
<td valign="top">CL </td><td>Kyoto, JAPAN</td>
</tr>

<tr>
<td valign="top">DE </td><td>grapheme-to-phoneme conversion; pronunciation model; name recognition;
   discriminative training</td>
</tr>

<tr>
<td valign="top">AB </td><td>This work investigates the use of acoustic data to improve grapheme-to-phoneme conversion for name recognition. We introduce a joint model of acoustics and graphonemes, and present two approaches, maximum likelihood training and discriminative training, in adapting graphoneme model parameters. Experiments on a large-scale voice-dialing system show that the maximum likelihood approach yields a relative 7% reduction in SER compared to the best baseline result we obtained without leveraging acoustic data, while discriminative training enlarges the SER reduction to 12%.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Li, Xiao; Gunawardana, Asela; Acero, Alex] Microsoft Res, Redmond, WA
   98052 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Li, X (reprint author), Microsoft Res, 1 Microsoft Way, Redmond, WA 98052 USA.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">BP </td><td>130</td>
</tr>

<tr>
<td valign="top">EP </td><td>135</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000255861600023</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Jian, ZH
   <br>Yang, Z</td>
</tr>

<tr>
<td valign="top">AF </td><td>Jian, Zhi-Hua
   <br>Yang, Zhen</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Conversion without Parallel Speech Corpus Based on Mixtures of
   Linear Transform</td>
</tr>

<tr>
<td valign="top">SO </td><td>2007 INTERNATIONAL CONFERENCE ON WIRELESS COMMUNICATIONS, NETWORKING AND
   MOBILE COMPUTING, VOLS 1-15</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Wireless Communications, Networking and
   Mobile Computing</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>3rd International Conference on Wireless Communications, Networking and
   Mobile Computing (WiCOM 2007)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 21-25, 2007</td>
</tr>

<tr>
<td valign="top">CL </td><td>Shanghai, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; multimedia application; Ms-LT; EM algorithm</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents an algorithm for voice conversion based on mixtures of linear transform (Ms-LT) which avoids the need for parallel training data inherent in conventional approaches. In maximum likelihood framework, the EM algorithm is used to compute the parameters of the conversion function. And the chirp z-transform is utilized to enhance the averaged spectral envelop due to the linear weighting. The proposed voice conversion system is evaluated using both objective and subjective measures. The experimental results demonstrate that our approach is capable of effectively transforming speaker identity and can achieve comparable results of the conventional methods where a parallel corpus exists.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Jian, Zhi-Hua; Yang, Zhen] Nanjing Univ Post &amp; Telecommun, Inst Signal
   Proc &amp; Transmiss, Nanjing, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Jian, ZH (reprint author), Nanjing Univ Post &amp; Telecommun, Inst Signal Proc &amp; Transmiss, Nanjing, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jianzh@njupt.edu.cn; yangz@njupt.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">BP </td><td>2825</td>
</tr>

<tr>
<td valign="top">EP </td><td>2828</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/WICOM.2007.701</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000262098302001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Jian, ZH
   <br>Yang, Z</td>
</tr>

<tr>
<td valign="top">AF </td><td>Jian Zhi-Hua
   <br>Yang Zhen</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice conversion using Viterbi algorithm based on Gaussian mixture model</td>
</tr>

<tr>
<td valign="top">SO </td><td>2007 INTERNATIONAL SYMPOSIUM ON INTELLIGENT SIGNAL PROCESSING AND
   COMMUNICATION SYSTEMS, VOLS 1 AND 2</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Symposium on Intelligent Signal Processing and
   Communication Systems-ISPACS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Symposium on Intelligent Signal Processing and
   Communication Systems</td>
</tr>

<tr>
<td valign="top">CY </td><td>NOV 28-DEC 01, 2007</td>
</tr>

<tr>
<td valign="top">CL </td><td>Xiamen, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>speech processing; voice conversion; Viterbi algorithm; Gaussian mixture
   model</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH</td>
</tr>

<tr>
<td valign="top">AB </td><td>A novel method for voice conversion based on Viterbi algorithm is proposed in this paper, which uses the matrix of transition probabilities of the target speaker to represent the relationship between the subsequent frames, and then determines the most appropriate component of the GMM by utilizing the Viterbi algorithm for each frame of the source speaker. It avoids the spectral discontinuities between adjacent frames and the spectral averaging. Both objective and subjective evaluations have demonstrated that the proposed method improves the performance of the conventional voice conversion system based on GMM.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Jian Zhi-Hua; Yang Zhen] Nanjing Univ Post &amp; Telecom, Inst Signal Proc
   &amp; Transmiss, Nanjing, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Jian, ZH (reprint author), Nanjing Univ Post &amp; Telecom, Inst Signal Proc &amp; Transmiss, Nanjing, Peoples R China.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">BP </td><td>40</td>
</tr>

<tr>
<td valign="top">EP </td><td>43</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/ISPACS.2007.4445818</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000255388100011</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Gong, CH
   <br>Zhao, HM
   <br>Lue, G
   <br>Liu, JX</td>
</tr>

<tr>
<td valign="top">AF </td><td>Gong, Chenghui
   <br>Zhao, Heming
   <br>Lue, Gang
   <br>Liu, Jianxin</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Preliminary Study on Vocal Tract System of Chinese Whispered Vowels</td>
</tr>

<tr>
<td valign="top">SO </td><td>2007 SECOND INTERNATIONAL CONFERENCE ON BIO-INSPIRED COMPUTING: THEORIES
   AND APPLICATIONS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>2nd International Conference on Bio-Inspired Computing</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 14-17, 2007</td>
</tr>

<tr>
<td valign="top">CL </td><td>Zhengzhou, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">ID </td><td>FORMANT ESTIMATION; VOICE CONVERSION; SPEECH; TRANSFORMATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper concentrates on the abstraction of parameters from vocal tract transfer function of Chinese whispered vowels. As there is no fundamental frequency in whispered speech, these parameters become more prominent in speech analysis and synthesis. It is proved that the proposed algorithm for formant estimation is effectual and the gain of vocal tract transfer function can be utilized for tune analysis. The comparison of these parameters between Chinese whispered vowels and voiced ones is the basis for whispering recognition and conversion. The ratios of formants excursion, bandwidths movement, gain and energy variation are calculated for scalar weight coefficients of voice personality transformation.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Gong, Chenghui] Soochow Univ, Coll Phys, Suzhou 215006, Peoples R China.
   <br>[Zhao, Heming; Lue, Gang; Liu, Jianxin] Soochow Univ, Coll Elect &amp;
   Informat Engn, Suzhou 215006, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Gong, CH (reprint author), Soochow Univ, Coll Phys, Suzhou 215006, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>homanhuihui@suda.edu.cn; hmzhao@suda.eud.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">BP </td><td>177</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/BICTA.2007.4806445</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000268062100040</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Orphanidou, C
   <br>Moroz, IM
   <br>Roberts, SJ</td>
</tr>

<tr>
<td valign="top">AF </td><td>Orphanidou, Christina
   <br>Moroz, Irene M.
   <br>Roberts, Stephen J.</td>
</tr>

<tr>
<td valign="top">BE </td><td>Iske, A
   <br>Levesley, J</td>
</tr>

<tr>
<td valign="top">TI </td><td>Multiscale voice morphing using radial basis function analysis</td>
</tr>

<tr>
<td valign="top">SO </td><td>ALGORITHMS FOR APPROXIMATION, PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>5th International Conference on Algorithms for Approximation</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUL 17-21, 2005</td>
</tr>

<tr>
<td valign="top">CL </td><td>Chester, ENGLAND</td>
</tr>

<tr>
<td valign="top">ID </td><td>TRANSFORM</td>
</tr>

<tr>
<td valign="top">AB </td><td>A new multiscale voice morphing algorithm using radial basis function (RBF) analysis is presented in this paper. The approach copes well with small training sets of high dimension, which is a problem often encountered in voice morphing. The aim of this algorithm is to transform one person's speech pattern so that it is perceived as if it was spoken by another speaker. The voice morphing system we propose assumes parallel training data from source and target speakers and uses the theory of wavelets in order to extract speaker feature information. The spectral conversion is modelled using RBF analysis. Independent listener tests demonstrate effective transformation of the perceived speaker identity.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Orphanidou, Christina; Moroz, Irene M.] Univ Oxford, Oxford Ctr Ind &amp;
   Appl Math, Oxford OX1 3LB, England.
   <br>[Orphanidou, Christina; Roberts, Stephen J.] Univ Oxford, Pattern Anal &amp;
   Machine Learning Res Grp, Oxford OX1 3PJ, England.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Orphanidou, C (reprint author), Univ Oxford, Oxford Ctr Ind &amp; Appl Math, Oxford OX1 3LB, England.</td>
</tr>

<tr>
<td valign="top">EM </td><td>orphanid@maths.ox.ac.uk; moroz@maths.ox.ac.uk; sjrob@robots.ox.ac.uk</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">BP </td><td>61</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/978-3-540-46551-5_5</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Mathematics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000244716900005</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Derichs, C</td>
</tr>

<tr>
<td valign="top">AF </td><td>Derichs, Claudia</td>
</tr>

<tr>
<td valign="top">TI </td><td>Malaysia in 2006 - An old tiger roars</td>
</tr>

<tr>
<td valign="top">SO </td><td>ASIAN SURVEY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Ninth Malaysia Plan; price hikes; scenic bridge; conversion; Islamic
   Family Law</td>
</tr>

<tr>
<td valign="top">AB </td><td>Prime Minister Baclawi's policies in 2006 were accompanied by an uninvited yet hardly ignorable critique, voiced by none other than his predecessor, Mahathir Mohamad. Apart from this controversy, the provisions of the Ninth Malaysia Plan, inter-ethnic relations, and price hikes for fuel occupied minds and media.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Univ Duisburg Essen, Inst E Asian Studies, Duisburg, Germany.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Derichs, C (reprint author), Univ Duisburg Essen, Inst E Asian Studies, Duisburg, Germany.</td>
</tr>

<tr>
<td valign="top">EM </td><td>claudia.derichs@uni-due.de</td>
</tr>

<tr>
<td valign="top">TC </td><td>4</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>4</td>
</tr>

<tr>
<td valign="top">PD </td><td>JAN-FEB</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">VL </td><td>47</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>148</td>
</tr>

<tr>
<td valign="top">EP </td><td>154</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1525/as.2007.47.1.148</td>
</tr>

<tr>
<td valign="top">SC </td><td>Area Studies</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000245247300018</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Liu, K
   <br>Zhang, JP
   <br>Yan, YH</td>
</tr>

<tr>
<td valign="top">AF </td><td>Liu, Kun
   <br>Zhang, Jianping
   <br>Yan, Yonghong</td>
</tr>

<tr>
<td valign="top">BE </td><td>Lei, JS
   <br>Yu, J
   <br>Zhou, SG</td>
</tr>

<tr>
<td valign="top">TI </td><td>High quality voice conversion through phoneme-based linear mapping
   functions with STRAIGHT for mandarin</td>
</tr>

<tr>
<td valign="top">SO </td><td>FOURTH INTERNATIONAL CONFERENCE ON FUZZY SYSTEMS AND KNOWLEDGE
   DISCOVERY, VOL 4, PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>4th International Conference on Fuzzy Systems and Knowledge Discovery</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 24-27, 2007</td>
</tr>

<tr>
<td valign="top">CL </td><td>Haikou, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; formant transitions; main vowel; phoneme-based mapping
   functions</td>
</tr>

<tr>
<td valign="top">ID </td><td>TRANSFORMATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>A novel voice conversion system using, phoneme-based linear mapping functions on main vowel phonemes is proposed in this paper. Our voice conversion algorithm has the, following three improvements. First, instead of has no all the Vocal Tract Resonance (VTR) vectors in the portion of a phoneme, we use the VTR vector at the steady-state of each phoneme to train phoneme-based GMM. Second, different linear mapping functions have been trained to describe the mapping relationships for corresponding phonemes. Third, in the transformation procedure. the transformed formant frequencies at the main vowel phonemes are obtained using the corresponding GMM. Besides, prosody parameters are also transformed. Finally the converted speech is re-synthesized with the transformed parameters by high quality speech manipulation framework STRAIGHT (Speech Transformation and Representation based on Adaptive Interpolation of weiGHTed spectrogram). Perceptual results for F-M and M-F conversion show that our MOS score of the converted voice is improved from 3.8 to 4.1 and ABX score front 3.3 to 3.8 compared with IBM's system. Comparisons with other systems are also given in this paper.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Liu, Kun; Zhang, Jianping; Yan, Yonghong] Chinese Acad Sci, Inst
   Acoust, Beijing 100083, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Liu, K (reprint author), Chinese Acad Sci, Inst Acoust, Beijing 100083, Peoples R China.</td>
</tr>

<tr>
<td valign="top">TC </td><td>5</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>5</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">BP </td><td>410</td>
</tr>

<tr>
<td valign="top">EP </td><td>414</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/FSKD.2007.347</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000252461000081</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Inanoglu, Z
   <br>Young, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Inanoglu, Zeynep
   <br>Young, Steve</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA</td>
</tr>

<tr>
<td valign="top">TI </td><td>A System for Transforming the Emotion in Speech: Combining Data-Driven
   Conversion Techniques for Prosody and Voice Quality</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2007: 8TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH
   COMMUNICATION ASSOCIATION, VOLS 1-4</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Interspeech Conference 2007</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 27-31, 2007</td>
</tr>

<tr>
<td valign="top">CL </td><td>Antwerp, BELGIUM</td>
</tr>

<tr>
<td valign="top">DE </td><td>expressive speech synthesis; emotion conversion; voice conversion</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper describes a system that combines independent transformation techniques to endow a neutral utterance with some required target emotion. The system consists of three modules that are each trained on a limited amount of speech data and act on differing temporal layers. F0 contours are mode-lied and generated using context-sensitive syllable HMMs, while durations are transformed using phone-based relative decision trees. For spectral conversion which is applied at the segmental level, two methods were investigated: a GMM-based voice conversion approach and a codebook selection approach. Converted test data were evaluated for three emotions using an independent emotion classifier as well as perceptual listening tests. The listening test results show that perception of sadness output by our system was comparable with the perception of human sad speech while the perception of surprise and anger was around 5% worse than that of a human speaker.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Inanoglu, Zeynep; Young, Steve] Univ Cambridge, Dept Engn, Cambridge
   CB2 1PZ, England.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Inanoglu, Z (reprint author), Univ Cambridge, Dept Engn, Cambridge CB2 1PZ, England.</td>
</tr>

<tr>
<td valign="top">EM </td><td>zi201@cam.ac.uk; sjy@eng.cam.ac.uk</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">BP </td><td>457</td>
</tr>

<tr>
<td valign="top">EP </td><td>460</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000269998600115</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hanzlicek, Z
   <br>Matousek, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hanzlicek, Zdenek
   <br>Matousek, Jindrich</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA</td>
</tr>

<tr>
<td valign="top">TI </td><td>F0 Transformation within the Voice Conversion Framework</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2007: 8TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH
   COMMUNICATION ASSOCIATION, VOLS 1-4</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Interspeech Conference 2007</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 27-31, 2007</td>
</tr>

<tr>
<td valign="top">CL </td><td>Antwerp, BELGIUM</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; f0 transformation; residual prediction</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, several experiments on F-0 transformation within the voice conversion framework are presented. The conversion system is based on a probabilistic transformation of line spectral frequencies and residual prediction. Three probabilistic methods of instantaneous F-0 transformation are described and compared. Moreover, a new modification of inter-speaker residual prediction is proposed which utilizes the information on target F-0 directly during the determination of suitable residuum. Preference listening tests confirmed that this modification outperformed the standard version of residual prediction.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Hanzlicek, Zdenek; Matousek, Jindrich] Univ W Bohemia, Dept Cybernet,
   Plzen, Czech Republic.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hanzlicek, Z (reprint author), Univ W Bohemia, Dept Cybernet, Plzen, Czech Republic.</td>
</tr>

<tr>
<td valign="top">EM </td><td>zhanzlic@kky.zcu.cz; jmatouse@kky.zcu.cz</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Matousek, Jindrich</display_name>&nbsp;</font></td><td><font size="3">C-2146-2011&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Matousek, Jindrich</display_name>&nbsp;</font></td><td><font size="3">0000-0002-7408-7730&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>6</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>6</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">BP </td><td>681</td>
</tr>

<tr>
<td valign="top">EP </td><td>684</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000269998600171</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Barra, R
   <br>Montero, JM
   <br>Macias-Guarasa, J
   <br>Gutierrez-Arriola, J
   <br>Ferreiros, J
   <br>Pardo, JM</td>
</tr>

<tr>
<td valign="top">AF </td><td>Barra, R.
   <br>Montero, J. M.
   <br>Macias-Guarasa, J.
   <br>Gutierrez-Arriola, J.
   <br>Ferreiros, J.
   <br>Pardo, J. M.</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA</td>
</tr>

<tr>
<td valign="top">TI </td><td>On the limitations of voice conversion techniques in emotion
   identification tasks</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2007: 8TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH
   COMMUNICATION ASSOCIATION, VOLS 1-4</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Interspeech Conference 2007</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 27-31, 2007</td>
</tr>

<tr>
<td valign="top">CL </td><td>Antwerp, BELGIUM</td>
</tr>

<tr>
<td valign="top">DE </td><td>speech synthesis; voice conversion; emotional speech; perceptual test</td>
</tr>

<tr>
<td valign="top">AB </td><td>The growing interest in emotional speech synthesis urges effective emotion conversion techniques to be explored. This paper estimates the relevance of three speech components (spectral envelope, residual excitation and prosody) for synthesizing identifiable emotional speech, in order to be able to customize voice conversion techniques to the specific characteristics of each emotion. The analysis has been based on a listening test with a set of synthetic mixed-emotion utterances that draw their speech components from emotional and neutral recordings. Results prove the importance of transforming residual excitation for the identification of emotions that are not fully conveyed through prosodic means (such as cold anger or sadness in our Spanish corpus).</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Barra, R.; Montero, J. M.; Macias-Guarasa, J.; Gutierrez-Arriola, J.;
   Ferreiros, J.; Pardo, J. M.] Univ Politecn Madrid, Speech Technol Grp,
   E-28040 Madrid, Spain.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Barra, R (reprint author), Univ Politecn Madrid, Speech Technol Grp, E-28040 Madrid, Spain.</td>
</tr>

<tr>
<td valign="top">EM </td><td>barra@die.upm.es; juancho@die.upm.es; macias@die.upm.es;
   jmga@ics.upm.es; jfl@die.upm.es; pardo@die.upm.es</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Montero, Juan M</display_name>&nbsp;</font></td><td><font size="3">K-2381-2014&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Pardo, Jose</display_name>&nbsp;</font></td><td><font size="3">H-3745-2013&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Barra-Chicote, Roberto</display_name>&nbsp;</font></td><td><font size="3">L-4963-2014&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Macias-Guarasa, Javier</display_name>&nbsp;</font></td><td><font size="3">J-4625-2012&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Montero, Juan M</display_name>&nbsp;</font></td><td><font size="3">0000-0002-7908-5400&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Barra-Chicote, Roberto</display_name>&nbsp;</font></td><td><font size="3">0000-0003-0844-7037&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Gutierrez-Arriola, Juana</display_name>&nbsp;</font></td><td><font size="3">0000-0003-0888-6582&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>FERREIROS LOPEZ, JAVIER</display_name>&nbsp;</font></td><td><font size="3">0000-0001-8834-3080&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Macias-Guarasa, Javier</display_name>&nbsp;</font></td><td><font size="3">0000-0002-3303-3963&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">BP </td><td>1241</td>
</tr>

<tr>
<td valign="top">EP </td><td>1244</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000269998600311</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Erro, D
   <br>Moreno, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Erro, Daniel
   <br>Moreno, Asuncion</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA</td>
</tr>

<tr>
<td valign="top">TI </td><td>Weighted Frequency Warping for Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2007: 8TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH
   COMMUNICATION ASSOCIATION, VOLS 1-4</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Interspeech Conference 2007</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 27-31, 2007</td>
</tr>

<tr>
<td valign="top">CL </td><td>Antwerp, BELGIUM</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; speech synthesis; harmonic model; GMM; weighted
   frequency warping</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a new voice conversion method called Weighted Frequency Warping (WFW), which combines the well known GMM approach and the frequency warping approach. The harmonic plus stochastic model has been used to analyze, modify and synthesize the speech signal. Special phase manipulation procedures have been designed to allow the system to work in pitch-asynchronous mode. The experiments show that the proposed technique reaches a high degree of similarity between the converted and target speakers, and the naturalness and quality of the resynthesized speech is much higher than those of classical GMM-based systems.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Erro, Daniel; Moreno, Asuncion] Univ Politecn Cataluna, Dept Signal
   Theory &amp; Commun, Barcelona, Spain.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Erro, D (reprint author), Univ Politecn Cataluna, Dept Signal Theory &amp; Commun, Barcelona, Spain.</td>
</tr>

<tr>
<td valign="top">EM </td><td>derro@gps.tsc.upc.edu; asuncion@gps.tsc.upc.edu</td>
</tr>

<tr xmlns:date="http://exslt.org/dates-and-times">
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Erro, Daniel</display_name>&nbsp;</font></td><td><font size="3">H-7043-2015&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Erro, Daniel</display_name>&nbsp;</font></td><td><font size="3">0000-0003-0954-6942&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">BP </td><td>1465</td>
</tr>

<tr>
<td valign="top">EP </td><td>1468</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000269998600367</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Erro, D
   <br>Moreno, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Erro, Daniel
   <br>Moreno, Asuncion</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA</td>
</tr>

<tr>
<td valign="top">TI </td><td>Frame Alignment Method for Cross-lingual Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2007: 8TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH
   COMMUNICATION ASSOCIATION, VOLS 1-4</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Interspeech Conference 2007</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 27-31, 2007</td>
</tr>

<tr>
<td valign="top">CL </td><td>Antwerp, BELGIUM</td>
</tr>

<tr>
<td valign="top">DE </td><td>speech synthesis; cross-lingual voice conversion; alignment; GMM;
   weighted frequency warping</td>
</tr>

<tr>
<td valign="top">AB </td><td>Most of the existing voice conversion methods calculate the optimal transformation function from a given set of paired acoustic vectors of the source and target speakers. The alignment of the phonetically equivalent source and target frames is problematic when the training corpus available is not parallel, although this is the most realistic situation. The alignment task is even more difficult in cross-lingual applications because the phoneme sets may be different in the involved languages. In this paper, a new iterative alignment method based on acoustic distances is proposed. The method is shown to be suitable for text-independent and cross-lingual voice conversion, and the conversion scores obtained in our evaluation experiments are not far from the performance achieved by using parallel training corpora.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Erro, Daniel; Moreno, Asuncion] Univ Politecn Cataluna, Dept Signal
   Theory &amp; Commun, Barcelona, Spain.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Erro, D (reprint author), Univ Politecn Cataluna, Dept Signal Theory &amp; Commun, Barcelona, Spain.</td>
</tr>

<tr>
<td valign="top">EM </td><td>derro@gps.tsc.upc.edu; asuncion@gps.tsc.upc.edu</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Erro, Daniel</display_name>&nbsp;</font></td><td><font size="3">H-7043-2015&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Erro, Daniel</display_name>&nbsp;</font></td><td><font size="3">0000-0003-0954-6942&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">BP </td><td>1533</td>
</tr>

<tr>
<td valign="top">EP </td><td>1536</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000269998601011</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nurminen, J
   <br>Tian, J
   <br>Popa, V</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nurminen, Jani
   <br>Tian, Jilei
   <br>Popa, Victor</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voicing Level Control with Application in Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2007: 8TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH
   COMMUNICATION ASSOCIATION, VOLS 1-4</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Interspeech Conference 2007</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 27-31, 2007</td>
</tr>

<tr>
<td valign="top">CL </td><td>Antwerp, BELGIUM</td>
</tr>

<tr>
<td valign="top">DE </td><td>voicing; spectral modification; voice conversion</td>
</tr>

<tr>
<td valign="top">AB </td><td>Speech processing related changes in the speech spectra may often lead to unwanted changes in the effective degree of voicing, which in turn may degrade the speech quality. This phenomenon is studied more closely in this paper, first on a theoretical level and then in the context of voice conversion. Moreover, a simple but efficient approach for avoiding the unwanted changes in the effective level of voicing is proposed. The usefulness of the proposed voicing level control is demonstrated in a practical voice conversion system. The compensation of the changes in the degree of voicing is found to reduce the average level of noise in the output and to enhance the perceptual speech quality.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nurminen, Jani] Nokia Technol Platforms, Tampere, Finland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nurminen, J (reprint author), Nokia Technol Platforms, Tampere, Finland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jani.k.nurminen@nokia.com; jilei.tian@nokia.com;
   ext-victor.popa@nokia.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">BP </td><td>1713</td>
</tr>

<tr>
<td valign="top">EP </td><td>1716</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000269998601056</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Percybrooks, WS
   <br>Moore, E</td>
</tr>

<tr>
<td valign="top">AF </td><td>Percybrooks, Winston S.
   <br>Moore, Elliot, II</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA</td>
</tr>

<tr>
<td valign="top">TI </td><td>New Algorithm for LPC Residual Estimation from LSF Vectors for a Voice
   Conversion System</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2007: 8TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH
   COMMUNICATION ASSOCIATION, VOLS 1-4</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Interspeech Conference 2007</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 27-31, 2007</td>
</tr>

<tr>
<td valign="top">CL </td><td>Antwerp, BELGIUM</td>
</tr>

<tr>
<td valign="top">DE </td><td>LPC residual; LSF parameters; GMM; voice conversion</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion involves transforming segments of speech from a source speaker to make them to be perceived as if spoken by a target speaker. Generally, this process involves the estimation of vocal tract parameters and an excitation signal that match the target speaker. The work presented here proposes an algorithm for estimating the excitation residuals of the target speaker using a weighted combination of clustered residuals. The algorithm is subjected to objective and subjective comparisons to other basic types of residual estimation techniques for voice conversion. Tests were carried on 2 male and 2 female target speakers in an ideal setting. The overall goal of this work is to create an improved algorithm for estimating excitation residuals during voice conversion that maintain speaker recognizability and high synthesis quality.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Percybrooks, Winston S.; Moore, Elliot, II] Georgia Inst Technol, Dept
   Elect &amp; Comp Engn, Savannah, GA USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Percybrooks, WS (reprint author), Georgia Inst Technol, Dept Elect &amp; Comp Engn, Savannah, GA USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>wpercybrooks@gatech.edu; elliot.moore@gtsav.gatech.edu</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Percybrooks, Winston</display_name>&nbsp;</font></td><td><font size="3">0000-0002-0169-7562&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">BP </td><td>2480</td>
</tr>

<tr>
<td valign="top">EP </td><td>2483</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000269998601248</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ohtani, Y
   <br>Toda, T
   <br>Saruwatari, H
   <br>Shikano, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ohtani, Yamato
   <br>Toda, Tomoki
   <br>Saruwatari, Hiroshi
   <br>Shikano, Kiyohiro</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA</td>
</tr>

<tr>
<td valign="top">TI </td><td>Speaker Adaptive Training for One-to-Many Eigenvoice Conversion Based on
   Gaussian Mixture Model</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2007: 8TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH
   COMMUNICATION ASSOCIATION, VOLS 1-4</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Interspeech Conference 2007</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 27-31, 2007</td>
</tr>

<tr>
<td valign="top">CL </td><td>Antwerp, BELGIUM</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speech synthesis; Voice conversion; Eigenvoice; Gaussian mixture model;
   Speaker adaptive training</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION; SPEECH</td>
</tr>

<tr>
<td valign="top">AB </td><td>One-to-many eigenvoice conversion (EVC) allows the conversion of a specific source speaker into arbitrary target speakers. Eigenvoice Gaussian mixture model (EV-GMM) is trained in advance with multiple parallel data sets consisting of the source speaker and many pre-stored target speakers. The EV-GMM is adapted for arbitrary target speakers using only a few utterances by estimating a small number of free parameters. Therefore, the initial EV-GMM directly affects the conversion performance of the adapted EV-GMM. In order to prepare a better initial model, this paper proposes Speaker Adaptive Training (SAT) of a canonical EV-GMM in one-to-many EVC. Results of objective and subjective evaluations demonstrate that SAT causes significant improvements in the performance of EVC.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ohtani, Yamato; Toda, Tomoki; Saruwatari, Hiroshi; Shikano, Kiyohiro]
   Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ohtani, Y (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>yamato-o@is.naist.jp; tomoki@is.naist.jp; sawatari@is.naist.jp;
   shikano@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">BP </td><td>2496</td>
</tr>

<tr>
<td valign="top">EP </td><td>2499</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000269998601252</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nakamura, K
   <br>Toda, T
   <br>Saruwatari, H
   <br>Shikano, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nakamura, Keigo
   <br>Toda, Tomoki
   <br>Saruwatari, Hiroshi
   <br>Shikano, Kiyohiro</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA</td>
</tr>

<tr>
<td valign="top">TI </td><td>Impact of Various Small Sound Source Signals on Voice Conversion
   Accuracy in Speech Communication Aid for Laryngectomees</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2007: 8TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH
   COMMUNICATION ASSOCIATION, VOLS 1-4</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Interspeech Conference 2007</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 27-31, 2007</td>
</tr>

<tr>
<td valign="top">CL </td><td>Antwerp, BELGIUM</td>
</tr>

<tr>
<td valign="top">DE </td><td>speaking aid; laryngectomees; sound sources; voice conversion</td>
</tr>

<tr>
<td valign="top">AB </td><td>We proposed a speaking aid system using statistical voice conversion for laryngectomees, whose vocal folds have been removed. This paper investigates the influence of various small sound sources on the voice conversion accuracy. Spectral envelopes and power of sound sources are controlled independently. In total 8 different kinds of sound source signals, e.g. pulse train, sierra wave and so on, are examined. Results of objective and subjective evaluations demonstrate that for voice conversion, sound sources with various spectral envelopes and power in a large degree are acceptable unless the power of them is comparable to that of silence parts.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nakamura, Keigo; Toda, Tomoki; Saruwatari, Hiroshi; Shikano, Kiyohiro]
   Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nakamura, K (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>kei-naka@is.naist.jp; tomoki@is.naist.jp; sawatari@is.naist.jp;
   shikano@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">BP </td><td>2640</td>
</tr>

<tr>
<td valign="top">EP </td><td>2643</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000269998601288</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Mesbahi, L
   <br>Barreaud, V
   <br>Boeffard, O</td>
</tr>

<tr>
<td valign="top">AF </td><td>Mesbahi, Larbi
   <br>Barreaud, Vincent
   <br>Boeffard, Olivier</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA</td>
</tr>

<tr>
<td valign="top">TI </td><td>Comparing GMM-based speech transformation systems</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2007: 8TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH
   COMMUNICATION ASSOCIATION, VOLS 1-4</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Interspeech Conference 2007</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 27-31, 2007</td>
</tr>

<tr>
<td valign="top">CL </td><td>Antwerp, BELGIUM</td>
</tr>

<tr>
<td valign="top">DE </td><td>GMM-based voice conversion; Over-fitting; Over-smoothing</td>
</tr>

<tr>
<td valign="top">ID </td><td>CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This article deals with a study on GMM-based voice conversion systems. We compare the main linear conversion functions found in the literature on an identical speech corpus. We insist in particular on the risks of over-fitting and over-smoothing. We propose three alternatives for robust conversion functions in order to minimize these risks. We show, on two experimental speech databases, that the approach suggested by Kain remains the more precise but leads to an over-fitting ratio of 1.72%. The alternatives which we propose, present an average degradation of 2.8% for a 0.52% over-fitting ratio.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Mesbahi, Larbi; Barreaud, Vincent; Boeffard, Olivier] Univ Rennes 1,
   IRISA, ENSSAT, F-22305 Lannion, France.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Mesbahi, L (reprint author), Univ Rennes 1, IRISA, ENSSAT, 6 Rue Kerampont,BP 80518, F-22305 Lannion, France.</td>
</tr>

<tr>
<td valign="top">EM </td><td>larbi.mesbahi@irisa.fr; vincent.barreaud@irisa.fr;
   olivier.boeffard@irisa.fr</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">BP </td><td>2852</td>
</tr>

<tr>
<td valign="top">EP </td><td>2855</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000269998601341</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Panfilov, O
   <br>Turgeon, A
   <br>Hickling, R
   <br>Alexandrov, I
   <br>McClellan, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Panfilov, Oleg
   <br>Turgeon, Antonio
   <br>Hickling, Ron
   <br>Alexandrov, Igor
   <br>McClellan, Kelly</td>
</tr>

<tr>
<td valign="top">BE </td><td>Labiod, H
   <br>Badra, M</td>
</tr>

<tr>
<td valign="top">TI </td><td>Direct RF conversion transceivers as a base for designing dynamic
   spectrum allocation systems</td>
</tr>

<tr>
<td valign="top">SO </td><td>NEW TECHNOLOGIES, MOBILITY AND SECURITY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>1st IFIP International Conference on New Technologies, Mobility and
   Security</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 02-04, 2007</td>
</tr>

<tr>
<td valign="top">CL </td><td>Paris, FRANCE</td>
</tr>

<tr>
<td valign="top">DE </td><td>direct RF conversion; dynamic spectrum allocation; software radio</td>
</tr>

<tr>
<td valign="top">AB </td><td>A new solution for implementing dynamic spectrum allocation (DSA) principles is described. It is based on using the Technoconcepts' direct conversion from RF to baseband transceiver chips. Such RF/D (TM) chips convert the received signals into digital form immediately after an antenna making it possible to provide all required signal processing in digital form. It yields systems capable of operating in different dynamically allocatable frequency bands and allowing integration of different types of services including voice and multimedia. These systems may be not only frequency agile and protocol independent, but configuration agnostic as well.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Panfilov, Oleg; Turgeon, Antonio; Hickling, Ron; Alexandrov, Igor;
   McClellan, Kelly] TechnoConcepts Inc, Sherman Oaks, CA 91403 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Panfilov, O (reprint author), TechnoConcepts Inc, 14945 Ventura, Sherman Oaks, CA 91403 USA.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">BP </td><td>75</td>
</tr>

<tr>
<td valign="top">EP </td><td>88</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/978-1-4020-6270-4_7</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000252161400007</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Mangayyagari, S
   <br>Sankar, R</td>
</tr>

<tr>
<td valign="top">AF </td><td>Mangayyagari, Srikanth
   <br>Sankar, Ravi</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Pitch conversion based on pitch mark mapping</td>
</tr>

<tr>
<td valign="top">SO </td><td>PROCEEDINGS IEEE SOUTHEASTCON 2007, VOLS 1 AND 2</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE SoutheastCon 2007</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 22-25, 2007</td>
</tr>

<tr>
<td valign="top">CL </td><td>IEEE Virginia Council, Richmond, VA</td>
</tr>

<tr>
<td valign="top">HO </td><td>IEEE Virginia Council</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion is a subject of major research interest these days, due to its numerous applications including dubbing, text to speech synthesis and multimedia. Pitch conversion is an important building block for efficient voice conversion. This paper focuses on the implementation of pitch scaling and pitch conversion. Effective silence removal, voice/unvoiced detection, pitch extraction, pitch marking and PSOLA techniques have been implemented for pitch scaling along with an implementation of pitch mark mapping method for pitch conversion. A graphical user interface has been developed for evaluation and illustration purposes. Evaluation of the pitch scaling and pitch conversion technique was performed on various speech data. Pitch scaling and pitch conversion for both the male and female speakers was implemented and verified with the help of pitch contour plots of the transformed speech.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Mangayyagari, Srikanth; Sankar, Ravi] Univ S Florida, Dept Elect Engn,
   Tampa, FL 33620 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Mangayyagari, S (reprint author), Univ S Florida, Dept Elect Engn, Tampa, FL 33620 USA.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">BP </td><td>8</td>
</tr>

<tr>
<td valign="top">EP </td><td>13</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/SECON.2007.342842</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000254279900003</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ananthakrishnan, G
   <br>Ramakrishnan, AG</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ananthakrishnan, G.
   <br>Ramakrishnan, A. G.</td>
</tr>

<tr>
<td valign="top">BE </td><td>Sablatnig, R
   <br>Scherzer, O</td>
</tr>

<tr>
<td valign="top">TI </td><td>Relative pitch tracking for singing voice as an application in query by
   humming systems</td>
</tr>

<tr>
<td valign="top">SO </td><td>PROCEEDINGS OF THE FOURTH IASTED INTERNATIONAL CONFERENCE ON SIGNAL
   PROCESSING, PATTERN RECOGNITION, AND APPLICATIONS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>4th IASTED International Conference on Signal Processing, Pattern
   Recognition and Applications</td>
</tr>

<tr>
<td valign="top">CY </td><td>FEB 14-16, 2007</td>
</tr>

<tr>
<td valign="top">CL </td><td>Innsbruck, AUSTRIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>query by humming; transcription; pitch-tracking; Bach filters</td>
</tr>

<tr>
<td valign="top">AB </td><td>Pitch extraction from singing voice has traditionally been viewed from the perspective of absolute pitch of the voice. However, for the query by humming applications, the absolute pitch is not of as much importance as compared to that of relative pitch. The algorithm presented in this paper addresses the issue of relative pitch tracking for singing voice and attempts to improve the transcription accuracy. It also tries to bring robustness to transcription, in the several forms of querying by a user. The algorithm makes use of specially designed Bach filters, which associate a relative frequency with a corresponding musical semitone. This simplifies the conversion of the transcribed pitch values to the MIDI format. Preliminary results show up to 85% accuracy in the automated transcription when compared with manual transcription.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ananthakrishnan, G.; Ramakrishnan, A. G.] Indian Inst Sci, Dept Elect
   Engn, Bangalore 560012, Karnataka, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ananthakrishnan, G (reprint author), Indian Inst Sci, Dept Elect Engn, Bangalore 560012, Karnataka, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ananthg@ee.iisc.ernet.in; ramkiag@ee.iisc.ernet.in</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Ganesan, Ramakrishnan Angarai</display_name>&nbsp;</font></td><td><font size="3">B-8317-2013&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Ganesan, Ramakrishnan Angarai</display_name>&nbsp;</font></td><td><font size="3">0000-0002-3646-1955&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">BP </td><td>275</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering; Imaging Science &amp; Photographic Technology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000246297800048</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Fujii, K
   <br>Okawa, J
   <br>Suigetsu, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Fujii, Kei
   <br>Okawa, Jun
   <br>Suigetsu, Kaori</td>
</tr>

<tr>
<td valign="top">BE </td><td>Ardil, C</td>
</tr>

<tr>
<td valign="top">TI </td><td>High-Individuality Voice Conversion Based on Concatenative Speech
   Synthesis</td>
</tr>

<tr>
<td valign="top">SO </td><td>PROCEEDINGS OF WORLD ACADEMY OF SCIENCE, ENGINEERING AND TECHNOLOGY, VOL
   26, PARTS 1 AND 2, DECEMBER 2007</td>
</tr>

<tr>
<td valign="top">SE </td><td>Proceedings of World Academy of Science Engineering and Technology</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Conference of the World-Academy-of-Science-Engineering-and-Technology</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 14-16, 2007</td>
</tr>

<tr>
<td valign="top">CL </td><td>Bangkok, THAILAND</td>
</tr>

<tr>
<td valign="top">DE </td><td>concatenative speech synthesis; join cost; speaker individuality; unit
   selection; voice conversion</td>
</tr>

<tr>
<td valign="top">AB </td><td>Concatenative speech synthesis is a method that can make speech sound which has naturalness and high-individuality of a speaker by introducing a large speech corpus. Based on this method, in this paper, we propose a voice conversion method whose conversion speech has high-individuality and naturalness. The authors also have two subjective evaluation experiments for evaluating individuality and sound quality of conversion speech. From the results, following three facts have be confirmed: (a) the proposal method can convert the individuality of speakers well, (b) employing the framework of unit selection (especially join cost) of concatenative speech synthesis into conventional voice conversion improves the sound quality of conversion speech, and (c) the proposal method is robust against the difference of genders between a source speaker and a target speaker.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Fujii, Kei; Okawa, Jun; Suigetsu, Kaori] Kumamoto Natl Coll Technol,
   Dept Informat &amp; Comp Sci, Kohshi City, Kumamoto 8611102, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Fujii, K (reprint author), Kumamoto Natl Coll Technol, Dept Informat &amp; Comp Sci, 2659-2 Suya, Kohshi City, Kumamoto 8611102, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>fujii@cs.knct.ac.jp; jokawa17@pr.cs.knct.ac.jp; suige@pr.cs.knct.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">VL </td><td>26</td>
</tr>

<tr>
<td valign="top">BP </td><td>483</td>
</tr>

<tr>
<td valign="top">EP </td><td>488</td>
</tr>

<tr>
<td valign="top">PN </td><td>1 &amp; 2</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000259869900091</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Jian, ZH
   <br>Yang, Z</td>
</tr>

<tr>
<td valign="top">AF </td><td>Jian, ZhiHua
   <br>Yang, Zhen</td>
</tr>

<tr>
<td valign="top">BE </td><td>Feng, WY
   <br>Gao, F</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice conversion using canonical correlation analysis based on Gaussian
   mixture model</td>
</tr>

<tr>
<td valign="top">SO </td><td>SNPD 2007: EIGHTH ACIS INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING,
   ARTIFICIAL INTELLIGENCE, NETWORKING, AND PARALLEL/DISTRIBUTED COMPUTING,
   VOL 1, PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>8th ACIS International Conference on Software Engineering, Artificial
   Intelligence, Networking and Parallel/Distributed Computing/3rd ACIS
   International Workshop on Self-Assembling Wireless Networks</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUL 30-AUG 01, 2007</td>
</tr>

<tr>
<td valign="top">CL </td><td>Qungdao, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH; TRANSFORMATION; ADAPTATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>A novel algorithm for voice conversion is proposed in this paper. The mapping function of spectral vectors of the source and target speakers is calculated by the canonical correlation analysis (CCA) estimation based on Gaussian mixture models. Since the spectral envelope feature remains a majority of second order statistical information contained in speech after linear prediction (LPC) analysis, the CCA method is more suitable for spectral conversion than MMSE because CCA explicitly considers the variance of each component of the spectral vectors during conversion procedure. Both subjective and objective evaluations are conducted. The experimental results demonstrate that the proposed scheme can achieve better performance than the previous method which uses MMSE estimation criterion.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Jian, ZhiHua; Yang, Zhen] Nanjing Univ Post &amp; Telecommun, Inst Signal
   Proc &amp; Transmiss, Nanjing, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Jian, ZH (reprint author), Nanjing Univ Post &amp; Telecommun, Inst Signal Proc &amp; Transmiss, Nanjing, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jianzh@njupt.edu.cn; yangz@njupt.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>4</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>4</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">BP </td><td>210</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000250429200041</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Zhao, L
   <br>Gao, YQ</td>
</tr>

<tr>
<td valign="top">AF </td><td>Zhao, Lei
   <br>Gao, Yinqiu</td>
</tr>

<tr>
<td valign="top">BE </td><td>Feng, WY
   <br>Gao, F</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice conversion adopting SOLAFS</td>
</tr>

<tr>
<td valign="top">SO </td><td>SNPD 2007: EIGHTH ACIS INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING,
   ARTIFICIAL INTELLIGENCE, NETWORKING, AND PARALLEL/DISTRIBUTED COMPUTING,
   VOL 1, PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>8th ACIS International Conference on Software Engineering, Artificial
   Intelligence, Networking and Parallel/Distributed Computing/3rd ACIS
   International Workshop on Self-Assembling Wireless Networks</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUL 30-AUG 01, 2007</td>
</tr>

<tr>
<td valign="top">CL </td><td>Qungdao, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">AB </td><td>An improved method of voice conversion is proposed to make the speech of a source speaker sound like uttered by a target speaker. Speaker individuality transformation is achieved by altering the spectral envelope and prosodic information. The main advantage of this method is to firstly apply the synchronized overlap-add fixed synthesis (SOLAFS) to modify the source speaker's speaking rate to match that of the target speaker, which enhances the performance of the whole conversion system compared with conventional systems without such a procedure. Besides, a precise estimation for the target excitation is advanced only with the information of the matched source's excitation and the average pitch period of the target speaker. The proposed scheme is evaluated using both subjective and objective measures. The experimental results show that the system is capable of effectively transforming speaker identity whilst the converted speech maintains high quality.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Zhao, Lei] Qingdao Technol Univ, Shandong, Peoples R China.
   <br>[Gao, Yinqiu] Nanjing Univ Posts &amp; Telecomm, Inst Signal &amp; Informat
   Proc, Nanjing, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Zhao, L (reprint author), Qingdao Technol Univ, Shandong, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>zh1996@qtech.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">BP </td><td>543</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/SNPD.2007.64</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000250429200104</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Feldhoffer, G
   <br>Oroszi, B
   <br>Takacs, G
   <br>Tihanyi, A
   <br>Bardi, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Feldhoffer, Gergely
   <br>Oroszi, Balazs
   <br>Takacs, Gyoergy
   <br>Tihanyi, Attila
   <br>Bardi, Tamas</td>
</tr>

<tr>
<td valign="top">BE </td><td>Matousek, V
   <br>Mautner, P</td>
</tr>

<tr>
<td valign="top">TI </td><td>Inter-speaker synchronization in audiovisual database for lip-readable
   speech to animation conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>TEXT, SPEECH AND DIALOGUE, PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">SE </td><td>Lecture Notes in Artificial Intelligence</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>10th International Conference on Text, Speech, and Dialogue</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 03-07, 2007</td>
</tr>

<tr>
<td valign="top">CL </td><td>Pilsen, CZECH REPUBLIC</td>
</tr>

<tr>
<td valign="top">AB </td><td>The present study proposes an inter-speaker audiovisual synchronization method to decrease the speaker dependency of our direct speech to animation conversion system. Our aim is to convert an everyday speaker's voice to lip-readable facial animation for hearing impaired users. This conversion needs mixed training data: acoustic features from normal speakers coupled with visual features from professional lip-speakers. Audio and video data of normal and professional speakers were synchronized with Dynamic Time Warping method. Quality and usefulness of the synchronization were investigated in subjective test with measuring noticeable conflicts between the audio and visual part of speech stimuli. An objective test was done also, training neural network on the synchronized audiovisual data with increasing number of speakers.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Feldhoffer, Gergely; Oroszi, Balazs; Takacs, Gyoergy; Tihanyi, Attila;
   Bardi, Tamas] Peter Pazmany Catholic Univ, Fac Informat Technol,
   Budapest, Hungary.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Feldhoffer, G (reprint author), Peter Pazmany Catholic Univ, Fac Informat Technol, Budapest, Hungary.</td>
</tr>

<tr>
<td valign="top">EM </td><td>flugi@digitus.itk.ppke.hu; oroba@digitus.itk.ppke.hu;
   takacsgy@digitus.itk.ppke.hu; tihanyia@digitus.itk.ppke.hu;
   bardi@digitus.itk.ppke.hu</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">VL </td><td>4629</td>
</tr>

<tr>
<td valign="top">BP </td><td>447</td>
</tr>

<tr>
<td valign="top">EP </td><td>454</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000251315900057</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hanzlicek, Z
   <br>Matousek, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hanzlicek, Zdenek
   <br>Matousek, Jindrich</td>
</tr>

<tr>
<td valign="top">BE </td><td>Matousek, V
   <br>Mautner, P</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice conversion based on probabilistic parameter transformation and
   extended inter-speaker residual prediction</td>
</tr>

<tr>
<td valign="top">SO </td><td>TEXT, SPEECH AND DIALOGUE, PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">SE </td><td>LECTURE NOTES IN ARTIFICIAL INTELLIGENCE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>10th International Conference on Text, Speech, and Dialogue</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 03-07, 2007</td>
</tr>

<tr>
<td valign="top">CL </td><td>Pilsen, CZECH REPUBLIC</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion is a process which modifies speech produced by one speaker so that it sounds as if it is uttered by another speaker. In this paper a new voice conversion system is presented. The system requires parallel training data. By using linear prediction analysis, speech is described with line spectral frequencies and the corresponding residua. LSFs are converted together with instantaneous F-0 by joint probabilistic function. The residua are transformed by employing residual prediction. In this paper, a new modification of residual prediction is introduced which uses information on the desired target F-0 to determine a proper residuum and it also allows an efficient control of F-0 in resulting speech.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Hanzlicek, Zdenek; Matousek, Jindrich] Univ W Bohemia, Fac Sci Appl,
   Dept Cybernet, Plzen 30614, Czech Republic.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hanzlicek, Z (reprint author), Univ W Bohemia, Fac Sci Appl, Dept Cybernet, Univ 8, Plzen 30614, Czech Republic.</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Matousek, Jindrich</display_name>&nbsp;</font></td><td><font size="3">C-2146-2011&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Matousek, Jindrich</display_name>&nbsp;</font></td><td><font size="3">0000-0002-7408-7730&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">VL </td><td>4629</td>
</tr>

<tr>
<td valign="top">BP </td><td>480</td>
</tr>

<tr>
<td valign="top">EP </td><td>487</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000251315900061</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Si, WW
   <br>Mehta, S
   <br>Samavati, H
   <br>Terrovitis, M
   <br>Mack, M
   <br>Onodera, K
   <br>Jen, S
   <br>Luschas, S
   <br>Hwang, J
   <br>Mendis, S
   <br>Su, D
   <br>Wooley, B</td>
</tr>

<tr>
<td valign="top">AF </td><td>Si, William W.
   <br>Mehta, Srenik
   <br>Samavati, Hirad
   <br>Terrovitis, Manolis
   <br>Mack, Michael
   <br>Onodera, Keith
   <br>Jen, Steve
   <br>Luschas, Susan
   <br>Hwang, Justin
   <br>Mendis, Suni
   <br>Su, David
   <br>Wooley, Bruce</td>
</tr>

<tr>
<td valign="top">TI </td><td>A 1.9-GHz single-chip CMOS PHS cellphone</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE JOURNAL OF SOLID-STATE CIRCUITS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>CMOS integrated circuits; direct conversion; fast settling synthesizer;
   fractional-N synthesizer; frequency synthesizer; personal handy-phone
   (PHS); phase-locked loop (PLL); phase noise; RF transceiver; transceiver</td>
</tr>

<tr>
<td valign="top">ID </td><td>PLL</td>
</tr>

<tr>
<td valign="top">AB </td><td>A single-chip CMOS PHS cellphone, integrated in a 0.18-mu m CMOS technology, implements all handset functions including radio, voice, audio, MODEM, TDMA controller, CPU, and digital interfaces. Both the receiver and transmitter are based on a direct conversion architecture. The RF transceiver achieves -106 dBm receive sensitivity and +4 dBm EVM-compliant transmit power. The local oscillator, based on a sigma-delta fractional-N synthesizer, has a phase noise of -118 dBc/Hz at 600 kHz offset and settling time of 15 Its. The current consumption for the receiver, transmitter and synthesizer are 32 mA, 29 mA, and 25 mA, respectively, from a 3.0 V supply.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Atheros Commun, Santa Clara, CA 95054 USA.
   <br>Stanford Univ, Stanford, CA 94305 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Si, WW (reprint author), Atheros Commun, Santa Clara, CA 95054 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>wsi@atheros.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>5</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>5</td>
</tr>

<tr>
<td valign="top">PD </td><td>DEC</td>
</tr>

<tr>
<td valign="top">PY </td><td>2006</td>
</tr>

<tr>
<td valign="top">VL </td><td>41</td>
</tr>

<tr>
<td valign="top">IS </td><td>12</td>
</tr>

<tr>
<td valign="top">BP </td><td>2737</td>
</tr>

<tr>
<td valign="top">EP </td><td>2745</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/JSSC.2006.884790</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000242543500013</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Pribilova, A
   <br>Pribil, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Pribilova, Anna
   <br>Pribil, Jiri</td>
</tr>

<tr>
<td valign="top">TI </td><td>Non-linear frequency scale mapping for voice conversion in
   text-to-speech system with cepstral description</td>
</tr>

<tr>
<td valign="top">SO </td><td>SPEECH COMMUNICATION</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article; Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Non-Linear Speech Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 19-22, 2005</td>
</tr>

<tr>
<td valign="top">CL </td><td>Barcelona, SPAIN</td>
</tr>

<tr>
<td valign="top">DE </td><td>speech synthesis; source-filter speech model; harmonic speech model;
   text-to-speech system; voice conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>TRANSFORMATION; NETWORKS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion, i.e. modification of a speech signal to sound as if spoken by a different speaker, finds its use in speech synthesis with a new voice without necessity of a new database. This paper introduces two new simple non-linear methods of frequency scale mapping for transformation of voice characteristics between male and female or childish. The frequency scale mapping methods were developed primarily for use in the Czech and Slovak text-to-speech (TTS) system designed for the blind and based on the Pocket PC device platform. It uses cepstral description of the diphone speech inventory of the male speaker using the source-filter speech model or the harmonic speech model. Three new diphone speech inventories corresponding to female, childish and young male voices are created from the original male speech inventory. Listening tests are used for evaluation of voice transformation and quality of synthetic speech. (C) 2006 Elsevier B.V. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Slovak Univ Technol Bratislava, Dept Radio Elect, Bratislava 81219,
   Slovakia.
   <br>Acad Sci Czech Republic, Inst Radio Engn &amp; Elect, CR-18251 Prague 8,
   Czech Republic.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Pribilova, A (reprint author), Slovak Univ Technol Bratislava, Dept Radio Elect, Ilkovicova 3, Bratislava 81219, Slovakia.</td>
</tr>

<tr>
<td valign="top">EM </td><td>pribilova@kre.elf.stuba.sk</td>
</tr>

<tr>
<td valign="top">TC </td><td>16</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>18</td>
</tr>

<tr>
<td valign="top">PD </td><td>DEC</td>
</tr>

<tr>
<td valign="top">PY </td><td>2006</td>
</tr>

<tr>
<td valign="top">VL </td><td>48</td>
</tr>

<tr>
<td valign="top">IS </td><td>12</td>
</tr>

<tr>
<td valign="top">BP </td><td>1691</td>
</tr>

<tr>
<td valign="top">EP </td><td>1703</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.specom.2006.08.001</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000243246600008</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Okubo, T
   <br>Mochizuki, R
   <br>Kobayashi, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Okubo, Tadashi
   <br>Mochizuki, Ryo
   <br>Kobayashi, Tetsunori</td>
</tr>

<tr>
<td valign="top">TI </td><td>Hybrid voice conversion of unit selection and generation using prosody
   dependent HMM</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEICE TRANSACTIONS ON INFORMATION AND SYSTEMS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; speech synthesis; HMM; unit selection; MLLR</td>
</tr>

<tr>
<td valign="top">AB </td><td>We propose a hybrid voice conversion method which employs a combination of techniques using HMM-based unit selection and spectrum generation. In the proposed method, the HMM-based unit selection selects the most likely unit for the required phoneme context from the target speaker's corpus when candidates of the target unit exist in the corpus. Unit selection is performed based on the sequence of the spectral probability distribution obtained from the adapted HMMs. On the other hand, when a target unit does not exist in a corpus, a target waveform is generated from the adapted HMM sequence by maximizing the spectral likelihood. The proposed method also employs the HMM in which the spectral probability distribution is adjusted to the target prosody using the weight defined by the prosodic probability of each distribution. To show the effectiveness of the proposed method, sound quality and speaker individuality tests were conducted. The results revealed that the proposed method could produce high-quality speech and individuality of the synthesized sound was more similar to the target speaker compared to conventional methods.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Waseda Univ, Dept Comp Sci, Tokyo 1698555, Japan.
   <br>Matsushita Elect Ind Co Ltd, AV Core Technol Dev Ctr, Tokyo 1408632,
   Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Okubo, T (reprint author), Waseda Univ, Dept Comp Sci, Tokyo 1698555, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>mochizuki.ryo@jp.panasonic.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>6</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>6</td>
</tr>

<tr>
<td valign="top">PD </td><td>NOV</td>
</tr>

<tr>
<td valign="top">PY </td><td>2006</td>
</tr>

<tr>
<td valign="top">VL </td><td>E89D</td>
</tr>

<tr>
<td valign="top">IS </td><td>11</td>
</tr>

<tr>
<td valign="top">BP </td><td>2775</td>
</tr>

<tr>
<td valign="top">EP </td><td>2782</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1093/ietisy/e89-d.11.2775</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000242507400008</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Louis, ED
   <br>Rios, E
   <br>Applegate, LM
   <br>Hernandez, NC
   <br>Andrews, HF</td>
</tr>

<tr>
<td valign="top">AF </td><td>Louis, Elan D.
   <br>Rios, Eileen
   <br>Applegate, LaKeisha M.
   <br>Hernandez, Nora C.
   <br>Andrews, Howard F.</td>
</tr>

<tr>
<td valign="top">TI </td><td>Jaw tremor: Prevalence and clinical correlates in three essential tremor
   case samples</td>
</tr>

<tr>
<td valign="top">SO </td><td>MOVEMENT DISORDERS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>essential tremor; epidemiology; prevalence; correlates; jaw tremor</td>
</tr>

<tr>
<td valign="top">ID </td><td>CENTRALIZED BRAIN REPOSITORY; PARKINSONS-DISEASE; REST TREMOR;
   COMMUNITY; DIAGNOSIS; VALIDITY</td>
</tr>

<tr>
<td valign="top">AB </td><td>The spectrum of involuntary movements seen in essential tremor (ET) is limited. Jaw tremor is one such movement. The prevalence and clinical correlates of jaw tremor have not been studied in detail. The objective of this study was to estimate the prevalence and examine the clinical correlates of jaw tremor in ET using ET cases from three distinct settings (population, tertiary-referral center, brain repository). All ET cases underwent a videotaped tremor examination in which tremors (including limb, head, voice, and jaw) were assessed. The prevalence [95% confidence interval (Q] of jaw tremor was lowest in the population sample (7.5%; 3.9%-14.2%), intermediate in the tertiary-referral center (10.1%; 6.8%-14.7%), and highest in the brain repository (18.0%; 12.3%-25.5%; P = 0.03). Jaw tremor was associated with older age (P &lt; 0.001), more severe action tremor of the arms (P &lt; 0.001), and presence of head and voice tremor (P &lt; 0.001). Jaw tremor was present in 4/14 (28.6%) ET cases with consistent rest tremor vs. 15/193 (7.8%) cases without rest tremor (odds ratio = 4.8; 95% CI = 1.3-7.0; P = 0.009). The prevalence of jaw tremor was 7.5% to 18.0% and was dependent on the mode of ascertainment, being least prevalent in a population-based sample. ET cases with jaw tremor had a more clinically severe and more topographically widespread disorder. The association in our study between jaw tremor and rest tremor, along with the published observation that jaw tremor can occur in Parkinson's disease (PD), raises the question whether jaw tremor in ET is a marker for subsequent conversion to PD.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Columbia Univ, Gertrude H Sergievsky Ctr, New York, NY 10027 USA.
   <br>Columbia Univ, Taub Inst Res Alzheimers Dis &amp; Aging Brain, New York, NY
   10027 USA.
   <br>Columbia Univ, Dept Neurol, Coll Phys &amp; Surg, New York, NY 10027 USA.
   <br>Columbia Univ, Dept Biostat, Mailman Sch Publ Hlth, New York, NY 10027
   USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Louis, ED (reprint author), Columbia Univ, Gertrude H Sergievsky Ctr, New York, NY 10027 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>EDL2@columbia.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>31</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>31</td>
</tr>

<tr>
<td valign="top">PD </td><td>NOV</td>
</tr>

<tr>
<td valign="top">PY </td><td>2006</td>
</tr>

<tr>
<td valign="top">VL </td><td>21</td>
</tr>

<tr>
<td valign="top">IS </td><td>11</td>
</tr>

<tr>
<td valign="top">BP </td><td>1872</td>
</tr>

<tr>
<td valign="top">EP </td><td>1878</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1002/mds.21069</td>
</tr>

<tr>
<td valign="top">SC </td><td>Neurosciences &amp; Neurology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000242229000011</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Turk, O
   <br>Arslan, LM</td>
</tr>

<tr>
<td valign="top">AF </td><td>Turk, Oytun
   <br>Arslan, Levent M.</td>
</tr>

<tr>
<td valign="top">TI </td><td>Robust processing techniques for voice conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>COMPUTER SPEECH AND LANGUAGE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH RECOGNITION; TRANSFORMATION; INDIVIDUALITY; PREDICTION; TUTORIAL;
   QUALITY</td>
</tr>

<tr>
<td valign="top">AB </td><td>Differences in speaker characteristics, recording conditions, and signal processing algorithms affect. output quality in voice conversion systems. This study focuses on formulating robust techniques for a codebook mapping based voice conversion algorithm. Three different methods are used to improve voice conversion performance: confidence measures, pre-emphasis, and, spectral equalization. Analysis is performed for each method and the implementation details are discussed. The first method employs confidence measures in the training stage to eliminate problematic pairs of source and target speech units that might result from possible misalignments, speaking style differences or pronunciation variations. Four confidence measures are developed based on the spectral distance, fundamental frequency (f(0)) distance, energy distance, and duration distance between the source and target speech units. The second method focuses on the importance of pre-emphasis in line-spectral frequency (LSF) based vocal tract modeling and transformmation. The last method, spectral equalization, is aimed at reducing the differences in the source and target long-term spectra when the source and target recording conditions are significantly different. The voice conversion algorithm that employs the proposed techniques, is compared with the baseline voice conversion algorithm with objective tests as well as three subjective listening tests. First, similarity to the target voice is evaluated in a subjective listening test and it is shown that the proposed algorithm improves similarity to the target voice by 23.0%. An ABX test is performed and the proposed algorithm is preferred over the baseline algorithm by 76.4%. In the third test, the two algorithms are compared in terms of the subjective quality. of the voice conversion output. The proposed algorithm improves the subjective output quality by 46.8% in terms of mean opinion score (MOS). (c) 2005 Elsevier Ltd. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Bogazici Univ, Dept Elect &amp; Elect Engn, Istanbul, Turkey.
   <br>Sestek Inc, R&amp;D Dept, TR-34469 Istanbul, Turkey.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Turk, O (reprint author), Bogazici Univ, Dept Elect &amp; Elect Engn, Istanbul, Turkey.</td>
</tr>

<tr>
<td valign="top">EM </td><td>oytun@sestek.com.tr; arslanle@boun.edu.tr</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Arslan, Levent</display_name>&nbsp;</font></td><td><font size="3">D-6377-2015&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Arslan, Levent</display_name>&nbsp;</font></td><td><font size="3">0000-0002-6086-8018&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>29</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>31</td>
</tr>

<tr>
<td valign="top">PD </td><td>OCT</td>
</tr>

<tr>
<td valign="top">PY </td><td>2006</td>
</tr>

<tr>
<td valign="top">VL </td><td>20</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">BP </td><td>441</td>
</tr>

<tr>
<td valign="top">EP </td><td>467</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.csl.2005.06.001</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000240727800005</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Salor, O
   <br>Demirekler, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Salor, Ozgul
   <br>Demirekler, Mubeccel</td>
</tr>

<tr>
<td valign="top">TI </td><td>Dynamic programming approach to voice transformation</td>
</tr>

<tr>
<td valign="top">SO </td><td>SPEECH COMMUNICATION</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice transformation; speaker transformation; codebook; line spectral
   frequencies; dynamic programming</td>
</tr>

<tr>
<td valign="top">ID </td><td>CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a voice transformation algorithm which modifies the speech of a source speaker such that it is perceived as if spoken by a target speaker. A novel method which is based on dynamic programming approach is proposed. The designed system obtains speaker-specific codebooks of line spectral frequencies (LSFs) for both source and target speakers. Those codebooks are used to train a mapping histogram matrix, which is used for LSF transformation from one speaker to the other. The baseline system uses the maxima of the histogram matrix for LSF transformation. The shortcomings of this system, which are the limitations of the target LSF space and the spectral discontinuities due to independent mapping of subsequent frames, have been overcome by applying the dynamic programming approach. Dynamic programming approach tries to model the long-term behaviour of LSFs of the target speaker, while it is trying to preserve the relationship between the subsequent frames of the source LSFs, during transformation. Both objective and subjective evaluations have been conducted and it has been shown that dynamic programming approach improves the performance of the system in terms of both the speech quality and speaker similarity. (c) 2006 Elsevier B.V. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Middle E Tech Univ, Dept Elect &amp; Elect Engn, TR-06531 Ankara, Turkey.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Salor, O (reprint author), TUBITAK, Inst Space Technol Res, METU Campus, Ankara, Turkey.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ozgul.salor@bilten.metu.edu.tr</td>
</tr>

<tr>
<td valign="top">TC </td><td>4</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>4</td>
</tr>

<tr>
<td valign="top">PD </td><td>OCT</td>
</tr>

<tr>
<td valign="top">PY </td><td>2006</td>
</tr>

<tr>
<td valign="top">VL </td><td>48</td>
</tr>

<tr>
<td valign="top">IS </td><td>10</td>
</tr>

<tr>
<td valign="top">BP </td><td>1262</td>
</tr>

<tr>
<td valign="top">EP </td><td>1272</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.specom.2006.06.003</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000241586500003</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wu, CH
   <br>Hsia, CC
   <br>Liu, TH
   <br>Wang, JF</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wu, Chung-Hsien
   <br>Hsia, Chi-Chun
   <br>Liu, Te-Hsien
   <br>Wang, Jhing-Fa</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice conversion using duration-embedded Bi-HMMs for expressive speech
   synthesis</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Bi-HMM voice conversion; embedded duration model; expressive speech
   synthesis; prosody conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>EMOTION; MODELS; SYSTEM</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents an expressive voice conversion model (DeBi-HMM) as the post processing of a text-to-speech (TTS) system for expressive speech synthesis. DeBi-HMM is named for its duration-embedded characteristic of the two HMMs for modeling the source and target speech signals, respectively. joint estimation of source and target HMMs is exploited for spectrum conversion from neutral to expressive speech. Gamma distribution is embedded as the duration model for each state in source and target HMMs. The expressive style-dependent decision trees achieve prosodic conversion. The STRAIGHT algorithm is adopted for the analysis and synthesis process. A set of small-sized speech databases for each expressive style is designed and collected to train the DeBi-HMM voice conversion models. Several experiments with statistical hypothesis testing are conducted to evaluate the quality of synthetic speech as perceived by human subjects. Compared with previous voice conversion methods, the proposed method exhibits encouraging potential in expressive speech synthesis.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Natl Cheng Kung Univ, Dept Comp Sci &amp; Informat Engn, Tainan 701, Taiwan.
   <br>Natl Cheng Kung Univ, Dept Elect Engn, Tainan 701, Taiwan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Wu, CH (reprint author), Natl Cheng Kung Univ, Dept Comp Sci &amp; Informat Engn, Tainan 701, Taiwan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>chwu@csie.ncku.edu.tw; shiacj@csie.ncku.edu.tw; liu@csie.ncku.edu.tw;
   wangjf@csie.ncku.edu.tw</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Wu, Chung-Hsien</display_name>&nbsp;</font></td><td><font size="3">E-7970-2013&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>50</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>56</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUL</td>
</tr>

<tr>
<td valign="top">PY </td><td>2006</td>
</tr>

<tr>
<td valign="top">VL </td><td>14</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">BP </td><td>1109</td>
</tr>

<tr>
<td valign="top">EP </td><td>1116</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASL.2006.876112</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000238709200003</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Tao, JH
   <br>Kang, YG
   <br>Li, AJ</td>
</tr>

<tr>
<td valign="top">AF </td><td>Tao, Jianhua
   <br>Kang, Yongguo
   <br>Li, Aijun</td>
</tr>

<tr>
<td valign="top">TI </td><td>Prosody conversion from neutral speech to emotional speech</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>emotional speech; prosody analysis; speech synthesis</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Emotion is an important element in expressive speech synthesis. Unlike traditional discrete emotion simulations, this 5 paper attempts to synthesize emotional speech by using "strong" "medium," and "weak" classifications. This paper tests different models, a linear modification model (LMM), a Gaussian mixture model (GMM), and a classification and regression tree (CART) model. The linear modification model makes direct modification of sentence F0 contours and syllabic durations from acoustic distributions of emotional speech, such as, F0 topline, F0 baseline, durations, and intensities. Further analysis shows that emotional speech is also related to stress and linguistic information. Unlike the linear modification method, the GMM and CART models try to map the subtle prosody distributions between neutral and emotional speech. While the GMM just uses the features, the CART model integrates linguistic features into the mapping. A pitch target model which is optimized to describe Mandarin F0 contours is also introduced. For all conversion methods, a deviation of perceived expressiveness (DPE) measure is created to evaluate the expressiveness of the output speech., The results show that the LMM gives the worst results among the three methods. The GMM method is more suitable for a small training set, while the CART method gives the better emotional speech output if trained With a large context-balanced corpus. The methods discussed in this paper indicate ways to generate emotional speech in speech synthesis. The objective and subjective evaluation processes are also analyzed. These results support the use of a neutral semantic content text in databases for emotional speech synthesis.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing
   100080, Peoples R China.
   <br>Chinese Acad Social Sci, Inst Linguist, Beijing 100732, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Tao, JH (reprint author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100080, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jhtao@nlpr.ia.ac.cn; ygkang@nlpr.ia.ac.cn; liaj@cass.org.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>95</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>103</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUL</td>
</tr>

<tr>
<td valign="top">PY </td><td>2006</td>
</tr>

<tr>
<td valign="top">VL </td><td>14</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">BP </td><td>1145</td>
</tr>

<tr>
<td valign="top">EP </td><td>1154</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASL.2006.876113</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000238709200007</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ye, H
   <br>Young, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ye, Hui
   <br>Young, Steve</td>
</tr>

<tr>
<td valign="top">TI </td><td>Quality-enhanced voice morphing using maximum likelihood transformations</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>linear transformation; phase dispersion; voice conversion; voice
   morphing</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice morphing is a technique for modifying a source speaker's speech to sound as if it was spoken by some designated target speaker. The core process in a voice morphing system is the transformation of the spectral envelope of the source speaker to match that of the target speaker and linear transformations estimated from time-aligned parallel training data are commonly used to achieve this. However, the naive application of envelope transformation combined with the necessary pitch and duration modifications will result in noticeable artifacts. This paper studies the linear transformation approach to voice morphing and investigates these two specific issues. First, a general maximum likelihood framework is proposed for transform estimation which avoids the need for parallel training data inherent in conventional least mean square approaches. Second, the main causes of artifacts are identified as being due to glottal coupling, unnatural phase dispersion and the high spectral variance of unvoiced sounds, and compensation techniques are developed to mitigate these. The resulting voice morphing system is evaluated using both subjective and objective measures. These tests show that the proposed approaches are capable of effectively transforming speaker identity whilst maintaining high quality. Furthermore, they do not require carefully prepared parallel training data.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Univ Cambridge, Dept Engn, Cambridge CB2 1PZ, England.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ye, H (reprint author), Univ Cambridge, Dept Engn, Cambridge CB2 1PZ, England.</td>
</tr>

<tr>
<td valign="top">EM </td><td>hy216@eng.cam.ac.uk; sjy@eng.cam.ac.uk</td>
</tr>

<tr>
<td valign="top">TC </td><td>54</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>59</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUL</td>
</tr>

<tr>
<td valign="top">PY </td><td>2006</td>
</tr>

<tr>
<td valign="top">VL </td><td>14</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">BP </td><td>1301</td>
</tr>

<tr>
<td valign="top">EP </td><td>1312</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TSA.2005.860839</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000238709200020</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Mouchtaris, A
   <br>Van der Spiegel, J
   <br>Mueller, P</td>
</tr>

<tr>
<td valign="top">AF </td><td>Mouchtaris, A
   <br>Van der Spiegel, J
   <br>Mueller, P</td>
</tr>

<tr>
<td valign="top">TI </td><td>Nonparallel training for voice conversion based on a parameter
   adaptation approach</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Gaussian mixture model; speaker adaptation; text-to-speech synthesis;
   voice conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH ANALYSIS SYNTHESIS; TRANSFORMATION; INDIVIDUALITY; MODELS</td>
</tr>

<tr>
<td valign="top">AB </td><td>The objective of voice conversion algorithms is to modify the speech by a particular source speaker so that it sounds as if spoken by a different target speaker. Current conversion algorithms employ a training procedure, during which the same utterances spoken by both the source and target speakers are needed for deriving the desired conversion parameters. Such a (parallel) corpus, is often difficult or impossible to collect. Here, we propose an algorithm that relaxes this constraint, i.e., the training corpus does not necessarily contain the same utterances from both speakers. The proposed algorithm is based on speaker adaptation techniques, adapting the conversion parameters derived for a particular pair of speakers to a different pair, for which only a nonparallel corpus is available. We show that adaptation reduces the error obtained when simply applying the conversion parameters of one pair of speakers to another by a factor that can reach 30%. A speaker identification measure is also employed that more insightfully portrays the importance of adaptation, while listening tests confirm the success of our method. Both the objective and subjective tests employed, demonstrate that the proposed algorithm achieves comparable results with the ideal case when a parallel corpus is available.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Univ Penn, Dept Elect &amp; Syst Engn, Philadelphia, PA 19104 USA.
   <br>Corticon Inc, King Of Prussia, PA 19406 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Mouchtaris, A (reprint author), Fdn Res &amp; Technol Hellas, Inst Comp Sci, GR-71110 Iraklion, Greece.</td>
</tr>

<tr>
<td valign="top">EM </td><td>mouchtar@ieee.org; jan@seas.upenn.edu; cortion@aol.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>57</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>60</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAY</td>
</tr>

<tr>
<td valign="top">PY </td><td>2006</td>
</tr>

<tr>
<td valign="top">VL </td><td>14</td>
</tr>

<tr>
<td valign="top">IS </td><td>3</td>
</tr>

<tr>
<td valign="top">BP </td><td>952</td>
</tr>

<tr>
<td valign="top">EP </td><td>963</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TSA.2005.857790</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000237140500019</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Rao, KS
   <br>Yegnanarayana, B</td>
</tr>

<tr>
<td valign="top">AF </td><td>Rao, KS
   <br>Yegnanarayana, B</td>
</tr>

<tr>
<td valign="top">TI </td><td>Prosody modification using instants of significant excitation</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>duration; excitation source; instants of significant excitation
   (epochs); linear prediction pitch synchronous overlap and add
   (LP-PSOLA); LP residual; pitch period; prosody modification</td>
</tr>

<tr>
<td valign="top">ID </td><td>TIME-SCALE MODIFICATION; FOURIER-ANALYSIS; SPEECH SYNTHESIS; PITCH;
   EXTRACTION; CONVERSION; SIGNALS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Prosody modification involves changing the pitch and duration of speech without affecting the message and naturalness. This paper proposes a method for prosody (pitch and duration) modification using the instants of significant excitation of the vocal tract system during the production of speech. The instants of significant excitation correspond to the instants of glottal closure (epochs) in the case of voiced speech, and to some random excitations like onset of burst in the case of nonvoiced speech. Instants of significant excitation are computed from the linear prediction (LP) residual of speech signals by using the property of average group-delay of minimum phase signals. The modification of pitch and duration is achieved by manipulating the LP residual with the help of the knowledge of the instants of significant excitation. The modified residual is used to excite the time-varying filter, whose parameters are derived from the original speech signal. Perceptual quality of the synthesized speech is good and is without any significant distortion. The proposed method is evaluated using waveforms, spectrograms, and listening tests. The performance of the method is compared with linear prediction pitch synchronous overlap and add (LP-PSOLA) method, which is another method for prosody manipulation based on the modification of the LP residual. The original and the synthesized speech signals obtained by the proposed method and by the LP-PSOLA method are available for listening at http://speech.cs.iitm.ernet.in/Main/result/prosody.html.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Indian Inst Technol Guwahati, Dept Elect &amp; Commun Engn, Gauhati 781039,
   India.
   <br>Indian Inst Technol, Dept Comp Sci &amp; Engn, Madras 600036, Tamil Nadu,
   India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Rao, KS (reprint author), Indian Inst Technol Guwahati, Dept Elect &amp; Commun Engn, Gauhati 781039, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ksrao@iitg.ernet.in; yegna@cs.iitm.ernet.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>97</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>98</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAY</td>
</tr>

<tr>
<td valign="top">PY </td><td>2006</td>
</tr>

<tr>
<td valign="top">VL </td><td>14</td>
</tr>

<tr>
<td valign="top">IS </td><td>3</td>
</tr>

<tr>
<td valign="top">BP </td><td>972</td>
</tr>

<tr>
<td valign="top">EP </td><td>980</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TSA.2005.858051</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000237140500021</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Chang, GK
   <br>Yu, JJ
   <br>Yeo, YK
   <br>Chowdhury, A
   <br>Jia, ZS</td>
</tr>

<tr>
<td valign="top">AF </td><td>Chang, Gee-Kung
   <br>Yu, Jianjun
   <br>Yeo, Yong-Kee
   <br>Chowdhury, Arshad
   <br>Jia, Zhensheng</td>
</tr>

<tr>
<td valign="top">TI </td><td>Enabling technologies for next-generation optical packet-switching
   networks</td>
</tr>

<tr>
<td valign="top">SO </td><td>PROCEEDINGS OF THE IEEE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>clock recovery; core networks; generalized multiprotocol label switching
   (GMPLS); IP over wavelength division multiplexing (WDM); optical buffer;
   optical Internet; optical label generation and swapping; optical
   label-switching network; optical packet switching; wavelength conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>40GBIT/S WAVELENGTH CONVERSION; CARRIER SUPPRESSION; ELECTROABSORPTION
   MODULATOR; CLOCK RECOVERY; WDM NETWORK; LOOP MIRROR; HIGH-SPEED; LABEL;
   DELAY; SYSTEM</td>
</tr>

<tr>
<td valign="top">AB </td><td>The optical packet-switching network is considered to be one of the most promising solutions for end-to-end delivery of high-bitrate data, video, and voice signals across optical networks of the future. optical label switching (OLS) technology incurs simpler extraction and processing of the labels so that the optical packets can be routed with low latency to the destinations. we have developed several key enabling technologies for integrated optical networks, including optical label generation, label swapping, optical buffering, clock recovery, and wavelength conversion. we have designed and experimentally demonstrated these enabling techniques that can provide efficient broadband services in future optical networks.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Georgia Inst Technol, Sch Elect &amp; Comp Engn, Atlanta, GA 30332 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Chang, GK (reprint author), Georgia Inst Technol, Sch Elect &amp; Comp Engn, Atlanta, GA 30332 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>gkchang@ece.gatech.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>38</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>38</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAY</td>
</tr>

<tr>
<td valign="top">PY </td><td>2006</td>
</tr>

<tr>
<td valign="top">VL </td><td>94</td>
</tr>

<tr>
<td valign="top">IS </td><td>5</td>
</tr>

<tr>
<td valign="top">BP </td><td>892</td>
</tr>

<tr>
<td valign="top">EP </td><td>910</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/JPROC.2006.873433</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000237857800013</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Dearnley, CA</td>
</tr>

<tr>
<td valign="top">AF </td><td>Dearnley, CA</td>
</tr>

<tr>
<td valign="top">TI </td><td>Knowing nursing and finding the professional voice: A study of enrolled
   nurses converting to first level registration</td>
</tr>

<tr>
<td valign="top">SO </td><td>NURSE EDUCATION TODAY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>enrolled nurse conversion; ways of knowing nursing; reflection;
   professional development</td>
</tr>

<tr>
<td valign="top">AB </td><td>This study explored the experiences of a group of second level registered nurses converting to first level registration in a two-year, part-time open learning programme. It aimed to examine the relationship between the mode of course delivery and the personal and professional development experienced by the learners. Data were collected by semi-structured interviews, conducted at five points over the two-year period and analyzed by constant comparative analysis. A theoretical model was developed to demonstrate the stages of knowing nursing, experienced by the students as they made the transition from passive to autonomous professional practice.
   <br>In their 'Women's Ways of Knowing' [Belenky M. F., Clinchy B.A., Goldberger N. R., Tarule J.M., 1986. Women's Ways of Knowing. Basic Books, New York] identified a progression through different stages of thinking about the nature of knowledge and evidence. This progression was evident in the nurses in this study. It was applied and developed to reflect the progression through ways of knowing nursing. This process consisted of three positions of development, each underpinned by their associated ways of knowing [Belenky M.F., Clinchy B.A., Goldberger N.R., Tarule J.M., 1986. Women's Ways of Knowing. Basic Books, New York] and characterized by a specific approach to professional nursing practice.
   <br>The changing ways of knowing nursing, described in this paper, are likely to be observed in registered nurses and health care assistants who are now entering Higher Education (HE) to undertake higher awards for the first time. A clearer understanding of this process of personal and professional change, and factors that impact upon it, will be a useful guide for those supporting learners both in the clinical and academic arenas. (c) 2005 Elsevier Ltd. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Univ Bradford, Sch Hlth Studies, Bradford BD5 0BB, W Yorkshire, England.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Dearnley, CA (reprint author), Univ Bradford, Sch Hlth Studies, Trin Rd, Bradford BD5 0BB, W Yorkshire, England.</td>
</tr>

<tr>
<td valign="top">EM </td><td>c.a.dearnley1@bradford.ac.uk</td>
</tr>

<tr xmlns:date="http://exslt.org/dates-and-times">
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Dearnley, Christine</display_name>&nbsp;</font></td><td><font size="3">0000-0002-3067-4554&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>18</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>18</td>
</tr>

<tr>
<td valign="top">PD </td><td>APR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2006</td>
</tr>

<tr>
<td valign="top">VL </td><td>26</td>
</tr>

<tr>
<td valign="top">IS </td><td>3</td>
</tr>

<tr>
<td valign="top">BP </td><td>209</td>
</tr>

<tr>
<td valign="top">EP </td><td>217</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.nedt.2005.10.002</td>
</tr>

<tr>
<td valign="top">SC </td><td>Education &amp; Educational Research; Nursing</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000236289100005</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Angkititrakul, P
   <br>Hansen, JHL</td>
</tr>

<tr>
<td valign="top">AF </td><td>Angkititrakul, P
   <br>Hansen, JHL</td>
</tr>

<tr>
<td valign="top">TI </td><td>Advances in phone-based modeling for automatic accent classification</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>automatic accent classification; dialect modeling; open accent
   classification; phoneme recognition; spectral trajectory modeling;
   speech recognition</td>
</tr>

<tr>
<td valign="top">ID </td><td>CONTINUOUS SPEECH RECOGNITION; AMERICAN ENGLISH; FOREIGN ACCENT; WORD;
   FEATURES; FRENCH</td>
</tr>

<tr>
<td valign="top">AB </td><td>It is suggested that algorithms capable of estimating and characterizing accent knowledge would provide valuable information in the development of more effective speech systems such as speech recognition, speaker identification, audio stream tagging in spoken document retrieval, channel monitoring, or voice conversion. Accent knowledge could be used for selection of alternative pronunciations in a lexicon, engage adaptation for acoustic modeling, or provide information for biasing a language model in large vocabulary speech recognition. In this paper, we propose a text-independent automatic accent classification system using phone-based models. Algorithm formulation begins with a series of experiments focused on capturing the spectral evolution information as potential accent sensitive cues. Alternative subspace representations using principal component analysis and linear discriminant analysis with projected trajectories are considered. Finally, an experimental study is performed to compare the spectral trajectory model framework to a traditional hidden Markov model recognition framework using an accent sensitive word corpus. System evaluation is performed using a corpus representing five English speaker groups with native American English, and English spoken with Mandarin Chinese, French, Thai, and Turkish accents for both male and female speakers.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Univ Texas, Sch Engn &amp; Comp Sci, Richardson, TX 75083 USA.
   <br>Univ Colorado, Ctr Spoken Language Res, Robust Speech Proc Grp, Boulder,
   CO 80302 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Angkititrakul, P (reprint author), Univ Texas, Sch Engn &amp; Comp Sci, Richardson, TX 75083 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>angkitit@utdallas.edu; john.hansen@utdallas.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>33</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>33</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2006</td>
</tr>

<tr>
<td valign="top">VL </td><td>14</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>634</td>
</tr>

<tr>
<td valign="top">EP </td><td>646</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TSA.2005.851980</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000235615000024</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Lee, CL
   <br>Chang, WW
   <br>Chiang, YC</td>
</tr>

<tr>
<td valign="top">AF </td><td>Lee, CL
   <br>Chang, WW
   <br>Chiang, YC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Spectral and prosodic transformations of hearing-impaired Mandarin
   speech</td>
</tr>

<tr>
<td valign="top">SO </td><td>SPEECH COMMUNICATION</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; prosodic modification; spectral conversion;
   hearing-impaired speaker; sinusoidal model</td>
</tr>

<tr>
<td valign="top">ID </td><td>CONVERSION; ALGORITHM</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper studies the combined use of spectral and prosodic conversions to enhance the hearing-impaired Mandarin speech. The analysis-synthesis system is based on a sinusoidal representation of the speech production mechanism. By taking advantage of the tone structure in Mandarin speech, pitch contours are orthogonally transformed and applied within the sinusoidal framework to perform pitch modification. Also proposed is a time-scale modification algorithm that finds accurate alignments between hearing-impaired and normal utterances. Using the alignments, spectral conversion is performed on subsyllabic acoustic units by a continuous probabilistic transform based on a Gaussian mixture model. Results of perceptual evaluation indicate that the proposed system greatly improves the intelligibility and the naturalness of hearing-impaired Mandarin speech. (c) 2005 Elsevier B.V. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Natl Chiao Tung Univ, Dept Commun Engn, Hsinchu 300, Taiwan.
   <br>Natl Hsinchu Teachers Coll, Dept Special Educ, Hsinchu, Taiwan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Chang, WW (reprint author), Natl Chiao Tung Univ, Dept Commun Engn, Hsinchu 300, Taiwan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>wwchang@cc.nctu.edu.tw</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PD </td><td>FEB</td>
</tr>

<tr>
<td valign="top">PY </td><td>2006</td>
</tr>

<tr>
<td valign="top">VL </td><td>48</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>207</td>
</tr>

<tr>
<td valign="top">EP </td><td>219</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.specom.2005.08.001</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000234772500007</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sundermann, D
   <br>Hoge, H
   <br>Bonafonte, A
   <br>Ney, H
   <br>Black, A
   <br>Narayanan, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Suendermann, David
   <br>Hoege, Harald
   <br>Bonafonte, Antonio
   <br>Ney, Hermann
   <br>Black, Alan
   <br>Narayanan, Shri</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Text-independent voice conversion based on unit selection</td>
</tr>

<tr>
<td valign="top">SO </td><td>2006 IEEE International Conference on Acoustics, Speech and Signal
   Processing, Vols 1-13</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   (ICASSP)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>31st IEEE International Conference on Acoustics, Speech and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 14-19, 2006</td>
</tr>

<tr>
<td valign="top">CL </td><td>Toulouse, FRANCE</td>
</tr>

<tr>
<td valign="top">AB </td><td>So far, most of the voice conversion training procedures are text-dependent, i.e., they are based on parallel training utterances of source and target speaker. Since several applications (e.g. speech-to-speech translation or dubbing) require text-independent training, over the last two years, training techniques that use non-parallel data were proposed. In this paper, we present a new approach that applies unit selection to find corresponding time frames in source and target speech. By means of a subjective experiment it is shown that this technique achieves the same performance as the conventional text-dependent training.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Siemens AG, Corp Technol, D-8000 Munich, Germany.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sundermann, D (reprint author), Siemens AG, Corp Technol, D-8000 Munich, Germany.</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Narayanan, Shrikanth</display_name>&nbsp;</font></td><td><font size="3">D-5676-2012&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>20</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>20</td>
</tr>

<tr>
<td valign="top">PY </td><td>2006</td>
</tr>

<tr>
<td valign="top">BP </td><td>81</td>
</tr>

<tr>
<td valign="top">EP </td><td>84</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science; Engineering; Imaging Science &amp; Photographic
   Technology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000245559900021</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Duxans, H
   <br>Bonafonte, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Duxans, Helenca
   <br>Bonafonte, Antonio</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Residual conversion versus prediction on voice morphing systems</td>
</tr>

<tr>
<td valign="top">SO </td><td>2006 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING, VOLS 1-13</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>31st IEEE International Conference on Acoustics, Speech and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 14-19, 2006</td>
</tr>

<tr>
<td valign="top">CL </td><td>Toulouse, FRANCE</td>
</tr>

<tr>
<td valign="top">AB </td><td>Many of the research efforts in Voice Morphing, or also called Voice Conversion (VC), has been carried out in the field of vocal tract mapping. It has been studied that in the vocal tract parameters there is the most relevant part of the information about speaker identity. However, to achieve an effective personality change it is also needed to modify the glottal flow characteristics of the source speaker. In this paper two strategies of transformation of LPC residual signals for a voice morphing system based in LSF mapping are compared: conversion of the source residual by codebook mapping and prediction of the target residual from LSF vectors. Experimental results demonstrates that the relationship between LSF parameters and their residual signals is higher that the relationship between LPC residual signals of two different aligned speakers.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Univ Politecn Cataluna, TALP Res Ctr, Dept Signal Theory &amp; Commun,
   Barcelona, Spain.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Duxans, H (reprint author), Univ Politecn Cataluna, TALP Res Ctr, Dept Signal Theory &amp; Commun, Barcelona, Spain.</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2006</td>
</tr>

<tr>
<td valign="top">BP </td><td>85</td>
</tr>

<tr>
<td valign="top">EP </td><td>88</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science; Engineering; Imaging Science &amp; Photographic
   Technology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000245559900022</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Zuo, GY
   <br>Chen, Y
   <br>Ruan, XG
   <br>Liu, WJ</td>
</tr>

<tr>
<td valign="top">AF </td><td>Zuo, Guoyu
   <br>Chen, Yao
   <br>Ruan, Xiaogang
   <br>Liu, Wenju</td>
</tr>

<tr>
<td valign="top">BE </td><td>Yeung, DS
   <br>Liu, ZQ
   <br>Wang, XZ
   <br>Yan, H</td>
</tr>

<tr>
<td valign="top">TI </td><td>Mandarin voice conversion using tone codebook mapping</td>
</tr>

<tr>
<td valign="top">SO </td><td>ADVANCES IN MACHINE LEARNING AND CYBERNETICS</td>
</tr>

<tr>
<td valign="top">SE </td><td>Lecture Notes in Artificial Intelligence</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article; Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>4th International Conference on Machine Learning and Cybernetics</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 18-21, 2005</td>
</tr>

<tr>
<td valign="top">CL </td><td>Canton, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">AB </td><td>A tone codebook mapping method is proposed to obtain a better performance in voice conversion of Mandarin speech than the conventional conversion method which deals mainly with short-time spectral envelopes. The pitch contour of the whole Mandarin syllable is used as a unit type for pitch conversion. The syllable pitch contours are first extracted from the source and target utterances. Time normalization and moving average filtering are then performed on them. These preprocessed pitch contours are classified to generate the source and target tone codebooks, and by associating them, a Mandarin tone mapping codebook is finally obtained in terms of speech alignment. Experiment results show that the proposed method for voice conversion can deliver a satisfactory performance in Mandarin speech.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Beijing Univ Technol, Inst Artificial Intelligence &amp; Robot, Beijing
   100022, Peoples R China.
   <br>Beijing Univ Technol, Sch Comp Sci, Beijing 10022, Peoples R China.
   <br>Chinese Acad Sci, Inst Automat, Beijing 100080, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Zuo, GY (reprint author), Beijing Univ Technol, Inst Artificial Intelligence &amp; Robot, Beijing 100022, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>zuoguoyu@bjut.edu.cn; yaochen@bjut.edu.cn; adrxg@bjut.edu.cn;
   lwj@nlpr.ia.ac.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2006</td>
</tr>

<tr>
<td valign="top">VL </td><td>3930</td>
</tr>

<tr>
<td valign="top">BP </td><td>965</td>
</tr>

<tr>
<td valign="top">EP </td><td>973</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000238282100101</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Abeysekera, SS
   <br>Charoensak, C</td>
</tr>

<tr>
<td valign="top">AF </td><td>Abeysekera, Saman S.
   <br>Charoensak, Charayaphan</td>
</tr>

<tr>
<td valign="top">TI </td><td>Efficient realization of sigma-delta (Sigma-Delta) Kalman lowpass filter
   in hardware using FPGA</td>
</tr>

<tr>
<td valign="top">SO </td><td>EURASIP JOURNAL ON APPLIED SIGNAL PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">ID </td><td>MODULATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Sigma-delta (Sigma-Delta) modulation techniques have moved into mainstream applications in signal processing and have found many practical uses in areas such as high-resolution A/D, D/A conversions, voice communication, and software radio. Sigma-Delta modulators produce a single, or few bits output resulting in hardware saving and thus making them suitable for implementation in very large scale integration (VLSI) circuits. To reduce quantization noise produced, higher-order modulators such as multiloop and multistage architectures are commonly used. The quantization noise behavior of higher-order Sigma-Delta modulators is well understood. Based on these quantization noise characteristics, various demodulator architectures, such as sinc filter, optimal FIR filter, and Laguerre filter are reported in literature. In this paper, theory and design of an efficient Kalman recursive demodulator filter is shown. Hardware implementation of Kalman lowpass filter, using field programmable gate array (FPGA), is explained. The FPGA synthesis results from Kalman filter design are compared with previous designs for sinc filter, optimum FIR filter, and Laguerre filter.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Nanyang Technol Univ, Sch Elect &amp; Elect Engn, Singapore 639798,
   Singapore.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Abeysekera, SS (reprint author), Nanyang Technol Univ, Sch Elect &amp; Elect Engn, Block S1,Nanyang Ave, Singapore 639798, Singapore.</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Abeysekera, Saman</display_name>&nbsp;</font></td><td><font size="3">A-5164-2011&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2006</td>
</tr>

<tr>
<td valign="top">AR </td><td>52736</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1155/ASP/2006/52736</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000240451300001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ueda, Y
   <br>Hirota, M
   <br>Sakata, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ueda, Yuichi
   <br>Hirota, Minako
   <br>Sakata, Tadashi</td>
</tr>

<tr>
<td valign="top">BE </td><td>Pan, JS
   <br>Shi, P
   <br>Zhao, Y</td>
</tr>

<tr>
<td valign="top">TI </td><td>Vowel synthesis based on the spectral morphing and its application to
   speaker conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>ICICIC 2006: FIRST INTERNATIONAL CONFERENCE ON INNOVATIVE COMPUTING,
   INFORMATION AND CONTROL, VOL 2, PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>1st International Conference on Innovative Computing, Information and
   Control (ICICIC 2006)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 30-SEP 01, 2006</td>
</tr>

<tr>
<td valign="top">CL </td><td>Beijing, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper we propose a speaker conversion and morphing technique for speech synthesis based on the spectral morphing, although the target synthesized voice is restricted to vowel and vowel-like speech sound at present. In the experiments, we have prepared the databases of four speakers, which consisted of the normalized spectra on Japanese fundamental five vowels and the mean fundamental frequency (pitch). The raw speech parameters including three formant frequencies, pitch pattern and amplitude one were extracted from the connected vowels, /aiueo/ uttered by a male and converted or morphed into another speaker voice using each database. As results of the dissimilarity tests, the desirable performances in speaker conversion or speaker morphing were confirmed in the proposed vowel synthesis. It is expected that this type of voice synthesis method will be applicable to a hybrid TTS, where a specific speaker voice is synthesized according to the synthetic rule.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ueda, Yuichi] Kumamoto Univ, Grad Sch Sci &amp; Technol, Kumamoto, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ueda, Y (reprint author), Kumamoto Univ, Grad Sch Sci &amp; Technol, Kumamoto, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ueda@cs.kumamoto-u.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2006</td>
</tr>

<tr>
<td valign="top">BP </td><td>738</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Automation &amp; Control Systems; Computer Science; Engineering; Imaging
   Science &amp; Photographic Technology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000240868000179</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Rao, KS
   <br>Yegnanarayana, B</td>
</tr>

<tr>
<td valign="top">AF </td><td>Rao, K. Sreenivasa
   <br>Yegnanarayana, B.</td>
</tr>

<tr>
<td valign="top">BE </td><td>Mohanty, SP
   <br>Sahoo, A</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice conversion by prosody and vocal tract modification</td>
</tr>

<tr>
<td valign="top">SO </td><td>ICIT 2006: 9TH INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY,
   PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th International Conference on Information Technology (ICIT 2006)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 18-21, 2006</td>
</tr>

<tr>
<td valign="top">CL </td><td>Bhubaneswar, INDIA</td>
</tr>

<tr>
<td valign="top">ID </td><td>GROUP DELAY FUNCTION; SIGNIFICANT EXCITATION; INSTANTS; EXTRACTION</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper we proposed some flexible methods, which are useful in the process of voice conversion. The proposed methods modify the shape of the vocal tract system and the characteristics of the prosody according to the desired requirement. The shape of the vocal tract system is modified by shifting the major resonant frequencies (formants) of the short term spectrum, and altering their bandwidths accordingly. In the case of prosody modification, the required durational and intonational characteristics are imposed on the given speech signal. In the proposed method, the prosodic characteristics are manipulated using instants of significant excitation. The instants of significant excitation correspond to the instants of glottal closure (epochs) in the case of voiced speech, and to some random excitations like onset of burst in the case of nonvoiced speech. Instants of significant excitation are computed from the Linear Prediction (LP) residual of the speech signals by using the property of average group delay of minimum phase signals. The manipulations of durational characteristics and pitch contour (intonation pattern) are achieved by manipulating the LP residual with the help of the knowledge of the instants of significant excitation, The modified LP residual is used to excite the time varying filter. The filter parameters are updated according to the desired vocal tract characteristics. The proposed methods are evaluated using listening tests.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Rao, K. Sreenivasa] Indian Inst Technol, Dept Elect Commun Engn,
   Gauhati 781039, India.
   <br>[Yegnanarayana, B.] Indian Inst Technol, Dept Comp Sci &amp; Engn, Madras
   600036, Tamil Nadu, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Rao, KS (reprint author), Indian Inst Technol, Dept Elect Commun Engn, Gauhati 781039, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ksrao@iitg.ernet.in; yegna@cs.iitm.ernet.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>4</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>4</td>
</tr>

<tr>
<td valign="top">PY </td><td>2006</td>
</tr>

<tr>
<td valign="top">BP </td><td>111</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000245510400028</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Bay, M
   <br>Beauchamp, JW</td>
</tr>

<tr>
<td valign="top">AF </td><td>Bay, M
   <br>Beauchamp, JW</td>
</tr>

<tr>
<td valign="top">BE </td><td>Rosca, J
   <br>Erdogmus, D
   <br>Principe, JC
   <br>Haykin, S</td>
</tr>

<tr>
<td valign="top">TI </td><td>Harmonic source separation using prestored spectra</td>
</tr>

<tr>
<td valign="top">SO </td><td>INDEPENDENT COMPONENT ANALYSIS AND BLIND SIGNAL SEPARATION, PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">SE </td><td>Lecture Notes in Computer Science</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article; Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>6th International Conference on Independent Component Analysis and Blind
   Signal Separation</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 05-08, 2006</td>
</tr>

<tr>
<td valign="top">CL </td><td>Charleston, SC</td>
</tr>

<tr>
<td valign="top">AB </td><td>Detecting multiple pitches (FOs) and segregating musical instrument lines from monaural recordings of contrapuntal polyphonic music into separate tracks is a difficult problem in music signal processing. Applications include audio-to-MIDI conversion, automatic music transcription, and audio enhancement and transformation. Past attempts at separation have been limited to separating two harmonic signals in a contrapuntal duet (Maher, 1990) or several harmonic signals in a single chord (Virtanen and Klapuri, 2001, 2002). Several researchers have attempted polyphonic pitch detection (Klapuri, 2001; Eggink and Brown, 2004a), predominant melody extraction (Goto, 2001; Marolt, 2004; Eggink and Brown, 2004b), and instrument recognition (Eggink and Brown, 2003). Our solution assumes that each instrument is represented as a time-varying harmonic series and that errors can be corrected using prior knowledge of instrument spectra. Fundamental frequencies (FOs) for each time frame are estimated from input spectral data using an Expectation-Maximization (EM) based algorithm with Gaussian distributions used to represent the harmonic series, Collisions (i.e., overlaps) between instrument harmonics, which frequently occur, are predicted from the estimated FOs. The uncollided harmonics are matched to ones contained in a pre-stored spectrum library in order that each FO's harmonic series is assigned to the appropriate instrument. Corrupted harmonics are restored using data taken from the library. Finally, each voice is additively resynthesized to a separate track. This algorithm is demonstrated for a monaural signal containing three contrapuntal musical instrument voices with distinct timbres.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Univ Illinois, Sch Mus, Urbana, IL 61801 USA.
   <br>Univ Illinois, Dept Elect &amp; Comp Engn, Urbana, IL 61801 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Bay, M (reprint author), Univ Illinois, Sch Mus, Urbana, IL 61801 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>mertbay@uiuc.edu; jwbeauch@uiuc.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>6</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>6</td>
</tr>

<tr>
<td valign="top">PY </td><td>2006</td>
</tr>

<tr>
<td valign="top">VL </td><td>3889</td>
</tr>

<tr>
<td valign="top">BP </td><td>561</td>
</tr>

<tr>
<td valign="top">EP </td><td>568</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000236486300070</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Cheng, YM
   <br>Ma, CX
   <br>Melnar, L</td>
</tr>

<tr>
<td valign="top">AF </td><td>Cheng, Yan Ming
   <br>Ma, Changxue
   <br>Melnar, Lynette</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA</td>
</tr>

<tr>
<td valign="top">TI </td><td>Cross-Language Evaluation of Voice-To-Phoneme Conversions for Voice-Tag
   Application in Embedded Platforms</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2006 AND 9TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE
   PROCESSING, VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th International Conference on Spoken Language Processing/INTERSPEECH
   2006</td>
</tr>

<tr>
<td valign="top">CY </td><td>2006</td>
</tr>

<tr>
<td valign="top">CL </td><td>Pittsburgh, PA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice-to-phoneme; voice-tag; speech recognition; embedded platform;
   cross-language; multilingual</td>
</tr>

<tr>
<td valign="top">AB </td><td>Previously, we proposed two voice-to-phoneme conversion algorithms for speaker-independent voice-tag creation specifically targeted at applications on embedded platforms, an environment sensitive to CPU and memory resource consumption [1]. These two algorithms (batch mode and sequential) were applied in a same-language context, i.e., both acoustic model training and voice-tag creation and application were performed on the same language.
   <br>In this paper, we investigate the cross-language application of these two voice-to-phoneme conversion algorithms, where the acoustic models are trained on a particular source language while the voice-tags are created and applied on a different target language. Here, both algorithms create phonetic representations of a voice-tag of a target language based on the speaker-independent acoustic models of a distinct source language. Our experiments show that recognition performances of these voice-tags vary depending on the source-target language pair, with the variation reflecting the predicted phonological similarity between the source and target languages. Among the most similar languages, performance nears that of the native-trained models and surpasses the native reference baseline.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Cheng, Yan Ming; Ma, Changxue; Melnar, Lynette] Motorola Labs, Human
   Interact Res, Schaumburg, IL 60196 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Cheng, YM (reprint author), Motorola Labs, Human Interact Res, 1925 Algonquin Rd, Schaumburg, IL 60196 USA.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2006</td>
</tr>

<tr>
<td valign="top">BP </td><td>121</td>
</tr>

<tr>
<td valign="top">EP </td><td>124</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000269965900031</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nakamura, K
   <br>Toda, T
   <br>Saruwatari, H
   <br>Shikano, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nakamura, Keigo
   <br>Toda, Tomoki
   <br>Saruwatari, Hiroshi
   <br>Shikano, Kiyohiro</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA</td>
</tr>

<tr>
<td valign="top">TI </td><td>Speaking Aid System for Total Laryngectomees Using Voice Conversion of
   Body Transmitted Artificial Speech</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2006 AND 9TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE
   PROCESSING, VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th International Conference on Spoken Language Processing/INTERSPEECH
   2006</td>
</tr>

<tr>
<td valign="top">CY </td><td>2006</td>
</tr>

<tr>
<td valign="top">CL </td><td>Pittsburgh, PA</td>
</tr>

<tr>
<td valign="top">DE </td><td>total laryngectomees; voice conversion; Non-Audible Murmur (NAM)
   microphone; electrolarynx</td>
</tr>

<tr>
<td valign="top">AB </td><td>The aim of this paper is to improve the naturalness of speech using a medical device such as an electrolarynx. There are several problems associated with using existing electrolarynxes, such as the fact the loud volume of the electrolarynx itself might disturb smooth interpersonal communication, and that the generated speech is unnatural. We propose a novel speaking-aid system for total laryngectomees using a new sound source as an alternative to the existing electrolarynx and a statistical voice-conversion technique. The new sound-source unit outputs extremely small signals that cannot be heard by people around the speaker. Artificial speech is recorded with a NAM microphone through soft tissues of the head. From the result of voice conversion, the body-transmitted artificial speech is consistently converted to a more natural voice. We also demonstrate that the speech recognition performance of the proposed system substantially increases in terms of objective evaluation.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nakamura, Keigo; Toda, Tomoki; Saruwatari, Hiroshi; Shikano, Kiyohiro]
   Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nakamura, K (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>kei-naka@is.naist.jp; tomoki@is.naist.jp; sawatari@is.naist.jp;
   shikano@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>8</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>8</td>
</tr>

<tr>
<td valign="top">PY </td><td>2006</td>
</tr>

<tr>
<td valign="top">BP </td><td>1395</td>
</tr>

<tr>
<td valign="top">EP </td><td>1398</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000269965901086</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nicolao, M
   <br>Drioli, C
   <br>Cosi, P</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nicolao, Mauro
   <br>Drioli, Carlo
   <br>Cosi, Piero</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice GMM modelling for FESTIVAL/MBROLA emotive TTS synthesis</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2006 AND 9TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE
   PROCESSING, VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th International Conference on Spoken Language Processing/INTERSPEECH
   2006</td>
</tr>

<tr>
<td valign="top">CY </td><td>2006</td>
</tr>

<tr>
<td valign="top">CL </td><td>Pittsburgh, PA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Emotive Speech Synthesis; Voice Conversion; GMM; Italian Festival;
   MBROLA</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice quality is recognized to play an important role for the rendering of emotions in verbal communication. In this paper we explore the effectiveness of a processing framework for voice transformations finalized to the analysis and synthesis of emotive speech. We use a GMM-based model to compute the differences between an MBROLA voice and an anger voice, and we address the modification of the MBROLA voice spectra by using a set of spectral conversion functions trained on the data.
   <br>We propose to organize the speech data for the training in such way that the target emotive speech data and the diphone database used for the text-to-speech synthesis, both come from the same speaker. A copy-synthesis procedure is used to produce synthesis speech utterances where pitch patterns, phoneme duration, and principal speaker characteristics are the same as in the target emotive utterances. This results in a better isolation of the voice quality differences due to the emotive arousal.
   <br>Three different models to represent voice quality differences are applied and compared. The models are all based on a GMM representation of the acoustic space. The performance of these models is discussed and the experimental results and assessment are presented.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nicolao, Mauro; Drioli, Carlo; Cosi, Piero] CNR, Ist Sci &amp; Tecnol
   Cogniz Sede Padova Fonet &amp; Diale, I-35121 Padua, Italy.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nicolao, M (reprint author), CNR, Ist Sci &amp; Tecnol Cogniz Sede Padova Fonet &amp; Diale, Via G Anghinoni 10, I-35121 Padua, Italy.</td>
</tr>

<tr>
<td valign="top">EM </td><td>nicolao@pd.istc.cnr.it; drioli@pd.istc.cnr.it; cosi@pd.istc.cnr.it</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2006</td>
</tr>

<tr>
<td valign="top">BP </td><td>1794</td>
</tr>

<tr>
<td valign="top">EP </td><td>1797</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000269965901186</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Qin, L
   <br>Wu, YJ
   <br>Ling, ZH
   <br>Wang, RH</td>
</tr>

<tr>
<td valign="top">AF </td><td>Qin, Long
   <br>Wu, Yi-jian
   <br>Ling, Zhen-Hua
   <br>Wang, Ren-Hua</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA</td>
</tr>

<tr>
<td valign="top">TI </td><td>Improving the Performance of HMM-Based Voice Conversion using Context
   Clustering Decision Tree and Appropriate Regression Matrix Format</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2006 AND 9TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE
   PROCESSING, VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th International Conference on Spoken Language Processing/INTERSPEECH
   2006</td>
</tr>

<tr>
<td valign="top">CY </td><td>2006</td>
</tr>

<tr>
<td valign="top">CL </td><td>Pittsburgh, PA</td>
</tr>

<tr>
<td valign="top">DE </td><td>model adaptation; regression matrix clustering; and regression matrix
   format</td>
</tr>

<tr>
<td valign="top">AB </td><td>To improve the performance of the HMM-based voice conversion system in which the LSP coefficient is introduced as the spectral representation, a model clustering technique to tie HMMs into classes for the model adaptation, considering the phonetic and linguistic contextual factors of HMMs, is adopted in this paper. Besides, due to the relationship between the LSP coefficients of adjacent orders, an appropriate format of the regression matrix is suggested according to the small amount of the adaptation training data. Subjective and objective tests prove that the source HMMs can be adapted more accurately using the proposed method, meanwhile the synthetic speech generated from the adapted model has better discrimination and speech quality.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Qin, Long; Wu, Yi-jian; Ling, Zhen-Hua; Wang, Ren-Hua] Univ Sci &amp;
   Technol China, iFLYTEK Speech Lab, Hefei 230026, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Qin, L (reprint author), Univ Sci &amp; Technol China, iFLYTEK Speech Lab, Hefei 230026, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>qinlong@mail.ustc.edu.cn; jasonwu@mail.ustc.edu.cn; zhling@ustc.edu;
   rhw@ustc.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2006</td>
</tr>

<tr>
<td valign="top">BP </td><td>2250</td>
</tr>

<tr>
<td valign="top">EP </td><td>2253</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000269965901300</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Lee, CH
   <br>Wu, CH</td>
</tr>

<tr>
<td valign="top">AF </td><td>Lee, Chung-Han
   <br>Wu, Chung-Hsien</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA</td>
</tr>

<tr>
<td valign="top">TI </td><td>MAP-BASED ADAPTATION FOR SPEECH CONVERSION USING ADAPTATION DATA
   SELECTION AND NON-PARALLEL TRAINING</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2006 AND 9TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE
   PROCESSING, VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th International Conference on Spoken Language Processing/INTERSPEECH
   2006</td>
</tr>

<tr>
<td valign="top">CY </td><td>2006</td>
</tr>

<tr>
<td valign="top">CL </td><td>Pittsburgh, PA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; non-parallel training; data selection</td>
</tr>

<tr>
<td valign="top">ID </td><td>MODELS</td>
</tr>

<tr>
<td valign="top">AB </td><td>This study presents an approach to GMM-based speech conversion using maximum a posteriori probability (MAP) adaptation. First, a conversion function is trained using a parallel corpus containing the same utterances spoken by both the source and the reference speakers. Then a non-parallel corpus from a new target speaker is used for the adaptation of the conversion function which models the voice conversion between the source speaker and the new target speaker. The consistency among the adaptation data is estimated to select suitable data from the non-parallel corpus for NW-based adaptation of the GMMs. In speech conversion evaluation, experimental results show that MAP adaptation using a small non-parallel corpus can reduce the conversion error and improve the speech quality for speaker identification compared to the method without adaptation. Objective and subjective tests also confirm the promising performance of the proposed approach.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Lee, Chung-Han; Wu, Chung-Hsien] Natl Cheng Kung Univ, Dept Comp Sci &amp;
   Informat Engn, Tainan 70101, Taiwan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Lee, CH (reprint author), Natl Cheng Kung Univ, Dept Comp Sci &amp; Informat Engn, Tainan 70101, Taiwan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>chlee@csie.ncku.edu.tw; chwu@csie.ncku.edu.tw</td>
</tr>

<tr>
<td valign="top">TC </td><td>39</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>39</td>
</tr>

<tr>
<td valign="top">PY </td><td>2006</td>
</tr>

<tr>
<td valign="top">BP </td><td>2254</td>
</tr>

<tr>
<td valign="top">EP </td><td>2257</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000269965901301</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nurminen, J
   <br>Tian, JL
   <br>Popa, V</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nurminen, Jani
   <br>Tian, Jilei
   <br>Popa, Victor</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA</td>
</tr>

<tr>
<td valign="top">TI </td><td>Novel Method for Data Clustering and Mode Selection with Application in
   Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2006 AND 9TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE
   PROCESSING, VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th International Conference on Spoken Language Processing/INTERSPEECH
   2006</td>
</tr>

<tr>
<td valign="top">CY </td><td>2006</td>
</tr>

<tr>
<td valign="top">CL </td><td>Pittsburgh, PA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; data-driven; clustering</td>
</tr>

<tr>
<td valign="top">AB </td><td>Since the statistical properties of speech signals are variable and depend heavily on the content, it is hard to design speech processing techniques that would perforin well on all inputs. For example, in voice conversion, where the aim is to transform the speech uttered by a source speaker to sound as if it was spoken by a target speaker, different types of inter-speaker relationships can be found from different types of speech segments. To tackle this problem in a robust manner, we have developed a novel scheme for data clustering and mode selection. When applied in the voice conversion application, the main idea of the proposed approach is to first cluster the target data to achieve a minimized intra-cluster variability. Then, a mode selector or a classifier is trained on aligned source-related data to recognize the target-based clusters. Auxiliary speech features can be used to enhance the classification accuracy, in addition to the source data. Finally, a separate conversion scheme is trained and used for each cluster. The proposed scheme is fully data-driven and it avoids the need to use heuristic solutions. The superior performance of the proposed scheme has been verified in a practical voice conversion system.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nurminen, Jani; Tian, Jilei; Popa, Victor] Nokia Res Ctr, Multimedia
   Technol Lab, Tampere, Finland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nurminen, J (reprint author), Nokia Res Ctr, Multimedia Technol Lab, Tampere, Finland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jani.k.nurminen@nokia.com; jilei.tian@nokia.com;
   ext-victor.popa@nokia.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2006</td>
</tr>

<tr>
<td valign="top">BP </td><td>2258</td>
</tr>

<tr>
<td valign="top">EP </td><td>2261</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000269965901302</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sundermann, D
   <br>Hoge, H
   <br>Bonafonte, A
   <br>Ney, H
   <br>Hirschberg, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Suendermann, David
   <br>Hoege, Harald
   <br>Bonafonte, Antonio
   <br>Ney, Hermann
   <br>Hirschberg, Julia</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA</td>
</tr>

<tr>
<td valign="top">TI </td><td>Text-Independent Cross-Language Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2006 AND 9TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE
   PROCESSING, VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th International Conference on Spoken Language Processing/INTERSPEECH
   2006</td>
</tr>

<tr>
<td valign="top">CY </td><td>2006</td>
</tr>

<tr>
<td valign="top">CL </td><td>Pittsburgh, PA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; unit selection; TC-Star</td>
</tr>

<tr>
<td valign="top">AB </td><td>So far, cross-language voice conversion requires at least one bilingual speaker and parallel speech data to perform the training. This paper shows how these obstacles can be overcome by means of a recently presented text-independent training method based on unit selection. The new method is evaluated in the framework of the European speech-to-speech translation project TC-Star and achieves a performance similar to that of text-dependent intra-lingual voice conversion.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Suendermann, David; Hoege, Harald] Siemens Corp Technol, Munich,
   Germany.
   <br>[Suendermann, David; Bonafonte, Antonio] Tech Univ Catalonia, Barcelona,
   Spain.
   <br>[Ney, Hermann] Rhein Westfal TH Aachen, Aachen, Germany.
   <br>[Hirschberg, Julia] Columbia Univ, New York, NY USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sundermann, D (reprint author), Siemens Corp Technol, Munich, Germany.</td>
</tr>

<tr>
<td valign="top">EM </td><td>david@suendermann.com; harald.hoege@siemens.com;
   antonio.bonafonte@upc.edu; ney@cs.rwth-aachen.de; julia@cs.columbia.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>4</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>4</td>
</tr>

<tr>
<td valign="top">PY </td><td>2006</td>
</tr>

<tr>
<td valign="top">BP </td><td>2262</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000269965901303</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ohtani, Y
   <br>Toda, T
   <br>Saruwatari, H
   <br>Shikano, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ohtani, Yamato
   <br>Toda, Tomoki
   <br>Saruwatari, Hiroshi
   <br>Shikano, Kiyohiro</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA</td>
</tr>

<tr>
<td valign="top">TI </td><td>Maximum Likelihood Voice Conversion Based on GMM with STRAIGHT Mixed
   Excitation</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2006 AND 9TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE
   PROCESSING, VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th International Conference on Spoken Language Processing/INTERSPEECH
   2006</td>
</tr>

<tr>
<td valign="top">CY </td><td>2006</td>
</tr>

<tr>
<td valign="top">CL </td><td>Pittsburgh, PA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speech synthesis; Voice conversion; Gaussian mixture model; STRAIGHT;
   Mixed excitation</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH</td>
</tr>

<tr>
<td valign="top">AB </td><td>The performance of voice conversion has been considerably improved through statistical modeling of spectral sequences. However, the converted speech still contains traces of artificial sounds. To alleviate this, it is necessary to statistically model a source sequence as well as a spectral sequence. In this paper, we introduce STRAIGHT mixed excitation to a framework of the voice conversion based on a Gaussian Mixture Model (GMM) on joint probability density of source and target features. We convert both spectral and source feature sequences based on Maximum Likelihood Estimation (MLE). Objective and subjective evaluation results demonstrate that the proposed source conversion produces strong improvements in both the converted speech quality and the conversion accuracy for speaker individuality.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ohtani, Yamato; Toda, Tomoki; Saruwatari, Hiroshi; Shikano, Kiyohiro]
   Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ohtani, Y (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>yamato-o@is.naist.jp; tomoki@is.naist.jp; sawatari@is.naist.jp;
   shikano@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>41</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>41</td>
</tr>

<tr>
<td valign="top">PY </td><td>2006</td>
</tr>

<tr>
<td valign="top">BP </td><td>2266</td>
</tr>

<tr>
<td valign="top">EP </td><td>2269</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000269965901304</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nakagiri, M
   <br>Toda, T
   <br>Kashioka, H
   <br>Shikano, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nakagiri, Mikihiro
   <br>Toda, Tomoki
   <br>Kashioka, Hideki
   <br>Shikano, Kiyohiro</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA</td>
</tr>

<tr>
<td valign="top">TI </td><td>Improving Body Transmitted Unvoiced Speech with Statistical Voice
   Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2006 AND 9TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE
   PROCESSING, VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th International Conference on Spoken Language Processing/INTERSPEECH
   2006</td>
</tr>

<tr>
<td valign="top">CY </td><td>2006</td>
</tr>

<tr>
<td valign="top">CL </td><td>Pittsburgh, PA</td>
</tr>

<tr>
<td valign="top">DE </td><td>silent speech telephone; body transmitted unvoiced speech; voice
   conversion; F(0) estimation; whisper</td>
</tr>

<tr>
<td valign="top">AB </td><td>The conversion method from Non-Audible Murmur (NAM) to ordinary speech based on the statistical voice conversion (NAM-to-Speech) has been proposed towards realization of "silent speech telephone, " Although NAM-to-Speech converts NAM to intelligible voices with similar quality to speech, there is still a large problem, i.e., difficulties of the F(0) estimation from unvoiced speech. In order to avoid this problem, we propose a conversion method from NAM to whisper that is a familiar and intelligible unvoiced speech (NAM-to-Whisper). Moreover, we enhance NAM-to-Whisper so that multiple types of body-transmitted unvoiced speech such as NAM and Body Transmitted Whisper (BTW) are accepted as input voices. We evaluate the performance of the proposed conversion method. Experimental results demonstrate that 1) intelligibility and naturalness of NAM are significantly improved by NAM-to-Whisper, 2) NAM-to-Whisper outperforms NAM-to-Speech, and 3) we can train a single conversion model successfully converting both NAM and BTW to the target voice.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nakagiri, Mikihiro; Toda, Tomoki; Kashioka, Hideki; Shikano, Kiyohiro]
   Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>tomoki@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>4</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>4</td>
</tr>

<tr>
<td valign="top">PY </td><td>2006</td>
</tr>

<tr>
<td valign="top">BP </td><td>2270</td>
</tr>

<tr>
<td valign="top">EP </td><td>2273</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000269965901305</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Uto, Y
   <br>Nankaku, Y
   <br>Toda, T
   <br>Lee, A
   <br>Tokuda, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Uto, Yosuke
   <br>Nankaku, Yoshihiko
   <br>Toda, Tomoki
   <br>Lee, Akinobu
   <br>Tokuda, Keiichi</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Conversion Based on Mixtures of Factor Analyzers</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2006 AND 9TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE
   PROCESSING, VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th International Conference on Spoken Language Processing/INTERSPEECH
   2006</td>
</tr>

<tr>
<td valign="top">CY </td><td>2006</td>
</tr>

<tr>
<td valign="top">CL </td><td>Pittsburgh, PA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; GMM (Gaussian Mixture Model); MFA (Mixtures of Factor
   Analyzers)</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper describes the voice conversion based on the Mixtures of Factor Analyzers (MFA) which can provide an efficient modeling with a limited amount of training data. As a typical spectral conversion method, a mapping algorithm based on the Gaussian Mixture Model (GMM) has been proposed. In this method two kinds of covariance matrix structures are often used : the diagonal and full covariance matrices. GMM with diagonal covariance matrices requires a large number of mixture components for accurately estimating spectral features. On the other hand, GMM with full covariance matrices needs sufficient training data to estimate model parameters. In order to cope with these problems, we apply MFA to voice conversion. MFA can be regarded as intermediate model between GMM with diagonal covariance and with full covariance. Experimental results show that MFA can improve the conversion accuracy compared with the conventional GMM.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Uto, Yosuke; Nankaku, Yoshihiko; Lee, Akinobu; Tokuda, Keiichi] Nagoya
   Inst Technol, Showa Ku, Gokiso Cho, Nagoya, Aichi 4668555, Japan.
   <br>[Toda, Tomoki] Nara Inst Sci &amp; Technol, Nara 6300101, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Uto, Y (reprint author), Nagoya Inst Technol, Showa Ku, Gokiso Cho, Nagoya, Aichi 4668555, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>uto@ics.nitech.ac.jp; nankaku@ics.nitech.ac.jp; tomoki@is.naist.jp;
   ri@ics.nitech.ac.jp; tokuda@ics.nitech.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2006</td>
</tr>

<tr>
<td valign="top">BP </td><td>2278</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000269965901307</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Tian, JL
   <br>Nurminen, J
   <br>Popa, V</td>
</tr>

<tr>
<td valign="top">AF </td><td>Tian, Jilei
   <br>Nurminen, Jani
   <br>Popa, Victor</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA</td>
</tr>

<tr>
<td valign="top">TI </td><td>Efficient Gaussian Mixture Model Evaluation in Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2006 AND 9TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE
   PROCESSING, VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th International Conference on Spoken Language Processing/INTERSPEECH
   2006</td>
</tr>

<tr>
<td valign="top">CY </td><td>2006</td>
</tr>

<tr>
<td valign="top">CL </td><td>Pittsburgh, PA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; speech subjective evaluation; Gaussian mixture model</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion refers to the adaptation of the characteristics of a source speaker's voice to those of a target speaker. Gaussian mixture models (GMM) have been found to be efficient in the voice conversion task. The GMM parameters are estimated from a training set with the goal to minimize the mean squared error (MSE) between the transformed and target vectors. Obviously, the quality of the GMM model plays an important role in achieving better voice conversion quality. This paper presents a very efficient approach for the evaluation of GMM models directly from the model parameters without using any test data, facilitating the improvement of the transformation performance especially in the case of embedded implementations. Though the proposed approach can be used in any application that utilizes GMM based transformation, we take voice conversion as an example application throughout the paper. The proposed approach is experimented with in this context and evaluated against an MSE based evaluation method. The results show that the proposed method is in line with all subjective observations and MSE results.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Tian, Jilei; Nurminen, Jani; Popa, Victor] Nokia Res Ctr, Multimedia
   Technol Lab, Tampere, Finland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Tian, JL (reprint author), Nokia Res Ctr, Multimedia Technol Lab, Tampere, Finland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jilei.tian@nokia.com; jani.k.nurminen@nokia.com;
   ext-victor.popa@nokia.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2006</td>
</tr>

<tr>
<td valign="top">BP </td><td>2282</td>
</tr>

<tr>
<td valign="top">EP </td><td>2285</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000269965901308</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Toda, T
   <br>Ohtani, Y
   <br>Shikano, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Toda, Tomoki
   <br>Ohtani, Yamato
   <br>Shikano, Kiyohiro</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA</td>
</tr>

<tr>
<td valign="top">TI </td><td>Eigenvoice Conversion Based on Gaussian Mixture Model</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2006 AND 9TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE
   PROCESSING, VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th International Conference on Spoken Language Processing/INTERSPEECH
   2006</td>
</tr>

<tr>
<td valign="top">CY </td><td>2006</td>
</tr>

<tr>
<td valign="top">CL </td><td>Pittsburgh, PA</td>
</tr>

<tr>
<td valign="top">DE </td><td>speech synthesis; voice conversion; GMM; eigenvoice; unsupervised
   training</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper describes a novel framework of voice conversion (VC). We call it eigenvoice conversion (EVC). We apply EVC to the conversion from a source speaker's voice to arbitrary target speakers' voices. Using multiple parallel data sets consisting of utterance-pairs of the source and multiple pre-stored target speakers, a canonical eigenvoice GMM (EV-GMM) is trained in advance. That conversion model enables us to flexibly control the speaker individuality of the convened speech by manually setting weight parameters. In addition, the optimum weight set for a specific target speaker is estimated using only speech data of the target speaker without any linguistic restrictions. We evaluate the performance of EVC by a spectral distortion measure. Experimental results demonstrate that EVC works very well even if we use only a few utterances of the target speaker for the weight estimation.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Toda, Tomoki; Ohtani, Yamato; Shikano, Kiyohiro] Nara Inst Sci &amp;
   Technol, Grad Sch Informat Sci, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Toda, T (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>tomoki@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>46</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>46</td>
</tr>

<tr>
<td valign="top">PY </td><td>2006</td>
</tr>

<tr>
<td valign="top">BP </td><td>2446</td>
</tr>

<tr>
<td valign="top">EP </td><td>2449</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000269965901349</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Jinachitra, P</td>
</tr>

<tr>
<td valign="top">AF </td><td>Jinachitra, Pamornpol</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA</td>
</tr>

<tr>
<td valign="top">TI </td><td>Glottal Closure and Opening Detection for Flexible Parametric Voice
   Coding</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2006 AND 9TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE
   PROCESSING, VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th International Conference on Spoken Language Processing/INTERSPEECH
   2006</td>
</tr>

<tr>
<td valign="top">CY </td><td>2006</td>
</tr>

<tr>
<td valign="top">CL </td><td>Pittsburgh, PA</td>
</tr>

<tr>
<td valign="top">DE </td><td>speech coding; glottal closure instant detection; glottal opening
   instant; voice transformation</td>
</tr>

<tr>
<td valign="top">AB </td><td>The knowledge of glottal closure and opening instants (GCI/GOI) is useful for many speech analysis applications. A Pitch-synchronous waveform encoding of voice is one such application. In this paper, a dynamic programming is employed to solve for the global close/open phase segmentation based on the polynomial parametric waveform of the derivative glottal waveform and its quasi-periodicity. Not only does the algorithm identify GCIs, but also the elusive GOIs, and as a by-product, the parameters of the glottal excitation waveform. The results show its effectiveness compared with a classical method. Its application to parametric voice encoding which allows for simple time-pitch scaling as well as voicing quality conversion is demonstrated.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Stanford Univ, Ctr Comp Res Mus &amp; Acoust, Stanford, CA 94305 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Jinachitra, P (reprint author), Stanford Univ, Ctr Comp Res Mus &amp; Acoust, Stanford, CA 94305 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>pj97@ccrma.stanford.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2006</td>
</tr>

<tr>
<td valign="top">BP </td><td>2482</td>
</tr>

<tr>
<td valign="top">EP </td><td>2485</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000269965901358</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Presicce, M
   <br>De Matteis, M
   <br>D'Amico, S
   <br>Giotta, D
   <br>Gaggl, R
   <br>Baschirotto, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Presicce, M.
   <br>De Matteis, M.
   <br>D'Amico, S.
   <br>Giotta, D.
   <br>Gaggl, R.
   <br>Baschirotto, A.</td>
</tr>

<tr>
<td valign="top">BE </td><td>Malcovati, P
   <br>Baschirotto, A</td>
</tr>

<tr>
<td valign="top">TI </td><td>A single-ended-to-differential transimpedence amplifier for a voice
   transceiver analog interface</td>
</tr>

<tr>
<td valign="top">SO </td><td>PRIME 2006: 2ND CONFERENCE ON PH.D. RESEARCH IN MICROELECTRONIC AND
   ELECTRONICS, PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>2nd Conference on Ph D Research in MicroElectronics and Electronics</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUN 12-15, 2006</td>
</tr>

<tr>
<td valign="top">CL </td><td>Otranto, ITALY</td>
</tr>

<tr>
<td valign="top">AB </td><td>This target of the work is the replacing of a voltage amplifier with a here proposed transimpedance one in the voice transceiver analog interface, in order to simplify the structure avoiding the use of high cost external components. This target required the development of an innovative transimpedence architecture here presented: it operates a single-ended-to-differential current conversion, leading the circuit to operate as a fully differential transimpedance structure, improving signal immunity. The proposed device has been designed in a 0.13 mu m CMOS technology and operates from a single 1.5V supply voltage.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Presicce, M.; De Matteis, M.; D'Amico, S.; Baschirotto, A.] Univ Lecce,
   Dept Elect, I-73100 Lecce, Italy.
   <br>[Giotta, D.; Gaggl, R.] Infineon, Villach, Austria.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Presicce, M (reprint author), Univ Lecce, Dept Elect, I-73100 Lecce, Italy.</td>
</tr>

<tr xmlns:date="http://exslt.org/dates-and-times">
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>De Matteis, Marcello</display_name>&nbsp;</font></td><td><font size="3">0000-0003-1061-1262&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Baschirotto, Andrea</display_name>&nbsp;</font></td><td><font size="3">0000-0002-8844-5754&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>D'Amico, Stefano</display_name>&nbsp;</font></td><td><font size="3">0000-0003-2886-2166&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2006</td>
</tr>

<tr>
<td valign="top">BP </td><td>169</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000240385600043</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hanzlicek, Z
   <br>Matousek, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hanzlicek, Zdenek
   <br>Matousek, Jindrich</td>
</tr>

<tr>
<td valign="top">BE </td><td>Sojka, P
   <br>Kopecek, I
   <br>Pala, K</td>
</tr>

<tr>
<td valign="top">TI </td><td>First steps towards new Czech voice conversion system</td>
</tr>

<tr>
<td valign="top">SO </td><td>TEXT, SPEECH AND DIALOGUE, PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">SE </td><td>Lecture Notes in Artificial Intelligence</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article; Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th International Conference on Text, Speech and Dialogue</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 11-15, 2006</td>
</tr>

<tr>
<td valign="top">CL </td><td>Brno, CZECH REPUBLIC</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper we deal with initial experiments on creating a new Czech voice conversion system. Voice conversion (VC) is a process which modifies the speech signal produced by one (source) speaker so that it sounds like another (target) speaker. Using VC technique a new voice for speech synthesizer can be prepared with no need to record a huge amount of new speech data. The transformation is determined using equal sentences from both speakers; these sentences are time-aligned using modified dynamic time warping algorithm. The conversion is divided into two stages corresponding to the source-filter model of speech production. Within this work we employ conversion function based on Gaussian mixture model for transforming the spectral envelope described by line spectral frequencies. Residua are converted using so called residual prediction techniques. Unlike in other similar research works, we predict residua not from the transformed spectral envelope, but directly from the source speech. Four versions of residual prediction are described and compared in this study. Objective evaluation of converted speech using performance metrics shows that our system is comparable with similar existing VC systems.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Univ W Bohemia, Dept Cybernet, Plzen 30614, Czech Republic.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hanzlicek, Z (reprint author), Univ W Bohemia, Dept Cybernet, Univerzitni 8, Plzen 30614, Czech Republic.</td>
</tr>

<tr>
<td valign="top">EM </td><td>zhanzlic@kky.zcu.cz; jmatouse@kky.zcu.cz</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Matousek, Jindrich</display_name>&nbsp;</font></td><td><font size="3">C-2146-2011&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Matousek, Jindrich</display_name>&nbsp;</font></td><td><font size="3">0000-0002-7408-7730&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2006</td>
</tr>

<tr>
<td valign="top">VL </td><td>4188</td>
</tr>

<tr>
<td valign="top">BP </td><td>383</td>
</tr>

<tr>
<td valign="top">EP </td><td>390</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000241103500048</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sun, J
   <br>Dai, BQ
   <br>Zhang, J
   <br>Xie, YL</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sun, Jun
   <br>Dai, Beiqian
   <br>Zhang, Jian
   <br>Xie, Yanlu</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Modeling glottal source for high quality voice conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>WCICA 2006: SIXTH WORLD CONGRESS ON INTELLIGENT CONTROL AND AUTOMATION,
   VOLS 1-12, CONFERENCE PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>6th World Congress on Intelligent Control and Automation</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUN 21-23, 2006</td>
</tr>

<tr>
<td valign="top">CL </td><td>Dalian, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>glottal source; vocal tract parameters; LSF-glottal codebook; voice
   conversion</td>
</tr>

<tr>
<td valign="top">AB </td><td>A novel modeling method for glottal source is proposed for improving the naturalness and quality of synthetic speech. This paper utilizes the high correlation between vocal tract parameters and glottal source to model glottal source. Vocal tract parameters (LSF) are clustered into some classes. Within each class, a LSF vector closest to centroid and its corresponding glottal wave derivative are selected as a code vector representing different phonetic class of voiced speech. At the stage of voice conversion or synthesis, we can find the relevant glottal source by virtual of finding the closest matched vocal tract parameters. Experiment results show that this vocal tract related glottal source model significantly outperform Rosenberg model and LF model. Correlation coefficients between vocal tract related glottal source and original glottal source increase 27% and 30.13%, spectral distance between synthetic speech and original speech reduce 50.5% and 51.48% respectively, comparing with Rosenberg model and LF model.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Sun, Jun; Dai, Beiqian; Zhang, Jian; Xie, Yanlu] Univ Sci &amp; Technol
   China, MOE Microsoft Key Lab Multimedia Comp &amp; Commun, Hefei 230027,
   Anhui, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sun, J (reprint author), Univ Sci &amp; Technol China, MOE Microsoft Key Lab Multimedia Comp &amp; Commun, Hefei 230027, Anhui, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>sunjun@ustc.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2006</td>
</tr>

<tr>
<td valign="top">BP </td><td>319</td>
</tr>

<tr>
<td valign="top">EP </td><td>319</td>
</tr>

<tr>
<td valign="top">SC </td><td>Automation &amp; Control Systems; Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000241773209329</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Dubois, TD</td>
</tr>

<tr>
<td valign="top">AF </td><td>Dubois, TD</td>
</tr>

<tr>
<td valign="top">TI </td><td>Hegemony, imperialism, and the construction of religion in East and
   Southeast Asia</td>
</tr>

<tr>
<td valign="top">SO </td><td>HISTORY AND THEORY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article; Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Workshop on Casting Faiths</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUN 07-08, 2005</td>
</tr>

<tr>
<td valign="top">CL </td><td>Natl Univ Singapore, Singapore, SINGAPORE</td>
</tr>

<tr>
<td valign="top">HO </td><td>Natl Univ Singapore</td>
</tr>

<tr>
<td valign="top">ID </td><td>MODERNITY; JAPAN</td>
</tr>

<tr>
<td valign="top">AB </td><td>Edward Said's concept of Orientalism portrays the high tide of nineteenth-century imperialism as the defining moment in the establishment of a global discursive hegemony, in which European attitudes and concepts gained a universal validity. The idea of "religion" was central to the civilizing mission of imperialism, and was shaped by the interests of a number of colonial actors in a way that remains visibly relevant today. In East and Southeast Asia, however, many of the concerns that statecraft, law, scholarship, and conversion had for religion transcended the European impact. Both before and after the period of European imperialism, states used religion to engineer social ethics and legitimate rule, scholars elaborated and enforced state theologies, and the missionary faithful voiced the need for and nature of religious conversion. The real impact of this period was to integrate pre-existing concerns into larger discourses, transforming them in the process. The ideals of national citizenship and of legal and scholarly impartiality recast the state and its institutions with a modernist sacrality, which had the effect of banishing the religious from the public space. At the same time, the missionary discourse of transformative conversion located it in the very personal realm of sincerity and belief. The evolution of colonial-era discourses of religion and society in Asia since the departure of European imperial power demonstrates both their lasting power and the degree of agency that remains implicit in the idea of hegemony.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Natl Univ Singapore, Singapore 117548, Singapore.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Dubois, TD (reprint author), Natl Univ Singapore, Singapore 117548, Singapore.</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>DuBois, Thomas</display_name>&nbsp;</font></td><td><font size="3">0000-0002-7970-2472&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>12</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>12</td>
</tr>

<tr>
<td valign="top">PD </td><td>DEC</td>
</tr>

<tr>
<td valign="top">PY </td><td>2005</td>
</tr>

<tr>
<td valign="top">VL </td><td>44</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">BP </td><td>113</td>
</tr>

<tr>
<td valign="top">EP </td><td>131</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1111/j.1468-2303.2005.00345.x</td>
</tr>

<tr>
<td valign="top">SC </td><td>History</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000233742200006</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Willinger, U
   <br>Aschauer, HN</td>
</tr>

<tr>
<td valign="top">AF </td><td>Willinger, U
   <br>Aschauer, HN</td>
</tr>

<tr>
<td valign="top">TI </td><td>Personality, anxiety and functional dysphonia</td>
</tr>

<tr>
<td valign="top">SO </td><td>PERSONALITY AND INDIVIDUAL DIFFERENCES</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>functional dysphonia; conversion disorder; symptoms of depression;
   symptoms of anxiety; personality</td>
</tr>

<tr>
<td valign="top">ID </td><td>PSYCHOGENIC VOICE DISORDER; GERMAN VERSION; QUESTIONNAIRE; VARIABLES;
   APHONIA; MODEL; MOOD; TPQ</td>
</tr>

<tr>
<td valign="top">AB </td><td>Psychological factors are considered for the predisposition and perpetuation of functional dysphonia. In the present study 61 patients with functional dysphonia were compared with 61 healthy controls, matched by age, sex, and occupation with respect to Cloninger's personality model, mood, and anxiety.
   <br>The patients with functional dysphonia presented significantly higher scores than the healthy controls with respect to "harm avoidance" (HA); depressive symptoms; symptoms of unspecific and general anxiety; symptoms of specific anxiety concerning "health", "illness", and "extraversion versus introversion". No significant differences were found in "novelty seeking" (NS), "reward dependence" (RD), "persistence" (PE), or in state-anxiety and anxiety of social situations. These results were found considering univariate and multivariate analyses and confirm the relationship of psychological factors such as personality traits, mood, and anxiety on one hand and conversion disorder in general and functional dysphonia in particular on the other hand. This important relationship should be considered in the diagnostic and therapeutic interventions of functional dysphonia. (c) 2005 Elsevier Ltd. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Med Univ Vienna, Univ Ear Nose &amp; Throat Clin, A-1090 Vienna, Austria.
   <br>Univ Hosp Psychiat, Dept Gen Psychiat, Vienna, Austria.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Willinger, U (reprint author), Med Univ Vienna, Univ Ear Nose &amp; Throat Clin, Waehringer Guertel 18-20, A-1090 Vienna, Austria.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ulrike.willinger@univie.ac.at</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Aschauer, Harald</display_name>&nbsp;</font></td><td><font size="3">G-8850-2016&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>4</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>4</td>
</tr>

<tr>
<td valign="top">PD </td><td>DEC</td>
</tr>

<tr>
<td valign="top">PY </td><td>2005</td>
</tr>

<tr>
<td valign="top">VL </td><td>39</td>
</tr>

<tr>
<td valign="top">IS </td><td>8</td>
</tr>

<tr>
<td valign="top">BP </td><td>1441</td>
</tr>

<tr>
<td valign="top">EP </td><td>1449</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.paid.2005.06.011</td>
</tr>

<tr>
<td valign="top">SC </td><td>Psychology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000233417200010</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Razdan, S
   <br>Bagley, DH
   <br>McGinnis, DE</td>
</tr>

<tr>
<td valign="top">AF </td><td>Razdan, S
   <br>Bagley, DH
   <br>McGinnis, DE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Minimizing minimally invasive surgery: The 5-mm trocar laparoscopic
   pyeloplasty</td>
</tr>

<tr>
<td valign="top">SO </td><td>JOURNAL OF ENDOUROLOGY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article; Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>22nd World Congress of Endourology/SWL 20th Basic Research Symposium</td>
</tr>

<tr>
<td valign="top">CY </td><td>NOV 02-05, 2004</td>
</tr>

<tr>
<td valign="top">CL </td><td>Bombay, INDIA</td>
</tr>

<tr>
<td valign="top">ID </td><td>URETEROPELVIC JUNCTION OBSTRUCTION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Background and Purpose: Laparoscopic pyeloplasty has evolved into the procedure of choice when definitive repair of the obstructed ureteropelvic junction is contemplated. Its main advantage over the gold standard of open pyeloplasty is decreased morbidity. We have utilized only three 5-mm ports in our last 15 pyeloplasties in an effort to further reduce morbidity and improve acceptance by an often-younger patient population.
   <br>Patients and Methods: Fifteen consecutive patients underwent laparoscopic transperitoneal pyeloplasty by the 5-mm port technique. Three trocars were utilized, two for the working instruments and one for a 5-mm telescope mounted on a voice-activated robotic arm (AESOP; Intuitive Surgical, Sunnyvale, CA). Three patients required an additional trocar for liver retraction. All patients underwent dismembered pyeloplasty and had indwelling double-pigtail stents placed for 4 to 6 weeks.
   <br>Results: The mean operative time was 195 minutes (range 120-240 minutes). The average blood loss was 30 mL. None of our patients required open conversion. With a median follow-up of 10 months (range 3-15 months), all 15 patients have shown both subjective (freedom from symptoms) and objective (renal scan) improvement.
   <br>Conclusion: We believe our technique has further minimized the morbidity of laparoscopic pyeloplasty without compromising the outcome. The 5-mm trocars obviate fascial closure, decrease patient discomfort, and improve cosmesis. Furthermore, the use of the robotic arm eliminates the need for a surgical assistant and makes this an essentially "one-person" procedure.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Thomas Jefferson Med Coll, Dept Urol, Philadelphia, PA 19107 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Bagley, DH (reprint author), Thomas Jefferson Med Coll, Dept Urol, 1025 Walnut St, Philadelphia, PA 19107 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>Demetrius.bagley@jefferson.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>8</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>9</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2005</td>
</tr>

<tr>
<td valign="top">VL </td><td>19</td>
</tr>

<tr>
<td valign="top">IS </td><td>5</td>
</tr>

<tr>
<td valign="top">BP </td><td>533</td>
</tr>

<tr>
<td valign="top">EP </td><td>536</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1089/end.2005.19.533</td>
</tr>

<tr>
<td valign="top">SC </td><td>Urology &amp; Nephrology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000230410400001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Lavner, Y
   <br>Porat, G</td>
</tr>

<tr>
<td valign="top">AF </td><td>Lavner, Y
   <br>Porat, G</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice morphing using 3D waveform interpolation surfaces and lossless
   tube area functions</td>
</tr>

<tr>
<td valign="top">SO </td><td>EURASIP JOURNAL ON APPLIED SIGNAL PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice morphing; prototype waveform interpolation; lossless tube area
   function; speech synthesis</td>
</tr>

<tr>
<td valign="top">ID </td><td>PITCH DETERMINATION; CONVERSION; SOUNDS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice morphing is the process of producing intermediate or hybrid voices between the utterances of two speakers. It can also be defined as the process of gradually transforming the voice of one speaker to that of another. The ability to change the speaker's individual characteristics and to produce high-quality voices can be used in many applications. Examples include multimedia and video entertainment, as well as enrichment of speech databases in text-to-speech systems. In this study we present a new technique which enables production of a given number of intermediate voices or of utterances which gradually change from one voice to another. This technique is based on two components: (1) creation of a 3D prototype waveform interpolation (PWI) surface from the LPC residual signal, to produce an intermediate excitation signal; (2) a representation of the vocal tract by a lossless tube area function, and an interpolation of the parameters of the two speakers. The resulting synthesized signal sounds like a natural voice lying between the two original voices.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Tel Hai Acad Coll, Dept Comp Sci, IL-12210 Upper Galilee, Israel.
   <br>Technion Israel Inst Technol, Dept Elect Engn, Signal &amp; Image Proc Lab,
   IL-32000 Haifa, Israel.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Lavner, Y (reprint author), Tel Hai Acad Coll, Dept Comp Sci, IL-12210 Upper Galilee, Israel.</td>
</tr>

<tr>
<td valign="top">EM </td><td>yizhar_l@kyiftah.org.il; gidon.porat@intel.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAY 21</td>
</tr>

<tr>
<td valign="top">PY </td><td>2005</td>
</tr>

<tr>
<td valign="top">VL </td><td>2005</td>
</tr>

<tr>
<td valign="top">IS </td><td>8</td>
</tr>

<tr>
<td valign="top">BP </td><td>1174</td>
</tr>

<tr>
<td valign="top">EP </td><td>1184</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1155/ASP.2005.1174</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000231617100002</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hasan, MM
   <br>Nasr, AM
   <br>Sultana, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hasan, MM
   <br>Nasr, AM
   <br>Sultana, S</td>
</tr>

<tr>
<td valign="top">TI </td><td>An approach to voice conversion using feature statistical mapping</td>
</tr>

<tr>
<td valign="top">SO </td><td>APPLIED ACOUSTICS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; linear prediction; pitch contour modification; speech
   synthesis</td>
</tr>

<tr>
<td valign="top">ID </td><td>INDIVIDUALITY; SPEECH</td>
</tr>

<tr>
<td valign="top">AB </td><td>The voice conversion (VC) technique recently has emerged as a new branch of speech synthesis dealing with speaker identity. In this work, a linear prediction (LP) analysis is carried out on speech signals to obtain acoustical parameters related to speaker identity - the speech fundamental frequency, or pitch, voicing decision, signal energy, and vocal tract parameters. Once these parameters are established for two different speakers designated as source and target speakers, statistical mapping functions can then be applied to modify the established parameters. The mapping functions are derived from these parameters in such a way that the source parameters resemble those of the target. Finally, the modified parameters are used to produce the new speech signal. To illustrate the feasibility of the proposed approach, a simple to use voice conversion software has been developed. This VC technique has shown satisfactory results. The synthesized speech signal virtually matching that of the target speaker. (C) 2004 Elsevier Ltd. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Univ Brunei Darussalam, Fac Sci, Dept Math, Gadong, Brunei.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hasan, MM (reprint author), Univ Brunei Darussalam, Fac Sci, Dept Math, BE 1410, Gadong, Brunei.</td>
</tr>

<tr>
<td valign="top">EM </td><td>mdmahmud@fos.ubd.edu.bn</td>
</tr>

<tr>
<td valign="top">TC </td><td>6</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>8</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAY</td>
</tr>

<tr>
<td valign="top">PY </td><td>2005</td>
</tr>

<tr>
<td valign="top">VL </td><td>66</td>
</tr>

<tr>
<td valign="top">IS </td><td>5</td>
</tr>

<tr>
<td valign="top">BP </td><td>513</td>
</tr>

<tr>
<td valign="top">EP </td><td>532</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.apacoust.2004.09.005</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000227203600004</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Mol, ST
   <br>Born, MP
   <br>van der Molen, HT</td>
</tr>

<tr>
<td valign="top">AF </td><td>Mol, ST
   <br>Born, MP
   <br>van der Molen, HT</td>
</tr>

<tr>
<td valign="top">TI </td><td>Developing criteria for expatriate effectiveness: time to jump off the
   adjustment bandwagon</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERNATIONAL JOURNAL OF INTERCULTURAL RELATIONS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>expatriate; selection; criteria; adjustment; performance; bandwidth;
   fidelity</td>
</tr>

<tr>
<td valign="top">ID </td><td>PERSONALITY DIMENSIONS; ADAPTIVE PERFORMANCE; RELATIVE IMPORTANCE;
   JOB-PERFORMANCE; ANTECEDENTS; ADAPTABILITY; CONSEQUENCES; ASSIGNMENT;
   SELECTION; EMPLOYEES</td>
</tr>

<tr>
<td valign="top">AB </td><td>While job performance is quintessential to assessing expatriate effectiveness, significant domestic advances in performance measurement have seldom been applied to evaluating expatriate training and selection practices. Based on a critical assessment of expatriate research and deliberations about the conversion of these domestic taxonomies to the expatriate domain, this theoretical paper voices a number of propositions that should serve to benefit the field. Specifically, it is proposed that: (1) Dependent variables that have been employed thus far within the field of expatriate effectiveness are best construed as mediators between their predictors and yet to be delineated criteria of expatriate effectiveness that actually sample expatriate job performance; more adequate sampling of the expatriate job performance domain is called for; (2) behaviorally specific criteria, such as those developed by Tett et al., (Human Performance, 2000, 13(3) 205) are essential to the adequate assessment of expatriate job performance; (3) the dimensions of adaptive performance which were developed by Pulakos and colleagues (Journal of Applied Psychology, 2000, 85(4) 612; Human Performance, 2002, 15(4) 299) constitute an important subdomain of expatriate job performance; and (4) an over reliance on the generalization of domestic taxonomies will result in criterion deficiency, as expatriate specific criteria to complement these generalized criteria need to be developed. (c) 2005 Elsevier Ltd. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Erasmus Univ, Inst Psychol, Rotterdam, Netherlands.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Mol, ST (reprint author), Erasmus Univ, Inst Psychol, Rotterdam, Netherlands.</td>
</tr>

<tr>
<td valign="top">EM </td><td>Mol@fsw.eur.nl</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Mol, Stefan T.</display_name>&nbsp;</font></td><td><font size="3">C-3385-2009&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Born, Marise</display_name>&nbsp;</font></td><td><font size="3">Q-3746-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Mol, Stefan T.</display_name>&nbsp;</font></td><td><font size="3">0000-0002-9375-3516&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Born, Marise</display_name>&nbsp;</font></td><td><font size="3">0000-0002-5539-3388&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>28</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>28</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAY</td>
</tr>

<tr>
<td valign="top">PY </td><td>2005</td>
</tr>

<tr>
<td valign="top">VL </td><td>29</td>
</tr>

<tr>
<td valign="top">IS </td><td>3</td>
</tr>

<tr>
<td valign="top">BP </td><td>339</td>
</tr>

<tr>
<td valign="top">EP </td><td>353</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.ijnjintrel.2005.05.004</td>
</tr>

<tr>
<td valign="top">SC </td><td>Psychology; Social Sciences - Other Topics; Sociology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000231481700005</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Willinger, U
   <br>Volkl-Kernstock, S
   <br>Aschauer, HN</td>
</tr>

<tr>
<td valign="top">AF </td><td>Willinger, U
   <br>Volkl-Kernstock, S
   <br>Aschauer, HN</td>
</tr>

<tr>
<td valign="top">TI </td><td>Marked depression and anxiety in patients with functional dysphonia</td>
</tr>

<tr>
<td valign="top">SO </td><td>PSYCHIATRY RESEARCH</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>conversion disorder; somatoform disorder; mood; general and specific
   anxiety</td>
</tr>

<tr>
<td valign="top">ID </td><td>CONVERSION SYMPTOM; VOICE DISORDERS; APHONIA</td>
</tr>

<tr>
<td valign="top">AB </td><td>The etiology of functional dysphonia is still unclear, but psychological factors are assumed to play an important role [Wilson, J.A., Deary, I.J., Scott, S., Mackenzie, K., 1995. Functional dysphonia. British Medical Journal 311, 1039-1040]. The purpose of this report is to investigate the impact of depression and anxiety in functional dysphonia. Sixty-one patients with functional dysphonia were screened for additional psychiatric disorders (besides 300.11) by a clinical psychiatric interview. They were then compared with healthy controls, matched by age, sex and occupation, with respect to self-reported symptoms of depression, generalized anxiety, and specific anxiety concerning health. The patients had significantly higher scores than the controls in depressive symptoms, in the symptoms of nonspecific and general anxiety, and in the symptoms of specific anxiety concerning health. Fifty-seven percent of the patients also fulfilled DSM-IV criteria for a mood disorder, an anxiety disorder, or an adjustment disorder. Multivariate analysis of covariance, performed to correct for the influence of co-morbid psychiatric diagnoses on self-rated symptoms of depression and anxiety, confirmed significant differences between patients and controls in the symptoms of depression and specific anxiety concerning "somatic complaints." Both symptoms of depression and anxiety should be taken into consideration in the diagnostic as well as the therapeutic process of patients with functional dysphonia. (c) 2005 Elsevier Ireland Ltd. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Univ Vienna, Ear Nose &amp; Throat Clin, Dept Phoniatr &amp; Logoped, A-1090
   Vienna, Austria.
   <br>Univ Vienna, Clin Neuropsychiat Children &amp; Adolescents, A-1090 Vienna,
   Austria.
   <br>Univ Hosp Psychiat, Dept Gen Psychiat, Vienna, Austria.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Willinger, U (reprint author), Univ Vienna, Ear Nose &amp; Throat Clin, Dept Phoniatr &amp; Logoped, Wahringer Gurtel 18-20, A-1090 Vienna, Austria.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ulrike.willinger@univie.ac.at</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Aschauer, Harald</display_name>&nbsp;</font></td><td><font size="3">G-8850-2016&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>32</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>36</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAR 30</td>
</tr>

<tr>
<td valign="top">PY </td><td>2005</td>
</tr>

<tr>
<td valign="top">VL </td><td>134</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>85</td>
</tr>

<tr>
<td valign="top">EP </td><td>91</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.psychres.2003.07.007</td>
</tr>

<tr>
<td valign="top">SC </td><td>Psychiatry</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000228502400009</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Brodey, BB
   <br>Rosen, CS
   <br>Winters, KC
   <br>Brodey, IS
   <br>Sheetz, BM
   <br>Steinfeld, RR
   <br>Kaminer, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Brodey, BB
   <br>Rosen, CS
   <br>Winters, KC
   <br>Brodey, IS
   <br>Sheetz, BM
   <br>Steinfeld, RR
   <br>Kaminer, Y</td>
</tr>

<tr>
<td valign="top">TI </td><td>Conversion and validation of the teen-addiction severity index (T-ASI)
   for Internet and automated-telephone self-report administration</td>
</tr>

<tr>
<td valign="top">SO </td><td>PSYCHOLOGY OF ADDICTIVE BEHAVIORS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">ID </td><td>RELIABILITY; VALIDITY</td>
</tr>

<tr>
<td valign="top">AB </td><td>This study converted the Teen-Addiction Severity Index (T-ASI) into self-report formats using Internet (Net) and interactive voice response (IVR) automated-telephone technologies. Reliability and convergent validity were assessed among 95 inpatient adolescent participants. Current functioning scores obtained by clinician interview correlated well with self-report Net (mean r = .74, SD = .14) and IVR (mean r = .72, SD = .16). Lifetime history items obtained by clinicians were consistent with self-report Net (mean r = .60, SD = .32; mean K = .67, SD = .24) and IVR formats (mean r = .60, SD = .30; mean K = .64, SD = .26). Participants rated "ease of use" as being high for both Net and IVR formats. These findings suggest that automated T-ASI administration is a valid and potentially less expensive alternative to clinician-administered T-ASI interviews.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Univ Washington, Dept Psychiat &amp; Behav Sci, Chapel Hill, NC USA.
   <br>TeleSage, Res Dept, Chapel Hill, NC USA.
   <br>Stanford Univ, Sch Med, Dept Psychiat &amp; Behav Sci, Stanford, CA 94305
   USA.
   <br>Univ Minnesota, Dept Psychiat, Minneapolis, MN 55455 USA.
   <br>Univ N Carolina, Dept Comparat Literature, Chapel Hill, NC USA.
   <br>Univ Connecticut, Dept Psychiat, Storrs, CT 06269 USA.
   <br>Univ Connecticut, Alcohol Res Ctr, Storrs, CT 06269 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Brodey, BB (reprint author), 716 Gimghoul Rd, Chapel Hill, NC 27514 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>bbrodey@telesage.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>19</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>19</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2005</td>
</tr>

<tr>
<td valign="top">VL </td><td>19</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>54</td>
</tr>

<tr>
<td valign="top">EP </td><td>61</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1037/0893-164X.19.1.54</td>
</tr>

<tr>
<td valign="top">SC </td><td>Substance Abuse; Psychology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000227729600007</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Verma, A
   <br>Kumar, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Verma, A
   <br>Kumar, A</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Introducing roughness in individuality transformation through jitter
   modeling and modification</td>
</tr>

<tr>
<td valign="top">SO </td><td>2005 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING, VOLS 1-5: SPEECH PROCESSING</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>30th IEEE International Conference on Acoustics, Speech, and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 19-23, 2005</td>
</tr>

<tr>
<td valign="top">CL </td><td>Philadelphia, PA</td>
</tr>

<tr>
<td valign="top">ID </td><td>CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Individuality transformation is a process to modify the speech signal in a person's voice so that it sounds as if spoken by another person. In most individuality transformation methods, pitch transformation is performed through a simple scaling considering the Global pitch characteristics of the source and target speakers without considering the short-term pitch variation or jitter. In this paper we present a novel method to model and modify jitter in the speech signal to introduce a handle on roughness in the process of individuality transformation. The proposed method is based upon computing the average intensity in a band around the fundamental frequency in the spectrum of a speaker's mean normalized pitch contour. The validity of the proposed method to model jitter has been established by subjective tests for perceived roughness in the speaker's voice. It is also shown that modification of jitter by the proposed method results in an improved subjective rating for individuality transformation.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Indian Inst Technol, IBM India Res Lab, New Delhi, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Verma, A (reprint author), Indian Inst Technol, IBM India Res Lab, New Delhi, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>vashish@in.ibm.corn; arunkm@care.iitd.ernet.in</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Verma, Ashish</display_name>&nbsp;</font></td><td><font size="3">L-3943-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2005</td>
</tr>

<tr>
<td valign="top">BP </td><td>5</td>
</tr>

<tr>
<td valign="top">EP </td><td>8</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000229404200002</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Toda, T
   <br>Black, AW
   <br>Tokuda, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Toda, T
   <br>Black, AW
   <br>Tokuda, K</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Spectral conversion based on maximum likelihood estimation considering
   global variance of converted parameter</td>
</tr>

<tr>
<td valign="top">SO </td><td>2005 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING, VOLS 1-5: SPEECH PROCESSING</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>30th IEEE International Conference on Acoustics, Speech, and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 19-23, 2005</td>
</tr>

<tr>
<td valign="top">CL </td><td>Philadelphia, PA</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper describes a novel spectral conversion method for the voice transformation. We perform spectral conversion between speakers using a Gaussian Mixture Model (GMM) on joint probability density of source and target features. A smooth spectral sequence can be estimated by applying maximum likelihood (ML) estimation using dynamic features to the GMM-based mapping. However, the degradation of the converted speech quality is still caused due to an over-smoothing of the converted spectra, which is inevitable in the conventional ML-based parameter estimation. In order to alleviate the over-smoothing, we propose an ML-based conversion taking account of the global variance of the converted parameter in each utterance. Experimental results show that the performance of the voice conversion can be improved by using the global variance information. Moreover, it is demonstrated that the proposed algorithm is more effective than spectral enhancement by postfiltering.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Carnegie Mellon Univ, Language Technol Inst, Pittsburgh, PA 15213 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Toda, T (reprint author), Carnegie Mellon Univ, Language Technol Inst, Pittsburgh, PA 15213 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>tomoki@cs.cmu.edu; awb@cs.cmu.edu; tokuda@ics.nitech.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>29</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>30</td>
</tr>

<tr>
<td valign="top">PY </td><td>2005</td>
</tr>

<tr>
<td valign="top">BP </td><td>9</td>
</tr>

<tr>
<td valign="top">EP </td><td>12</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000229404200003</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sundermann, D
   <br>Bonafonte, A
   <br>Ney, H
   <br>Hoge, H</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sundermann, D
   <br>Bonafonte, A
   <br>Ney, H
   <br>Hoge, H</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>A study on residual prediction techniques for voice conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>2005 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING, VOLS 1-5: SPEECH PROCESSING</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>30th IEEE International Conference on Acoustics, Speech, and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 19-23, 2005</td>
</tr>

<tr>
<td valign="top">CL </td><td>Philadelphia, PA</td>
</tr>

<tr>
<td valign="top">AB </td><td>Several well-studied voice conversion techniques use line spectral frequencies as features to represent the spectral envelopes of the processed speech frames. In order to return to the time domain, these features are converted to linear predictive coefficients that serve as coefficients of a filter applied to an unknown residual signal. In this study, we compare several residual prediction approaches that have already been proposed in the literature dealing with voice conversion. We also present a novel technique that outperforms the others in terms of voice conversion performance and sound quality.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Univ Politecn Cataluna, Dept Signal Theory &amp; Commun, ES-08034 Barcelona,
   Spain.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sundermann, D (reprint author), Univ Politecn Cataluna, Dept Signal Theory &amp; Commun, C Jordi Girona 1 3, ES-08034 Barcelona, Spain.</td>
</tr>

<tr>
<td valign="top">EM </td><td>suendermann@gps.tsc.upc.es; antonio@gps.tsc.upc.es;
   ney@cs.rwth-aachen.de; harald.hoege@siemens.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2005</td>
</tr>

<tr>
<td valign="top">BP </td><td>13</td>
</tr>

<tr>
<td valign="top">EP </td><td>16</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000229404200004</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Qin, L
   <br>Chen, GP
   <br>Ling, ZH
   <br>Dai, LR</td>
</tr>

<tr>
<td valign="top">AF </td><td>Qin, L
   <br>Chen, GP
   <br>Ling, ZH
   <br>Dai, LR</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>An improved spectral and prosodic transformation method in
   straight-based voice conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>2005 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING, VOLS 1-5: SPEECH PROCESSING</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>30th IEEE International Conference on Acoustics, Speech, and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 19-23, 2005</td>
</tr>

<tr>
<td valign="top">CL </td><td>Philadelphia, PA</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a novel spectral conversion method by considering the glottal effect on STRAIGHT spectrum to improve the performance of former voice conversion system based on codebook mapping. By introducing MoG model into spectral representation, STRAIGHT spectrum is decomposed into excitation-dependent and excitation-independent components, which are transformed separately. Besides, SFC model is adopted to measure the prosodic characteristics of different speakers and realize prosodic conversion. Listening test proves that proposed method can effectively improve the discrimination and speech quality of converted speech at the same time.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Univ Sci &amp; Technol China, iFLYTEK Speech Lab, Hefei 230026, Peoples R
   China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Qin, L (reprint author), Univ Sci &amp; Technol China, iFLYTEK Speech Lab, Hefei 230026, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>qinlong@mail.ustc.edu.cn; gpchen@ustc.edu; zhling@ustc.edu;
   lrdai@ustc.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2005</td>
</tr>

<tr>
<td valign="top">BP </td><td>21</td>
</tr>

<tr>
<td valign="top">EP </td><td>24</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000229404200006</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sano, M
   <br>Sumiyoshi, H
   <br>Shibata, M
   <br>Yagi, N</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sano, M
   <br>Sumiyoshi, H
   <br>Shibata, M
   <br>Yagi, N</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Generating metadata from acoustic and speech data in live broadcasting</td>
</tr>

<tr>
<td valign="top">SO </td><td>2005 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING, VOLS 1-5: SPEECH PROCESSING</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>30th IEEE International Conference on Acoustics, Speech, and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 19-23, 2005</td>
</tr>

<tr>
<td valign="top">CL </td><td>Philadelphia, PA</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper describes a method to generate metadata for TV programs in real-time by utilizing acoustic and speech data in live broadcasting. Various styles of watching TV programs can be provided by using metadata related to the content of the program. The acoustic data to be processed in our case is crowd noise in a football (soccer) stadium, and the speech data is an announcer's voice. The crowd noise is closely related to not only spectators' emotions but also their attention and expectations. In other words, a part in which the crowd noise rises corresponds to an important event in the game. Because the crowd noise conveys no further information about what happened in the scene, the announcer's voice, after speech-to-text conversion, is processed to extract further meaning. By combining these two processes of identifying and extracting, content-based segment metadata is generated automatically. This method was applied to generating metadata for six professional football games, by which its effectiveness was verified.</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2005</td>
</tr>

<tr>
<td valign="top">BP </td><td>1145</td>
</tr>

<tr>
<td valign="top">EP </td><td>1148</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000229404201287</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sundermann, D
   <br>Hoge, H
   <br>Bonafonte, A
   <br>Duxans, H</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sundermann, D
   <br>Hoge, H
   <br>Bonafonte, A
   <br>Duxans, H</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Residual prediction</td>
</tr>

<tr>
<td valign="top">SO </td><td>2005 IEEE International Symposium on Signal Processing and Information
   Technology (ISSPIT), Vols 1 and 2</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>5th IEEE International Symposium on Signal Processing and Information
   Technology</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 18-21, 2005</td>
</tr>

<tr>
<td valign="top">CL </td><td>Athens, GREECE</td>
</tr>

<tr>
<td valign="top">AB </td><td>Residual prediction is a technique that aims at recovering the spectral details of speech that was encoded using parameterizations as linear predictive coefficients. Example applications of residual prediction are hidden Markov model-based speech synthesis or voice conversion. Our voice conversion experiments showed that only one of the seven compared techniques was capable of successfully converting the voice while achieving a fair speech quality (i.e. mean opinion score = 3).</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Siemens AG, D-8000 Munich, Germany.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sundermann, D (reprint author), Siemens AG, Otto Hahn Ring 6, D-8000 Munich, Germany.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2005</td>
</tr>

<tr>
<td valign="top">BP </td><td>512</td>
</tr>

<tr>
<td valign="top">EP </td><td>516</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering; Imaging Science &amp; Photographic
   Technology; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000236568000090</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nordstrom, KI
   <br>Rutledge, GA
   <br>Driessen, PF</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nordstrom, KI
   <br>Rutledge, GA
   <br>Driessen, PF</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Using voice conversion as a paradigm for analyzing breathy singing
   voices</td>
</tr>

<tr>
<td valign="top">SO </td><td>2005 IEEE PACIFIC RIM CONFERENCE ON COMMUNICATIONS, COMPUTERS AND SIGNAL
   PROCESSING (PACRIM)</td>
</tr>

<tr>
<td valign="top">SE </td><td>IEEE PACIFIC RIM CONFERENCE ON COMMUNICATIONS, COMPUTERS AND SIGNAL
   PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE Pacific Rim Conference on Communications, Computers, and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 24-26, 2005</td>
</tr>

<tr>
<td valign="top">CL </td><td>Victoria, CANADA</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOCAL-TRACT; PERCEPTION</td>
</tr>

<tr>
<td valign="top">AB </td><td>it is generally thought that breathiness can be added to voices by modifying the glottal source within a source-filter model. However, this does not work well when the original voice is very different from the desired breathy voice. In this experiment, a voice conversion algorithm is used to investigate the relationship between the glottal source and the vocal tract filter. The LPC residual from one voice is fed into the LPC filter of another voice. According to a source-filter theory of the voice, the synthesized voice should take on the glottal quality of the LPC source. This hypothesis is evaluated through a perceptual test with a linguistics expert. The results suggest that the vocal tract does have an influence on the perception of breathy voices. Given the narrow nature of this experiment, further testing is recommended to verify these results.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Univ Victoria, Dept Elect &amp; Comp Engn, Victoria, BC V8W 3P6, Canada.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nordstrom, KI (reprint author), Univ Victoria, Dept Elect &amp; Comp Engn, POB 3055 STN CSC, Victoria, BC V8W 3P6, Canada.</td>
</tr>

<tr>
<td valign="top">EM </td><td>knordstr@ece.uvic.ca; GRutledge@tc-helicon.com; peter@ece.uvic.ca</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2005</td>
</tr>

<tr>
<td valign="top">BP </td><td>428</td>
</tr>

<tr>
<td valign="top">EP </td><td>431</td>
</tr>

<tr>
<td valign="top">SC </td><td>Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000234267800108</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sundermann, D
   <br>Hoge, H
   <br>Bonafonte, A
   <br>Ney, H
   <br>Black, AW</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sundermann, D
   <br>Hoge, H
   <br>Bonafonte, A
   <br>Ney, H
   <br>Black, AW</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Residual prediction based on unit selection</td>
</tr>

<tr>
<td valign="top">SO </td><td>2005 IEEE Workshop on Automatic Speech Recognition and Understanding
   (ASRU)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE Workshop on Automatic Speech Recognition and Understanding</td>
</tr>

<tr>
<td valign="top">CY </td><td>NOV 27-DEC 01, 2005</td>
</tr>

<tr>
<td valign="top">CL </td><td>San Juan, PR</td>
</tr>

<tr>
<td valign="top">AB </td><td>Recently, we presented a study on residual prediction techniques that can be applied to voice conversion based on linear transformation or hidden Markov model-based speech synthesis. Our voice conversion experiments showed that none of the six compared techniques was capable of successfully converting the voice while achieving a fair speech quality. In this paper, we suggest a novel residual prediction technique based on unit selection that outperforms the others in terms of speech quality (mean opinion score = 3) while keeping the conversion performance.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Siemens Corp Technol, Munich, Germany.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sundermann, D (reprint author), Siemens Corp Technol, Munich, Germany.</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2005</td>
</tr>

<tr>
<td valign="top">BP </td><td>369</td>
</tr>

<tr>
<td valign="top">EP </td><td>374</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000235936000071</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Cheng, YM
   <br>Ma, CX
   <br>Melnar, L</td>
</tr>

<tr>
<td valign="top">AF </td><td>Cheng, YM
   <br>Ma, CX
   <br>Melnar, L</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice-to-phoneme conversion algorithms for speaker-independent voice-tag
   applications in embedded platforms</td>
</tr>

<tr>
<td valign="top">SO </td><td>2005 IEEE Workshop on Automatic Speech Recognition and Understanding
   (ASRU)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE Workshop on Automatic Speech Recognition and Understanding</td>
</tr>

<tr>
<td valign="top">CY </td><td>NOV 27-DEC 01, 2005</td>
</tr>

<tr>
<td valign="top">CL </td><td>San Juan, PR</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper we present two voice-to-phoneme conversion algorgithms that extract voice-tag abstractions for speaker-independent voice-tag applications in embedded platforms, which are very sensitive to memory and CPU consumptions. In the first approach, a voice-to-phoneme conversion in batch mode manages this task by preserving the commonality of input feature vectors of multiple voice-tag example utterances. Given multiple example utterances, a developed feature combination strategy produces an "average" utterance, which is converted to phonetic strings as a voice-tag representation via a speaker-independent phonetic decoder. In the second approach, a sequential voice-to-phoneme conversion algorithm uncovers the hierarchy of phonetic consensus embedded among multiple phonetic hypotheses generated by a speaker-independent phonetic decoder from multiple example utterances of a voice-tag. The most relevant phonetic hypotheses are then chosen to represent the voice-tag.
   <br>The voice-tag representations obtained by these two voice-to-phoneme conversion algorithms are compared in speech recognition experiments to phonetic transcriptions of voice-tag reference prepared by an expert phonetician. Both algorithms either per-form comparably to or significantly better than the manual transcription approach. We conclude from this that both algorithms are very effective for the targeted purposes.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Ctr Human Interact Res, Motorola Labs, Schaumburg, IL 60196 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Cheng, YM (reprint author), Ctr Human Interact Res, Motorola Labs, 1295 E Algonquin Rd, Schaumburg, IL 60196 USA.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2005</td>
</tr>

<tr>
<td valign="top">BP </td><td>403</td>
</tr>

<tr>
<td valign="top">EP </td><td>408</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000235936000077</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kang, YG
   <br>Shuang, ZW
   <br>Tao, JH
   <br>Zhang, W
   <br>Xu, B</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kang, YG
   <br>Shuang, ZW
   <br>Tao, JH
   <br>Zhang, W
   <br>Xu, B</td>
</tr>

<tr>
<td valign="top">BE </td><td>Tao, J
   <br>Picard, RW</td>
</tr>

<tr>
<td valign="top">TI </td><td>A hybrid GMM and codebook mapping method for spectral conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>AFFECTIVE COMPUTING AND INTELLIGENT INTERACTION, PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">SE </td><td>Lecture Notes in Computer Science</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article; Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>1st International Conference on Affective Computing and Intelligent
   Interaction</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 22-24, 2005</td>
</tr>

<tr>
<td valign="top">CL </td><td>Beijing, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION; TRANSFORMATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper proposes a new mapping method combining GMM and codebook mapping methods to transform spectral envelope for voice conversion system. After analyzing overly smoothing problem of GMM mapping method in detail, we propose to convert the basic spectral envelope by GMM method and convert envelope-subtracted spectral details by GMM and phone-tied codebook mapping method. Objective evaluations based on performance indices show that the performance of proposed mapping method averagely improves 27.2017% than GMM mapping method, and listening tests prove that the proposed method can effectively reduce over smoothing problem of GMM method while it can avoid the discontinuity problem of codebook mapping method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Chinese Acad Sci, Inst Automat, Beijing 100864, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kang, YG (reprint author), Chinese Acad Sci, Inst Automat, Beijing 100864, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ygkang@nlpr.ia.ac.cn; shuangzw@cn.ibm.com; jhtao@nlpr.ia.ac.cn;
   zhangwei@cn.ibm.com; xubo@hitic.ia.ac.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>4</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>4</td>
</tr>

<tr>
<td valign="top">PY </td><td>2005</td>
</tr>

<tr>
<td valign="top">VL </td><td>3784</td>
</tr>

<tr>
<td valign="top">BP </td><td>303</td>
</tr>

<tr>
<td valign="top">EP </td><td>310</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000234342700039</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Zhang, J
   <br>Sun, J
   <br>Dai, BQ</td>
</tr>

<tr>
<td valign="top">AF </td><td>Zhang, J
   <br>Sun, J
   <br>Dai, BQ</td>
</tr>

<tr>
<td valign="top">BE </td><td>Tao, J
   <br>Picard, RW</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice conversion based on weighted least squares estimation criterion
   and residual prediction from pitch contour</td>
</tr>

<tr>
<td valign="top">SO </td><td>AFFECTIVE COMPUTING AND INTELLIGENT INTERACTION, PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">SE </td><td>LECTURE NOTES IN COMPUTER SCIENCE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article; Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>1st International Conference on Affective Computing and Intelligent
   Interaction</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 22-24, 2005</td>
</tr>

<tr>
<td valign="top">CL </td><td>Beijing, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper describes an enhanced system for more efficient voice conversion. A weighted LMSE (Least Mean Squared Error) criterion is adopted, instead of conventional LMSE, for the spectral conversion function training. In addition, a short-term pitch contour mapping algorithm together with a new residual codebook formed from pitch contour is presented. Informal listening tests prove that convincing voice conversion is achieved while maintaining high speech quality. Evaluations by objective tests also show that the proposed system reduces speaker individual discrimination compared with the baseline system in LPC based analysis/synthesis framework.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Univ Sci &amp; Technol China, Elect Sci &amp; Technol Dept, Hefei 230026, Anhui,
   Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Zhang, J (reprint author), Univ Sci &amp; Technol China, Elect Sci &amp; Technol Dept, Hefei 230026, Anhui, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jianzhang@ust.edu; sunjun@ust.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2005</td>
</tr>

<tr>
<td valign="top">VL </td><td>3784</td>
</tr>

<tr>
<td valign="top">BP </td><td>326</td>
</tr>

<tr>
<td valign="top">EP </td><td>333</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000234342700042</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kim, SJ
   <br>Kim, KK
   <br>Han, HB
   <br>Hahn, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kim, SJ
   <br>Kim, KK
   <br>Han, HB
   <br>Hahn, M</td>
</tr>

<tr>
<td valign="top">BE </td><td>Tao, J
   <br>Picard, RW</td>
</tr>

<tr>
<td valign="top">TI </td><td>Study on emotional speech features in Korean with its application to
   voice conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>AFFECTIVE COMPUTING AND INTELLIGENT INTERACTION, PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">SE </td><td>Lecture Notes in Computer Science</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article; Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>1st International Conference on Affective Computing and Intelligent
   Interaction</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 22-24, 2005</td>
</tr>

<tr>
<td valign="top">CL </td><td>Beijing, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">AB </td><td>Recent researches in speech synthesis are mainly focused on naturalness, and the emotional speech synthesis becomes one of the highlighted research topics. Although quite a many studies on emotional speech in English or Japanese have been addressed, the studies in Korean can seldom be found. This paper presents an analysis of emotional speech in Korean. Emotional speech features related to human speech prosody, such as F0, the duration, and the amplitude with their variations, are exploited. Their attribution to three different types of typical human speech is tried to be quantified and modeled. By utilizing the analysis results, emotional voice conversion from the neutral speech to the emotional one is also performed and tested.</td>
</tr>

<tr>
<td valign="top">EM </td><td>sangjin@icu.ac.kr; kkkim@icu.ac.kr; sunshine@kt.co.kr; mshahn@icu.ac.kr</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Hahn, Minsoo</display_name>&nbsp;</font></td><td><font size="3">C-1746-2011&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2005</td>
</tr>

<tr>
<td valign="top">VL </td><td>3784</td>
</tr>

<tr>
<td valign="top">BP </td><td>342</td>
</tr>

<tr>
<td valign="top">EP </td><td>349</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000234342700044</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wang, YP
   <br>Ling, ZH
   <br>Wang, RH</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wang, YP
   <br>Ling, ZH
   <br>Wang, RH</td>
</tr>

<tr>
<td valign="top">BE </td><td>Tao, J
   <br>Picard, RW</td>
</tr>

<tr>
<td valign="top">TI </td><td>Emotional speech synthesis based on improved codebook mapping voice
   conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>AFFECTIVE COMPUTING AND INTELLIGENT INTERACTION, PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">SE </td><td>LECTURE NOTES IN COMPUTER SCIENCE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article; Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>1st International Conference on Affective Computing and Intelligent
   Interaction</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 22-24, 2005</td>
</tr>

<tr>
<td valign="top">CL </td><td>Beijing, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a spectral transformation method for emotional speech synthesis based on voice conversion framework. Three emotions are studied, including anger, happiness and sadness. For the sake of high naturalness, superior speech quality and emotion expressiveness, our original STASC system is modified by introducing a new feature selection strategy and hierarchical codebook mapping procedure. Our result shows that the LSF coefficients at low frequency carry more emotion-relative information, and therefore only these coefficients are converted. Listening tests prove that the proposed method can achieve a satisfactory balance between emotional expression and speech quality of converted speech signals.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Univ Sci &amp; Technol China, iFlytek Speech Lab, Hefei 230026, Peoples R
   China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Wang, YP (reprint author), Univ Sci &amp; Technol China, iFlytek Speech Lab, Hefei 230026, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ypwang2@ustc.edu; zhling@ustc.edu; rhw@ustc.edn.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2005</td>
</tr>

<tr>
<td valign="top">VL </td><td>3784</td>
</tr>

<tr>
<td valign="top">BP </td><td>374</td>
</tr>

<tr>
<td valign="top">EP </td><td>381</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000234342700048</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sato, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sato, Y</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice quality conversion using interactive evolution of prosodic control</td>
</tr>

<tr>
<td valign="top">SO </td><td>APPLIED SOFT COMPUTING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>interactive evolution; evolutionary computation; voice quality
   conversion; prosodic control</td>
</tr>

<tr>
<td valign="top">AB </td><td>Recent years have seen the birth of new markets using voice quality conversion technology for a variety of application fields including man-personal machine interfaces, addition of narration in multimedia-content editing, and computer games. Optimal parameters for voice quality conversion, however, are speaker dependent, and consequently, no clear-cut algorithm has existed in the past and parameter adjustment has usually been performed by an experienced designer on a trial and error basis. This paper proposes the application of evolutionary computation, a stochastic search technique based on organic evolution, to parameter adjustment for voice conversion, and reports on several experimental results applicable to the fitting of prosodic coefficients. Evolutionary computation is said to be "applicable to even cases where the properties of the target function are not well known,'' and we decided to apply it considering that this feature might be effective in our study. Providing an explicit evaluative function for evolutionary computation, however, is difficult, and we here adopt an interactive-evolution system in which genetic manipulation is performed repeatedly while evaluating results based on human emotions. Evaluation experiments were performed on raw human speech recorded by a microphone and speech mechanically synthesized from text. It was found that the application of evolutionary computation could achieve voice conversion satisfying specific targets with relatively little degradation of sound quality and no impression of artificial processing in comparison to parameter adjustment based on designer experience or trial and error. This paper also shows that prosodic conversion coefficients determined by the interactive evolution technique, while exhibiting speaker dependency, is not text dependent. (C) 2004 Elsevier B.V. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Hosei Univ, Fac Comp &amp; Informat Sci, Koganei, Tokyo 1848584, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sato, Y (reprint author), Hosei Univ, Fac Comp &amp; Informat Sci, 3-7-2 Kajino Cho, Koganei, Tokyo 1848584, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>yuji@k.hosei.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>9</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>9</td>
</tr>

<tr>
<td valign="top">PD </td><td>JAN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2005</td>
</tr>

<tr>
<td valign="top">VL </td><td>5</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>181</td>
</tr>

<tr>
<td valign="top">EP </td><td>192</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.asoc.2004.06.005</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000227208700004</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Huang, F
   <br>Yin, JX</td>
</tr>

<tr>
<td valign="top">AF </td><td>Huang, F
   <br>Yin, JX</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Statistical eigenvoice: Speaker features within S+N framework and a way
   towards language-independent voice conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>ISPACS 2005: PROCEEDINGS OF THE 2005 INTERNATIONAL SYMPOSIUM ON
   INTELLIGENT SIGNAL PROCESSING AND COMMUNICATION SYSTEMS</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Symposium on Intelligent Signal Processing and
   Communication Systems ISPACS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Symposium on Intelligent Signal Processing and
   Communication Systems</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 13-16, 2005</td>
</tr>

<tr>
<td valign="top">CL </td><td>Chinese Univ Hong Kong, Chung Chi Coll, Hong Kong, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">HO </td><td>Chinese Univ Hong Kong, Chung Chi Coll</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a statistical method for speaker feature extraction and voice conversion within Sinusoidal+Noise (S+N) modeling framework. With fundamental researches on speaker characteristics embedded in the parameter sets of S+N model, we found the vector sets of Statistical EigenVoice (SEV) and Weighted Statistical EigenVoice (wSEV), which are basis vectors of GMM representation, have significant properties: approximately speaker-dependent and language-independent. Piered by the feature vectors of SEV and wSEV, we address a new algorithm for context-free voice conversion. Subjective tests suggest that the SEV based method achieves convincing results while maintaining high synthesis quality in comparison to the traditional LPC approaches.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>S China Univ Technol, Sch Elect &amp; Informat Engn, Guangzhou 510640,
   Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Huang, F (reprint author), S China Univ Technol, Sch Elect &amp; Informat Engn, Guangzhou 510640, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>eefhuang@scut.edu.cn; eejyin@scut.edu.cn</td>
</tr>

<tr xmlns:date="http://exslt.org/dates-and-times">
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Huang, Feng</display_name>&nbsp;</font></td><td><font size="3">0000-0003-2813-3107&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2005</td>
</tr>

<tr>
<td valign="top">BP </td><td>33</td>
</tr>

<tr>
<td valign="top">EP </td><td>36</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering; Imaging Science &amp; Photographic
   Technology; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000235933100009</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Xia, J
   <br>Yin, JX</td>
</tr>

<tr>
<td valign="top">AF </td><td>Xia, J
   <br>Yin, JX</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>A GMM based residual prediction method for voice conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>ISPACS 2005: PROCEEDINGS OF THE 2005 INTERNATIONAL SYMPOSIUM ON
   INTELLIGENT SIGNAL PROCESSING AND COMMUNICATION SYSTEMS</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Symposium on Intelligent Signal Processing and
   Communication Systems ISPACS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Symposium on Intelligent Signal Processing and
   Communication Systems</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 13-16, 2005</td>
</tr>

<tr>
<td valign="top">CL </td><td>Chinese Univ Hong Kong, Chung Chi Coll, Hong Kong, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">HO </td><td>Chinese Univ Hong Kong, Chung Chi Coll</td>
</tr>

<tr>
<td valign="top">AB </td><td>The purpose of a voice conversion (VC) system is to change the perceived speaker identity of a speech signal to sound as if a target speaker had spoken it. In this paper, we propose a residual prediction method for the spectral detail transformation component of a VC system. The algorithm described here is based on the LPC analysis/synthesis framework, and achieves residual prediction from LPC parameters during voiced speech. This step consists of a GMM based LPC parameter classifier and a LPC residual codebook. The predicted residual is then combined with all-pole LPC spectrum to synthesize speech signal. Several aspects of this residual prediction method, including the validation of the codebook and the performance of the coded speech are tested using objective measures. The converted speech is found nearly indistinguishable from the target speaker's individuality in informal listening tests.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>S China Univ Technol, Guangzhou 510641, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Xia, J (reprint author), S China Univ Technol, Guangzhou 510641, Peoples R China.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2005</td>
</tr>

<tr>
<td valign="top">BP </td><td>389</td>
</tr>

<tr>
<td valign="top">EP </td><td>392</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering; Imaging Science &amp; Photographic
   Technology; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000235933100098</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Caratozzolo, E
   <br>Recordare, A
   <br>Massani, M
   <br>Bonariol, L
   <br>Jelmoni, A
   <br>Antoniutti, M
   <br>Bassi, N</td>
</tr>

<tr>
<td valign="top">AF </td><td>Caratozzolo, E
   <br>Recordare, A
   <br>Massani, M
   <br>Bonariol, L
   <br>Jelmoni, A
   <br>Antoniutti, M
   <br>Bassi, N</td>
</tr>

<tr>
<td valign="top">TI </td><td>Telerobotic-assisted laparoscopic cholecystectomy: our experience on 29
   patients</td>
</tr>

<tr>
<td valign="top">SO </td><td>JOURNAL OF HEPATO-BILIARY-PANCREATIC SURGERY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>telerobotic surgery; computer-assisted cholecystectomy; laparoscopic
   cholecystectomy</td>
</tr>

<tr>
<td valign="top">ID </td><td>BILE-DUCT INJURIES; SURGERY; ROBOTICS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Background/Purpose. The role of computer-assisted surgery (CAS) is still debated and not clearly defined. Methods. The authors report their initial experience with CAS, comparing 29 patients submitted to cholecystectomy, using a Zeus remote-controlled robot and an Aesop remote voice-activated endoscope robot, with 29 patients submitted to standard laparoscopic cholecystectomy (LC). The surgical field and the arms of the robot were under the direct and real-time control of the surgeon, who stayed at the workstation and maneuvered the Zeus, using joysticks. The workstation was in the same room as the patient. Results. Twenty-nine patients underwent telerobotic-assisted cholecystectomy (TLAC); 1 procedure was converted to standard LC and 1 to open cholecystectomy. The conversions were due to choledocholithiasis and cholecystitis. During TLAC, the mean operating time and transition time (from the induction of anesthesia to incision of the skin) were, respectively, 75 min (range, 60-170 min) and 45 min (range, 25-60 min). We did not observe any complications related to TLAC. The limitations of TLAC were the lack of tactile feedback, the increase in surgical time, and the expensive cost of the procedure to reach the same result as that of LC. Conclusions. After this initial experience, we believe that TLAC could be considered only for training in CAS, but that it is without advantages in terms of its higher cost compared with LC.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Reg Hosp Ca Foncello Piazza Osped, Reg Reference Ctr Hepato Biliary
   Pancreat Surg, Unit Surg 4, I-31100 Treviso, Italy.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Caratozzolo, E (reprint author), Reg Hosp Ca Foncello Piazza Osped, Reg Reference Ctr Hepato Biliary Pancreat Surg, Unit Surg 4, I-31100 Treviso, Italy.</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Recordare, Alfonso</display_name>&nbsp;</font></td><td><font size="3">N-6453-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>8</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>8</td>
</tr>

<tr>
<td valign="top">PY </td><td>2005</td>
</tr>

<tr>
<td valign="top">VL </td><td>12</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>163</td>
</tr>

<tr>
<td valign="top">EP </td><td>166</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/s00534-004-0932-5</td>
</tr>

<tr>
<td valign="top">SC </td><td>Gastroenterology &amp; Hepatology; Surgery</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000228811700012</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Vondra, M
   <br>Vich, R</td>
</tr>

<tr>
<td valign="top">AF </td><td>Vondra, M
   <br>Vich, R</td>
</tr>

<tr>
<td valign="top">BE </td><td>Chollet, G
   <br>Esposito, A
   <br>FaundezZanuy, M
   <br>Marinaro, M</td>
</tr>

<tr>
<td valign="top">TI </td><td>Speech identity conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>NONLINEAR SPEECH MODELING AND APPLICATIONS</td>
</tr>

<tr>
<td valign="top">SE </td><td>LECTURE NOTES IN ARTIFICIAL INTELLIGENCE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article; Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th International Summer School Neural Nets E R Caianiello on Nonlinear
   Speech Processing - Algorithms and Analysis</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 13-18, 2004</td>
</tr>

<tr>
<td valign="top">CL </td><td>Vietri sul Mare, ITALY</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper a new voice conversion algorithm will be presented, which transforms the utterance of a source speaker into the utterance of a target speaker or into the utterance of a new unknown speaker. Presented voice conversion algorithm is based on spectral speech analysis, spectral envelope warping, spectrum interpolation and parametrical high quality IIR or FIR cepstral speech synthesis. Several approaches to frequency warping of the speech spectrum axe compared, e.g. linear frequency transformation, piecewise linear frequency modification and nonlinear frequency low-pass to low-pass transformation. Prosodic transformation. i.e. fundamental frequency, time and intensity scale modifications are not mentioned.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Brno Univ Techn, Brno, Czech Republic.
   <br>Acad Sci Czech Republ, Inst Radio Engn &amp; Elect, Prague 18251 8, Czech
   Republic.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Vondra, M (reprint author), Brno Univ Techn, Purkynova 118, Brno, Czech Republic.</td>
</tr>

<tr>
<td valign="top">EM </td><td>vondra@feec.vutbr.cz; vich@ure.cas.cz</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2005</td>
</tr>

<tr>
<td valign="top">VL </td><td>3445</td>
</tr>

<tr>
<td valign="top">BP </td><td>421</td>
</tr>

<tr>
<td valign="top">EP </td><td>426</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000230863300028</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Zuo, GY
   <br>Chen, Y
   <br>Ruan, XG
   <br>Liu, WJ</td>
</tr>

<tr>
<td valign="top">AF </td><td>Zuo, GY
   <br>Chen, Y
   <br>Ruan, XG
   <br>Liu, WJ</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Learning Mandarin tone mapping codebook for voice conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>Proceedings of 2005 International Conference on Machine Learning and
   Cybernetics, Vols 1-9</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>4th International Conference on Machine Learning and Cybernetics</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 18-21, 2005</td>
</tr>

<tr>
<td valign="top">CL </td><td>Canton, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; Mandarin tone; codebook mapping</td>
</tr>

<tr>
<td valign="top">AB </td><td>The phonetics properties of Chinese Mandarin features clearly demarcated syllables with tonal phonemes. A tone codebook mapping method is proposed to obtain a better performance in voice conversion of Chinese Mandarin speech than the conventional conversion method, which focuses mainly on the short-time spectral envelopes. The pitch contour of the whole Mandarin syllable is used as a unit type for pitch conversion. The syllable pitch contours are first extracted from the source and target utterances. Time normalization and moving average filtering are then performed on them. These preprocessed pitch contours are classified to generate the source and target tone code-books, and by associating them a Mandarin tone mapping codebook is finally obtained in terms of speech alignment. Experimental results show that the proposed tone codebook mapping method in voice conversion can give a good performance in Chinese Mandarin speech.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Beijing Univ Technol, Inst Artificial Intelligence &amp; Robot, Beijing
   100022, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Zuo, GY (reprint author), Beijing Univ Technol, Inst Artificial Intelligence &amp; Robot, Beijing 100022, Peoples R China.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2005</td>
</tr>

<tr>
<td valign="top">BP </td><td>4824</td>
</tr>

<tr>
<td valign="top">EP </td><td>4828</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000235325607047</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ma, JC
   <br>Liu, WJ</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ma, JC
   <br>Liu, WJ</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice conversion based on joint pitch and spectral transformation with
   component - Group-GMM</td>
</tr>

<tr>
<td valign="top">SO </td><td>PROCEEDINGS OF THE 2005 IEEE INTERNATIONAL CONFERENCE ON NATURAL
   LANGUAGE PROCESSING AND KNOWLEDGE ENGINEERING (IEEE NLP-KE'05)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Natural Language Processing and Knowledge
   Engineering</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 30-NOV 01, 2005</td>
</tr>

<tr>
<td valign="top">CL </td><td>Wuhan, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">AB </td><td>Spectral and pitch are two most important features in voice conversion which including a majority of speaker identity information. Some researchers use the GMM (Gaussian Mixture Model) to model the joint spectral and pitch. But these two features have the discrepancy of unit and meaning, so should do some processing before training the model. In this paper, a new framework CG-GMM (Component-Group GMM) is used for the joint pitch and spectral transformation. Experiments are setup and compared with the previous approach of voice conversion. The converted speeches indicate satisfactory speech quality and speaker identifiability. Meanwhile the speaking style is much like to the target speaker.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing
   100080, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ma, JC (reprint author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, 95 E Zhong Guan Cun Rd,Haidian Dist, Beijing 100080, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jcma@nlpr.ia.ac.cn; lwj@nlpr.ia.ac.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>6</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>6</td>
</tr>

<tr>
<td valign="top">PY </td><td>2005</td>
</tr>

<tr>
<td valign="top">BP </td><td>199</td>
</tr>

<tr>
<td valign="top">EP </td><td>203</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000235577200039</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Rurainsky, J
   <br>Eisert, P</td>
</tr>

<tr>
<td valign="top">AF </td><td>Rurainsky, J
   <br>Eisert, P</td>
</tr>

<tr>
<td valign="top">BE </td><td>Li, SP
   <br>Pereira, F
   <br>Shum, HY
   <br>Tescher, AG</td>
</tr>

<tr>
<td valign="top">TI </td><td>Text2Video: Text-driven facial animation using MPEG-4</td>
</tr>

<tr>
<td valign="top">SO </td><td>Visual Communications and Image Processing 2005, Pts 1-4</td>
</tr>

<tr>
<td valign="top">SE </td><td>PROCEEDINGS OF THE SOCIETY OF PHOTO-OPTICAL INSTRUMENTATION ENGINEERS
   (SPIE)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Conference on Visual Communications and Image Processing 2005</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUL 12-15, 2005</td>
</tr>

<tr>
<td valign="top">CL </td><td>Beijing, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>MPEG-4; facial animation; text-driven animation; SMS; MMS</td>
</tr>

<tr>
<td valign="top">AB </td><td>We present a complete system for the automatic creation of talking head video sequences from text messages. Our system converts the text into MPEG-4 Facial Animation Parameters and synthetic voice. A user selected 3D character will perform lip movements synchronized to the speech data. The 3D models created from a single image vary from realistic people to cartoon characters. A voice selection for different languages and gender as well as a pitch shift component enables a personalization of the animation. The animation can be shown on different displays and devices ranging from 3GPP players on mobile phones to real-time 3D render engines. Therefore, our system can be used in mobile communication for the conversion of regular SMS messages to MMS animations.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Heinrich Hertz Inst Nachrichtentech Berlin GmbH, Fraunhofer Inst
   Telecommun, Image Proc Dept, D-10587 Berlin, Germany.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Rurainsky, J (reprint author), Heinrich Hertz Inst Nachrichtentech Berlin GmbH, Fraunhofer Inst Telecommun, Image Proc Dept, D-10587 Berlin, Germany.</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Eisert, Peter</display_name>&nbsp;</font></td><td><font size="3">0000-0001-8378-4805&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2005</td>
</tr>

<tr>
<td valign="top">VL </td><td>5960</td>
</tr>

<tr>
<td valign="top">BP </td><td>492</td>
</tr>

<tr>
<td valign="top">EP </td><td>500</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1117/12.631413</td>
</tr>

<tr>
<td valign="top">PN </td><td>1-4</td>
</tr>

<tr>
<td valign="top">SC </td><td>Imaging Science &amp; Photographic Technology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000232176400053</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Verma, A
   <br>Kumar, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Verma, A
   <br>Kumar, A</td>
</tr>

<tr>
<td valign="top">TI </td><td>Modification of harmonic peak-to-valley ratio for controlling roughness
   in voice conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>ELECTRONICS LETTERS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOCAL QUALITY</td>
</tr>

<tr>
<td valign="top">AB </td><td>A method for modifying voice quality attributes, i.e. breathiness and roughness, is presented in the context of voice conversion. Both breathiness and roughness of a speaker are collectively modelled by harmonic peak-to-valley ratio (HPVR) in the speaker's speech spectrum. The average HPVR is modified through a post-filtering operation after the conversion of spectral envelope, pitch and other speaker individuality features. It is shown that average HPVR for various speakers correlates well with the perceived breathiness and roughness in their voices. Further, it is shown through subjective experiments that the proposed modification in HPVR results in improved subjective rating of the transformed speech signal.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Indian Inst Technol, IBM India Res Lab, New Delhi 110016, India.
   <br>Indian Inst Technol, Ctr Appl Res Elect, New Delhi 110016, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Verma, A (reprint author), Indian Inst Technol, IBM India Res Lab, New Delhi 110016, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>vashish@in.ibm.com</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Verma, Ashish</display_name>&nbsp;</font></td><td><font size="3">L-3943-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>DEC 9</td>
</tr>

<tr>
<td valign="top">PY </td><td>2004</td>
</tr>

<tr>
<td valign="top">VL </td><td>40</td>
</tr>

<tr>
<td valign="top">IS </td><td>25</td>
</tr>

<tr>
<td valign="top">BP </td><td>1613</td>
</tr>

<tr>
<td valign="top">EP </td><td>1615</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1049/el:20047285</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000225964200033</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Rassweiler, J
   <br>Marrero, R
   <br>Hammady, A
   <br>Erdogru, T
   <br>Teber, D
   <br>Frede, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Rassweiler, J
   <br>Marrero, R
   <br>Hammady, A
   <br>Erdogru, T
   <br>Teber, D
   <br>Frede, T</td>
</tr>

<tr>
<td valign="top">TI </td><td>Transperitoneal laparoscopic radical prostatectomy: Ascending technique</td>
</tr>

<tr>
<td valign="top">SO </td><td>JOURNAL OF ENDOUROLOGY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">AB </td><td>Background and Purpose: To demonstrate the operative steps of transperitoneal laparoscopic radical prostatectomy with the ascending approach (Heilbronn technique).
   <br>Patients and Methods: The technique is based on our experience with more than 1000 cases of clinically localized prostate cancer from March 1999 to April 2004. The technical steps, instrumental requirements, patient data, complications, and reintervention rate were reviewed.
   <br>Results: The principles of the technique include the routine use of a voice-controlled robot (AESOP(R)) for the camera, exposure of the prostatic apex with 120degrees retracting forceps, free-hand suturing for Santorini plexus control, application of 5-mm clips during the nerve-sparing technique, control of the prostatic pedicles by 12-mm Hem-o-Lock clips, the bladder neck-sparing technique in patients with stage T-1c and T-2a tumors, and use of interrupted sutures for the urethrovesical anastomosis. A considerable improvement was observed when comparing the first 300 with the most recent 300 cases (mean operating time 280 v 208 minutes; conversion rate 2.7% v 0.3%; reintervention rate 3.7% v 1.0%).
   <br>Conclusions: Through our experience with more than 1000 cases, transperitoneal access for laparoscopic radical prostatectomy has proven to be feasible and transferable with results comparable to those of the original open approach. Besides the well-known advantages of minimally invasive surgery, the video endoscopic approach may offer further benefits in permitting optimization of the technique by video assessment.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Heidelberg Univ, SLK Klinikum Heilbronn, Urol Dept, Heidelberg, Germany.
   <br>Hosp Gran Canaria Dr Negin, Urol Dept, Las Palmas Gran Canaria, Spain.
   <br>S Valley Univ Hosp, Urol Dept, Alexandria, Egypt.
   <br>Akdeniz Univ, Tip Fak Urol ABD, Antalya, Turkey.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Rassweiler, J (reprint author), Urol Clin, Am Gesundbrunnen 20, D-74074 Heilbronn, Germany.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jens.rassweiler@skl-klinken.de</td>
</tr>

<tr>
<td valign="top">TC </td><td>35</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>35</td>
</tr>

<tr>
<td valign="top">PD </td><td>SEP</td>
</tr>

<tr>
<td valign="top">PY </td><td>2004</td>
</tr>

<tr>
<td valign="top">VL </td><td>18</td>
</tr>

<tr>
<td valign="top">IS </td><td>7</td>
</tr>

<tr>
<td valign="top">BP </td><td>593</td>
</tr>

<tr>
<td valign="top">EP </td><td>599</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1089/end.2004.18.593</td>
</tr>

<tr>
<td valign="top">SC </td><td>Urology &amp; Nephrology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000224227400001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Shiraki, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Shiraki, Y</td>
</tr>

<tr>
<td valign="top">TI </td><td>Optimal temporal decomposition for voice morphing preserving Delta
   cepstrum</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEICE TRANSACTIONS ON FUNDAMENTALS OF ELECTRONICS COMMUNICATIONS AND
   COMPUTER SCIENCES</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice morphing; voice conversion; interpolation of spectra; temporal
   decomposition; spectrum distortion; DP matching; Delta cepstrum</td>
</tr>

<tr>
<td valign="top">ID </td><td>CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>We propose Optimal Temporal Decomposition (OTD) of speech for voice morphing preserving Delta cepstrum. OTD is an optimal modification of the original Temporal Decomposition (TD) by B.Atal. It is theoretically shown that OTD can achieve minimal spectral distortion for the TD-based approximation of time-varying LPC parameters. Moreover, by applying OTD to preserving A cepstrum, it is also theoretically shown that Delta cepstrum of a target speaker can be reflected to that of a source speaker. In frequency domain interpolation, the Laplacian Spectral Distortion (LSD) measure is introduced to improve the Inverse Function of Integrated Spectrum (IFIS) based non-uniform frequency warping. Experimental results indicate that Delta cepstrum of the OTD-based morphing spectra of a source speaker is mostly equal to that of a target speaker except for a piecewise constant factor and subjective listening tests show that the speech intelligibility of the proposed morphing method is superior to the conventional method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>NTT Corp, NTT Commun Sci Lab, Atsugi, Kanagawa 2430198, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Shiraki, Y (reprint author), NTT Corp, NTT Commun Sci Lab, Atsugi, Kanagawa 2430198, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>shira@idea.BRL.ntt.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2004</td>
</tr>

<tr>
<td valign="top">VL </td><td>E87A</td>
</tr>

<tr>
<td valign="top">IS </td><td>3</td>
</tr>

<tr>
<td valign="top">BP </td><td>577</td>
</tr>

<tr>
<td valign="top">EP </td><td>583</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000220163700007</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Veuillet, E
   <br>Magnan, A
   <br>Ecalle, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Veuillet, E
   <br>Magnan, A
   <br>Ecalle, J</td>
</tr>

<tr>
<td valign="top">TI </td><td>(Auditory perceptual impairment and reading capacity in dyslexic
   children: effect of therapeutic-remedial rehabilitation</td>
</tr>

<tr>
<td valign="top">SO </td><td>REVUE DE NEUROPSYCHOLOGIE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>auditory temporal processing; phonology; VOT; descending pathways;
   laterality; training; dyslexia; categorical perception</td>
</tr>

<tr>
<td valign="top">ID </td><td>PERIPHERAL AUDITORY ASYMMETRY; DEVELOPMENTAL DYSLEXIA;
   SPEECH-PERCEPTION; PHONOLOGICAL AWARENESS; FREQUENCY DISCRIMINATION;
   LANGUAGE IMPAIRMENT; PROCESSING DEFICITS; LEARNING-PROBLEMS; POOR
   READERS; ADULTS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Learning to read is based on mechanisms the lack of which may lead to dyslexia, a specific disturbance of written language. Dyslexic children have great difficulty with the segmental analysis of words, and hence with acquiring grapheme-phoneme conversion strategies. Among the various hypotheses which have been proposed for dyslexia, there is a basic deficit affecting the temporal processing of information by the brain. At the auditory level, such a temporal integration impairment makes the dyslexic child unable to hear the acoustic distinctions between the brief successive sounds involved in speech. But such a phoneme processing deficit might come from an elementary auditory problem which was not only sensory (low-level) but also based on central auditory processes involving the cortex and thus high-level processing. One may wonder whether difficulty in phoneme contrast perception might not derive from abnormal masking of interference in the coded acoustic information flow in the auditory pathways. Such disturbed acoustic message integration might then be attributable to abnormal functioning on the part of certain inhibitory mechanisms. As it happens, the auditory pathways comprise inhibitory fibres, some of which - such as those of the medial olivocochlear efferent system - are thought to play a role in the central control of cochlear functioning. This presentation seeks, firstly, to compare between normal-reading and dyslexic children the phonological competencies and the sensitivity to voicing contrast. We confirm the low scores of dyslexic children in the phonological tasks and observe that some of them are particularly impaired in voicing contrast perception. Moreover we compare between these two populations the functioning of the descending auditory pathways and observe functioning which are abnormally lateralized in the majority of dyslexic children. Secondly we present preliminary results concerning the follow-up of the performance of four dyslexic children undergoing training based on the principle of implicit phonologic learning. Positive and related modifications are observed between the different parameters.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Hop Edouard Herriot, CNRS, UMR 5020, F-69437 Lyon 03, France.
   <br>Univ Lyon 2, CNRS, UMR 5596, Lab EMC DDL, F-69365 Lyon, France.
   <br>Hop Edouard Herriot, Serv Audiol &amp; Explorat Orofaciales, Ctr Ref
   Troubles Apprentissages Chez Enfant, Lyon, France.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Veuillet, E (reprint author), Hop Edouard Herriot, CNRS, UMR 5020, 3 Pl Arsonval,Pavillon U, F-69437 Lyon 03, France.</td>
</tr>

<tr>
<td valign="top">EM </td><td>evelyne.veuillet@chu-lyon.fr</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>MAGNAN, Annie</display_name>&nbsp;</font></td><td><font size="3">H-1577-2017&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Ecalle, Jean</display_name>&nbsp;</font></td><td><font size="3">H-4166-2017&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>MAGNAN, Annie</display_name>&nbsp;</font></td><td><font size="3">0000-0002-0486-0871&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAR-JUN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2004</td>
</tr>

<tr>
<td valign="top">VL </td><td>14</td>
</tr>

<tr>
<td valign="top">IS </td><td>1-2</td>
</tr>

<tr>
<td valign="top">BP </td><td>103</td>
</tr>

<tr>
<td valign="top">EP </td><td>132</td>
</tr>

<tr>
<td valign="top">SC </td><td>Neurosciences &amp; Neurology; Psychology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000222421800005</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Lei, HY
   <br>Zhao, Y
   <br>Dai, YW
   <br>Wang, ZQ</td>
</tr>

<tr>
<td valign="top">AF </td><td>Lei, HY
   <br>Zhao, Y
   <br>Dai, YW
   <br>Wang, ZQ</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>A secure voice communication system based on DSP</td>
</tr>

<tr>
<td valign="top">SO </td><td>2004 8TH INTERNATIONAL CONFERENCE ON CONTROL, AUTOMATION, ROBOTICS AND
   VISION, VOLS 1-3</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Control Automation Robotics and Vision</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>8th International Conference on Control, Automation, Robotics and Vision
   (ICARCV 2004)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 06-09, 2004</td>
</tr>

<tr>
<td valign="top">CL </td><td>Kunming, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, a secure voice communication system based on TMS320VC5410 is proposed, which includes the hardware design and the software design. Among them, the A/D and D/A conversions of voice signals are implemented by the MC145480, while data transmission is performed by the TL16C554. A chaotic voice encryption algorithm combining cat map and Logistic map is implemented in this system. Experiments of both encrypted and non-encrypted voice communication over short distance have been performed, which demonstrates the feasibility of the proposed system.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Nanjing Univ Sci &amp; Technol, Dept Automat, Nanjing 210094, Peoples R
   China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Lei, HY (reprint author), Nanjing Univ Sci &amp; Technol, Dept Automat, Nanjing 210094, Peoples R China.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2004</td>
</tr>

<tr>
<td valign="top">BP </td><td>132</td>
</tr>

<tr>
<td valign="top">EP </td><td>137</td>
</tr>

<tr>
<td valign="top">SC </td><td>Automation &amp; Control Systems; Computer Science; Robotics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000230484500024</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Mouchtaris, A
   <br>Van der Spiegel, J
   <br>Mueller, P</td>
</tr>

<tr>
<td valign="top">AF </td><td>Mouchtaris, A
   <br>Van der Spiegel, J
   <br>Mueller, P</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Non-parallel training for voice conversion by maximum likelihood
   constrained adaptation</td>
</tr>

<tr>
<td valign="top">SO </td><td>2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING, VOL I, PROCEEDINGS: SPEECH PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 17-21, 2004</td>
</tr>

<tr>
<td valign="top">CL </td><td>Montreal, CANADA</td>
</tr>

<tr>
<td valign="top">ID </td><td>MODELS</td>
</tr>

<tr>
<td valign="top">AB </td><td>The objective of voice conversion methods is to modify the speech characteristics of a particular speaker in such manner, as to sound like speech by a different target speaker. Current voice conversion algorithms are based on deriving a conversion function by estimating its parameters through a corpus that contains the same utterances spoken by both speakers. Such a corpus, usually referred to as a parallel corpus, has the disadvantage that many times it is difficult or even impossible to collect. Here, we propose a voice conversion method that does not require a parallel corpus for training, i.e. the spoken utterances by the two speakers need not be the same, by employing speaker adaptation techniques to adapt to a particular pair of source and target speakers, the derived conversion parameters from a different pair of speakers. We show that adaptation reduces the error obtained when simply applying the conversion parameters of one pair of speakers to another by a factor that can reach 30% in many cases, and with performance comparable with the ideal case when a parallel corpus is available.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Univ Penn, Dept Elect &amp; Syst Engn, Philadelphia, PA 19104 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Mouchtaris, A (reprint author), Univ Penn, Dept Elect &amp; Syst Engn, Philadelphia, PA 19104 USA.</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2004</td>
</tr>

<tr>
<td valign="top">BP </td><td>1</td>
</tr>

<tr>
<td valign="top">EP </td><td>4</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science; Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000222173500001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Eichner, M
   <br>Wolff, M
   <br>Hoffmann, R</td>
</tr>

<tr>
<td valign="top">AF </td><td>Eichner, M
   <br>Wolff, M
   <br>Hoffmann, R</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice characteristics conversion for TTS using reverse VTLN</td>
</tr>

<tr>
<td valign="top">SO </td><td>2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING, VOL I, PROCEEDINGS: SPEECH PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 17-21, 2004</td>
</tr>

<tr>
<td valign="top">CL </td><td>Montreal, CANADA</td>
</tr>

<tr>
<td valign="top">AB </td><td>In the past, several approaches have been proposed for voice conversion in TTS systems. Mostly, conversion is done by modification of the spectral properties and pitch to match a certain target voice. This conversion causes distortions that deteriorate the quality of the synthesized speech. In this paper we investigate a very simple and straightforward method for voice conversion. It generates a new voice from the source speaker instead of generating a certain target speaker's voice. For application in TTS systems it is often sufficient to synthesize new voices that sound sufficiently different to be distinguishable from each other. This is done by applying a spectral warping technique that is commonly used for speaker normalization in speech recognition systems called vocal tract length normalization (VTLN). Due to the low requirements of resources this method is especially suited for embedded systems.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Dresden Univ Technol, D-8027 Dresden, Germany.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Eichner, M (reprint author), Dresden Univ Technol, D-8027 Dresden, Germany.</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2004</td>
</tr>

<tr>
<td valign="top">BP </td><td>17</td>
</tr>

<tr>
<td valign="top">EP </td><td>20</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science; Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000222173500005</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Rentzos, D
   <br>Vaseghi, S
   <br>Yan, Q
   <br>Ho, CH</td>
</tr>

<tr>
<td valign="top">AF </td><td>Rentzos, D
   <br>Vaseghi, S
   <br>Yan, Q
   <br>Ho, CH</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice conversion through transformation of spectral and intonation
   features</td>
</tr>

<tr>
<td valign="top">SO </td><td>2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING, VOL I, PROCEEDINGS: SPEECH PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 17-21, 2004</td>
</tr>

<tr>
<td valign="top">CL </td><td>Montreal, CANADA</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a voice conversion method based on transformation of the characteristic features of a source speaker towards a target. Voice characteristic features are grouped into two main categories: (a) the spectral features at formants and (b) the pitch and intonation patterns. Signal modelling and transformation methods for each group of voice features are outlined. The spectral features at formants are modelled using a set of two-dimensional phoneme-dependent HMMs. Subband frequency warping is used for spectrum transformation with the subbands centred on the estimates of the formant trajectories. The F0 contour is used for modelling the pitch and intonation patterns of speech. A PSOLA based method is employed for transformation of pitch, intonation patterns and speaking rate. The experiments present illustrations and perceptual evaluations of the results of transformations of the various voice features.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Brunel Univ, Dept Elect &amp; Comp Engn, Uxbridge UB8 3PH, Middx, England.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Rentzos, D (reprint author), Brunel Univ, Dept Elect &amp; Comp Engn, Uxbridge UB8 3PH, Middx, England.</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2004</td>
</tr>

<tr>
<td valign="top">BP </td><td>21</td>
</tr>

<tr>
<td valign="top">EP </td><td>24</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science; Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000222173500006</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Zuo, GY
   <br>Liu, WJ
   <br>Ruan, XG</td>
</tr>

<tr>
<td valign="top">AF </td><td>Zuo, GY
   <br>Liu, WJ
   <br>Ruan, XG</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Improving the performance of MGM-based voice conversion by preparing
   training data method</td>
</tr>

<tr>
<td valign="top">SO </td><td>2004 INTERNATIONAL SYMPOSIUM ON CHINESE SPOKEN LANGUAGE PROCESSING,
   PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>4th International Symposium on Chinese Spoken Language Processing
   (ISCSLP 2004)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 15-18, 2004</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hong Kong, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper proposes an approach to improve both the target speaker's individuality and the quality of the converted speech by preparing the training data. In mixture Gaussian spectral mapping (MGM) based voice conversion, spectral features representations are analyzed to obtain the right feature associations between the source and target characteristics. A voiced and unvoiced (V/UV) decision scheme for time-alignment is provided to obtain the right data for training mixture Gaussian spectral mapping function while removing the misaligned data. Experiments are conducted in terms of the applications of spectral representation methods and V/UV decisions strategies to the MGM functions. When linear predictive cepstral coefficients (LPCC) are used for time-alignment and the V/UV decisions are adopted for removing bad data, results show that the conversion function can get a better accuracy and the proposed method can effectively improve the overall performance of voice conversion.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing,
   Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Zuo, GY (reprint author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>gyzuo@nlpr.ia.ac.cn; lwj@nlpr.ia.ac.cn; adrxg@bjut.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2004</td>
</tr>

<tr>
<td valign="top">BP </td><td>181</td>
</tr>

<tr>
<td valign="top">EP </td><td>184</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000228097200046</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sato, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sato, Y</td>
</tr>

<tr>
<td valign="top">BE </td><td>Deb, K
   <br>Poli, R
   <br>Banzhaf, W
   <br>Beyer, HG
   <br>Burke, E
   <br>Darwen, P
   <br>Dasgupta, D
   <br>Floreano, D
   <br>Foster, O
   <br>Harman, M
   <br>Holland, O
   <br>Lanzi, PL
   <br>Spector, L
   <br>Tettamanzi, A
   <br>Thierens, D
   <br>Tyrrell, A</td>
</tr>

<tr>
<td valign="top">TI </td><td>Achieving shorter search times in voice conversion using interactive
   evolution</td>
</tr>

<tr>
<td valign="top">SO </td><td>GENETIC AND EVOLUTIONARY COMPUTATION GECCO 2004 , PT 2, PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">SE </td><td>Lecture Notes in Computer Science</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article; Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>6th Annual Genetic and Evolutionary Computation Conference (GECCO 2004)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUN 26-30, 2004</td>
</tr>

<tr>
<td valign="top">CL </td><td>Seattle, WA</td>
</tr>

<tr>
<td valign="top">AB </td><td>We have already proposed using evolutionary computation to adjust the voice quality conversion parameters, and we have reported that this approach produces results that are not only closer to the desired target than the results of parameter adjustment based on designer experience or trial and error, but which also have relatively little sound quality degradation. In this paper we propose improved techniques for the generation of initial entities and genetic manipulation in order to reducing the workload associated with human evaluation in interactive evolution. We perform voice quality conversion experiments both on natural speech recorded with a microphone and on synthetic speech generated from text data. As a result, we confirm that the proposed improvements make it possible to perform voice quality conversion more efficiently than when using the technique proposed earlier.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Hosei Univ, Fac Comp &amp; Informat Sci, Koganei, Tokyo 1848584, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sato, Y (reprint author), Hosei Univ, Fac Comp &amp; Informat Sci, 3-7-2 Kajino Cho, Koganei, Tokyo 1848584, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>yuji@k.hosei.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2004</td>
</tr>

<tr>
<td valign="top">VL </td><td>3103</td>
</tr>

<tr>
<td valign="top">BP </td><td>1328</td>
</tr>

<tr>
<td valign="top">EP </td><td>1329</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000225040600150</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Otake, Y
   <br>Tajima, Y
   <br>Terada, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Otake, Y
   <br>Tajima, Y
   <br>Terada, M</td>
</tr>

<tr>
<td valign="top">BE </td><td>Kahng, HK
   <br>Goto, S</td>
</tr>

<tr>
<td valign="top">TI </td><td>A SIP-based voice-mail system with voice recognition</td>
</tr>

<tr>
<td valign="top">SO </td><td>INFORMATION NETWORKING: NETWORKING TECHNOLOGIES FOR BROADBAND AND MOBILE
   NETWORKS</td>
</tr>

<tr>
<td valign="top">SE </td><td>Lecture Notes in Computer Science</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article; Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Information Networking (ICOIN 2004)</td>
</tr>

<tr>
<td valign="top">CY </td><td>FEB 18-20, 2004</td>
</tr>

<tr>
<td valign="top">CL </td><td>Busan, SOUTH KOREA</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper we propose a new voice mail service incorporating voice recognition, a function not realized in existing unified messaging systems. The proposed service, which we have also implemented and evaluated, is characterized by a voice-to-text conversion function with delivery of text and associated voice message by email. A Web-based GUI (Graphical User Interface) provides easy user access to voice messages. Telephone call, to the service are VoIP (Voice over IP) using SIP (Session Initiation Protocol) call signaling.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Tokyo Univ Agr &amp; Technol, Grad Sch Technol, Koganei, Tokyo 1848588,
   Japan.
   <br>Tokyo Univ Agr &amp; Technol, Dept Comp Informat &amp; Commun sci, Koganei,
   Tokyo 1848588, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Otake, Y (reprint author), Tokyo Univ Agr &amp; Technol, Grad Sch Technol, 2-24-16 Naka Machi, Koganei, Tokyo 1848588, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>yasutaka@cc.tuat.ac.jp; ytajima@cc.tuat.ac.jp; m-tera@cc.tuat.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2004</td>
</tr>

<tr>
<td valign="top">VL </td><td>3090</td>
</tr>

<tr>
<td valign="top">BP </td><td>985</td>
</tr>

<tr>
<td valign="top">EP </td><td>994</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000223980100099</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Bryant, M
   <br>Cox, JW</td>
</tr>

<tr>
<td valign="top">AF </td><td>Bryant, M
   <br>Cox, JW</td>
</tr>

<tr>
<td valign="top">TI </td><td>Conversion stories as shifting narratives of organizational change</td>
</tr>

<tr>
<td valign="top">SO </td><td>JOURNAL OF ORGANIZATIONAL CHANGE MANAGEMENT</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article; Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>63rd Annual Meeting of the Academy-of-Management</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 01-06, 2003</td>
</tr>

<tr>
<td valign="top">CL </td><td>SEATTLE, WA</td>
</tr>

<tr>
<td valign="top">DE </td><td>narratives; organizational change; employees</td>
</tr>

<tr>
<td valign="top">ID </td><td>NEGLECT; LOYALTY; WORLD; VOICE; EXIT</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper is concerned with how employees talk about their experiences of organizational change and focuses specifically, on the construction of conversion stories. These are particularly positive narratives that consider change as a turning point in which individuals depart from an old way of life pre-change to embrace a post-change organization. In this study, employees seek conversion into management groups and report the values and philosophies of management in their narratives, thus highlighting the benefits of change while suppressing any negative aspects. This paper draws attention to the dramatic nature of the conversion story and explores the sharp distinction between the reporting of experiences prior to and after change. We also investigate the relationship between constructing conversion stories and gaining personal and career advancement at work and suggest that beneath the positive exterior of the conversion narratives lies a theme of silence, which may be related to career advancement Our findings suggest that such stories of silence complicate the conversion story as an example of positive organizational change and discuss implications for both the theory and practice of narrative change research.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Monash Univ, Dept Management, Churchill, Vic, Australia.
   <br>RMIT Univ, RMIT Business, Sch Management, Melbourne, Vic, Australia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Bryant, M (reprint author), Monash Univ, Dept Management, Churchill, Vic, Australia.</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Wolfram Cox, Julie</display_name>&nbsp;</font></td><td><font size="3">0000-0002-5738-9745&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Bryant, Melanie</display_name>&nbsp;</font></td><td><font size="3">0000-0002-0897-8092&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>17</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>17</td>
</tr>

<tr>
<td valign="top">PY </td><td>2004</td>
</tr>

<tr>
<td valign="top">VL </td><td>17</td>
</tr>

<tr>
<td valign="top">IS </td><td>6</td>
</tr>

<tr>
<td valign="top">BP </td><td>578</td>
</tr>

<tr>
<td valign="top">EP </td><td>592</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1108/09534810410564569</td>
</tr>

<tr>
<td valign="top">SC </td><td>Business &amp; Economics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000225279700003</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Leonard, WA
   <br>Pitts, CJ
   <br>Chimento, P</td>
</tr>

<tr>
<td valign="top">AF </td><td>Leonard, WA
   <br>Pitts, CJ
   <br>Chimento, P</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Designing a net-centric DoD Teleport</td>
</tr>

<tr>
<td valign="top">SO </td><td>MILCOM 2004 - 2004 IEEE MILITARY COMMUNICATIONS CONFERENCE, VOLS 1- 3</td>
</tr>

<tr>
<td valign="top">SE </td><td>IEEE Military Communications Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE Military Communications Conference (MILCOM 2004)</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 31-NOV 03, 2004</td>
</tr>

<tr>
<td valign="top">CL </td><td>Monterey, CA</td>
</tr>

<tr>
<td valign="top">AB </td><td>The Department of Defense (DoD) Teleport Program Office (TPO) under the Defense Information Systems Agency is in the process of defining a transformational net-centric architecture for the future. The DoD Teleport system provides Definse Information Service Network (DISN) and Global Information Grid Bandwidth Expansion (GIG-BE) transport access to deployed Warfighters via military and commercial satellite earth stations from around the world. The TPO has begun to design a net-centric Teleport architecture based on providing all the voice, video, and data requirements of deployed user via a converged Internet Protocol (IP)-based transport system, consisting of:
   <br>IP-based networking modems,
   <br>A black convergence router suite,
   <br>Voice over IP (VoIP) gateway for unclassified voice, and
   <br>Serial-to-IP conversion for circuit-based network services.
   <br>This architecture will exist in parallel with the current implementation for some time to enable a gradual transition by the deployed warfighter.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Booz Allen &amp; Hamilton Inc, Mclean, VA 22102 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Leonard, WA (reprint author), Booz Allen &amp; Hamilton Inc, Mclean, VA 22102 USA.</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2004</td>
</tr>

<tr>
<td valign="top">BP </td><td>1689</td>
</tr>

<tr>
<td valign="top">EP </td><td>1693</td>
</tr>

<tr>
<td valign="top">SC </td><td>Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000230724200260</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sheng, L
   <br>Yin, JX
   <br>Huang, JC</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sheng, L
   <br>Yin, JX
   <br>Huang, JC</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice conversion algorithm using phoneme Gaussian mixture model</td>
</tr>

<tr>
<td valign="top">SO </td><td>PROCEEDINGS OF THE 2004 INTERNATIONAL SYMPOSIUM ON INTELLIGENT
   MULTIMEDIA, VIDEO AND SPEECH PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Symposium on Intelligent Multimedia, Video and Speech
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 20-22, 2004</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hong Kong Polytech Univ, Hong Kong, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">HO </td><td>Hong Kong Polytech Univ</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a new, voice conversion algorithm which modifies the utterance of a source speaker to sound like speech from a target speaker. Our method uses speech models based on phoneme units of speech, which finds accurate alignments between source and tat-get speaker utterances. Using the alignments, vocal tract and glottal excitation characteristics are mapped across speakers. Objective and subjective tests suggest that convincing voice conversion is achieved while maintaining high speech quality, which is comparable to other frame-based approaches.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>S China Univ Technol, Sch Elect &amp; Informat, Guangzhou, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sheng, L (reprint author), S China Univ Technol, Sch Elect &amp; Informat, Guangzhou, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>Lvsheng@scut.edu.cn; eejyin@scut.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2004</td>
</tr>

<tr>
<td valign="top">BP </td><td>5</td>
</tr>

<tr>
<td valign="top">EP </td><td>8</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/ISIMP.2004.1433986</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Imaging Science &amp; Photographic Technology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000227714000002</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Salor, O
   <br>Demirekler, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Salor, O
   <br>Demirekler, M</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Spectral modification for context-free voice conversion using MELP
   speech coding framework</td>
</tr>

<tr>
<td valign="top">SO </td><td>PROCEEDINGS OF THE 2004 INTERNATIONAL SYMPOSIUM ON INTELLIGENT
   MULTIMEDIA, VIDEO AND SPEECH PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Symposium on Intelligent Multimedia, Video and Speech
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 20-22, 2004</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hong Kong Polytech Univ, Hong Kong, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">HO </td><td>Hong Kong Polytech Univ</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this work, we have focused on spectral modification of speech for voice con version from one speaker to another. Speech conversion aims to modify the speech of one speaker such that the modified speech sounds as if spoken by another speaker. MELP (Mixed Excitation Linear Prediction) speech coding algorithm has been used as speech analysis and synthesis framework. Using a 230-sentence triphone balanced database of the two speakers, a mapping between the 4-stage vector quantization indexes for line spectral frequencies (LSF's) of the two speakers have been obtained. This mapping provides a context-free speech conversion for spectral properties of the speakers. Two different methods have been proposed to obtain the LSF mapping. The first method determines the corresponding source and the target LSF codeword indexes, while the second method finds a new LSF codebook for the target speaker. After the spectral modification, pitch modification is applied to the source speaker's residual to approximate the target speaker's pitch range and then the modified filter is driven by the modified residual signal. Subjective ABX listening tests have been carried out and the correct speaker perception rate has been obtained as 80% and 77% for the first and the second spectral conversion methods respectively. For future work, we are planning to integrate our previous work, on LPC filter and residual relationship analysis to increase the correct speaker perception rate.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Middle E Tech Univ, Dept Elect &amp; Elect Engn, Ankara, Turkey.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Salor, O (reprint author), Middle E Tech Univ, Dept Elect &amp; Elect Engn, Ankara, Turkey.</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2004</td>
</tr>

<tr>
<td valign="top">BP </td><td>314</td>
</tr>

<tr>
<td valign="top">EP </td><td>317</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/ISIMP.2004.1434063</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Imaging Science &amp; Photographic Technology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000227714000079</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sundermann, D
   <br>Bonafonte, A
   <br>Ney, H
   <br>Hoge, H</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sundermann, D
   <br>Bonafonte, A
   <br>Ney, H
   <br>Hoge, H</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Time domain vocal tract length normalization</td>
</tr>

<tr>
<td valign="top">SO </td><td>Proceedings of the Fourth IEEE International Symposium on Signal
   Processing and Information Technology</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>4th IEEE International Symposium on Signal Processing and Information
   Technology</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 08-21, 2004</td>
</tr>

<tr>
<td valign="top">CL </td><td>Rome, ITALY</td>
</tr>

<tr>
<td valign="top">AB </td><td>Recently, the speaker normalization technique VTLN (vocal tract length normalization), known from speech recognition, was applied to voice conversion. So far, VTLN has been performed in frequency domain. However, to accelerate the conversion process, it is helpful to apply VTLN directly to the time frames of a speech signal. In this paper, we propose a technique which directly manipulates the time signal. By means of subjective tests, it is shown that the performance of voice conversion techniques based on frequency domain and time domain VTLN are equivalent in terms of speech quality, while the latter requires about 20 times less processing time.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Univ Politecn Catalunya, Dept Signal Theory &amp; Commun, ES-08034
   Barcelona, Spain.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sundermann, D (reprint author), Univ Politecn Catalunya, Dept Signal Theory &amp; Commun, C Jordi Girona,1 &amp; 3, ES-08034 Barcelona, Spain.</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2004</td>
</tr>

<tr>
<td valign="top">BP </td><td>191</td>
</tr>

<tr>
<td valign="top">EP </td><td>194</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/ISSPIT.2004.1433719</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000228482900046</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Turk, O
   <br>Arslan, LM</td>
</tr>

<tr>
<td valign="top">AF </td><td>Turk, O
   <br>Arslan, LM</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Robust voice conversion methods</td>
</tr>

<tr>
<td valign="top">SO </td><td>PROCEEDINGS OF THE IEEE 12TH SIGNAL PROCESSING AND COMMUNICATIONS
   APPLICATIONS CONFERENCE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE 12th Signal Processing and Communications Applications Conference</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 28-30, 2004</td>
</tr>

<tr>
<td valign="top">CL </td><td>Kusadasi, TURKEY</td>
</tr>

<tr>
<td valign="top">AB </td><td>Several problems in the training and transformation stages of voice conversion algorithms cause reduction in the output quality. This study focuses on the improvement of output quality in STASC based voice conversion and proposes five new methods. Robust end-point detection, pre-emphasis and spectral equalization are the three new methods that are employed in the training stage. The fourth method employs confidence measures for eliminating source and target HMM states that are significantly different in terms of duration, vocal tract spectrum, pitch, and energy. The last method focuses on the improvement of the pitch detection method. The optimal parameters of an autocorrelation based pitch detector are determined for male and female speakers separately with detailed analysis. The f(0) values obtained from electro-glottograph signals are used as the reference. The algorithm that employs the proposed methods is compared with STASC in subjective listening tests. The similarity to the target voice is increased by 23.0% and the subjective quality by 28.8% with the new methods.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Bogazici Univ, Elekt Elekt Muh Bolumu, Istanbul, Turkey.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Turk, O (reprint author), Bogazici Univ, Elekt Elekt Muh Bolumu, Istanbul, Turkey.</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Arslan, Levent</display_name>&nbsp;</font></td><td><font size="3">D-6377-2015&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Arslan, Levent</display_name>&nbsp;</font></td><td><font size="3">0000-0002-6086-8018&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2004</td>
</tr>

<tr>
<td valign="top">BP </td><td>264</td>
</tr>

<tr>
<td valign="top">EP </td><td>267</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/SIU.2004.1338310</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000225861200068</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Eb Taher, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Eb Taher, A</td>
</tr>

<tr>
<td valign="top">BE </td><td>Sojka, P
   <br>Kopecek, I
   <br>Pala, K</td>
</tr>

<tr>
<td valign="top">TI </td><td>New refinement schemes for voice conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>TEXT, SPEECH AND DIALOGUE, PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">SE </td><td>Lecture Notes in Computer Science</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article; Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>7th International Conference on Texts, Speech and Dialogue</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 08-11, 2004</td>
</tr>

<tr>
<td valign="top">CL </td><td>Brno, CZECH REPUBLIC</td>
</tr>

<tr>
<td valign="top">AB </td><td>New refinement schemes for voice conversion are proposed in this paper. I take mel-frequency cepstral coefficients (MFCC) as the basic feature and adopt cepstral mean subtraction to compensate the channel effects. I propose S/U/V (Silence/Unvoiced/Voiced) decision rule such that two sets of codebooks are used to capture the difference between unvoiced and voiced segments of the source speaker. Moreover, I apply three schemes to refine the synthesized voice, including pitch refinement with PSOLA, energy equalization, and frame concatenation based on synchronized pitch marks. The satisfactory performance of the voice conversion system can be demonstrated through ABX listening test and MOS grade.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Brno Univ Technol, Fac Elect Engn &amp; Commun, Dept Telecommun, Brno, Czech
   Republic.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Eb Taher, A (reprint author), Brno Univ Technol, Fac Elect Engn &amp; Commun, Dept Telecommun, Purkynova 118, Brno, Czech Republic.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jawad2004@yahoo.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2004</td>
</tr>

<tr>
<td valign="top">VL </td><td>3206</td>
</tr>

<tr>
<td valign="top">BP </td><td>481</td>
</tr>

<tr>
<td valign="top">EP </td><td>488</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000224026300061</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Bryant, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Bryant, M</td>
</tr>

<tr>
<td valign="top">TI </td><td>Persistence and silence: A narrative analysis of employee responses to
   organisational change</td>
</tr>

<tr>
<td valign="top">SO </td><td>SOCIOLOGICAL RESEARCH ONLINE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>atrocity tales; conversion stories; narratives; organisational change;
   silence; voice</td>
</tr>

<tr>
<td valign="top">ID </td><td>ATROCITY TALES; VOICE; EXIT; LOYALTY; STRESS; NEGLECT; CONVERSION;
   EMOTIONS; STORIES; IMPACT</td>
</tr>

<tr>
<td valign="top">AB </td><td>This article is concerned with how employees talk about organisational change and focuses specifically on how employees discuss reactions and responses to change through the construction of narratives. Employees included in this study suggest that the use of voice as an attempt to inform managers of their discontent, or remaining silent and passive are the most common responses to organisational change. Within sociology and management literature, voice has been considered as a constructive response to change, providing invaluable feedback to managers about declining conditions or performance lapses. Alternatively, remaining silent or passive has been documented as a weak strategy in which the individual renounces control and forms a dependency relationship with powerful groups such as managers. The primary aim of this paper is to challenge the argument that voice is a constructive response to change and suggest that voice is likely to be perceived as destructive, thus leading to the removal of responsibilities and career opportunities. Furthermore, this paper argues that silence is the more constructive response to change, which is documented in this research as leading to the advancement of careers. Relationships between the way employees respond to organisational change and the type of narrative that they construct is also discussed. Those who report remaining silent construct 'conversion stories' suggesting that organisational change provided a turning point in which employees could embrace management practices and gain career advancement. Alternatively, those who reported using voice construct 'atrocity tales' in which change is associated with stories of workplace bullying, removal of career opportunities and workplace violence. These narratives suggest that the use of voice as a response to change is more complex than its original intent and explanation in the literature, providing challenges for researchers in understanding where voice as a constructive response ends and where resistance to change begins.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Monash Univ, Dept Management, Clayton, Vic 3168, Australia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Bryant, M (reprint author), Monash Univ, Dept Management, Clayton, Vic 3168, Australia.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>NOV 28</td>
</tr>

<tr>
<td valign="top">PY </td><td>2003</td>
</tr>

<tr>
<td valign="top">VL </td><td>8</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">SC </td><td>Sociology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000187722400006</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Unhino, E
   <br>Yano, K
   <br>Azetsu, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Unhino, E
   <br>Yano, K
   <br>Azetsu, T</td>
</tr>

<tr>
<td valign="top">TI </td><td>Twin unit self-organising map for voice conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>ELECTRONICS LETTERS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">AB </td><td>A self-organising map with twin unit (TW-SOM) is presented. TW-SOM enables voice conversion by fewer codebooks than the conventional LBG method, further it provides better voice conversion accuracy. The proposed method is-successfully applied to the actual voice conversion from bone conduction voice to air conduction voice.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Yamaguchi Univ, Dept Phys Biol &amp; Informat, Yamaguchi 7538512, Japan.
   <br>Yamaguchi Univ, Div Nat Sci &amp; Symbiosis, Yamaguchi 7538512, Japan.
   <br>Yamaguchi Prefectural Univ, Dept Environm Sci, Yamaguchi 7530021, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Unhino, E (reprint author), Yamaguchi Univ, Dept Phys Biol &amp; Informat, Yamaguchi 7538512, Japan.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>NOV 27</td>
</tr>

<tr>
<td valign="top">PY </td><td>2003</td>
</tr>

<tr>
<td valign="top">VL </td><td>39</td>
</tr>

<tr>
<td valign="top">IS </td><td>24</td>
</tr>

<tr>
<td valign="top">BP </td><td>1767</td>
</tr>

<tr>
<td valign="top">EP </td><td>1769</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1049/el:20031089</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000187081900049</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Chantawibul, S
   <br>Lokechareonlarp, S
   <br>Pokawatana, C</td>
</tr>

<tr>
<td valign="top">AF </td><td>Chantawibul, S
   <br>Lokechareonlarp, S
   <br>Pokawatana, C</td>
</tr>

<tr>
<td valign="top">TI </td><td>Total video endoscopic thyroidectomy by an axillary approach</td>
</tr>

<tr>
<td valign="top">SO </td><td>JOURNAL OF LAPAROENDOSCOPIC &amp; ADVANCED SURGICAL TECHNIQUES-PART A</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">ID </td><td>CARBON-DIOXIDE INSUFFLATION; ASSISTED NECK-SURGERY; PRIMARY
   HYPERPARATHYROIDISM; LIFTING METHOD; PARATHYROIDECTOMY; HYPERCARBIA;
   RESECTION; TUMORS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Background: A permanent transverse surgical scar is an unavoidable problem after conventional thyroidectomy. Endoscopic thyroidectomy performed via an axillary approach leaves no scarring at the neck and anterior chest wall and so provides an excellent cosmetic result. The axillary scars are usually not seen when the arm is in a normal position.
   <br>Materials and Methods: From April 2001 to February 2003, we used a four-port technique to perform 45 lobectomy and isthmectomy procedures. One 12-mm port for the flexible laparoscope (EL2-TF410, Fuji Photo Optical, Tokyo, Japan) and three additional 5-mm ports for instruments and suction were inserted through the axilla on the side of the nodule. The CO2 insufflation pressure was set at 4 mm Hg, and in most cases, a 5-mm Johnson &amp; Johnson Harmonic Scalpel (Ethicon Endo-Surgery, Cincinnati, Ohio, U.S.A.) was used for the dissection.
   <br>Results: Of 45 procedures, 44 were performed successfully. In one case, conversion to a conventional technique was required. The mean operating time was 131.2 minutes, and the mean blood loss was 51.6 mL. The recurrent laryngeal nerves were clearly identified in every case, and no case of permanent voice change occurred after surgery. In one patient, a 20-mL seroma developed on the 10th postoperative day, which was treated by simple aspiration. One patient experienced a transient voice change. The patients were discharged on average at 2.9 days after the operation.
   <br>Conclusions: Endoscopic thyroidectomy by an axillary approach to manage benign thyroid disease is feasible and safe and provides promising cosmetic results. We think that this approach may play an important role in the near future.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Rajavithi Hosp, Dept Surg, Bangkok 10400, Thailand.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Chantawibul, S (reprint author), Rajavithi Hosp, Dept Surg, Bangkok 10400, Thailand.</td>
</tr>

<tr>
<td valign="top">TC </td><td>25</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>29</td>
</tr>

<tr>
<td valign="top">PD </td><td>OCT</td>
</tr>

<tr>
<td valign="top">PY </td><td>2003</td>
</tr>

<tr>
<td valign="top">VL </td><td>13</td>
</tr>

<tr>
<td valign="top">IS </td><td>5</td>
</tr>

<tr>
<td valign="top">BP </td><td>295</td>
</tr>

<tr>
<td valign="top">EP </td><td>299</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1089/109264203769681655</td>
</tr>

<tr>
<td valign="top">SC </td><td>Surgery</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000185762400002</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Vlantis, AC
   <br>Gregor, RT
   <br>Elliot, H
   <br>Oudes, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Vlantis, AC
   <br>Gregor, RT
   <br>Elliot, H
   <br>Oudes, M</td>
</tr>

<tr>
<td valign="top">TI </td><td>Conversion from a non-indwelling to a Provox (R) 2 indwelling voice
   prosthesis for speech rehabilitation: comparison of voice quality and
   patient preference</td>
</tr>

<tr>
<td valign="top">SO </td><td>JOURNAL OF LARYNGOLOGY AND OTOLOGY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>laryngectomy; speech; alaryngeal; larynx; artificial</td>
</tr>

<tr>
<td valign="top">ID </td><td>TOTAL LARYNGECTOMY; BLOM-SINGER; RESTORATION; EXPERIENCE; BUTTON</td>
</tr>

<tr>
<td valign="top">AB </td><td>This prospective study assessed the advantages and problems associated with converting a patient using an older generation non-indwelling voice prosthesis to a newer generation indwelling voice prosthesis, in this case the Provox((R))2. The voice characteristics of each patient were measured using the old and then the new voice prosthesis. Technical aspects of the insertion of the indwelling prosthesis were noted. Each patient completed a questionnaire after a period of use with the indwelling prosthesis.
   <br>Changing the prosthesis was simple and uncomplicated in 15 of 17 patients. Acoustic analysis showed improved parameters with the indwelling prosthesis, but no perceptual difference between the two prostheses. The questionnaire revealed that most patients preferred the indwelling prosthesis.
   <br>Replacing a non-indwelling with an indwelling prosthesis is technically simple, leading to improvement in voice quality and patient satisfaction. It may be reasonable to offer this choice to patients currently using an older generation non-indwelling voice prosthesis.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Univ Stellenbosch, Dept Otolaryngol Head &amp; Neck Surg, ZA-7600
   Stellenbosch, South Africa.
   <br>Univ Stellenbosch, Dept Speech Pathol, ZA-7600 Stellenbosch, South
   Africa.
   <br>Tygerberg Acad Hosp, Tygerberg, South Africa.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Vlantis, AC (reprint author), Chinese Univ Hong Kong, Dept Surg, Prince Wales Hosp, Shatin, Hong Kong, Peoples R China.</td>
</tr>

<tr>
<td valign="top">TC </td><td>6</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>6</td>
</tr>

<tr>
<td valign="top">PD </td><td>OCT</td>
</tr>

<tr>
<td valign="top">PY </td><td>2003</td>
</tr>

<tr>
<td valign="top">VL </td><td>117</td>
</tr>

<tr>
<td valign="top">IS </td><td>10</td>
</tr>

<tr>
<td valign="top">BP </td><td>815</td>
</tr>

<tr>
<td valign="top">EP </td><td>820</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1258/002221503770716278</td>
</tr>

<tr>
<td valign="top">SC </td><td>Otorhinolaryngology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000186276200015</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Baker, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Baker, J</td>
</tr>

<tr>
<td valign="top">TI </td><td>Psychogenic voice disorders and traumatic stress experience: A
   discussion paper with two case reports</td>
</tr>

<tr>
<td valign="top">SO </td><td>JOURNAL OF VOICE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article; Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Speech Pathology Australia Conference</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY, 2001</td>
</tr>

<tr>
<td valign="top">CL </td><td>MELBOURNE, AUSTRALIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>psychogenic aphonia/dysphonia; functional aphonia/dysphonia; conversion
   reaction; hysterical aphonia/dysphonia; traumatic stress</td>
</tr>

<tr>
<td valign="top">ID </td><td>FUNCTIONAL DYSPHONIA; APHONIA; THERAPY; SYMPTOM</td>
</tr>

<tr>
<td valign="top">AB </td><td>Psychogenic dysphonia refers to loss of voice where there is insufficient structural or neurological pathology to account for the nature and severity of the dysphonia, and where loss of volitional control over phonation seems to be related to psychological processes such as anxiety, depression, conversion reaction, or personality disorder. Such dysphonias may often develop post-viral infection with laryngitis, and generally in close proximity to emotionally or psychologically taxing experiences, where "conflict over speaking out" is an issue.
   <br>In more rare instances, severe and persistent psychogenic dysphonia may develop under innocuous or unrelated circumstances, but over time, it may be traced back to traumatic stress experiences that occurred many months or years prior to the onset of the voice disorder. In such cases, the qualitative nature of the traumatic experience may be reflected in the way the psychogenic voice disorder presents.
   <br>The possible relationship between psychogenic dysphonia and earlier traumatic stress experience is discussed, and the reportedly low prevalence of conversion reaction (4% to 5%) as the basis for psychogenic dysphonia is challenged. Two cases are presented to illustrate the issues raised: the first, a young woman who was sexually assaulted and chose to "keep her secret," and the second, a 52-year-old woman who developed a psychogenic dysphonia following a second, modified thyroplasty for a unilateral vocal fold paresis.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Flinders Univ S Australia, Dept Speech Pathol, Sch Med, Adelaide, SA
   5001, Australia.
   <br>Flinders Univ S Australia, Dept Psychiat, Sch Med, Adelaide, SA 5001,
   Australia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Baker, J (reprint author), Flinders Univ S Australia, Dept Speech Pathol, Sch Med, GPO Box 2100, Adelaide, SA 5001, Australia.</td>
</tr>

<tr>
<td valign="top">TC </td><td>21</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>26</td>
</tr>

<tr>
<td valign="top">PD </td><td>SEP</td>
</tr>

<tr>
<td valign="top">PY </td><td>2003</td>
</tr>

<tr>
<td valign="top">VL </td><td>17</td>
</tr>

<tr>
<td valign="top">IS </td><td>3</td>
</tr>

<tr>
<td valign="top">BP </td><td>308</td>
</tr>

<tr>
<td valign="top">EP </td><td>318</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1067/S0892-1997(03)00015-8</td>
</tr>

<tr>
<td valign="top">SC </td><td>Audiology &amp; Speech-Language Pathology; Otorhinolaryngology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000185304500004</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Viscovich, N
   <br>Borod, J
   <br>Pihan, H
   <br>Peery, S
   <br>Brickman, AM
   <br>Tabert, M
   <br>Schmidt, M
   <br>Spielman, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Viscovich, N
   <br>Borod, J
   <br>Pihan, H
   <br>Peery, S
   <br>Brickman, AM
   <br>Tabert, M
   <br>Schmidt, M
   <br>Spielman, J</td>
</tr>

<tr>
<td valign="top">TI </td><td>Acoustical analysis of posed prosodic expressions: Effects of emotion
   and sex</td>
</tr>

<tr>
<td valign="top">SO </td><td>PERCEPTUAL AND MOTOR SKILLS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article; Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>30th Annual Meeting of the International-Neuropsychological-Society</td>
</tr>

<tr>
<td valign="top">CY </td><td>FEB   16, 2002</td>
</tr>

<tr>
<td valign="top">CL </td><td>TORONTO, CANADA</td>
</tr>

<tr>
<td valign="top">ID </td><td>RIGHT-HEMISPHERE; FUNDAMENTAL-FREQUENCY; VOCAL EMOTIONS; SPEECH; VOICE;
   LATERALIZATION; PERCEPTION; INTONATION; CHANNELS; LANGUAGE</td>
</tr>

<tr>
<td valign="top">AB </td><td>Prosodic expression is an important channel of emotional communication and can be assessed through computerized acoustical analysis. Fundamental frequency (F-0) is the most commonly studied acoustic measure and considered the best index of perceived pitch. In this study, we examined two factors that can influence prosodic expression, sex and emotion type. A special feature is the use of a neutral expression as a control measure. We also described procedures for eliciting posed prosodic expression via an experimental task from the New York Emotion Battery. Subjects were healthy men (n = 10) and women (n = 9), matched for age (M = 29.2 yr.) and education (M = 15.6 yr.). Subjects were asked to intone neutral-content sentences with happy, sad, and neutral prosody. F-0 mean and standard deviation were measured using the Computerized Speech Lab program. Initial findings indicated that women produced significantly higher F-0 values than did men and that happy sentences were produced with significantly higher F-0 values than were sad sentences. When semitone conversions were applied and neutral prosody was subtracted out, differences remained for emotion type but not for sex. Findings are discussed in terms of implications for the assessment and treatment of prosody in clinical populations.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>CUNY Queens Coll, Dept Psychol, Flushing, NY 11367 USA.
   <br>CUNY, Grad Ctr, New York, NY USA.
   <br>CUNY Mt Sinai Sch Med, Dept Neurol, Grad Ctr, New York, NY 10029 USA.
   <br>Univ Bern, Inselspital, Dept Neurol, Bern, Switzerland.
   <br>CUNY Queens Coll, Dept Psychol, New York, NY USA.
   <br>CUNY, Grad Ctr, New York, NY USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Borod, J (reprint author), CUNY Queens Coll, Dept Psychol, Flushing, NY 11367 USA.</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2003</td>
</tr>

<tr>
<td valign="top">VL </td><td>96</td>
</tr>

<tr>
<td valign="top">IS </td><td>3</td>
</tr>

<tr>
<td valign="top">BP </td><td>759</td>
</tr>

<tr>
<td valign="top">EP </td><td>771</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.2466/PMS.96.3.759-771</td>
</tr>

<tr>
<td valign="top">PN </td><td>1</td>
</tr>

<tr>
<td valign="top">SC </td><td>Psychology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000183372000007</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Rassweiler, J
   <br>Seemann, O
   <br>Hatzinger, M
   <br>Schulze, M
   <br>Frede, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Rassweiler, J
   <br>Seemann, O
   <br>Hatzinger, M
   <br>Schulze, M
   <br>Frede, T</td>
</tr>

<tr>
<td valign="top">TI </td><td>Technical evolution of laparoscopic radical prostatectorny after 450
   cases</td>
</tr>

<tr>
<td valign="top">SO </td><td>JOURNAL OF ENDOUROLOGY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article; Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>19th World Congress on Endourology and Shockwave Lithotripsy/17th Basic
   Research Symposium</td>
</tr>

<tr>
<td valign="top">CY </td><td>NOV, 2001</td>
</tr>

<tr>
<td valign="top">CL </td><td>BANGKOK, THAILAND</td>
</tr>

<tr>
<td valign="top">ID </td><td>EARLY CATHETER REMOVAL; RETROPUBIC PROSTATECTOMY; CONSECUTIVE PATIENTS;
   RISK-FACTORS; EXPERIENCE; MORBIDITY; COMPLICATIONS; MORTALITY</td>
</tr>

<tr>
<td valign="top">AB </td><td>Background and Purpose: In 1998, laparoscopic radical prostatectomy with primary access to the seminal vesicles was introduced. In 1999, we developed a laparoscopic technique similar to the classic retropubic radical prostatectomy. We focus here on the continuous technical evolution of our technique.
   <br>Patients and Methods: From March 1999 to May 2002, we performed 450 laparoscopic radical prostatectomies. All important data of the patients; data concerning the performance of the procedure, including technical modifications, conversion, reintervention, and complication rate; as well as follow-up information were documented contemporaneously. The patients were divided into three groups of 150 individuals each in order to analyze the influence of the technical evolution of the procedure. Additionally, we studied the transferability of our technique, comparing the learning curves of the three surgeons involved in the program.
   <br>Results: The technical modifications included the routine use of a voice-controlled robot (AESOP) for the camera, exposure of the apex with 120degrees retracting forceps, a free-hand suturing technique instead of the Endo-stitch device for the dorsal vein complex, 5-mm clipping instead of bipolar coagulation for the nerve-sparing technique, initial 6 o' clock suturing of the urethra before complete division, control of the prostatic pedicles by use of 12-mm Hemo-lok clips instead of the Ultracision or Endo-GIA, the bladder neck-sparing technique in cases of T-1c and T-2a tumors, and interrupted instead of continuous sutures for the vesicourethral anastomosis. All these modifications resulted in a significant decrease in operating time and the rates of transfusion, open conversion, and reintervention. The introduction of the nerve-sparing technique increased the number of tumor-positive margins. The mean operating time of the third surgeon was significantly less than that of the first surgeon, but the transfusion, conversion, and reintervention rates did not differ significantly among the surgeons.
   <br>Conclusions: Laparoscopic radical prostatectomy has undergone continuous technical evolution resulting in a significant improvement of the operative results. Although we were able to demonstrate the transferability of this difficult prodedure, we feel that it should be performed only at centers of expertise.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Univ Heidelberg, Klinikum Heilbronn, Dept Urol, Heidelberg, Germany.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Rassweiler, J (reprint author), Urol Clin, Gesundbrunnen 20, D-74074 Heilbronn, Germany.</td>
</tr>

<tr xmlns:date="http://exslt.org/dates-and-times">
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Hatzinger, Martin</display_name>&nbsp;</font></td><td><font size="3">0000-0002-6146-3602&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>39</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>43</td>
</tr>

<tr>
<td valign="top">PD </td><td>APR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2003</td>
</tr>

<tr>
<td valign="top">VL </td><td>17</td>
</tr>

<tr>
<td valign="top">IS </td><td>3</td>
</tr>

<tr>
<td valign="top">BP </td><td>143</td>
</tr>

<tr>
<td valign="top">EP </td><td>154</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1089/089277903321618707</td>
</tr>

<tr>
<td valign="top">SC </td><td>Urology &amp; Nephrology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000182535300003</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Luk, RWP
   <br>Yeung, DS
   <br>Lu, Q
   <br>Leung, HL
   <br>Li, SY
   <br>Leung, F</td>
</tr>

<tr>
<td valign="top">AF </td><td>Luk, RWP
   <br>Yeung, DS
   <br>Lu, Q
   <br>Leung, HL
   <br>Li, SY
   <br>Leung, F</td>
</tr>

<tr>
<td valign="top">TI </td><td>ASAB: a Chinese screen reader</td>
</tr>

<tr>
<td valign="top">SO </td><td>SOFTWARE-PRACTICE &amp; EXPERIENCE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>interface design; screen reader; graphical user interface;
   text-to-speech; visually-impaired; Chinese; ideographic languages</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper describes the design and development of a computer interface for blind and visually-impaired users, who are native speakers of Cantonese (i.e. a Chinese dialect). Apart from enabling the interface to (1) produce Chinese voice output, (2) convert Chinese characters to Braille codes, (3) facilitate Chinese Braille input, and (4) operate in a Microsoft Chinese Windows environment, the significant aspects of this paper include the following: (1) the description of an integrated architecture, which can be used for other languages; (2) a general bilingual Braille input mechanism; (3) a sentence-based input method that can be used for contracted-Braille-to-text conversion with an error rate of about 6%, operating at about 700 characters/second using a Pentium 11300 MHz PC; (4) a code-mixed synthesis module for general bilingual and multilingual applications; (5) the potential to directly adopt the system for use with other ideographic languages (like Japanese and Korean), as well as agglutinating languages like Finnish and Turkish, which have no space between words. Copyright (C) 2003 John Wiley Sons, Ltd.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Hong Kong Polytech Univ, Dept Comp, Kowloon, Hong Kong, Peoples R China.
   <br>Hong Kong Soc BLind, Kowloon, Hong Kong, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Luk, RWP (reprint author), Hong Kong Polytech Univ, Dept Comp, Kowloon, Hong Kong, Peoples R China.</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Luk, Robert</display_name>&nbsp;</font></td><td><font size="3">B-9382-2015&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Luk, Robert</display_name>&nbsp;</font></td><td><font size="3">0000-0002-9310-8867&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Lu, Qin</display_name>&nbsp;</font></td><td><font size="3">0000-0002-9092-2476&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2003</td>
</tr>

<tr>
<td valign="top">VL </td><td>33</td>
</tr>

<tr>
<td valign="top">IS </td><td>3</td>
</tr>

<tr>
<td valign="top">BP </td><td>201</td>
</tr>

<tr>
<td valign="top">EP </td><td>219</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1002/spe.497</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000181542200001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kumar, A
   <br>Verma, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kumar, A
   <br>Verma, A</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE
   IEEE
   IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Using phone and diphone based acoustic models for voice conversion: A
   step towards creating voice fonts</td>
</tr>

<tr>
<td valign="top">SO </td><td>2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING, VOL I, PROCEEDINGS: SPEECH PROCESSING I</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 06-10, 2003</td>
</tr>

<tr>
<td valign="top">CL </td><td>HONG KONG, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion techniques attempt to modify speech signal so that it is perceived as if spoken by another speaker, different from the original speaker. In this paper, we present a novel approach to perform voice conversion. Our approach, uses acoustic models based on units of speech, like phones and diphones, for voice conversion. These models can be computed and used independently for a given speaker without being concerned about the source or target speaker. It avoids the use of a parallel speech corpus in the voices of source and target speakers. It is shown that by using the proposed approach, voice fonts can be created and stored which will represent individual characteristics of a particular speaker, to be used for customization of synthetic speech. We also show through objective and subjective tests, that voice conversion quality is comparable to other approaches that require a parallel speech corpus.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Indian Inst Technol, Ctr Appl Res Elect, New Delhi 110016, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kumar, A (reprint author), Indian Inst Technol, Ctr Appl Res Elect, New Delhi 110016, India.</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Verma, Ashish</display_name>&nbsp;</font></td><td><font size="3">L-3943-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2003</td>
</tr>

<tr>
<td valign="top">BP </td><td>720</td>
</tr>

<tr>
<td valign="top">EP </td><td>723</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000185328700183</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Turajlic, E
   <br>Rentzos, D
   <br>Vaseghi, S
   <br>Ho, CH</td>
</tr>

<tr>
<td valign="top">AF </td><td>Turajlic, E
   <br>Rentzos, D
   <br>Vaseghi, S
   <br>Ho, CH</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE
   IEEE
   IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Evaluation of methods for parameteric formant transformation in voice
   conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING, VOL I, PROCEEDINGS: SPEECH PROCESSING I</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 06-10, 2003</td>
</tr>

<tr>
<td valign="top">CL </td><td>HONG KONG, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper explores methods of estimation and mapping. of parametric formant-based models for voice transformation. The main focus is the transformation of the parameters of a model of the vocal tract of a source speaker to a target speaker. The vocal tract parameters are represented with the linear prediction (LP) model coefficients and the associated formant frequencies, bandwidths, intensities and their temporal trajectories. Two methods are explored for vocal tract (formant) mapping. The first method is based on non-uniform frequency warping and the second is based on pole rotation. Both methods transform all parameters of the formants (frequency, bandwidth and intensity). In addition, the factors that affect the selection of the warping ratios for the mapping functions are presented. Experimental evaluation of voice morphing based on parametric models are presented.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Brunel Univ, Dept Elect &amp; Comp Engn, Uxbridge UB8 3PH, Middx, England.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Turajlic, E (reprint author), Brunel Univ, Dept Elect &amp; Comp Engn, Uxbridge UB8 3PH, Middx, England.</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2003</td>
</tr>

<tr>
<td valign="top">BP </td><td>724</td>
</tr>

<tr>
<td valign="top">EP </td><td>727</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000185328700184</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Loke, A
   <br>Abdelgany, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Loke, A
   <br>Abdelgany, M</td>
</tr>

<tr>
<td valign="top">BE </td><td>Quach, T</td>
</tr>

<tr>
<td valign="top">TI </td><td>Multi mode wireless terminals - Key technical challenges</td>
</tr>

<tr>
<td valign="top">SO </td><td>2003 IEEE RADIO FREQUENCY INTEGRATED CIRCUITS (RFIC) SYMPOSIUM, DIGEST
   OF PAPERS</td>
</tr>

<tr>
<td valign="top">SE </td><td>IEEE RADIO FREQUENCY INTEGRATED CIRCUITS SYMPOSIUM</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE Radio Frequency Integrated Circuits Symposium (RFIC)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUN 08-10, 2003</td>
</tr>

<tr>
<td valign="top">CL </td><td>PHILADELPHIA, PA</td>
</tr>

<tr>
<td valign="top">DE </td><td>code division Multiple access (CDMA) direct Conversion; global system
   for mobile communication (GSM) Wide Band code division multiple access
   (WCDMA); Wireless Local Area network ( WLAN)</td>
</tr>

<tr>
<td valign="top">AB </td><td>Increasing demand for seamless connectivity on mobile platforms is fueling phenomenal technological innovation in the wireless industry. In addition to second and third generation digital cellular standards striving to provide subscribers with higher data rates along with voice several competing standards like Wireless Local Area net works (WLAN) and Bluetooth are gaining consumer acceptance as viable solutions for data connectivity. Industry is rapidly moving towards convergence of radio technology to cater for multiple communication standards on single platform. Key technical challenges in implementation of radio architectures for multi mode radio are presented.</td>
</tr>

<tr>
<td valign="top">TC </td><td>6</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>6</td>
</tr>

<tr>
<td valign="top">PY </td><td>2003</td>
</tr>

<tr>
<td valign="top">BP </td><td>11</td>
</tr>

<tr>
<td valign="top">EP </td><td>14</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/RFIC.2003.1213882</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Optics; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000184031700002</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kumar, A
   <br>Verma, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kumar, A
   <br>Verma, A</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE
   IEEE
   IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Using phone and diphone based acoustic modets for voice conversion: a
   step towards creating voice fonts</td>
</tr>

<tr>
<td valign="top">SO </td><td>2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I, PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>4th International Conference on Multimedia and Expo (ICME 2003)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUL 06-09, 2003</td>
</tr>

<tr>
<td valign="top">CL </td><td>BALTIMORE, MD</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion techniques attempt to modify speech signal so that it is perceived as if spoken by another speaker, different from the original speaker. In this paper, we present a novel approach to perform voice conversion. Our approach uses acoustic models based on units of speech, like phones and diphones, for voice conversion. These models can be computed and used independently for a given speaker without being concerned about the source or target speaker. It avoids the use of a parallel speech corpus in the voices of source and target speakers. It is shown that by using the proposed approach, voice fonts can be created and stored which will represent individual characteristics of a particular speaker, to be used for customization of synthetic speech. We also show through objective and subjective tests, that voice conversion quality is comparable to other approaches that require a parallel speech corpus.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Indian Inst Technol, Ctr Appl Res Elect, New Delhi 110016, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kumar, A (reprint author), Indian Inst Technol, Ctr Appl Res Elect, New Delhi 110016, India.</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Verma, Ashish</display_name>&nbsp;</font></td><td><font size="3">L-3943-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2003</td>
</tr>

<tr>
<td valign="top">BP </td><td>393</td>
</tr>

<tr>
<td valign="top">EP </td><td>396</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering; Imaging Science &amp; Photographic Technology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000184737900099</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Lin, CY
   <br>Jang, JSR</td>
</tr>

<tr>
<td valign="top">AF </td><td>Lin, CY
   <br>Jang, JSR</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE
   IEEE
   IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>New refinement schemes for voice conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL II,
   PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>4th International Conference on Multimedia and Expo (ICME 2003)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUL 06-09, 2003</td>
</tr>

<tr>
<td valign="top">CL </td><td>BALTIMORE, MD</td>
</tr>

<tr>
<td valign="top">AB </td><td>New refinement schemes for voice conversion are proposed in this paper. We take mel-frequency cepstral coefficients (MFCC) as the basic feature and adopt cepstral mean subtraction to compensate the channel effects. We propose S/U/V (Silence/Unvoiced/Voiced) decision rule such that two sets of codebooks are used to capture the difference between unvoiced and voiced segments of the source speaker. Moreover, we apply three schemes to refine the synthesized voice, including pitch refinement with PSOLA, energy equalization, and frame concatenation based on synchronized pitch marks. The satisfactory performance of the voice conversion system can be demonstrated through ABX listening test and MOS grade.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu 30043, Taiwan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Lin, CY (reprint author), Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu 30043, Taiwan.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2003</td>
</tr>

<tr>
<td valign="top">BP </td><td>725</td>
</tr>

<tr>
<td valign="top">EP </td><td>728</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering; Imaging Science &amp; Photographic Technology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000184738000182</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sundermann, D
   <br>Ney, H
   <br>Hoge, H</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sundermann, D
   <br>Ney, H
   <br>Hoge, H</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>VTLN-based cross-language voice conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>ASRU'03: 2003 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND
   UNDERSTANDING ASRU '03</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU
   03)</td>
</tr>

<tr>
<td valign="top">CY </td><td>NOV 30-DEC 03, 2003</td>
</tr>

<tr>
<td valign="top">CL </td><td>St Thomas, VI</td>
</tr>

<tr>
<td valign="top">AB </td><td>In speech recognition, vocal tract length normalization (VTLN) is a well-studied technique for speaker normalization. As cross-language voice conversion aims at the transformation of a source speaker's voice into that of a target speaker using a different language, we want to investigate whether VTLN is an appropriate method to adapt the voice characteristics. After applying several conventional VTLN warping functions, we extend the conventional piece-wise linear function to several segments, allowing a more detailed warping of the source spectrum. Experiments on cross-language voice conversion are performed on three corpora of two languages and both speaker genders.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Rhein Westfal TH Aachen, Aachen Tech Univ, Dept Comp Sci, D-52056
   Aachen, Germany.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sundermann, D (reprint author), Rhein Westfal TH Aachen, Aachen Tech Univ, Dept Comp Sci, Ahornstr 55, D-52056 Aachen, Germany.</td>
</tr>

<tr>
<td valign="top">TC </td><td>10</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>11</td>
</tr>

<tr>
<td valign="top">PY </td><td>2003</td>
</tr>

<tr>
<td valign="top">BP </td><td>676</td>
</tr>

<tr>
<td valign="top">EP </td><td>681</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/ASRU.2003.1318521</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000222782800116</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Rentzos, D
   <br>Vaseghi, S
   <br>Turajlic, E
   <br>Yan, Q
   <br>Ho, CH</td>
</tr>

<tr>
<td valign="top">AF </td><td>Rentzos, D
   <br>Vaseghi, S
   <br>Turajlic, E
   <br>Yan, Q
   <br>Ho, CH</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Transformation of speaker characteristics for voice conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>ASRU'03: 2003 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND
   UNDERSTANDING ASRU '03</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU
   03)</td>
</tr>

<tr>
<td valign="top">CY </td><td>NOV 30-DEC 03, 2003</td>
</tr>

<tr>
<td valign="top">CL </td><td>St Thomas, VI</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a voice conversion method based on analysis and transformation of the characteristics that define a speaker's voice. Voice characteristic features are grouped into three main categories: (a) the spectral features at formants, (b) the pitch and intonation pattern and (c) the glottal pulse shape. Modelling and transformation methods of each group of voice features are outlined. The spectral features at formants are modelled using a two-dimensional phoneme-dependent HMMs. Subband frequency warping is used for spectrum transformation where the subbands are centred on estimates of formant trajectories. The F0 contour, extracted from autocorrelation-based pitchmarks, is used for modelling the pitch and intonation patterns of speech. A PSOLA based method is used for transformation of pitch, intonation patterns and speaking rate. Finally a method based on de-convolution of the vocal tract is used for modelling and mapping of the glottal pulse. The experimental results present illustrations of transformations of the various characteristics and perceptual evaluations.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Brunel Univ, Dept Elect &amp; Comp Engn, Uxbridge UB8 3PH, Middx, England.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Rentzos, D (reprint author), Brunel Univ, Dept Elect &amp; Comp Engn, Uxbridge UB8 3PH, Middx, England.</td>
</tr>

<tr>
<td valign="top">TC </td><td>5</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>5</td>
</tr>

<tr>
<td valign="top">PY </td><td>2003</td>
</tr>

<tr>
<td valign="top">BP </td><td>706</td>
</tr>

<tr>
<td valign="top">EP </td><td>711</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/ASRU.2003.1318526</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000222782800121</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Shafer, WE
   <br>Owsen, D</td>
</tr>

<tr>
<td valign="top">AF </td><td>Shafer, WE
   <br>Owsen, D</td>
</tr>

<tr>
<td valign="top">TI </td><td>Policy issues raised by for-profit spinoffs from professional
   associations: An evaluation of a recent AICPA initiative</td>
</tr>

<tr>
<td valign="top">SO </td><td>JOURNAL OF BUSINESS ETHICS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>nonprofit governance; nonprofit organizations; professional
   associations; professionalism; public policy</td>
</tr>

<tr>
<td valign="top">ID </td><td>NONPROFIT; CONVERSIONS; COMMERCIALISM; COMPETITION; HOSPITALS; FUTURE</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper provides an evaluation of the spinoff of a for-profit company from the American Institute of Certified Public Accountants (AICPA), a nonprofit professional association. The evaluation is based on a review of the literature on public policy issues surrounding organizational conversions from nonprofit to for-profit legal status. Many criticisms of this for-profit spinoff were voiced by professional leaders and accounting regulators, and we demonstrate that these criticisms are grounded in widely recognized policy principles relating to nonprofit conversions. The public policy issues raised by this study have implications for the governance of professional associations in all disciplines.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Pepperdine Univ, Graziadio Sch Business &amp; Management, Malibu, CA 90265
   USA.
   <br>Univ Portsmouth, Portsmouth PO1 2UP, Hants, England.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Shafer, WE (reprint author), Pepperdine Univ, Graziadio Sch Business &amp; Management, Malibu, CA 90265 USA.</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Shafer, William</display_name>&nbsp;</font></td><td><font size="3">0000-0002-0661-990X&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PD </td><td>JAN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2003</td>
</tr>

<tr>
<td valign="top">VL </td><td>42</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>181</td>
</tr>

<tr>
<td valign="top">EP </td><td>195</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1023/A:1021946917170</td>
</tr>

<tr>
<td valign="top">SC </td><td>Business &amp; Economics; Social Sciences - Other Topics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000180327300005</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sundermann, D
   <br>Ney, H</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sundermann, D
   <br>Ney, H</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE Computer Society</td>
</tr>

<tr>
<td valign="top">TI </td><td>VTLN-based voice conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>PROCEEDINGS OF THE 3RD IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING
   AND INFORMATION TECHNOLOGY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>3rd IEEE International Symposium on Signal Processing and Information
   Technology</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 14-17, 2003</td>
</tr>

<tr>
<td valign="top">CL </td><td>Darmstadt, GERMANY</td>
</tr>

<tr>
<td valign="top">AB </td><td>In speech recognition, vocal tract length normalization (VTLN) is a well-studied technique for speaker normalization. As voice conversion aims at the transformation of a source speaker's voice into that of a target speaker, we want to investigate whether VTLN is an appropriate method to adapt the voice characteristics. After applying several conventional VTLN warping functions, we extend the piecewise linear function to several segments, allowing a more detailed warping of the source spectrum. Experiments on voice conversion are performed on three corpora of two languages and both speaker genders.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Tech Univ, RWTH Aachen, Comp Sci Dept, D-52056 Aachen, Germany.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sundermann, D (reprint author), Tech Univ, RWTH Aachen, Comp Sci Dept, Ahornstr 55, D-52056 Aachen, Germany.</td>
</tr>

<tr>
<td valign="top">EM </td><td>suendermann@cs.rwth-aachen.de; ney@cs.rwth-aachen.de</td>
</tr>

<tr>
<td valign="top">TC </td><td>12</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>13</td>
</tr>

<tr>
<td valign="top">PY </td><td>2003</td>
</tr>

<tr>
<td valign="top">BP </td><td>556</td>
</tr>

<tr>
<td valign="top">EP </td><td>559</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000223641300137</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Tward, E
   <br>Petach, M
   <br>Backhaus, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Tward, E
   <br>Petach, M
   <br>Backhaus, S</td>
</tr>

<tr>
<td valign="top">BE </td><td>ElGenk, MS</td>
</tr>

<tr>
<td valign="top">TI </td><td>Thermoacoustic space power converter</td>
</tr>

<tr>
<td valign="top">SO </td><td>SPACE TECHNOLOGY AND APPLICATIONS INTERNATIONAL FORUM - STAIF 2003</td>
</tr>

<tr>
<td valign="top">SE </td><td>AIP CONFERENCE PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Space Technology and Applications International Forum (STAIF-2003)</td>
</tr>

<tr>
<td valign="top">CY </td><td>FEB 02-05, 2003</td>
</tr>

<tr>
<td valign="top">CL </td><td>ALBUQUERQUE, NM</td>
</tr>

<tr>
<td valign="top">AB </td><td>A thermoacoustic power converter for use in space in the conversion of radioisotope-generated heat to electricity is under development. The converter incorporates a thermoacoustic driver that converts heat to acoustic power without any moving parts. The acoustic power is used to drive a pair of flexure bearing supported pistons connected to voice coils in a vibrationally balanced pair of moving coil alternators. Initial tests of the small similar to100W thermoacoustic driver have demonstrated good efficiency. An alternator matched to the driver is now under construction. A description of the system and the results of development tests are presented.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>TRW Co Inc, Space &amp; Technol Div, Redondo Beach, CA 90278 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Tward, E (reprint author), TRW Co Inc, Space &amp; Technol Div, 1 Space Pk, Redondo Beach, CA 90278 USA.</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Backhaus, Scott</display_name>&nbsp;</font></td><td><font size="3">F-4285-2012&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>4</td>
</tr>

<tr>
<td valign="top">PY </td><td>2003</td>
</tr>

<tr>
<td valign="top">VL </td><td>654</td>
</tr>

<tr>
<td valign="top">BP </td><td>656</td>
</tr>

<tr>
<td valign="top">EP </td><td>661</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Thermodynamics; Energy &amp; Fuels</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000181267400082</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Yu, P
   <br>Revis, J
   <br>Wuyts, FL
   <br>Zanaret, M
   <br>Giovanni, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Yu, P
   <br>Revis, J
   <br>Wuyts, FL
   <br>Zanaret, M
   <br>Giovanni, A</td>
</tr>

<tr>
<td valign="top">TI </td><td>Correlation of instrumental voice evaluation with perceptual voice
   analysis using a modified visual analog scale</td>
</tr>

<tr>
<td valign="top">SO </td><td>FOLIA PHONIATRICA ET LOGOPAEDICA</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice; dysphonia; GRBAS scale; reliability</td>
</tr>

<tr>
<td valign="top">ID </td><td>ACOUSTIC PREDICTION; GRBAS SCALE; QUALITY; RELIABILITY; DYSPHONIA;
   SEVERITY; RATINGS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Various rating scales have been used for perceptual voice analysis including ordinal (ORD) scales and visual analog (VA) scales. The purpose of this study was to determine the most suitable scale for studies using perceptual voice analysis as a gold standard for validation of objective analysis protocols. The study was carried out on 74 female voice samples from 68 dysphonic patients and 6 controls. A panel of 4 raters with experience in perceptual analysis was asked to score voices according to the G component (overall quality) of the GRBAS system. Two rating scales were used. The first was a conventional 4-point ORD scale. The second was a modified VA (mVA) scale obtained by transforming the VA scale into an ORD scale using a weighted conversion scheme. Objective voice evaluation was performed using the EVA(R) workstation. Objective measurements included acoustic, aerodynamic, and physiologic parameters as well as parameters based on nonlinear mathematics (e.g., Lyapunov coefficient). Instrumental measurements were compared with results of perceptual analysis using either the conventional ORD scale or mVA scale. Results demonstrate that correlation between perceptual and objective voice judgments is better using a mVA scale than a conventional ORD scale (concordance, 88 vs. 64%). Data also indicate that the mVA scale described herein improves the correlation between objective and perceptual voice analysis. Copyright (C) 2002 S. Karger AG, Basel.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>CHU Timone, Lab Audio Phonol Clin, Marseille, France.
   <br>Kunming Med Univ, Yunnan, Peoples R China.
   <br>Univ Antwerp, B-2020 Antwerp, Belgium.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Giovanni, A (reprint author), Hop Enfants La Timone, Fed ORL, Lab Audio Phonol Clin, Blvd Jean Moulin, F-13385 Marseille 5, France.</td>
</tr>

<tr>
<td valign="top">TC </td><td>34</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>39</td>
</tr>

<tr>
<td valign="top">PD </td><td>NOV-DEC</td>
</tr>

<tr>
<td valign="top">PY </td><td>2002</td>
</tr>

<tr>
<td valign="top">VL </td><td>54</td>
</tr>

<tr>
<td valign="top">IS </td><td>6</td>
</tr>

<tr>
<td valign="top">BP </td><td>271</td>
</tr>

<tr>
<td valign="top">EP </td><td>281</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1159/000066150</td>
</tr>

<tr>
<td valign="top">SC </td><td>Audiology &amp; Speech-Language Pathology; Otorhinolaryngology;
   Rehabilitation</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000179074900001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Busbridge, SC
   <br>Huang, Y
   <br>Fryer, PA</td>
</tr>

<tr>
<td valign="top">AF </td><td>Busbridge, SC
   <br>Huang, Y
   <br>Fryer, PA</td>
</tr>

<tr>
<td valign="top">TI </td><td>Crossover systems in digital loudspeakers</td>
</tr>

<tr>
<td valign="top">SO </td><td>JOURNAL OF THE AUDIO ENGINEERING SOCIETY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">ID </td><td>COIL</td>
</tr>

<tr>
<td valign="top">AB </td><td>A digital loudspeaker is one that does not contain any form of embedded digital-to-analog converter. From a consideration of the mathematical operations describing the digital-to-analog conversion process in a digital loudspeaker, it is concluded that the use of N identical analog filters (where N is the number of bits) filtering each bit driver separately offers a practical alternative to digital signal processing. The implementation of a multiple-driver multiple-voice-coil digital loudspeaker is described. The effect of component tolerances in the crossover and compatibility with the requirements of the current drive are evaluated. The interactions between motion emf, driving current, and mutual coupling emfs are considered. It is concluded that this method of crossover implementation is both viable and achievable.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Univ Brighton, Sch Engn, Brighton BN2 4GJ, E Sussex, England.
   <br>B&amp;W Loudspeakers Ltd, Steyning BN44 3SA, England.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Busbridge, SC (reprint author), Univ Brighton, Sch Engn, Brighton BN2 4GJ, E Sussex, England.</td>
</tr>

<tr>
<td valign="top">TC </td><td>6</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>6</td>
</tr>

<tr>
<td valign="top">PD </td><td>OCT</td>
</tr>

<tr>
<td valign="top">PY </td><td>2002</td>
</tr>

<tr>
<td valign="top">VL </td><td>50</td>
</tr>

<tr>
<td valign="top">IS </td><td>10</td>
</tr>

<tr>
<td valign="top">BP </td><td>791</td>
</tr>

<tr>
<td valign="top">EP </td><td>798</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000178914800004</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nakanishi, Y
   <br>Matsuoka, Y
   <br>Maruoka, Y
   <br>Akiyama, K
   <br>Kimura, S
   <br>Haneda, H</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nakanishi, Y
   <br>Matsuoka, Y
   <br>Maruoka, Y
   <br>Akiyama, K
   <br>Kimura, S
   <br>Haneda, H</td>
</tr>

<tr>
<td valign="top">TI </td><td>Small size IP-PBX NEAX2000IPS (Internet protocol server)</td>
</tr>

<tr>
<td valign="top">SO </td><td>NEC RESEARCH &amp; DEVELOPMENT</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>IP-PBX; peer-to-peer; VoIP; IP network; CTI; maintenance; all in one;
   ToIP</td>
</tr>

<tr>
<td valign="top">AB </td><td>In recent years, with the popularization of IP-supported networks, voice communications have been recognized as a part of data communications, and "VoIP" technologies have spread out in order to converge voice and data. Even PBX being a core of corporate telephony system is required to go with the tide of IP technologies, so that IP-PBX which provides infrastructure to accelerate voice and data convergence and support multimedia in the seamless networks is highly expected. NEC's small IP-PBX "NEAX2000IPS" is a hybrid system which provides flexible combination of TDM switching, pure IP (peer-to-peer) switching, and conversion of TDM-based voice data to packet-based voice data and vice versa. This is the next-generation telephony system suitable for next-generation multimedia communications. This paper introduces the overview and new technologies of NEAX2000IPS.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>OCT</td>
</tr>

<tr>
<td valign="top">PY </td><td>2002</td>
</tr>

<tr>
<td valign="top">VL </td><td>43</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">BP </td><td>272</td>
</tr>

<tr>
<td valign="top">EP </td><td>279</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000178424600010</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Lee, KS
   <br>Doh, W
   <br>Youn, DH</td>
</tr>

<tr>
<td valign="top">AF </td><td>Lee, KS
   <br>Doh, W
   <br>Youn, DH</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice conversion using low dimensional vector mapping</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEICE TRANSACTIONS ON INFORMATION AND SYSTEMS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; KLT; soft clustering and pitch modification</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, a new voice personality transformation algorithm which uses the vocal tract characteristics and pitch period as feature parameters is proposed. The vocal tract transfer function is divided into time-invariant and time-varying parts. Conversion rules for the time-varying part are constructed by the classified-linear transformation matrix based on soft-clustering techniques for LPC cepstrum expressed in KL (Karhunen-Loeve) coefficients. An excitation signal containing prosodic information is transformed by average pitch ratio. In order to improve the naturalness, transformation on the excitation signal is separately applied to voiced and unvoiced bands to preserve the overall spectral structure. Objective tests show that the distance between the LPC cepstrum of a target speaker and that of the speech synthesized using the proposed method is reduced by about 70% compared with the distance between the target speaker's LPC cepstrum and the source speaker's. Also, subjective listening tests show that 60-70% of listeners identify the transformed speech as the target speaker's.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Konkuk Univ, Dept Elect Engn, Seoul, South Korea.
   <br>Yonsei Univ, Dept Elect Engn, Seoul 120749, South Korea.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Lee, KS (reprint author), Konkuk Univ, Dept Elect Engn, Seoul, South Korea.</td>
</tr>

<tr>
<td valign="top">TC </td><td>5</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>7</td>
</tr>

<tr>
<td valign="top">PD </td><td>AUG</td>
</tr>

<tr>
<td valign="top">PY </td><td>2002</td>
</tr>

<tr>
<td valign="top">VL </td><td>E85D</td>
</tr>

<tr>
<td valign="top">IS </td><td>8</td>
</tr>

<tr>
<td valign="top">BP </td><td>1297</td>
</tr>

<tr>
<td valign="top">EP </td><td>1305</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000177439400013</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sen, A
   <br>Samudravijaya, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sen, A
   <br>Samudravijaya, K</td>
</tr>

<tr>
<td valign="top">TI </td><td>Indian accent text-to-speech system for web browsing</td>
</tr>

<tr>
<td valign="top">SO </td><td>SADHANA-ACADEMY PROCEEDINGS IN ENGINEERING SCIENCES</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Web reader; text to speech; speech synthesis; speech recognition; Indian
   accent; human computer interaction</td>
</tr>

<tr>
<td valign="top">ID </td><td>RECOGNITION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Incorporation of speech and Indian scripts can greatly enhance the accessibility of web information among common people. This paper describes a 'web reader' which 'reads out' the textual contents of a selected web page in Hindi or in English with Indian accent. The content of the page is downloaded and parsed into suitable textual form. It is then passed on to an indigenously developed text-to-speech system for Hindi/Indian English, to generate spoken output. The text-to-speech conversion is performed in three stages: text analysis, to establish pronunciation, phoneme to acoustic-phonetic parameter conversion and, lastly, parameter-to-speech conversion through a production model. Different types of voices are used to read special messages. The web reader detects the hypertext links in the web pages and gives the user the option to follow the link or continue perusing the current web page. The user can exercise the option either through a keyboard or via spoken commands. Future plans include refining the web parser, improvement of naturalness of synthetic speech and improving the robustness of the speech recognition system.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Tata Inst Fundamental Res, Sch Technol &amp; Comp Sci, Bombay 400005,
   Maharashtra, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sen, A (reprint author), Tata Inst Fundamental Res, Sch Technol &amp; Comp Sci, Homi Bhabha Rd, Bombay 400005, Maharashtra, India.</td>
</tr>

<tr>
<td valign="top">TC </td><td>4</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>4</td>
</tr>

<tr>
<td valign="top">PD </td><td>FEB</td>
</tr>

<tr>
<td valign="top">PY </td><td>2002</td>
</tr>

<tr>
<td valign="top">VL </td><td>27</td>
</tr>

<tr>
<td valign="top">BP </td><td>113</td>
</tr>

<tr>
<td valign="top">EP </td><td>126</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/BF02703316</td>
</tr>

<tr>
<td valign="top">PN </td><td>1</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000175626100009</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hung, CY
   <br>Tan, CP
   <br>Chuang, LC
   <br>Lee, WT</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hung, CY
   <br>Tan, CP
   <br>Chuang, LC
   <br>Lee, WT</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE
   IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>The Implementation of the communication framework of SIP and MGCP in
   VoIP applications</td>
</tr>

<tr>
<td valign="top">SO </td><td>10TH IEEE INTERNATIONAL CONFERENCE ON NETWORKS (ICON 2002), PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>10th IEEE International Conference on Networks</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 27-30, 2002</td>
</tr>

<tr>
<td valign="top">CL </td><td>SINGAPORE, SINGAPORE</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice over IP (VoIP); Session Initiation Protocol (SIP); Media Gateway
   Control Protocol (MGCP); Session Description Protocol (SDP)</td>
</tr>

<tr>
<td valign="top">AB </td><td>Recently, because of the popularity of Internet, the increment of network bandwidth, and the improvement of voice compression technologies, the VOIP has an exploding growth and makes a lot of changes to our lives. We could not only transmit text-based applications but also voice/video- based real-time multimedia applications over the Internet by using VoIP technology. To begin with VoIP, we need some protocols such as H.323, SIP, and MGCP to support it. These protocols play important roles in the VoIP communications, yet the interoperability between these protocols also becomes an emerging problem. Between these different VoIP protocols, how to communicate with each other is a tough task we must face. In this paper, we propose a solution for protocol conversion between MGCP and SIP. We had already implemented the protocol conversion platform in our lab, and found that it is feasible in the real-world applications.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Feng Chia Univ, Dept Informat Engn, Taichung 40724, Taiwan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hung, CY (reprint author), Feng Chia Univ, Dept Informat Engn, 100 Wenhwa Rd, Taichung 40724, Taiwan.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2002</td>
</tr>

<tr>
<td valign="top">BP </td><td>449</td>
</tr>

<tr>
<td valign="top">EP </td><td>454</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000180247200070</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Yoshida, K
   <br>Kazama, M
   <br>Tohyama, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Yoshida, K
   <br>Kazama, M
   <br>Tohyama, M</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE
   IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Pitch and speech-rate conversion using envelope modulation modeling</td>
</tr>

<tr>
<td valign="top">SO </td><td>2002 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING, VOLS I-IV, PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   (ICASSP)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 13-17, 2002</td>
</tr>

<tr>
<td valign="top">CL </td><td>ORLANDO, FL</td>
</tr>

<tr>
<td valign="top">AB </td><td>This article describes a method of intelligible speech representation that uses narrow-band envelopes and their carriers, This method enables modification of the talker's voice pitch and speech-rate without sacrificing intelligibility. The carrier, which shows the instantaneous phase, conveys pitch information, while the temporal envelope conveys speech-rate information and preserves speech intelligibility. The carriers, however, can be replaced by sinusoidal signals without severely degrading intelligibility or voice quality. Consequently, we can modify the pitch by shifting each envelope's carrier-frequency and convert the speech-rate by stretching or shrinking the envelopes. These findings could be useful in frequency scaling of the speech spectrum to assist hearing-impaired listeners or in time scaling of the speech signal for speech signal reproduction.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Kogakuin Univ, Hachioji, Tokyo 1920015, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Yoshida, K (reprint author), Kogakuin Univ, Hachioji, Tokyo 1920015, Japan.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2002</td>
</tr>

<tr>
<td valign="top">BP </td><td>425</td>
</tr>

<tr>
<td valign="top">EP </td><td>428</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science; Engineering; Imaging Science &amp; Photographic
   Technology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000177510400107</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Liu, D
   <br>Nilsson, R
   <br>Norling, F</td>
</tr>

<tr>
<td valign="top">AF </td><td>Liu, D
   <br>Nilsson, R
   <br>Norling, F</td>
</tr>

<tr>
<td valign="top">BE </td><td>Li, L
   <br>Yu, JB</td>
</tr>

<tr>
<td valign="top">TI </td><td>Full digital driving voice and audio load on an IC</td>
</tr>

<tr>
<td valign="top">SO </td><td>2002 INTERNATIONAL CONFERENCE ON COMMUNICATIONS, CIRCUITS AND SYSTEMS
   AND WEST SINO EXPOSITION PROCEEDINGS, VOLS 1-4</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Communications, Circuits and Systems</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUN 29-JUL 01, 2002</td>
</tr>

<tr>
<td valign="top">CL </td><td>Chengdu, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>DAC; delta-sigma modulation</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a study of how digital to analog conversion for low cost voice-audio can be achieved without analog components. The idea is to use an 1-bit oversampling digital to analog converter and. utilize the fact that a loudspeaker has a very limited frequency bandwidth. Several methods to implement 1-bit digital to analog converters are thoroughly discussed. Functional design and verification as well as low power design are done. The design is implemented in a 0.35mu full-custom ASIC. The measured power consumption is 0.056mW when supply voltage is 2.5V The solution is suitable for voice and audio terminals for communications.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Linkoping Univ, Dept Elect Engn, SE-58183 Linkoping, Sweden.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Liu, D (reprint author), Linkoping Univ, Dept Elect Engn, SE-58183 Linkoping, Sweden.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2002</td>
</tr>

<tr>
<td valign="top">BP </td><td>1505</td>
</tr>

<tr>
<td valign="top">EP </td><td>1510</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering; Imaging Science &amp; Photographic
   Technology; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000189407400319</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Jung, E
   <br>Schwarzbacher, A
   <br>Lawlor, R</td>
</tr>

<tr>
<td valign="top">AF </td><td>Jung, E
   <br>Schwarzbacher, A
   <br>Lawlor, R</td>
</tr>

<tr>
<td valign="top">BE </td><td>Skodras, AN
   <br>Constantinides, AG</td>
</tr>

<tr>
<td valign="top">TI </td><td>Implementation of real-time AMDF pitch-detection for voice gender
   normalisation</td>
</tr>

<tr>
<td valign="top">SO </td><td>DSP 2002: 14TH INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING
   PROCEEDINGS, VOLS 1 AND 2</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>14th International Conference on Digital Signal Processing (DSP 2002)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUL 01-03, 2002</td>
</tr>

<tr>
<td valign="top">CL </td><td>Santorini, GREECE</td>
</tr>

<tr>
<td valign="top">AB </td><td>Traditionally the interest in voice gender conversion was of a more theoretical nature rather than founded in real-life applications. However, with the increase in mobile communication and the resulting limitation in transmission bandwidth new approaches to minimising data rates have to be developed. Here voice gender normalisation (VGN) presents a novel method of achieving higher compression rates by using the VGN algorithm to remove all gender specific components of a speech signal and thus leaving only the information content to be transmitted.
   <br>A second application for VGN is in the field of speech controlled systems, where current speech recognition algorithms have to deal with the voice characteristics of a speaker as well as the information content. Here again the use of VGN can remove the speakers voice characteristics leaving only the pure information. Therefore, such a system would be capable of achieving much higher recognition rates while being independent of the speaker. This paper presents the theory of a gender removal system based on VGN and furthermore, outlines an efficient real-time hardware implementation for the use in portable communications equipment.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Dublin Inst Technol, Dublin 8, Ireland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Jung, E (reprint author), Dublin Inst Technol, Kevin St, Dublin 8, Ireland.</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2002</td>
</tr>

<tr>
<td valign="top">BP </td><td>827</td>
</tr>

<tr>
<td valign="top">EP </td><td>830</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000179052600175</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ito, K
   <br>Kumagai, T
   <br>Harada, K
   <br>Sonobe, T
   <br>Tomita, T
   <br>Ikeda, E</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ito, K
   <br>Kumagai, T
   <br>Harada, K
   <br>Sonobe, T
   <br>Tomita, T
   <br>Ikeda, E</td>
</tr>

<tr>
<td valign="top">TI </td><td>Radio network control system</td>
</tr>

<tr>
<td valign="top">SO </td><td>FUJITSU SCIENTIFIC &amp; TECHNICAL JOURNAL</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">AB </td><td>Our radio network control system based on the W-CDMA global standard specifications of the 3GPP (3(rd) Generation Partnership Project) can provide multimedia services such as voice, TV telephone, packet, and multi-call at a higher quality and higher rate than those of the 2(nd) generation mobile telecommunication system.
   <br>Our radio network control system has a highly flexible and scalable structure. This has been achieved by dividing various functionalities such as diversity handover, common transport channel related transaction, user data transaction with protocol conversion, and bandwidth control based on ATM and other technologies into several transaction units. These transaction units use high-speed RISC processors and closely interact with each other under the control of the application part to achieve various high-performance functionalities. This paper describes the architectures, functionalities, and technologies of our radio network control system.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Fujitsu Ltd, Kawasaki, Kanagawa, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ito, K (reprint author), Fujitsu Ltd, Kawasaki, Kanagawa, Japan.</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2002</td>
</tr>

<tr>
<td valign="top">VL </td><td>38</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>174</td>
</tr>

<tr>
<td valign="top">EP </td><td>182</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000181111000008</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Vassiliades, TA</td>
</tr>

<tr>
<td valign="top">AF </td><td>Vassiliades, TA</td>
</tr>

<tr>
<td valign="top">TI </td><td>Technical aids to performing thoracoscopic robotically-assisted internal
   mammary artery harvesting</td>
</tr>

<tr>
<td valign="top">SO </td><td>HEART SURGERY FORUM</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article; Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>4th Annual Meeting of the
   International-Society-for-Minimally-Invasive-Cardiac-Surgery</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUN 27-30, 2001</td>
</tr>

<tr>
<td valign="top">CL </td><td>MUNICH, GERMANY</td>
</tr>

<tr>
<td valign="top">ID </td><td>SURGERY; SCALPEL</td>
</tr>

<tr>
<td valign="top">AB </td><td>Objective: This report outlines the procedures and technical aids used for performing thoracoscopic internal mammary artery (IMA) harvesting in a series of 308 patients.
   <br>Methods: As a part of atraumatic coronary artery bypass (ACAB) operations, thoracoscopic IMA harvests ( 294 left, 14 right, and 12 bilateral) were performed in 308 consecutive patients. Single-lung ventilation and carbon dioxide insufflation were employed in all cases to facilitate exposure and dissection. A voice-activated robotic arm controlled the camera view, and harvesting was accomplished with the electrocautery on a low setting.
   <br>Results: Harvest time decreased from a mean of 58.4 minutes in the first fifty procedures to 29.4 minutes in the last fifty procedures. There were no significant complications as a result of this technique, and no patients required a conversion to sternotomy as a result of IMA injury.
   <br>Conclusions: Thoracoscopic internal mammary artery harvesting is an essential basic skill for cardiac surgeons interested in performing minimally invasive and atraumatic coronary bypass procedures. Thoracoscopic IMA harvesting can be successfully performed with the use of the technical aids and procedures outlined in this report.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Pensacola Heart Inst, Pensacola, FL USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Vassiliades, TA (reprint author), Cardiothorac Surg Associates NW FL, 5151 N 9th Ave,Suite 200, Pensacola, FL 32504 USA.</td>
</tr>

<tr>
<td valign="top">TC </td><td>17</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>20</td>
</tr>

<tr>
<td valign="top">PY </td><td>2002</td>
</tr>

<tr>
<td valign="top">VL </td><td>5</td>
</tr>

<tr>
<td valign="top">SU </td><td>3</td>
</tr>

<tr>
<td valign="top">BP </td><td>119</td>
</tr>

<tr>
<td valign="top">EP </td><td>124</td>
</tr>

<tr>
<td valign="top">AR </td><td>UNSP 2001-6692</td>
</tr>

<tr>
<td valign="top">SC </td><td>Cardiovascular System &amp; Cardiology; Surgery</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000176842800002</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Titze, IR</td>
</tr>

<tr>
<td valign="top">AF </td><td>Titze, IR</td>
</tr>

<tr>
<td valign="top">TI </td><td>Acoustic interpretation of resonant voice</td>
</tr>

<tr>
<td valign="top">SO </td><td>JOURNAL OF VOICE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>resonance; nasality; placement; focus; voice quality</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOCAL-TRACT</td>
</tr>

<tr>
<td valign="top">AB </td><td>Resonant voice, often described in terms of vibratory sensations in the face, is investigated acoustically by calculating vocal tract inertance. It appears that the ease of production and vibrancy of resonant voice depends more on lowering phonation threshold pressure than on tissue or air resonance in or around the face. Phonation threshold pressure is lowered by increasing air column inertance in the laryngeal vestibule. The fact that the sensations are felt in the face is an indication of effective conversion of aerodynamic energy to acoustic energy, rather than sound resonation in the sinuses or the nasal airways.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Univ Iowa, Natl Ctr Voice &amp; Speech, Wendell Johnson Speech &amp; Hearing Ctr
   330, Iowa City, IA 52242 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Titze, IR (reprint author), Univ Iowa, Natl Ctr Voice &amp; Speech, Wendell Johnson Speech &amp; Hearing Ctr 330, Iowa City, IA 52242 USA.</td>
</tr>

<tr>
<td valign="top">TC </td><td>75</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>78</td>
</tr>

<tr>
<td valign="top">PD </td><td>DEC</td>
</tr>

<tr>
<td valign="top">PY </td><td>2001</td>
</tr>

<tr>
<td valign="top">VL </td><td>15</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">BP </td><td>519</td>
</tr>

<tr>
<td valign="top">EP </td><td>528</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/S0892-1997(01)00052-2</td>
</tr>

<tr>
<td valign="top">SC </td><td>Audiology &amp; Speech-Language Pathology; Otorhinolaryngology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000172802800007</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Cherif, A
   <br>Bouafif, L
   <br>Dabbabi, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Cherif, A
   <br>Bouafif, L
   <br>Dabbabi, T</td>
</tr>

<tr>
<td valign="top">TI </td><td>Pitch detection and formant analysis of Arabic speech processing</td>
</tr>

<tr>
<td valign="top">SO </td><td>APPLIED ACOUSTICS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">AB </td><td>Speech processing and synthesis has been a well researched area for several years linked to a renewal of interest especially in electronics, artificial intelligence, telecommunications, and even in medicine. For example, the implantation of speaker recognition systems, the development of new low bit rate coders, speech synthesis and assistance of the handicapped person, the identification of some neurological and ORL pathologies by vocal analysis are considered as the most promising applications in this field. In fact, for these applications, speech processing constitutes an essential stage of the extraction and the identification of vocal parameters (pitch, formants, stamp...) which depend on the physical, physiological and the linguistic structure of the spoken language. Moreover, the variability of the speech signal (children, male, female sounds) and its prosodic aspects (shouted, sung sounds...) render the task of treatment more difficult and oblige us to observe and acquire a large quantity of speech signals to extract that which is relevant. Hence, we have improved the processing part by the development of a convivial hard and soft environment under MATLAB 5-2. The originality of the work is that the developed program works in real time when associated with the MATLAB real time toolbox. In fact, the new speech processing program computes the pitch period, extracts the formant frequencies of Arabic speech and identifies the speaker vocal stamp. The database consists of Arabic sentences phonetically balanced, pronounced by several speakers. After acquisition, conversion and segmentation, we identify the voiced-unvoiced (V/UV) speech by analysing its zero-crossing evolution. Then we compute the fundamental frequency, the formants and the spectral envelope (vocal stamp). These parameters are not used only in speech synthesis and recognition but also in the prediction of the speaker's emotional and psychological state. (C) 2001 Elsevier Science Ltd. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Fac Sci Tunis, Signaux &amp; Syst Lab, Tunis 1060, Tunisia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Cherif, A (reprint author), Fac Sci Tunis, Signaux &amp; Syst Lab, Tunis 1060, Tunisia.</td>
</tr>

<tr>
<td valign="top">TC </td><td>11</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>11</td>
</tr>

<tr>
<td valign="top">PD </td><td>OCT</td>
</tr>

<tr>
<td valign="top">PY </td><td>2001</td>
</tr>

<tr>
<td valign="top">VL </td><td>62</td>
</tr>

<tr>
<td valign="top">IS </td><td>10</td>
</tr>

<tr>
<td valign="top">BP </td><td>1129</td>
</tr>

<tr>
<td valign="top">EP </td><td>1140</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/S0003-682X(01)00007-X</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000170413700001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Shlomot, E
   <br>Cuperman, V
   <br>Gersho, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Shlomot, E
   <br>Cuperman, V
   <br>Gersho, A</td>
</tr>

<tr>
<td valign="top">TI </td><td>Hybrid coding: Combined harmonic and waveform coding of speech at 4 kb/s</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE TRANSACTIONS ON SPEECH AND AUDIO PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>harmonic spectral quantization; hybrid coding of speech; low bit-rate
   speech coding; sinusoidal speech coding</td>
</tr>

<tr>
<td valign="top">ID </td><td>QUANTIZATION; VOCODER</td>
</tr>

<tr>
<td valign="top">AB </td><td>A new hybrid speech coding technique is presented in this paper, which combines a frequency-domain parametric coder (for stationary voiced and stationary unvoiced speech) with a time-domain waveform coder (for transition speech). Our hybrid coder uses a parametric representation for the excitation of a linear-prediction filter. The excitation of stationary voiced speech is a sum of harmonic cosines with interpolated magnitudes and a synthetic phase model, the excitation for stationary unvoiced speech is a spectrally shaped noise, and the excitation for transition speech is a set of signed pulses. Signal alignment when switching between the harmonic excitation of stationary voiced speech and the pulse model used for transition speech is required, and achieved by special alignment procedures.
   <br>A 4 kb/s hybrid coder, which achieves high-quality reconstructed speech, is described in this paper. The 4 kb/s hybrid coder employs a neural network classifier, and a novel pitch detection and harmonic bandwidth estimation algorithm. The locations of excitation pulses for coding transitions are determined by analysis-by-synthesis. A simple and efficient dimension conversion and quantization of the harmonic spectral magnitudes of voiced speech was devised, combining the general nonsquare transform (NST) or dimension conversion and a weighted vector quantization (VQ) approach. Subjective listening tests demonstrate that the 4 kb/s hybrid coding scheme competes favorably with CELP coders at low bit-rates.(1).</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Mindspeed Technol Conxant Syst, Newport Beach, CA 92660 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Shlomot, E (reprint author), Mindspeed Technol Conxant Syst, Newport Beach, CA 92660 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>shlomot@midspeed.com; vlad@cuperman.com; gersho@ece.ucsb.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>10</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>11</td>
</tr>

<tr>
<td valign="top">PD </td><td>SEP</td>
</tr>

<tr>
<td valign="top">PY </td><td>2001</td>
</tr>

<tr>
<td valign="top">VL </td><td>9</td>
</tr>

<tr>
<td valign="top">IS </td><td>6</td>
</tr>

<tr>
<td valign="top">BP </td><td>632</td>
</tr>

<tr>
<td valign="top">EP </td><td>646</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/89.943341</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000170651100003</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sistrom, CL
   <br>Honeyman, JC
   <br>Mancuso, A
   <br>Quisling, RG</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sistrom, CL
   <br>Honeyman, JC
   <br>Mancuso, A
   <br>Quisling, RG</td>
</tr>

<tr>
<td valign="top">TI </td><td>Managing predefined templates and macros for a departmental speech
   recognition system using common software</td>
</tr>

<tr>
<td valign="top">SO </td><td>JOURNAL OF DIGITAL IMAGING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>radiology information systems; radiographic image interpretation;
   computer-assisted software design; speech recognition; medical
   informatics applications</td>
</tr>

<tr>
<td valign="top">ID </td><td>RADIOLOGY REPORTING SYSTEM; VOICE RECOGNITION; IMPLEMENTATION;
   GENERATION; TRANSCRIPTION; INTEGRATION; LANGUAGE</td>
</tr>

<tr>
<td valign="top">AB </td><td>The authors have developed a networked database system to create, store, and manage predefined radiology report definitions. This was prompted by complete departmental conversion to a computer speech recognition system (SRS) for clinical reporting. The software complements and extends the capabilities of the SRS, and 2 systems are integrated by means of a simple text file format and import/export functions within each program. This report describes the functional requirements, design considerations, and implementation details of the structured report management software. The database and its interface are designed to allow all radiologists and division managers to define and update template structures relevant to their practice areas. Two key conceptual extensions supported by the template management system are the addition of a template type construct and allowing individual radiologists to dynamically share common organ system or modality-specific templates. In addition, the template manager software enables specifying predefined report structures that can be triggered at the time of dictation from printed lists of barcodes. Initial experience using the program in a regional, multisite, academic radiology practice has been positive. Copyright (C) 2001 by W.B. Sounders Company.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Univ Florida, Sch Med, Dept Radiol, Gainesville, FL 32610 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sistrom, CL (reprint author), Univ Florida, Sch Med, Dept Radiol, POB 100374, Gainesville, FL 32610 USA.</td>
</tr>

<tr>
<td valign="top">TC </td><td>12</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>12</td>
</tr>

<tr>
<td valign="top">PD </td><td>SEP</td>
</tr>

<tr>
<td valign="top">PY </td><td>2001</td>
</tr>

<tr>
<td valign="top">VL </td><td>14</td>
</tr>

<tr>
<td valign="top">IS </td><td>3</td>
</tr>

<tr>
<td valign="top">BP </td><td>131</td>
</tr>

<tr>
<td valign="top">EP </td><td>141</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/s10278-001-0012-1</td>
</tr>

<tr>
<td valign="top">SC </td><td>Radiology, Nuclear Medicine &amp; Medical Imaging</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000171971500004</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Choi, KH
   <br>Luo, Y
   <br>Hwang, JN</td>
</tr>

<tr>
<td valign="top">AF </td><td>Choi, KH
   <br>Luo, Y
   <br>Hwang, JN</td>
</tr>

<tr>
<td valign="top">TI </td><td>Hidden markov model inversion for audio-to-visual conversion in an
   MPEG-4 facial animation system</td>
</tr>

<tr>
<td valign="top">SO </td><td>JOURNAL OF VLSI SIGNAL PROCESSING SYSTEMS FOR SIGNAL IMAGE AND VIDEO
   TECHNOLOGY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article; Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE 3rd Workshop on Miultimedia Signal Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 13-15, 1999</td>
</tr>

<tr>
<td valign="top">CL </td><td>COPENHAGEN, DENMARK</td>
</tr>

<tr>
<td valign="top">DE </td><td>HMMI; audio-to-visual conversion; MPEG-4; facial animation</td>
</tr>

<tr>
<td valign="top">ID </td><td>LIP MOVEMENTS; SPEECH</td>
</tr>

<tr>
<td valign="top">AB </td><td>MPEG-4 standard allows composition of natural or synthetic video with facial animation. Based on this standard, an animated face can be inserted into natural or synthetic video to create new virtual working environments such as virtual meetings or virtual collaborative environments. For these applications, audio-to-visual conversion techniques can be used to generate a talking face that is synchronized with the voice. In this paper, we address audio-to-visual conversion problems by introducing a novel Hidden Markov Model Inversion (HMMI) method. In training audio-visual HMMs, the model parameters {lambda (av)} can be chosen to optimize some criterion such as maximum likelihood. In inversion of audio-visual HMMs, visual parameters that optimize some criterion can be found based on given speech and model parameters {lambda (av)}. By using the proposed HMMI technique, an animated talking face can be synchronized with audio and can be driven realistically. The HMMI technique combined with MPEG-4 standard to create a virtual conference system, named VIRTUAL-FACE, is introduced to show the role of HMMI for applications of MPEG-4 facial animation.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Univ Washington, Dept Elect Engn, Informat Proc Lab, Seattle, WA 98195
   USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Choi, KH (reprint author), Univ Washington, Dept Elect Engn, Informat Proc Lab, Box 352500, Seattle, WA 98195 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>khchoi@ee.washington.edu; luoying@ee.washington.edu;
   hwang@ee.washington.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>29</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>34</td>
</tr>

<tr>
<td valign="top">PD </td><td>AUG</td>
</tr>

<tr>
<td valign="top">PY </td><td>2001</td>
</tr>

<tr>
<td valign="top">VL </td><td>29</td>
</tr>

<tr>
<td valign="top">IS </td><td>1-2</td>
</tr>

<tr>
<td valign="top">BP </td><td>51</td>
</tr>

<tr>
<td valign="top">EP </td><td>61</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1023/A:1011171430700</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000169325400005</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hayt, DB
   <br>Alexander, S
   <br>Drakakis, J
   <br>Berdebes, N</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hayt, DB
   <br>Alexander, S
   <br>Drakakis, J
   <br>Berdebes, N</td>
</tr>

<tr>
<td valign="top">TI </td><td>Filmless in 60 days: The impact of picture archiving and communications
   systems within a large urban hospital</td>
</tr>

<tr>
<td valign="top">SO </td><td>JOURNAL OF DIGITAL IMAGING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>picture archiving and communications system; filmless; image conversion;
   report turn-around time; continuous speech recognition system; voice
   recognition dictation system</td>
</tr>

<tr>
<td valign="top">AB </td><td>Many large urban hospitals converting to filmless radiography use a phased approach for digital imaging implementation. In fact, this strategy often is recommended by picture archival communication systems (PACS) experts and vendors alike for large, busy hospitals installing PACS in existing physical facilities. The concern is that comprehensive conversion from film-based to digital imaging may be too overwhelming an adjustment in operations for a medical staff to effectively handle without serious disruption of workflow for patient treatment and care. Elmhurst Hospital Center is a 543-bed hospital located in the Borough of Queens in New York City. Owned by the New York City Health and Hospitals Corporation, this municipal teaching hospital provides services to a patient mix that is 38% indigent with no insurance, 50% covered by Medicaid or Medicare, and 12% affiliated with HMOs. Most inpatients are admitted through the emergency department. Forty-five percent of all radiology procedures conducted are for emergency patients. Historically, up to 25% of all diagnostic imaging examinations were never reported formally by radiologists. Report turnaround time for the remaining 75% was unacceptable, with only 3% of all imaging examinations reported within a 12-hour period in 1996. Both situations existed in great part because physicians and residents who felt they needed access to films simply took them. Many were never located or returned days after they were taken. In 1998, Elmhurst Hospital Center replaced its RIS and added voice recognition dictation capabilities in January 1999. A hospitalwide PACS was deployed 10 months later. With the exception of mammography, the hospital converted to filmless radiography within 60 days. The critical objectives to maintain control of films and radically improve the reporting process were achieved immediately. Over 99% of all examinations now are formally reviewed and reported. Only 7% of all reports take 1 or more days to generate. This report describes Elmhurst Hospital's efforts to make improvements in the delivery of radiology services and the reasons attributed to its rapid conversion to becoming a filmless (mammography excluded) medical center. The impact of the PACS on radiology department operations and service is discussed. Copyright (C) 2007 by W.B. Saunders Company.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Elmhurst Hosp Ctr, Radiol Dept E118, Elmhurst, NY 11373 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hayt, DB (reprint author), Elmhurst Hosp Ctr, Radiol Dept E118, 79-01 Broadway, Elmhurst, NY 11373 USA.</td>
</tr>

<tr>
<td valign="top">TC </td><td>20</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>21</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2001</td>
</tr>

<tr>
<td valign="top">VL </td><td>14</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>62</td>
</tr>

<tr>
<td valign="top">EP </td><td>71</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/s10278-001-0004-1</td>
</tr>

<tr>
<td valign="top">SC </td><td>Radiology, Nuclear Medicine &amp; Medical Imaging</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000169345100004</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Tan, DHM
   <br>Hui, SC
   <br>Lau, CT</td>
</tr>

<tr>
<td valign="top">AF </td><td>Tan, DHM
   <br>Hui, SC
   <br>Lau, CT</td>
</tr>

<tr>
<td valign="top">TI </td><td>Wireless messaging services for mobile users</td>
</tr>

<tr>
<td valign="top">SO </td><td>JOURNAL OF NETWORK AND COMPUTER APPLICATIONS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">ID </td><td>INTERNET; COMMUNICATION; QUALITY</td>
</tr>

<tr>
<td valign="top">AB </td><td>Unified messaging brings together the various communication services including e-mail, fax, voice and video messages into one single mechanism for transportation over the Internet. This helps reduce costs by circumventing expensive international toll charges and eliminates the need for multiple communication devices for sending and receiving different types of media data. In the age of mobile communication, there is a growing demand for wireless messaging support for mobile users. However, wireless links are typically low bandwidth, costly and unreliable. Although most of the existing messaging systems can be extended to support wireless messaging via wireless network, they are unable to operate efficiently over the wireless environment. This paper proposes a number of techniques such as compression, on-demand retrieval, multi-part retrieval, content summarization and attachment conversion to support wireless messaging. These techniques have been incorporated into the Wireless Messaging Gateway System (WMGS) which supports e-mail, fax, voice mail, paging and short messaging services for the integrated Internet and wireless environment. Performance results have shown that significant savings on bandwidth utilization and transmission time have been achieved. (C) 2001 Academic Press.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Tan, DHM (reprint author), Nanyang Technol Univ, Sch Comp Engn, Nanyang Ave, Singapore 639798, Singapore.</td>
</tr>

<tr xmlns:date="http://exslt.org/dates-and-times">
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Lau, CT</display_name>&nbsp;</font></td><td><font size="3">A-3681-2011&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>4</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>4</td>
</tr>

<tr>
<td valign="top">PD </td><td>APR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2001</td>
</tr>

<tr>
<td valign="top">VL </td><td>24</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>151</td>
</tr>

<tr>
<td valign="top">EP </td><td>166</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1006/jnca.2000.0126</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000168607400004</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Boyd, WD
   <br>Kiaii, B
   <br>Novick, RJ
   <br>Rayman, R
   <br>Ganapathy, S
   <br>Dobkowski, WB
   <br>Jablonsky, G
   <br>McKenzie, FN
   <br>Menkis, AH</td>
</tr>

<tr>
<td valign="top">AF </td><td>Boyd, WD
   <br>Kiaii, B
   <br>Novick, RJ
   <br>Rayman, R
   <br>Ganapathy, S
   <br>Dobkowski, WB
   <br>Jablonsky, G
   <br>McKenzie, FN
   <br>Menkis, AH</td>
</tr>

<tr>
<td valign="top">TI </td><td>RAVECAB: improving outcome in off-pump minimal access surgery with
   robotic assistance and video enhancement</td>
</tr>

<tr>
<td valign="top">SO </td><td>CANADIAN JOURNAL OF SURGERY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article; Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Evolving Techniques and Technologies in Minimally Invasive Cardiac
   Surgery Annual Meeting</td>
</tr>

<tr>
<td valign="top">CY </td><td>JAN 22-23, 1999</td>
</tr>

<tr>
<td valign="top">CL </td><td>SAN ANTONIO, TEXAS</td>
</tr>

<tr>
<td valign="top">ID </td><td>CORONARY-BYPASS; ARTERY; EXPERIENCE</td>
</tr>

<tr>
<td valign="top">AB </td><td>Objective: To determine the efficacy of using the harmonic scalpel and robotic assistance to facilitate thoracoscopic harvest of the internal thoracic artery (ITA). Design: A case series. Setting: London Health Sciences Centre, University of Western Ontario, London, Ont. Patients and methods: Fifteen consecutive patients requiring harvest of the ITA for coronary artery bypass grafting. Intervention: Robot-assisted, video-enhanced coronary artery bypass (RAVECAB) through limited-access incisions,using the harmonic scalpel and a voice-activated robotic assistant. Main outcome measures: Ease and duration of the harvesting technique, complications of the procedure, graft flow and patency, and duration of postoperative hospitalization. Results: RAVECAB facilitated thoracoscopic dissection of the ITA with the harmonic scalpel in all cases. There were no conversions to a standard approach and no reoperations for bleeding. The mean (and standard deviation) ITA harvest time was 64.1 (22.9) minutes (range from 40 to 118 minutes). Robotic voice command capture rate was greater than 95%. Mean (and SD) intraoperative graft flows were 33.1 (26.8) mL/min (range from 14 to 126 mL/min). There was 100% graft patency on postoperative angiography. There were no deaths, perioperative myocardial infarction or arrhythmias. Mean (and SD) postoperative hospitalization was 3.3 (0.8) days. Conclusions: RAVECAB is a demanding procedure that addresses many of the disadvantages of the "conventional" minimally invasive coronary artery bypass. It allows complete pedicle dissection with minimal ITA manipulation and assures sufficient conduit length and a tension-free coronary artery anastomosis. All anastomoses were performed under direct vision through a 5- to 8-cm inferior mammary incision.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Univ Western Ontario, London Hlth Sci Ctr, London, ON N6A 5A5, Canada.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Boyd, WD (reprint author), Univ Western Ontario, London Hlth Sci Ctr, Univ Campus,339 Windermere Rd, London, ON N6A 5A5, Canada.</td>
</tr>

<tr>
<td valign="top">TC </td><td>27</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>28</td>
</tr>

<tr>
<td valign="top">PD </td><td>FEB</td>
</tr>

<tr>
<td valign="top">PY </td><td>2001</td>
</tr>

<tr>
<td valign="top">VL </td><td>44</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>45</td>
</tr>

<tr>
<td valign="top">EP </td><td>50</td>
</tr>

<tr>
<td valign="top">SC </td><td>Surgery</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000166950800012</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Rombouts, P
   <br>De Wilde, W
   <br>Weyten, L</td>
</tr>

<tr>
<td valign="top">AF </td><td>Rombouts, P
   <br>De Wilde, W
   <br>Weyten, L</td>
</tr>

<tr>
<td valign="top">TI </td><td>A 13.5-b 1.2-V micropower extended counting A/D converter</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE JOURNAL OF SOLID-STATE CIRCUITS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>analog-to-digital; extended counting; low power; low voltage</td>
</tr>

<tr>
<td valign="top">ID </td><td>SWITCHED-OPAMP CIRCUITS; SIGMA-DELTA MODULATOR; LOCAL FEEDBACK LOOPS;
   LOW-POWER; DYNAMIC-RANGE; CMOS; ADC; FILTER; 10-B</td>
</tr>

<tr>
<td valign="top">AB </td><td>This work presents a study of the extended counting technique for a 1.2-V micropower voice band A/D converter, This extended counting technique is a blend of Sigma Delta modulation with its high resolution but relatively low Speed and algorithmic conversion with its higher speed but lower accuracy. To achieve this, the converter successively operates first as a first-order Sigma Delta modulator to convert the most significant bits, and then the same hardware is used as an algorithmic converter to convert the remaining least significant bits.
   <br>An experimental prototype was designed in 0.8-mum CMOS, With a 1.2-V power supply, it consumes 150 muW of power at a 16-kHz Nyquist sampling frequency. The measured peak S/(N + THD) was 80 dB and the dynamic range 82 dB, The converter core including the controller and all reconstruction logic occupies about 1.3 x 1 mm(2) of chip area. This is considerably less than a complete Sigma Delta modulation A/D converter where the digital decimation filter would occupy a significant amount of chip area.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>State Univ Ghent, Elect &amp; Informat Syst Lab, B-9000 Ghent, Belgium.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Rombouts, P (reprint author), State Univ Ghent, Elect &amp; Informat Syst Lab, B-9000 Ghent, Belgium.</td>
</tr>

<tr>
<td valign="top">TC </td><td>60</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>61</td>
</tr>

<tr>
<td valign="top">PD </td><td>FEB</td>
</tr>

<tr>
<td valign="top">PY </td><td>2001</td>
</tr>

<tr>
<td valign="top">VL </td><td>36</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>176</td>
</tr>

<tr>
<td valign="top">EP </td><td>183</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/4.902758</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000167163000002</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Guillonneau, B
   <br>Vallancien, G</td>
</tr>

<tr>
<td valign="top">AF </td><td>Guillonneau, B
   <br>Vallancien, G</td>
</tr>

<tr>
<td valign="top">TI </td><td>Laparoscopic radical prostatectomy: The Montsouris technique</td>
</tr>

<tr>
<td valign="top">SO </td><td>JOURNAL OF UROLOGY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>prostatic neoplasms; prostatectomy; laparoscopy; prostate</td>
</tr>

<tr>
<td valign="top">ID </td><td>RETROPUBIC PROSTATECTOMY; INITIAL EXPERIENCE</td>
</tr>

<tr>
<td valign="top">AB </td><td>Purpose: Laparoscopic radical prostatectomy has become standard at our institution based on experience with 260 consecutive cases operated on between January 1998 and December 1999. In view of the favorable short-term outcomes we describe our standardized laparoscopic radical prostatectomy technique.
   <br>Materials and Methods: Two urologists trained in open retropubic radical prostatectomy and laparoscopy combined their experience to develop a specific technique of nonincisional radical prostatectomy for localized prostate cancer. Patients presented with clinical stages T1b to T2 prostate cancer and tumor size was approximately 18 to 130 gm. Operations were performed by 1 senior surgeon and 1 assistant, with the help of a voice controlled robot and with the patient under general anesthesia. The 2, 10 mm, ports and 3, 5 mm. ports were placed in the umbilicus and iliac fossa. The laparoscopic procedure was performed transperitoneally, combining anterograde and retrograde approaches in 7 standardized steps. Urethrovesical anastomosis was performed with 3-zero interrupted sutures tied intracorporeally. Technical details were compiled, summarized and illustrated with schematic views.
   <br>Results: Operating time was approximately 3 hours for the last 120 cases. Estimated average blood loss was 250 ml. with a transfusion rate of less than 1%. The conversion rate was 0%. Postoperative pain was minimal and analgesics were generally not required by postoperative day 2. The accuracy of dissection and sutures allowed patients to be discharged home without urethral catheterization starting on postoperative day 3.
   <br>Conclusions: Laparoscopic radical prostatectomy is now not only feasible, but more importantly reproducible. Each step has been checked and validated, and the procedure is standardized and has definitively replaced the retropubic approach in our practice.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Univ Paris 06, Inst Mutualiste Montsouris, Dept Urol, Paris, France.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Guillonneau, B (reprint author), Univ Paris 06, Inst Mutualiste Montsouris, Dept Urol, Paris, France.</td>
</tr>

<tr>
<td valign="top">TC </td><td>415</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>442</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2000</td>
</tr>

<tr>
<td valign="top">VL </td><td>163</td>
</tr>

<tr>
<td valign="top">IS </td><td>6</td>
</tr>

<tr>
<td valign="top">BP </td><td>1643</td>
</tr>

<tr>
<td valign="top">EP </td><td>1649</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/S0022-5347(05)67512-X</td>
</tr>

<tr>
<td valign="top">SC </td><td>Urology &amp; Nephrology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000086984900003</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>McGoogan, JR
   <br>Merritt, JE
   <br>Dave, YJ</td>
</tr>

<tr>
<td valign="top">AF </td><td>McGoogan, JR
   <br>Merritt, JE
   <br>Dave, YJ</td>
</tr>

<tr>
<td valign="top">TI </td><td>Evolution of switching architecture to support voice telephony over ATM</td>
</tr>

<tr>
<td valign="top">SO </td><td>BELL LABS TECHNICAL JOURNAL</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper discusses the architectural complexities of extending Lucent's 5ESS(R)-2000 Digital Switch using the 7R/E(TM) Packet Driver to add voice telephony over ATM (VTOA) with switched virtual circuits (SVCs). While meeting customers' tight deadlines for introducing this technology, this project addressed many of today's most complex issues, including platform migration, circuit-to-packet switch conversion, and voice/data migration. To provide the media gateway function, the PacketStar(TM) Access Concentrator (developed at Yurie Systems, now part of Lucent Technologies) was incorporated into Lucent's long-standing, premier switching product, the 5ESS-2000 Switch. Additional forward-looking technology from lucent provided the connection/signaling gateway functionality.</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PD </td><td>APR-JUN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2000</td>
</tr>

<tr>
<td valign="top">VL </td><td>5</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>157</td>
</tr>

<tr>
<td valign="top">EP </td><td>168</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1002/bltj.2228</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000088892600012</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Tsuzuki, S
   <br>Yamada, Y
   <br>Tazaki, S
   <br>Yoshida, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Tsuzuki, S
   <br>Yamada, Y
   <br>Tazaki, S
   <br>Yoshida, S</td>
</tr>

<tr>
<td valign="top">TI </td><td>Synchronization of voice data on asynchronous networks and its
   implementation for a VSAT channel gateway</td>
</tr>

<tr>
<td valign="top">SO </td><td>ELECTRICAL ENGINEERING IN JAPAN</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Internet; packet voice communication; VSAT satellite communication;
   synchronization algorithm; voice speed conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH</td>
</tr>

<tr>
<td valign="top">AB </td><td>The variation of packet-arrival intervals is one of the problems to be solved in realizing real-time voice communications on asynchronous networks such as the Internet. Although the variation can be absorbed by a receiving buffer, the end-to-end delay increases in proportion to the amount of buffered data. Therefore, the guideline for the receiving buffer's design that considers the trade-off between voice quality and delay is needed.
   <br>In this paper, the design of the receiving buffer by means of the voice-speed-conversion technique is discussed. According to our scenario, the buffer size was expected to be equivalent to the time length in which the probability distribution function of the packet-arrival intervals was about 95% to 99%. However, it is shown that synchronized voice reproduction can be achieved within a preset delay, which is shorter than the corresponding time to the buffer size. This is achieved by keeping the rate of voice speed conversion within the range of 50% to 150% depending on the variation of the intervals. The proposed method was implemented in the conventional UNIX workstations (WSs) connected to an Ethernet segment. The average voice delay between WSs was 150 to 180 ms and the synchronized voice reproduction was achieved with a standard deviation of 13 to 25 ms. The proposed method is applied to VSAT-satellite-channel gateways to realize the real-time voice communication between the WSs through a VSAT channel. (C) 1999 Scripta Technica.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Ehime Univ, Matsuyama, Ehime 790, Japan.
   <br>Kyoto Univ, Kyoto, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Tsuzuki, S (reprint author), Ehime Univ, Matsuyama, Ehime 790, Japan.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>JAN 30</td>
</tr>

<tr>
<td valign="top">PY </td><td>2000</td>
</tr>

<tr>
<td valign="top">VL </td><td>130</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>68</td>
</tr>

<tr>
<td valign="top">EP </td><td>79</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000083898800009</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Endo, Y
   <br>Kasuya, H</td>
</tr>

<tr>
<td valign="top">AF </td><td>Endo, Y
   <br>Kasuya, H</td>
</tr>

<tr>
<td valign="top">TI </td><td>A speech analysis-conversion-synthesis system taking period-to-period
   fluctuations into account</td>
</tr>

<tr>
<td valign="top">SO </td><td>ELECTRONICS AND COMMUNICATIONS IN JAPAN PART III-FUNDAMENTAL ELECTRONIC
   SCIENCE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>speech analysis; speech conversion; speech synthesis; cycle-to-cycle
   perturbation; voice quality</td>
</tr>

<tr>
<td valign="top">ID </td><td>PATHOLOGICAL VOICE</td>
</tr>

<tr>
<td valign="top">AB </td><td>An analysis-conversion-synthesis system taking account of cycle-to-cycle perturbation and designed for speech research was constructed. This system can analyze, convert, and synthesize acoustic characteristics related to voice quality, such as the perturbation of the fundamental period, effective value and spectrum, mean fundamental frequency, average spectrum, and laryngeal noise. Analysis-synthesis experiments show that the spectral envelopes and perturbations can be reconstructed from a few parameters. The analyzed-synthesized speech signals retain the spectrum of the original speech signals. The perceptual difference between the original and the analyzed-synthesized signals is very small. In the future, we will investigate the relationships between acoustic characteristics and perceptual impressions and the relationships between the naturalness of synthesized voices and the characteristics of perturbation. (C) 1999 Scripta Technica.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Utsunomiya Univ, Dept Elect &amp; Elect Engn, Utsunomiya 3218585, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Endo, Y (reprint author), Utsunomiya Univ, Dept Elect &amp; Elect Engn, Utsunomiya 3218585, Japan.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>DEC</td>
</tr>

<tr>
<td valign="top">PY </td><td>1999</td>
</tr>

<tr>
<td valign="top">VL </td><td>82</td>
</tr>

<tr>
<td valign="top">IS </td><td>12</td>
</tr>

<tr>
<td valign="top">BP </td><td>1</td>
</tr>

<tr>
<td valign="top">EP </td><td>12</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1002/(SICI)1520-6440(199912)82:12&lt;1::AID-ECJC1&gt;3.0.CO;2-2</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000081095100001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Arslan, LM</td>
</tr>

<tr>
<td valign="top">AF </td><td>Arslan, LM</td>
</tr>

<tr>
<td valign="top">TI </td><td>Speaker Transformation Algorithm using Segmental Codebooks (STASC)</td>
</tr>

<tr>
<td valign="top">SO </td><td>SPEECH COMMUNICATION</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; speaker transformation; codebook; line spectral
   frequencies; hidden Markov models; time-varying filter; overlap-add
   analysis</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION; NETWORKS</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a new voice conversion algorithm which modifies the utterance of a source speaker to sound-like speech from a target speaker. We refer to the method as Speaker Transformation Algorithm using Segmental Codebooks (STASC). A novel method is proposed which finds accurate alignments between source and target speaker utterances. Using the alignments, source speaker acoustic characteristics are mapped to target speaker acoustic characteristics. The acoustic parameters included in the mapping are vocal tract, excitation, intonation, energy, and duration characteristics. Informal listening tests suggest that convincing voice conversion is achieved while maintaining high speech quality. The performance of the proposed system is also evaluated on a simple Gaussian mixture model-based speaker identification system, and the results show that the transformed speech is assigned higher likelihood by the target speaker model when compared to the source speaker model. (C) 1999 Elsevier Science B.V. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Bogazici Univ, Elect &amp; Elect Dept, TR-80815 Bebek, Turkey.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Arslan, LM (reprint author), Bogazici Univ, Elect &amp; Elect Dept, TR-80815 Bebek, Turkey.</td>
</tr>

<tr>
<td valign="top">EM </td><td>arslanle@boun.edu.tr</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Arslan, Levent</display_name>&nbsp;</font></td><td><font size="3">D-6377-2015&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Arslan, Levent</display_name>&nbsp;</font></td><td><font size="3">0000-0002-6086-8018&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>82</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>94</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUL</td>
</tr>

<tr>
<td valign="top">PY </td><td>1999</td>
</tr>

<tr>
<td valign="top">VL </td><td>28</td>
</tr>

<tr>
<td valign="top">IS </td><td>3</td>
</tr>

<tr>
<td valign="top">BP </td><td>211</td>
</tr>

<tr>
<td valign="top">EP </td><td>226</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/S0167-6393(99)00015-1</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000081568100003</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Beland, R
   <br>Bois, M
   <br>Seron, X
   <br>Damien, B</td>
</tr>

<tr>
<td valign="top">AF </td><td>Beland, R
   <br>Bois, M
   <br>Seron, X
   <br>Damien, B</td>
</tr>

<tr>
<td valign="top">TI </td><td>Phonological spelling in a DAT patient: The role of the segmentation
   subsystem in the phoneme-to-grapheme conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>COGNITIVE NEUROPSYCHOLOGY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">ID </td><td>BETA-APHASIA BATTERY; ALZHEIMERS-DISEASE; SCHOOL EDUCATION; NORMATIVE
   DATA; AGRAPHIA; CONSTRAINTS; WRITTEN; SUBSET; SPEECH; LEVEL</td>
</tr>

<tr>
<td valign="top">AB </td><td>We are presenting a single-case study of a DAT patient whose writing output is severely impaired while performance in reading aloud and repetition is almost flawless. The large corpus of errors collected from written and oral spelling tasks shows two important characteristics: (1) in both tasks, OE relies on the non-lexical route for spelling and produces 'phonologically plausible errors" (PPEs) and non-phonologically plausible errors" (NPPEs), and (2) the proportion of NPPEs affecting four phonological features [+/- voiced], [+/- nasal], [a continuant], and [+/- rounded] is higher in written than in oral spelling. Analysis of PPEs and NPPEs reveals that the proportion of PPEs varies in inverse relation to the phonological complexity of the stimuli, i.e. fewer PPEs are produced in syllabically complex stimuli. According to our proposal, OE's functional lesion is localised in the segmentation subsystem of the phoneme-to-grapheme conversion mechanism. More specifically, OE suffers from a phonological impairment, that is, a lowered tolerance to syllabic complexity, which is exacerbated in any task, including phonological spelling, that requires an explicit segmentation of the auditory input form. A second deficit affecting the phonological working memory system is responsible for the production of the single feature errors. We suggest that the single feature errors are more abundant in written than in oral spelling because OE suffers from a deficit affecting the transfer from abstract graphemic representations to letter forms without affecting the transfer to letter names.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Inst Univ Geriat Montreal, Ctr Rech, Montreal, PQ H3W 1W5, Canada.
   <br>Univ Montreal, Montreal, PQ, Canada.
   <br>Univ Catholique Louvain, B-1348 Louvain, Belgium.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Beland, R (reprint author), Inst Univ Geriat Montreal, Ctr Rech, 4565 Chem Queen Mary, Montreal, PQ H3W 1W5, Canada.</td>
</tr>

<tr>
<td valign="top">EM </td><td>belandr@eoa.umontreal.ca</td>
</tr>

<tr>
<td valign="top">TC </td><td>9</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>9</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAR</td>
</tr>

<tr>
<td valign="top">PY </td><td>1999</td>
</tr>

<tr>
<td valign="top">VL </td><td>16</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>115</td>
</tr>

<tr>
<td valign="top">EP </td><td>155</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1080/026432999380924</td>
</tr>

<tr>
<td valign="top">SC </td><td>Psychology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000079201100002</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Fetsch, T
   <br>Burschel, G
   <br>Breithardt, G
   <br>Engberding, R
   <br>Koch, HP
   <br>Lukl, J
   <br>Trappe, HJ
   <br>Treese, N</td>
</tr>

<tr>
<td valign="top">AF </td><td>Fetsch, T
   <br>Burschel, G
   <br>Breithardt, G
   <br>Engberding, R
   <br>Koch, HP
   <br>Lukl, J
   <br>Trappe, HJ
   <br>Treese, N</td>
</tr>

<tr>
<td valign="top">TI </td><td>Antiarrhythmic drug therapy after DC cardioversion of chronic atrial
   fibrillation - rationale and design of the PAFAC trial</td>
</tr>

<tr>
<td valign="top">SO </td><td>ZEITSCHRIFT FUR KARDIOLOGIE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>atrial fibrillation; antiarrhythmic drugs; sotalol; quinidine; Tele-EGG
   recording; cardioversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>SINUS RHYTHM; TRANSESOPHAGEAL ECHOCARDIOGRAPHY; FOLLOW-UP; SOTALOL;
   QUINIDINE; MAINTENANCE; FRAMINGHAM; STROKE; CONVERSION; FLUTTER</td>
</tr>

<tr>
<td valign="top">AB </td><td>Atrial fibrillation (AF) is the most frequent cardiac arrhythmia. However, despite manifold publications reflecting numerous clinical trials about treatment of AF, the management of this arrhythmia is still under controversial. discussion, in daily clinical work as well as in research. The present study concentrates on three major questions: 1. How frequent are recurrences of AF in long-term follow-up? Most of the previous studies used the occurrence of symptoms as a surrogate parameter for recurrences of AF, despite the expected high rate of asymptomatic relapses. In the present study a daily transtelephonic ECG transmission enables a rhythm monitoring independent of symptoms. 2. Is the frequency of AF recurrences significantly reduced by antiarrhythmic medication? A direct comparison of class I and III antiarrhythmic drugs, which still are most frequently used for this indication,and of placebo will answer this question. 3. How safe is the long-term treatment for the prevention of AF recurrences with special respect to proarrhythmic effects? The daily transtelephonic ECG transmission enables a quantitative and qualitative monitoring of tachy- and bradyarrhythmias independent of symptoms. Additionally, the daily analysis of ECG measures may detect parameters predicting subsequent life threatening arrhythmias.
   <br>The study design provides a prospective, randomised, double-blind, placebo controlled, multicenter parallel group comparison. In Germany and in the Czech Republic about 90 hospitals will include 900 patients with documented chronic AF, age 18 to 80 years, if they are eligible for electrical cardioversion without concomitant antiarrhythmic drug therapy and if they are anticoagulated for at least three weeks prior to inclusion. Neither the size of the left atrium nor the duration of chronic AF are exclusion criteria. A few hours after successful electrical cardioversion the patients are randomised either to sotalol (2x 160 mg) or quinidine + verapamil (3x 160 mg 3x 80 mg) or placebo. Starting at the day after cardioversion, the patient is asked to record and transmit electrocardiograms of one minute duration at least once a day using his personal transtelephonic ECG recording unit (Tele-ECG recorder, credit card size), in case of symptoms as often as necessary. The ECGs can be transmitted at any time by any regular phone without additional equipment using a toll free number. A custom made, computer based, fully automated receiving centre is handling the patient calls interactively with voice control, including a voice recording of the patient's symptoms. The ECG tracings and the patient's voice messages are subsequently computer based analysed by experienced technicians. All ECG measures are stored in a database. In case of AF recurrence, any other relevant arrhythmia or additional abnormalities (e.g. QT prolongation) the correspondent hospital is immediately informed by fax. In case of AF recurrence, a subsequent Holter recording discriminates in paroxysmal and permanent AE Study medication is ended if either permanent AF or the third episode of paroxysmal AF are detected or after 12 months of follow-up. Regular follow-up visits are performed monthly. Major endpoints are the time to first recurrence of AF or the time to death, secondary parameters are the number of AF recurrences, the time to end of medication and AF related symptoms.
   <br>The recruitment started in the last days of 1996. Until the end of June 1998, 424 patients have been randomised. It is expected to end recruitment in spring 1999 and to close the study in spring 2000. Final results will be available in summer 2000.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Univ Munster, D-48129 Munster, Germany.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Fetsch, T (reprint author), Univ Munster, D-48129 Munster, Germany.</td>
</tr>

<tr>
<td valign="top">EM </td><td>fetsch@uni-muenster.de</td>
</tr>

<tr>
<td valign="top">TC </td><td>12</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>12</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAR</td>
</tr>

<tr>
<td valign="top">PY </td><td>1999</td>
</tr>

<tr>
<td valign="top">VL </td><td>88</td>
</tr>

<tr>
<td valign="top">IS </td><td>3</td>
</tr>

<tr>
<td valign="top">BP </td><td>195</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/s003920050276</td>
</tr>

<tr>
<td valign="top">SC </td><td>Cardiovascular System &amp; Cardiology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000079658400005</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Baker, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Baker, J</td>
</tr>

<tr>
<td valign="top">TI </td><td>Psychogenic dysphonia: Peeling back the layers</td>
</tr>

<tr>
<td valign="top">SO </td><td>JOURNAL OF VOICE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>psychogenic dysphonia; functional dysphonia; conversion dysphonia;
   hysterical dysphonia</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE DISORDER; SPEECH-THERAPY</td>
</tr>

<tr>
<td valign="top">AB </td><td>Resolution of psychogenic dysphonia is often quick and effortless for client and therapist alike. In such instances, the therapeutic interventions are simple and straightforward, insights are reached without difficulty, and once normal voice has been established, resumption of dysphonia or other psychosomatic symptoms rarely occurs. Sometimes, however, psychogenic dysphonia is extremely difficult to overcome, requiring considerable time, effort, and determination on the part of the client, coupled with confident, skilled persistence and psychotherapeutic insight from the therapist. In such cases one feels a sense of working through many complex layers before obtaining satisfactory voice or reaching an understanding of the psychogenic factors that precipitated onset and/or maintenance of the dysphonia.
   <br>Two cases that illustrate this involved process of peeling back the layers are presented. For resolution of severe psychogenic dysphonia, the therapist must be able to recognize and establish the complex relationship between the neurophysiological, intrapsychic, and interpersonal levels of function as they affect the client's voice and person, as a whole. This work requires considerable courage and skill on the part of the therapist to question, explore, change direction, and select alternative approaches. It is important that the problem can be resolved with a depth of understanding which is relevant for the client, and with due attention to the social context and wider systems of which he or she is a part.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Baker, J (reprint author), E Adelaide Med Ctr, 50 Hutt St, Adelaide, SA 5000, Australia.</td>
</tr>

<tr>
<td valign="top">TC </td><td>13</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>13</td>
</tr>

<tr>
<td valign="top">PD </td><td>DEC</td>
</tr>

<tr>
<td valign="top">PY </td><td>1998</td>
</tr>

<tr>
<td valign="top">VL </td><td>12</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">BP </td><td>527</td>
</tr>

<tr>
<td valign="top">EP </td><td>535</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/S0892-1997(98)80061-1</td>
</tr>

<tr>
<td valign="top">SC </td><td>Audiology &amp; Speech-Language Pathology; Otorhinolaryngology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000078387700014</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kubick, WR</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kubick, WR</td>
</tr>

<tr>
<td valign="top">TI </td><td>The elegant machine: Applying technology to optimize clinical trials</td>
</tr>

<tr>
<td valign="top">SO </td><td>DRUG INFORMATION JOURNAL</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>data management; electronic data capture (EDC); optical imaging;
   interactive voice response (IVR); data repository</td>
</tr>

<tr>
<td valign="top">AB </td><td>A new model for conducting clinical research programs in the future is beginning to evolve which will emphasize collaboration, interdependency, and close interactive sharing of information among the various stakeholders in the research process: patient investigator; sponsor project manager medical monitor data management organization, laboratories, biostatisticians, and regulatory authorities. This model will conserve the use of information by minimizing redundancies, transcriptions, and data conversions in a continuous dataflow, simplifying the data clarification process, and using the "best fit" technology tool for capturing and accessing clinical information. This View of clinical research will be based on a generic data repository based upon evolving International Conference on Harmonization/Food and Drug Administration (ICH/FDA) data standards which will also support trials management and safety monitoring with respect to the data repository The current state of alternative data capture technologies such as interactive voice response systems (IVRS), remote data entry (RDE), image recognition, Internet technologies, hand-held computers, voice recognition, and wearable monitoring devices will be reviewed.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>PAREXEL Int Corp, Informat Technol, Waltham, MA 02154 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kubick, WR (reprint author), PAREXEL Int Corp, Informat Technol, 195 West St, Waltham, MA 02154 USA.</td>
</tr>

<tr>
<td valign="top">TC </td><td>11</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>11</td>
</tr>

<tr>
<td valign="top">PD </td><td>OCT-DEC</td>
</tr>

<tr>
<td valign="top">PY </td><td>1998</td>
</tr>

<tr>
<td valign="top">VL </td><td>32</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">BP </td><td>861</td>
</tr>

<tr>
<td valign="top">EP </td><td>869</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1177/009286159803200402</td>
</tr>

<tr>
<td valign="top">SC </td><td>Health Care Sciences &amp; Services; Pharmacology &amp; Pharmacy</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000165531300002</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Cox, RV
   <br>Haskell, BG
   <br>Lecun, Y
   <br>Shahraray, B
   <br>Rabiner, L</td>
</tr>

<tr>
<td valign="top">AF </td><td>Cox, RV
   <br>Haskell, BG
   <br>Lecun, Y
   <br>Shahraray, B
   <br>Rabiner, L</td>
</tr>

<tr>
<td valign="top">TI </td><td>On the applications of multimedia processing to communications</td>
</tr>

<tr>
<td valign="top">SO </td><td>PROCEEDINGS OF THE IEEE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>AAC; access; agents; audio coding; cable modems; communications
   networks; content-based video sampling; document compression; fax
   coding; H.261; HDTV; image coding; image processing; JBIG; JPEG; media
   conversion; MPEG; multimedia; multimedia browsing; multimedia indexing;
   multimedia searching; optical character recognition; PAC; packet
   networks; perceptual coding; POTS telephony; quality of service; speech
   coding; speech compression; speech processing; speech recognition;
   speech synthesis; spoken language interface; spoken language
   understanding; standards; streaming; teleconferencing; video coding;
   video telephony</td>
</tr>

<tr>
<td valign="top">ID </td><td>RECOGNITION; TELECOMMUNICATIONS; MODELS; NOISE</td>
</tr>

<tr>
<td valign="top">AB </td><td>The challenge of multimedia processing is to provide services that seamlessly integrate text sound, image, and video information and to do it in a way that preserves the ease of use and interactivity of conventional plain old telephone service (POTS) telephony, irrelevant of the bandwidth or means of access of the connection to the sen,ice. To achieve this goal, there are a number of technological problems that must be considered, including:
   <br>compression and coding of multimedia signals, including algorithmic issues, standards issues, and transmission issues;
   <br>synthesis and recognition of multimedia signals, including speech, images, handwriting, and text;
   <br>organization, storage, aid retrieval of multimedia signals, including the appropriate method and speed of delivery (e.g., streaming versus full downloading), resolution (including layering or embedded versions of the signal), and quality of service, i.e., perceived quality of the resulting signal;
   <br>access methods to the multimedia signal (i.e., matching the user to the machine), including spoken natural language interfaces, agent interfaces, and media conversion tools;
   <br>searching (i.e., based on machine intelligence) by text, speech, and image queries;
   <br>browsing (i.e., based on human intelligence) by accessing the text, by voice, or by indexed images.
   <br>In each of these areas, a great deal of progress has been made in the past few years, driven in parr by the relentless growth in multimedia personal computers and in part by the promise of broad-band access from the home and from wireless connections. Standards have also played a key role in driving new multimedia services, both on the POTS network and on the Internet.
   <br>It is the purpose of this paper to review the status of the technology in each of the areas listed above and to illustrate current capabilities by describing several multimedia applications that have been implemented at AT&amp;T Labs over the past several years.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>AT&amp;T Bell Labs, Speech &amp; Image Proc Serv Res Lab, Florham Pk, NJ 07932
   USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Cox, RV (reprint author), AT&amp;T Bell Labs, Speech &amp; Image Proc Serv Res Lab, Florham Pk, NJ 07932 USA.</td>
</tr>

<tr>
<td valign="top">TC </td><td>23</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>23</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAY</td>
</tr>

<tr>
<td valign="top">PY </td><td>1998</td>
</tr>

<tr>
<td valign="top">VL </td><td>86</td>
</tr>

<tr>
<td valign="top">IS </td><td>5</td>
</tr>

<tr>
<td valign="top">BP </td><td>755</td>
</tr>

<tr>
<td valign="top">EP </td><td>824</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/5.664272</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000073345700002</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Stylianou, Y
   <br>Cappe, O
   <br>Moulines, E</td>
</tr>

<tr>
<td valign="top">AF </td><td>Stylianou, Y
   <br>Cappe, O
   <br>Moulines, E</td>
</tr>

<tr>
<td valign="top">TI </td><td>Continuous probabilistic transform for voice conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE TRANSACTIONS ON SPEECH AND AUDIO PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">ID </td><td>EM ALGORITHM; SPEAKER RECOGNITION; MAXIMUM-LIKELIHOOD; INDIVIDUALITY;
   FREQUENCY</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion, as considered in this paper, is defined as modifying the speech signal of one speaker (source speaker) so that it sounds as if it had been pronounced by a different speaker (target speaker), Our contribution includes the design of a new methodology for representing the relationship between two sets of spectral envelopes, The proposed method is based on the use of a Gaussian mixture model of the source speaker spectral envelopes, The conversion itself is represented by a continuous parametric function which takes into account the probabilistic classification provided by the mixture model. The parameters of the conversion function are estimated by least squares optimization on the training data, This conversion method is implemented in the context of the HNM (harmonic + noise model) system, which allows high-quality modifications of speech signals, Compared to earlier methods based on vector quantization, the proposed conversion scheme results in a much better match between the converted envelopes and the target envelopes, Evaluation by objective tests and formal listening tests shows that the proposed transform greatly improves the quality and naturalness of the converted speech signals compared with previous proposed conversion methods.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>AT&amp;T Bell Labs, Res, Murray Hill, NJ 07974 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Stylianou, Y (reprint author), AT&amp;T Bell Labs, Res, 600 Mt Ave, Murray Hill, NJ 07974 USA.</td>
</tr>

<tr>
<td valign="top">TC </td><td>525</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>551</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAR</td>
</tr>

<tr>
<td valign="top">PY </td><td>1998</td>
</tr>

<tr>
<td valign="top">VL </td><td>6</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>131</td>
</tr>

<tr>
<td valign="top">EP </td><td>142</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/89.661472</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000072118200004</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Rosenbower, TJ
   <br>Morris, JA
   <br>Eddy, VA
   <br>Ries, WR</td>
</tr>

<tr>
<td valign="top">AF </td><td>Rosenbower, TJ
   <br>Morris, JA
   <br>Eddy, VA
   <br>Ries, WR</td>
</tr>

<tr>
<td valign="top">TI </td><td>The long-term complications of percutaneous dilatational tracheostomy</td>
</tr>

<tr>
<td valign="top">SO </td><td>AMERICAN SURGEON</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article; Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>65th Annual Scientific Meeting and Postgraduate Course Program of the
   Southeastern-Surgical-Congress</td>
</tr>

<tr>
<td valign="top">CY </td><td>FEB 01-05, 1997</td>
</tr>

<tr>
<td valign="top">CL </td><td>NASHVILLE, TENNESSEE</td>
</tr>

<tr>
<td valign="top">ID </td><td>CRITICALLY ILL PATIENTS; INTENSIVE-CARE UNIT; DILATIONAL TRACHEOSTOMY;
   BEDSIDE PROCEDURE; TRACHEOTOMY; TRIAL</td>
</tr>

<tr>
<td valign="top">AB </td><td>Percutaneous dilatational tracheostomy was adopted at our institution, because it was demonstrated to be more cost effective than standard open tracheostomy in critically ill patients. The objective of this study was to evaluate the long-term outcome and complication rate of percutaneous dilatational tracheostomy in critically ill patients. We performed a consecutive case study of all Level I trauma patients from August 1991 to May 1994 who underwent percutaneous dilatational tracheostomy. All patients were prospectively evaluated by a standard questionnaire a minimum of 1 year after the procedure. All symptomatic patients were offered fiberoptic laryngoscopy. Descriptive statistical methods and the Student's T test were used to analyze the data. Of 7054 consecutive trauma admissions, 237 tracheostomies were performed. A total of 143 tracheostomies (60%) were open, and 95 (40%) were percutaneous. Of the 95 patients, 20 were lost to follow-up, 12 died from causes unrelated to the procedure, 6 had severe traumatic brain injuries and were unable to participate, and 2 patients required conversion to an open procedure. This left a study group of 55 patients. At a minimum of 1 year follow-up, 40 patients (73%) were asymptomatic. Of the 15 (27%) symptomatic patients, two patients had acute airway compromise after decannulation secondary to subglottic stenosis. Both were recannulated and subsequently decannulated uneventfully. Six patients declined fiberoptic laryngoscopy, because their symptoms were minimal (minor voice changes in three and intermittent hoarseness in three). Nine patients underwent fiberoptic laryngoscopy, and all examinations were normal. The mean cost of standard open tracheostomy at our institution is $1134 (58%) more than the mean cost of percutaneous dilatational tracheostomy. Of the study group patients undergoing percutaneous dilatational tracheostomy, 27 per cent complained of symptoms a minimum of 1 year posttracheostomy, Of these patients, 60 per cent underwent fiberoptic laryngoscopy, and no subglottic lesions were identified. Our findings suggest that percutaneous dilatational tracheostomy is a safe, cost-effective alternative to standard tracheostomy in critically ill patients.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Vanderbilt Univ, Sch Med, Dept Surg, Nashville, TN 37212 USA.
   <br>Vanderbilt Univ, Sch Med, Dept Otolaryngol, Nashville, TN 37212 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Rosenbower, TJ (reprint author), 1103 N Elm St, Greensboro, NC 27401 USA.</td>
</tr>

<tr>
<td valign="top">TC </td><td>47</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>47</td>
</tr>

<tr>
<td valign="top">PD </td><td>JAN</td>
</tr>

<tr>
<td valign="top">PY </td><td>1998</td>
</tr>

<tr>
<td valign="top">VL </td><td>64</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>82</td>
</tr>

<tr>
<td valign="top">EP </td><td>86</td>
</tr>

<tr>
<td valign="top">SC </td><td>Surgery</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000071667500028</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<tr><td>EF</td><td></td></tr></table>