<table border="0" cellpadding="2" cellspacing="0"><tr><td>FN</td><td>Clarivate Analytics Web of Science</td></tr><tr><td>VR</td><td>1.0</td></tr><style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Stoecklin, D</td>
</tr>

<tr>
<td valign="top">AF </td><td>Stoecklin, Daniel</td>
</tr>

<tr>
<td valign="top">TI </td><td>Freely Expressed Views: Methodological Challenges for the Right of the
   Child to be Heard</td>
</tr>

<tr>
<td valign="top">SO </td><td>CHILD INDICATORS RESEARCH</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article; Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Exploring the Global Well-Being of Children
   and Youth - Methodological Challenges and Practices of Undertaking
   Qualitative Research on Well-Being from Multinational Perspectives</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUL, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Tech Univ Berlin, Berlin, GERMANY</td>
</tr>

<tr>
<td valign="top">HO </td><td>Tech Univ Berlin</td>
</tr>

<tr>
<td valign="top">DE </td><td>Capabilities; Child; Methodological challenges; Participation; Right to
   be heard; Subjective well-being</td>
</tr>

<tr>
<td valign="top">AB </td><td>The methodological challenges in the research on children's subjective understandings of well-being are very close to the ones surrounding the implementation of the right of the child to be heard. Therefore, identification of the factors favouring or impeding children's freely expressed views on the one hand, and preliminary results of research on children's subjective well-being on the other hand, reciprocally inform each other. The right to be heard is approached from the perspective of capabilities (Stoecklin &amp; Bonvin 2014) identifying factors that are converting this formal freedom into real freedom. They highlight preliminary results of a qualitative study conducted in Switzerland along the procotol of the Child's Subjective Well-Being study (Hunner-Kreisel et al. 2016). The inclusion of a participative research tool, the actor's system (Stoecklin 2013), has allowed to concentrate on children's subjective understandings of their experience. The results are not analysed in terms of statistical representativeness, but rather in terms of &lt;&lt; structural &gt;&gt; features they allow to highlight. The endeavour is methodological. The analysis shows that language itself can be a conversion factor in the implementation of the right to be heard, and similarly a methodological trap inducing specific translations of children's voices into the official vocabulary of well-being. Research protocols should therefore be adapted to the evolving capacities of children, considering that well-being is not given state but rather a subjective feeling stemming from processual social dynamics in which children play a part, even when they have little voice.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Stoecklin, Daniel] Univ Geneva, Ctr Childrens Rights Studies, Valais
   Campus,POB 4176, CH-1950 Sion 4, Switzerland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Stoecklin, D (reprint author), Univ Geneva, Ctr Childrens Rights Studies, Valais Campus,POB 4176, CH-1950 Sion 4, Switzerland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>daniel.stoecklin@unige.ch</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PD </td><td>APR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2019</td>
</tr>

<tr>
<td valign="top">VL </td><td>12</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">SI </td><td>SI</td>
</tr>

<tr>
<td valign="top">BP </td><td>569</td>
</tr>

<tr>
<td valign="top">EP </td><td>588</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/s12187-018-9527-6</td>
</tr>

<tr>
<td valign="top">SC </td><td>Social Sciences - Other Topics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000460896300011</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Luo, ZJ
   <br>Chen, JH
   <br>Takiguchi, T
   <br>Ariki, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Luo, Zhaojie
   <br>Chen, Jinhui
   <br>Takiguchi, Tetsuya
   <br>Ariki, Yasuo</td>
</tr>

<tr>
<td valign="top">TI </td><td>Neutral-to-emotional voice conversion with cross-wavelet transform F0
   using generative adversarial networks</td>
</tr>

<tr>
<td valign="top">SO </td><td>APSIPA TRANSACTIONS ON SIGNAL AND INFORMATION PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Continuous wavelet transform; Emotional voice conversion; Generative
   adversarial networks; Variational autoencoder; F0 features</td>
</tr>

<tr>
<td valign="top">ID </td><td>COHERENCE</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we propose a novel neutral-to-emotional voice conversion (VC) model that can effectively learn a mapping from neutral to emotional speech with limited emotional voice data. Although conventional VC techniques have achieved tremendous success in spectral conversion, the lack of representations in fundamental frequency (F0), which explicitly represents prosody information, is still a major limiting factor for emotional VC. To overcome this limitation, in our proposed model, we outline the practical elements of the cross-wavelet transform (XWT) method, highlighting how such a method is applied in synthesizing diverse representations of F0 features in emotional VC. The idea is (1) to decompose F0 into different temporal level representations using continuous wavelet transform (CWT); (2) to use XWT to combine different CWT-F0 features to synthesize interaction XWT-F0 features; (3) and then use both the CWT-F0 and corresponding XWT-F0 features to train the emotional VC model. Moreover, to better measure similarities between the converted and real F0 features, we applied a VA-GAN training model, which combines a variational autoencoder (VAE) with a generative adversarial network (GAN). In the VA-GAN model, VAE learns the latent representations of high-dimensional features (CWT-F0, XWT-F0), while the discriminator of the GAN can use the learned feature representations as a basis for a VAE reconstruction objective.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Luo, Zhaojie; Takiguchi, Tetsuya; Ariki, Yasuo] Kobe Univ, Grad Sch
   Syst Informat, Nada Ku, 1-1 Rokkodai, Kobe, Hyogo 6578501, Japan.
   <br>[Chen, Jinhui] Kobe Univ, RIEB, Nada Ku, 2-1 Rokkodai, Kobe, Hyogo
   6578501, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Luo, ZJ (reprint author), Kobe Univ, Grad Sch Syst Informat, Nada Ku, 1-1 Rokkodai, Kobe, Hyogo 6578501, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>luozhaojie@me.cs.scitec.kobe-u.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAR 4</td>
</tr>

<tr>
<td valign="top">PY </td><td>2019</td>
</tr>

<tr>
<td valign="top">VL </td><td>8</td>
</tr>

<tr>
<td valign="top">AR </td><td>e10</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1017/ATSIP.2019.3</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000460167900001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ben Othmane, I
   <br>Di Martino, J
   <br>Ouni, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ben Othmane, Imen
   <br>Di Martino, Joseph
   <br>Ouni, Kais</td>
</tr>

<tr>
<td valign="top">TI </td><td>Enhancement of esophageal speech obtained by a voice conversion
   technique using time dilated Fourier cepstra</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERNATIONAL JOURNAL OF SPEECH TECHNOLOGY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Esophageal speech; Voice conversion; Deep neural networks; Time dilation
   algorithm; Noise reduction; Excitation and phase; Gaussian mixture model</td>
</tr>

<tr>
<td valign="top">ID </td><td>SIGNAL ESTIMATION; NEURAL-NETWORKS; TRACHEOESOPHAGEAL; JUDGMENTS</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a novel speaking-aid system for enhancing esophageal speech (ES). The method adopted in this paper aims to improve the quality of esophageal speech using a combination of a voice conversion technique and a time dilation algorithm. In the proposed system, a Deep Neural Network (DNN) is used as a nonlinear mapping function for vocal tract vector transformation. Then the converted frames are used to determine realistic excitation and phase vectors from the target training space using a frame selection algorithm. Next, in order to preserve speaker identity of the esophageal speakers, we use the source vocal tract features and propose to apply on them a time dilation algorithm to reduce the unpleasant esophageal noises. Finally the converted speech is reconstructed using the dilated source vocal tract frames and the predicted excitation and phase. DNN and Gaussian mixture model (GMM) based voice conversion systems have been evaluated using objective and subjective measures. Such an experimental study has been realized also in order to evaluate the changes in speech quality and intelligibility of the transformed signals. Experimental results demonstrate that the proposed methods provide considerable improvement in intelligibility and naturalness of the converted esophageal speech.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ben Othmane, Imen; Ouni, Kais] ENICarthage Univ Carthage, Natl Engn Sch
   Carthage, Res Unit Signals &amp; Mechatron Syst, SMS,UR13ES49, Carthage,
   Tunisia.
   <br>[Di Martino, Joseph] Loria Lab Lorrain Rech Informat &amp; Applicat, BP 239,
   F-54506 Vandoeuvre Les Nancy, France.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ben Othmane, I (reprint author), ENICarthage Univ Carthage, Natl Engn Sch Carthage, Res Unit Signals &amp; Mechatron Syst, SMS,UR13ES49, Carthage, Tunisia.</td>
</tr>

<tr>
<td valign="top">EM </td><td>imen.benothmen@hotmail.fr; joseph.di-martino@loria.fr;
   kais.ouni@enicarthage.rnu.tn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2019</td>
</tr>

<tr>
<td valign="top">VL </td><td>22</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>99</td>
</tr>

<tr>
<td valign="top">EP </td><td>110</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/s10772-018-09579-1</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000460136300010</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Zhang, JX
   <br>Ling, ZH
   <br>Liu, LJ
   <br>Jiang, Y
   <br>Dai, LR</td>
</tr>

<tr>
<td valign="top">AF </td><td>Zhang, Jing-Xuan
   <br>Ling, Zhen-Hua
   <br>Liu, Li-Juan
   <br>Jiang, Yuan
   <br>Dai, Li-Rong</td>
</tr>

<tr>
<td valign="top">TI </td><td>Sequence-to-Sequence Acoustic Modeling for Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; sequence-to-sequence; attention; Mel-spectrogram</td>
</tr>

<tr>
<td valign="top">ID </td><td>NEURAL-NETWORKS</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, a neural network named sequence-to-sequence ConvErsion NeTwork (SCENT) is presented for acoustic modeling in voice conversion. At training stage, a SCENT model is estimated by aligning the feature sequences of source and target speakers implicitly using attention mechanism. At the conversion stage, acoustic features and durations of source utterances are converted simultaneously using the unified acoustic model. Mel-scale spectrograms are adopted as acoustic features, which contain both excitation and vocal tract descriptions of speech signals. The bottleneck features extracted from source speech using an automatic speech recognition model are appended as an auxiliary input. A WaveNet vocoder conditioned on Mel-spectrograms is built to reconstruct waveforms from the outputs of the SCENT model. It is worth noting that our proposed method can achieve appropriate duration conversion, which is difficult in conventional methods. Experimental results show that our proposed method obtained better objective and subjective performance than the baseline methods using Gaussian mixture models and deep neural networks as acoustic models. This proposed method also outperformed our previous work, which achieved the top rank in Voice Conversion Challenge 2018. Ablation tests further confirmed the effectiveness of several components in our proposed method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Zhang, Jing-Xuan; Ling, Zhen-Hua; Jiang, Yuan; Dai, Li-Rong] Univ Sci &amp;
   Technol China, Natl Engn Lab Speech &amp; Language Informat Proc, Hefei
   230027, Anhui, Peoples R China.
   <br>[Liu, Li-Juan; Jiang, Yuan] iFLYTEK Co Ltd, Hefei 230088, Anhui, Peoples
   R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ling, ZH (reprint author), Univ Sci &amp; Technol China, Natl Engn Lab Speech &amp; Language Informat Proc, Hefei 230027, Anhui, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>nosisi@mail.ustc.edu.cn; zhling@ustc.edu.cn; ljliu@iflytek.com;
   yuanjiang@iflytek.com; lrdai@ustc.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2019</td>
</tr>

<tr>
<td valign="top">VL </td><td>27</td>
</tr>

<tr>
<td valign="top">IS </td><td>3</td>
</tr>

<tr>
<td valign="top">BP </td><td>631</td>
</tr>

<tr>
<td valign="top">EP </td><td>644</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASLP.2019.2892235</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000457913900002</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hashimoto, T
   <br>Saito, D
   <br>Minematsu, N</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hashimoto, Tetsuya
   <br>Saito, Daisuke
   <br>Minematsu, Nobuaki</td>
</tr>

<tr>
<td valign="top">TI </td><td>Many-to-Many and Completely Parallel-Data-Free Voice Conversion Based on
   Eigenspace DNN</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; deep neural network; Gaussian mixture models;
   many-to-many conversion; eigenvoice; parallel-data-free</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH RECOGNITION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Media conversion of image, text, speech, etc., generally requires a large amount of parallel data for training a conversion model. Recently, methods for training the model using no or a small amount of parallel data draw researchers' attention. In many-to-many voice conversion, since it is often hard to collect parallel data from every pair of speakers, the conversion models requiring no parallel data are desired. Conventional many-to-many voice conversion models required a large amount of prestored parallel data to acquire prior knowledge of the entire speaker space. Then, a specific model from an arbitrary speaker to another can he realized by adapting a few model parameters. Although these conversion models certainly do not use parallel data in an adaptation step, they still use parallel data for prior training. In this study, we aim at realizing completely parallel-data-free and many-to-many voice conversion. The proposed method uses both Eigenvoice Gaussian mixture models (EVGMM) and Deep neural network (DNN). EVGMM is a many-to-many conversion model that constructs the entire speaker space (called eigenspace) by analyzing mean vectors of Gaussian mixture models and it is used in our method to decompose training speakers' features into their eigenspace components. By using the speaker features and the obtained components as pseudo parallel data, multiple DNNs are trained to realize conversion between them. With these DNNs, features of any target speaker can be represented by a weighted sum of the components. It should be noted that all the processes of our proposal do not require any parallel data. A key technique is to estimate covariance terms of EVGMM with no parallel data. Experiments indicate that individuality scores of the proposed method using no parallel data are comparable enough to those of a baseline system trained with parallel data.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Hashimoto, Tetsuya] Toyota Motor Co Ltd, Toyota, Aichi 4718571, Japan.
   <br>[Saito, Daisuke; Minematsu, Nobuaki] Univ Tokyo, Grad Sch Engn, Tokyo
   1138656, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hashimoto, T (reprint author), Toyota Motor Co Ltd, Toyota, Aichi 4718571, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>hashib@gavo.t.u-tokyo.ac.jp; dsk_saito@gavo.t.u-tokyo.ac.jp;
   mine@gavo.t.u-tokyo.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>FEB</td>
</tr>

<tr>
<td valign="top">PY </td><td>2019</td>
</tr>

<tr>
<td valign="top">VL </td><td>27</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>332</td>
</tr>

<tr>
<td valign="top">EP </td><td>341</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASLP.2018.2878949</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000451966300008</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Gao, SH
   <br>Wu, XL
   <br>Xiang, C
   <br>Huang, DY</td>
</tr>

<tr>
<td valign="top">AF </td><td>Gao, Shuhua
   <br>Wu, Xiaoling
   <br>Xiang, Cheng
   <br>Huang, Dongyan</td>
</tr>

<tr>
<td valign="top">TI </td><td>Development of a computationally efficient voice conversion system on
   mobile phones</td>
</tr>

<tr>
<td valign="top">SO </td><td>APSIPA TRANSACTIONS ON SIGNAL AND INFORMATION PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; GMM; Mobile application; Parallel computing; Weighted
   frequency warping</td>
</tr>

<tr>
<td valign="top">ID </td><td>NEURAL-NETWORKS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion aims to change a source speaker's voice to make it sound like the one of a target speaker while preserving linguistic information. Despite the rapid advance of voice conversion algorithms in the last decade, most of them are still too complicated to be accessible to the public. With the popularity of mobile devices especially smart phones, mobile voice conversion applications are highly desirable such that everyone can enjoy the pleasure of high-quality voice mimicry and people with speech disorders can also potentially benefit from it. Due to the limited computing resources on mobile phones, the major concern is the time efficiency of such a mobile application to guarantee positive user experience. In this paper, we detail the development of a mobile voice conversion system based on the Gaussian mixture model (GMM) and the weighted frequency warping methods. We attempt to boost the computational efficiency by making the best of hardware characteristics of today's mobile phones, such as parallel computing on multiple cores and the advanced vectorization support. Experimental evaluation results indicate that our system can achieve acceptable voice conversion performance while the conversion time for a five-second sentence only takes slightly more than one second on iPhone 7.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Gao, Shuhua; Wu, Xiaoling; Xiang, Cheng] Natl Univ Singapore, Dept
   Elect &amp; Comp Engn, Singapore, Singapore.
   <br>[Gao, Shuhua; Wu, Xiaoling; Huang, Dongyan] ASTAR, Inst Infocomm Res,
   Human Language Technol Dept, Singapore, Singapore.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Huang, DY (reprint author), ASTAR, Inst Infocomm Res, Human Language Technol Dept, Singapore, Singapore.</td>
</tr>

<tr>
<td valign="top">EM </td><td>huang@i2r.a-star.edu.sg</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>JAN 4</td>
</tr>

<tr>
<td valign="top">PY </td><td>2019</td>
</tr>

<tr>
<td valign="top">VL </td><td>8</td>
</tr>

<tr>
<td valign="top">AR </td><td>e4</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1017/ATSIP.2018.23</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000454901200002</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Miao, XK
   <br>Zhang, XW
   <br>Sun, M
   <br>Zheng, CY
   <br>Cao, TY</td>
</tr>

<tr>
<td valign="top">AF </td><td>Miao, Xiaokong
   <br>Zhang, Xiongwei
   <br>Sun, Meng
   <br>Zheng, Changyan
   <br>Cao, Tieyong</td>
</tr>

<tr>
<td valign="top">TI </td><td>A BLSTM and WaveNet-Based Voice Conversion Method With Waveform Collapse
   Suppression by Post-Processing</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE ACCESS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; speech synthesis; BLSTM; WaveNet</td>
</tr>

<tr>
<td valign="top">ID </td><td>NEURAL-NETWORKS</td>
</tr>

<tr>
<td valign="top">AB </td><td>In recent years, neural network-based voice conversion methods have been rapidly developed, and many different models and neural networks have been applied in parallel voice conversion. However, the over-smoothing of parametric methods [e.g., bidirectional long short-term memory (BLSTM)] and the waveform collapse of neural vocoders (e.g., WaveNet) still have negative impacts on the quality of the converted voices. To overcome this problem, we propose a BLSTM and WaveNet-based voice conversion method cooperated with waveform collapse suppression by post-processing. This method firstly uses BLSTM to convert the acoustic features between parallel speakers, and then synthesizes pre-converted voice with WaveNet. Subsequently, several alternative iterations of BLSTM post-processing is performed, and the final converted voice is generated by WaveNet. The proposed method can directly generate converted audio waveforms and avoid the waveform-collapsed speech caused by a single WaveNet generation effectively. The experimental results indicate that acoustic features trained by using the BLSTM network could achieve better results than conventional baselines. From our experiments on VCC2018, the usage of WaveNet could alleviate the problem of over-smoothing, which contributes to improving the similarity and naturalness of the final results of voice conversion.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Miao, Xiaokong; Zhang, Xiongwei; Sun, Meng; Zheng, Changyan; Cao,
   Tieyong] Army Engn Univ, Lab Intelligent Informat Proc, Nanjing 210007,
   Jiangsu, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Zhang, XW; Sun, M (reprint author), Army Engn Univ, Lab Intelligent Informat Proc, Nanjing 210007, Jiangsu, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>xwzhang9898@163.com; sunmengccjs@gmail.com</td>
</tr>

<tr xmlns:date="http://exslt.org/dates-and-times">
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Sun, Meng</display_name>&nbsp;</font></td><td><font size="3">0000-0002-7435-3752&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Zheng, Changyan</display_name>&nbsp;</font></td><td><font size="3">0000-0002-2088-9308&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2019</td>
</tr>

<tr>
<td valign="top">VL </td><td>7</td>
</tr>

<tr>
<td valign="top">BP </td><td>54321</td>
</tr>

<tr>
<td valign="top">EP </td><td>54329</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/ACCESS.2019.2912926</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000467040000001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Vijayan, K
   <br>Li, HZ
   <br>Toda, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Vijayan, Karthika
   <br>Li, Haizhou
   <br>Toda, Tomoki</td>
</tr>

<tr>
<td valign="top">TI </td><td>Speech-to-Singing Voice Conversion The challenges and strategies for
   improving vocal conversion processes</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE SIGNAL PROCESSING MAGAZINE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Vijayan, Karthika; Li, Haizhou] Natl Univ Singapore, Dept Elect &amp; Comp
   Engn, Singapore, Singapore.
   <br>[Vijayan, Karthika; Li, Haizhou] Int Speech Commun Assoc, Singapore,
   Singapore.
   <br>[Vijayan, Karthika; Li, Haizhou] Asia Pacific Signal &amp; Informat Proc
   Assoc, Hong Kong, Peoples R China.
   <br>[Toda, Tomoki] Nagoya Univ, Informat Technol Ctr, Nagoya, Aichi, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Vijayan, K (reprint author), Natl Univ Singapore, Dept Elect &amp; Comp Engn, Singapore, Singapore.</td>
</tr>

<tr>
<td valign="top">EM </td><td>vijayan.karthika@nus.edu.sg; haizhou.li@nus.edu.sg;
   tomoki@icts.nagoya-u.ac.jp</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Vijayan, Karthika</display_name>&nbsp;</font></td><td><font size="3">Y-4001-2018&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Li, Haizhou</display_name>&nbsp;</font></td><td><font size="3">Q-6438-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Vijayan, Karthika</display_name>&nbsp;</font></td><td><font size="3">0000-0001-7281-1329&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Li, Haizhou</display_name>&nbsp;</font></td><td><font size="3">0000-0001-9158-9401&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>JAN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2019</td>
</tr>

<tr>
<td valign="top">VL </td><td>36</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>95</td>
</tr>

<tr>
<td valign="top">EP </td><td>102</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/MSP.2018.2875195</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000455089800012</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Bao, JY
   <br>Xu, N</td>
</tr>

<tr>
<td valign="top">AF </td><td>Bao, Jingyi
   <br>Xu, Ning</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice conversion based on Gaussian processes by using kernels modeling
   the spectral density with Gaussian mixture models</td>
</tr>

<tr>
<td valign="top">SO </td><td>MODERN PHYSICS LETTERS B</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article; Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>3rd International Conference on Materials Science and Nanotechnology
   (ICMSNT)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 29-APR 01, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Chengdu, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; Gaussian process; flexible kernel; Gaussian mixture
   model</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion (VC) is a technique that aims to transform the individuality of a source speech so as to mimic that of a target speech while keeping the message unaltered. In our previous work, Gaussian process (GP) was introduced into the literature of VC for the first time, for the sake of overcoming the "over-fitting" problem inherent in the state-of-the-art VC methods, which gives very promising results. However, standard GP usually acts as somewhat a smoothing device more than a universal approximator. In this paper, we further attempt to improve the flexibility of GP-based VC by resorting to the expressive kernels that are derived to model the spectral density with Gaussian mixture model (GMM). Our new method benefits from the expressiveness of the new kernel while the inference of GP remains simple and analytic as usual. Experiments demonstrate both objectively and subjectively that the individualities of the converted speech are much more closer to those of the target while speech quality obtained is comparable to the standard GP-based method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Bao, Jingyi] Changzhou Inst Technol, Sch Elect Informat &amp; Elect Engn,
   Liaohe Rd 666, Changzhou City 213032, Peoples R China.
   <br>[Xu, Ning] Hohai Univ, Coll IoT Engn, Dept Commun Engn, North Jinling Rd
   200, Changzhou City 213022, Peoples R China.
   <br>[Xu, Ning] Hohai Univ, Coll IoT Engn, Changzhou Key Lab Robot &amp;
   Intelligent Technol, North Jinling Rd 200, Changzhou City 213022,
   Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Bao, JY (reprint author), Changzhou Inst Technol, Sch Elect Informat &amp; Elect Engn, Liaohe Rd 666, Changzhou City 213032, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>baojy@czu.cn; 20101832@hhu.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>DEC 30</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">VL </td><td>32</td>
</tr>

<tr>
<td valign="top">IS </td><td>34-36</td>
</tr>

<tr>
<td valign="top">SI </td><td>SI</td>
</tr>

<tr>
<td valign="top">AR </td><td>1840096</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1142/S0217984918400961</td>
</tr>

<tr>
<td valign="top">SC </td><td>Physics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000456014100039</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Zhao, YX
   <br>Kuruvilla-Dugdale, M
   <br>Song, MG</td>
</tr>

<tr>
<td valign="top">AF </td><td>Zhao, Yunxin
   <br>Kuruvilla-Dugdale, Mili
   <br>Song, Minguang</td>
</tr>

<tr>
<td valign="top">TI </td><td>Structured Sparse Spectral Transforms and Structural Measures for Voice
   Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; structured sparse spectral transform; NMF; frequency
   warping; objective measures</td>
</tr>

<tr>
<td valign="top">ID </td><td>REPRESENTATION; STRAIGHT</td>
</tr>

<tr>
<td valign="top">AB </td><td>We investigate a structured sparse spectral transform method for voice conversion (VC) to perform frequency warping and spectral shaping simultaneously on high-dimensional (D) STRAIGHT spectra. Learning a large transform matrix for high-D data often results in an overfit matrix with low sparsity, which leads to muffled speech in VC. We address this problem by using the frequency-warping characteristic of a source-target speaker pair to define a region of support (ROS) in a transform matrix, and further optimize it by nonnegative matrix factorization (NMF) to obtain structured sparse transform. We also investigate structural measures of spectral and temporal covariance and variance at different scales for assessing VC speech quality. Our experiments on ARCTIC dataset of 12 speaker pairs show that embedding the ROS in spectral transforms offers flexibility in tradeoffs between spectral distortion and structure preservation, and the structural measures provide quantitatively reasonable results on converted speech. Our subjective listening tests show that the proposed VC method achieves a mean opinion score of "very good" relative to natural speech, and in comparison with three other VC methods, it is the most preferred one in naturalness and in voice similarity to target speakers.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Zhao, Yunxin; Song, Minguang] Univ Missouri, Dept Elect Engn &amp; Comp
   Sci, Columbia, MO 65211 USA.
   <br>[Kuruvilla-Dugdale, Mili] Univ Missouri, Dept Commun Sci &amp; Disorders,
   Columbia, MO 65211 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Zhao, YX (reprint author), Univ Missouri, Dept Elect Engn &amp; Comp Sci, Columbia, MO 65211 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>zhaoy@missouri.edu; kuruvillam@health.missouri.edu;
   msong@mail.missouri.edu</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Kuruvilla-Dugdale, Mili</display_name>&nbsp;</font></td><td><font size="3">0000-0001-7900-2364&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>DEC</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">VL </td><td>26</td>
</tr>

<tr>
<td valign="top">IS </td><td>12</td>
</tr>

<tr>
<td valign="top">BP </td><td>2267</td>
</tr>

<tr>
<td valign="top">EP </td><td>2276</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASLP.2018.2860682</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000446325200002</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hwang, HT
   <br>Wu, YC
   <br>Peng, YH
   <br>Hsu, CC
   <br>Tsao, Y
   <br>Wang, HM
   <br>Wang, YR
   <br>Chen, SH</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hwang, Hsin-Te
   <br>Wu, Yi-Chiao
   <br>Peng, Yu-Huai
   <br>Hsu, Chin-Cheng
   <br>Tsao, Yu
   <br>Wang, Hsin-Min
   <br>Wang, Yih-Ru
   <br>Chen, Sin-Horng</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Conversion Based on Locally Linear Embedding</td>
</tr>

<tr>
<td valign="top">SO </td><td>JOURNAL OF INFORMATION SCIENCE AND ENGINEERING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; locally linear embedding; exemplar-based; many-to-one;
   manifold learning</td>
</tr>

<tr>
<td valign="top">ID </td><td>NONNEGATIVE MATRIX FACTORIZATION; NEURAL-NETWORKS; REPRESENTATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a novel locally linear embedding (LLE)-based framework for exemplar -based spectral conversion (SC). The key feature of the proposed SC framework is that it integrates the LLE algorithm, a manifold learning method, with the conventional exemplar -based SC method. One important advantage of the LLE-based SC framework is that it can be applied to either one-to-one SC or many-to-one SC. For one-to-one SC, a parallel speech corpus consisting of the pre-specified source and target speakers' speeches is used to construct the paired source and target dictionaries in advance. During online conversion, the LLE-based SC method converts the source spectral features to the target like spectral features based on the paired dictionaries. On the other hand, when applied to many-to-one SC, our system is capable of converting the voice of any unseen source speaker to that of a desired target speaker, without the requirement of collecting parallel training speech utterances from them beforehand. To further improve the quality of the converted speech, the maximum likelihood parameter generation (MLPG) and global variance (GV) methods are adopted in the proposed SC systems. Experimental results demonstrate that the proposed one-to-one SC system is comparable with the state-of-the-art Gaussian mixture model (GMM)-based one-to-one SC system in terms of speech quality and speaker similarity, and the many-to-one SC system can approximate the performance of the one-to-one SC system.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Hwang, Hsin-Te; Wu, Yi-Chiao; Peng, Yu-Huai; Hsu, Chin-Cheng; Wang,
   Hsin-Min] Acad Sinica, Inst Informat Sci, Taipei 115, Taiwan.
   <br>[Tsao, Yu] Acad Sinica, Res Ctr Informat Technol Innovat, Taipei 115,
   Taiwan.
   <br>[Hwang, Hsin-Te; Wang, Yih-Ru; Chen, Sin-Horng] Natl Chiao Tung Univ,
   Dept Elect &amp; Comp Engn, Hsinchu 300, Taiwan.
   <br>[Peng, Yu-Huai] Natl Tsing Hua Univ, Dept Elect Engn, Hsinchu 300,
   Taiwan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hwang, HT (reprint author), Acad Sinica, Inst Informat Sci, Taipei 115, Taiwan.; Hwang, HT (reprint author), Natl Chiao Tung Univ, Dept Elect &amp; Comp Engn, Hsinchu 300, Taiwan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>hwanght@iis.sinica.edu.tw; tedwu@iis.sinica.edu.tw;
   roland19930601@gmail.com; jeremycchsu@iis.sinica.edu.tw;
   yu.tsao@citi.sinica.edu.tw; whm@iis.sinica.edu.tw;
   yrwang@mail.nctu.edu.tw; schen@mail.nctu.edu.tw</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>NOV</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">VL </td><td>34</td>
</tr>

<tr>
<td valign="top">IS </td><td>6</td>
</tr>

<tr>
<td valign="top">BP </td><td>1493</td>
</tr>

<tr>
<td valign="top">EP </td><td>1516</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.6688/JISE.201811_34(6).0008</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000451364100008</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Xue, YW
   <br>Hamada, Y
   <br>Akagi, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Xue, Yawen
   <br>Hamada, Yasuhiro
   <br>Akagi, Masato</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice conversion for emotional speech: Rule-based synthesis with degree
   of emotion controllable in dimensional space</td>
</tr>

<tr>
<td valign="top">SO </td><td>SPEECH COMMUNICATION</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Emotional voice conversion; Rule-based speech synthesis; Emotion
   dimension; Three-layered model; Fujisaki F0 model; Target prediction
   model</td>
</tr>

<tr>
<td valign="top">ID </td><td>PERCEPTION; MODEL; COMMUNICATION; EXTRACTION; LANGUAGE; QUALITY; SYSTEM</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper proposes a rule-based voice conversion system for emotion which is capable of converting neutral speech to emotional speech using dimensional space (arousal and valence) to control the degree of emotion on a continuous scale. We propose an inverse three-layered model with acoustic features as output at the top layer, semantic primitives at the middle layer and emotion dimension as input at the bottom layer; an adaptive-based fuzzy inference system acts as connectors to extract the non-linear rules among the three layers. The rules are applied by modifying the acoustic features of neutral speech to create the different types of emotional speech. The prosody-related acoustic features of F0 and power envelope are parameterized using the Fujisaki model and target prediction model separately. Perceptual evaluation results show that the degree of emotion can be perceived well in the dimensional space of valence and arousal.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Xue, Yawen; Hamada, Yasuhiro; Akagi, Masato] Japan Adv Inst Sci &amp;
   Technol, Sch Informat Sci, 1-1 Asahidai, Nomi, Ishikawa 9231292, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Xue, YW (reprint author), Japan Adv Inst Sci &amp; Technol, Sch Informat Sci, 1-1 Asahidai, Nomi, Ishikawa 9231292, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>xue_yawen@jaist.ac.jp; y-hamada@jaist.ac.jp; akagi@jaist.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>SEP</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">VL </td><td>102</td>
</tr>

<tr>
<td valign="top">BP </td><td>54</td>
</tr>

<tr>
<td valign="top">EP </td><td>67</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.specom.2018.06.006</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000455419300005</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Meng, YF
   <br>Zhao, JQ
   <br>Yan, XX
   <br>Zhao, CL
   <br>Qin, SS
   <br>Cho, JH
   <br>Zhang, C
   <br>Sun, QJ
   <br>Wang, ZL</td>
</tr>

<tr>
<td valign="top">AF </td><td>Meng, Yanfang
   <br>Zhao, Junqing
   <br>Yan, XiXi
   <br>Zhao, Chunlin
   <br>Qin, Shanshan
   <br>Cho, Jeong Ho
   <br>Zhang, Chi
   <br>Sun, Qijun
   <br>Wang, Zhong Lin</td>
</tr>

<tr>
<td valign="top">TI </td><td>Mechanosensation-Active Matrix Based on Direct-Contact Tribotronic
   Planar Graphene Transistor Array</td>
</tr>

<tr>
<td valign="top">SO </td><td>ACS NANO</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>electronic skin; graphene transistor; direct-contact tribotronic
   devices; mechanosensation; triboelectric nanogenerator</td>
</tr>

<tr>
<td valign="top">ID </td><td>FIELD-EFFECT TRANSISTORS; PRESSURE SENSOR MATRIX; TRIBOELECTRIC
   NANOGENERATORS; ARTIFICIAL SKIN; CARBON NANOTUBE; OUTPUT POWER; ENERGY;
   TRANSPARENT; ELECTRONICS; POLYMER</td>
</tr>

<tr>
<td valign="top">AB </td><td>Mechanosensitive electronics aims at replicating the multifunctions of human skin to realize quantitative conversion of external stimuli into electronic signals and provide corresponding feedback instructions. Here, we report a mechanosensation-active matrix based on a direct-contact tribotronic planar graphene transistor array. Ion gel is utilized as both the dielectric in the graphene transistor and the friction layer for triboelectric potential coupling to achieve highly efficient gating and sensation properties. Different contact distances between the ion gel and other friction materials produce different triboelectric potentials, which are directly coupled to the graphene channel and lead to different output signals through modulating the Fermi level of graphene. Based on this mechanism, the tribotronic graphene transistor is capable of sensing approaching distances, recognizing the category of different materials, and even distinguishing voices. It possesses excellent sensing properties, including high sensitivity (0.16 mm(-1)), fast response time (similar to 15 ms), and excellent durability (over 1000 cycles). Furthermore, the fabricated mechanosensation-active matrix is demonstrated to sense spatial contact distances and visualize a 2D color mapping of the target object. The tribotronic active matrix with ion gel as dielectric/friction layer provides a route for efficient and low-power-consuming mechanosensation in a noninvasive fashion. It is of great significance in multifunction sensory systems, wearable human-machine interactive interfaces, artificial electronic skin, and future telemedicine for patient surveillance.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Meng, Yanfang; Zhao, Junqing; Yan, XiXi; Zhao, Chunlin; Qin, Shanshan;
   Zhang, Chi; Sun, Qijun; Wang, Zhong Lin] Chinese Acad Sci, Beijing Inst
   Nanoenergy &amp; Nanosyst, Beijing 100083, Peoples R China.
   <br>[Meng, Yanfang; Zhao, Junqing; Yan, XiXi; Zhao, Chunlin; Qin, Shanshan;
   Zhang, Chi; Sun, Qijun; Wang, Zhong Lin] Univ Chinese Acad Sci, Sch
   Nanosci &amp; Technol, Beijing 100049, Peoples R China.
   <br>[Meng, Yanfang; Zhao, Junqing; Yan, XiXi; Zhao, Chunlin; Qin, Shanshan]
   Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   <br>[Cho, Jeong Ho] Sungkyunkwan Univ, Sch Chem Engn, SKKU Adv Inst
   Nanotechnol SAINT, Suwon 440746, South Korea.
   <br>[Zhang, Chi; Sun, Qijun; Wang, Zhong Lin] Guangxi Univ, Ctr Nanoenergy
   Res, Sch Phys Sci &amp; Technol, Nanning 530004, Peoples R China.
   <br>[Wang, Zhong Lin] Georgia Inst Technol, Sch Mat Sci &amp; Engn, Atlanta, GA
   30332 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Zhang, C; Sun, QJ; Wang, ZL (reprint author), Chinese Acad Sci, Beijing Inst Nanoenergy &amp; Nanosyst, Beijing 100083, Peoples R China.; Zhang, C; Sun, QJ; Wang, ZL (reprint author), Univ Chinese Acad Sci, Sch Nanosci &amp; Technol, Beijing 100049, Peoples R China.; Zhang, C; Sun, QJ; Wang, ZL (reprint author), Guangxi Univ, Ctr Nanoenergy Res, Sch Phys Sci &amp; Technol, Nanning 530004, Peoples R China.; Wang, ZL (reprint author), Georgia Inst Technol, Sch Mat Sci &amp; Engn, Atlanta, GA 30332 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>czhang@binn.cas.cn; sunqijun@binn.cas.cn; zlwang@gatech.edu</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>SUN, QIJUN</display_name>&nbsp;</font></td><td><font size="3">C-6011-2015&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Zhang, Chi</display_name>&nbsp;</font></td><td><font size="3">F-5430-2013&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>SUN, QIJUN</display_name>&nbsp;</font></td><td><font size="3">0000-0003-2130-7389&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Zhang, Chi</display_name>&nbsp;</font></td><td><font size="3">0000-0002-7511-805X&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PD </td><td>SEP</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">VL </td><td>12</td>
</tr>

<tr>
<td valign="top">IS </td><td>9</td>
</tr>

<tr>
<td valign="top">BP </td><td>9381</td>
</tr>

<tr>
<td valign="top">EP </td><td>9389</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1021/acsnano.8b04490</td>
</tr>

<tr>
<td valign="top">SC </td><td>Chemistry; Science &amp; Technology - Other Topics; Materials Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000445972400057</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Farrus, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Farrus, Mireia</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Disguise in Automatic Speaker Recognition</td>
</tr>

<tr>
<td valign="top">SO </td><td>ACM COMPUTING SURVEYS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speaker recognition; voice disguise; voice imitation; voice conversion;
   channel degradation; robustness</td>
</tr>

<tr>
<td valign="top">ID </td><td>TO-NOISE RATIO; SYNTHETIC SPEECH; VERIFICATION; IDENTIFICATION;
   INTOXICATION; INFORMATION; VARIABILITY; CONVERSION; IMPOSTOR; FEATURES</td>
</tr>

<tr>
<td valign="top">AB </td><td>Humans are able to identify other people's voices even in voice disguise conditions. However, we are not immune to all voice changes when trying to identify people from voice. Likewise, automatic speaker recognition systems can also be deceived by voice imitation and other types of disguise. Taking into account the voice disguise classification into the combination of two different categories (deliberate/non-deliberate and electronic/non-electronic), this survey provides a literature review on the influence of voice disguise in the automatic speaker recognition task and the robustness of these systems to such voice changes. Additionally, the survey addresses existing applications dealing with voice disguise and analyzes some issues for future research.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Farrus, Mireia] Univ Pompeu Fabra, Catalonia, Spain.
   <br>[Farrus, Mireia] Dept Informat &amp; Commun Technol, C Roc Boronat 138,
   Barcelona 08018, Catalonia, Spain.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Farrus, M (reprint author), Univ Pompeu Fabra, Catalonia, Spain.; Farrus, M (reprint author), Dept Informat &amp; Commun Technol, C Roc Boronat 138, Barcelona 08018, Catalonia, Spain.</td>
</tr>

<tr>
<td valign="top">EM </td><td>mireia.farrus@upf.edu</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Farrus, Mireia</display_name>&nbsp;</font></td><td><font size="3">O-1402-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Farrus, Mireia</display_name>&nbsp;</font></td><td><font size="3">0000-0002-7160-9513&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>SEP</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">VL </td><td>51</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">AR </td><td>68</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1145/3195832</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000444694000003</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Tebbi, H
   <br>Hamadouche, M
   <br>Azzoune, H</td>
</tr>

<tr>
<td valign="top">AF </td><td>Tebbi, Hanane
   <br>Hamadouche, Maamar
   <br>Azzoune, Hamid</td>
</tr>

<tr>
<td valign="top">TI </td><td>An Arabic expert system for voice synthesis</td>
</tr>

<tr>
<td valign="top">SO </td><td>EXPERT SYSTEMS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>expert system; graphemes-to-phonemes transcription; knowledge base;
   prologue inference engine; Standard Arabic; text to speech</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, an efficient Arabic Expert System for Voice Synthesis, referred to us as AESVS, is proposed. This system, which is based on the Standard Arabic language, consists of 3 phases. The first phase is responsible for the automatic acquisition of human expert knowledge. The acquired knowledge consists of Arabic language pronunciation rules along with the required domain dictionary, to be used by the system. The second phase is concerned with the creation of the sound base that contains the acoustic units needed during the voice generation step. The third phase, which represents the speech synthesis operation, allows the generation of a phonemic sequence for any given word. All words in the input text are converted into their corresponding phonemes sequence by using orthographic phonetic transcription that takes into account their adaptation to the Arabic Language. To ensure this conversion from text to speech, we used approximately 350 predicates created by using the PROLOGUE inference engine, which are stored in a knowledge base that can be used upon request, during both the orthographic-phonetic transcription and the voice generation steps.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Tebbi, Hanane; Azzoune, Hamid] USTHB, LRIA, Comp Sci Dept, Makoudi City
   66, Algiers 16000, Algeria.
   <br>[Hamadouche, Maamar] USDB, Comp Sci Dept, Algiers, Algeria.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Tebbi, H (reprint author), USTHB, LRIA, Comp Sci Dept, Makoudi City 66, Algiers 16000, Algeria.</td>
</tr>

<tr>
<td valign="top">EM </td><td>tebbi_hanane@yahoo.fr</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Hanane, Tebbi</display_name>&nbsp;</font></td><td><font size="3">J-4092-2015&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Hanane, Tebbi</display_name>&nbsp;</font></td><td><font size="3">0000-0002-4614-3635&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>AUG</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">VL </td><td>35</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">SI </td><td>SI</td>
</tr>

<tr>
<td valign="top">AR </td><td>e12284</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1111/exsy.12284</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000442211200013</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hellbernd, N
   <br>Sammler, D</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hellbernd, Nele
   <br>Sammler, Daniela</td>
</tr>

<tr>
<td valign="top">TI </td><td>Neural bases of social communicative intentions in speech</td>
</tr>

<tr>
<td valign="top">SO </td><td>SOCIAL COGNITIVE AND AFFECTIVE NEUROSCIENCE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice; prosody; intention; theory of mind; auditory categorical
   perception; connectivity</td>
</tr>

<tr>
<td valign="top">ID </td><td>ANTERIOR TEMPORAL-LOBE; ACOUSTIC FEATURES; BRAIN MECHANISMS; AUDITORY
   OBJECTS; FRONTAL-CORTEX; HUMAN AMYGDALA; COGNITION; LANGUAGE;
   METAANALYSIS; PERCEPTION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Our ability to understand others' communicative intentions in speech is key to successful social interaction. Indeed, misunderstanding an 'excuse me' as apology, while meant as criticism, may have important consequences. Recent behavioural studies have provided evidence that prosody, that is, vocal tone, is an important indicator for speakers' intentions. Using a novel audio-morphing paradigm, the present functional magnetic resonance imaging study examined the neurocognitive mechanisms that allow listeners to 'read' speakers' intents from vocal prosodic patterns. Participants categorized prosodic expressions that gradually varied in their acoustics between criticism, doubt, and suggestion. Categorizing typical exemplars of the three intentions induced activations along the ventral auditory stream, complemented by amygdala and mentalizing system. These findings likely depict the stepwise conversion of external perceptual information into abstract prosodic categories and internal social semantic concepts, including the speaker's mental state. Ambiguous tokens, in turn, involved cingulo-opercular areas known to assist decision-making in case of conflicting cues. Auditory and decision-making processes were flexibly coupled with the amygdala, depending on prosodic typicality, indicating enhanced categorization efficiency of overtly relevant, meaningful prosodic signals. Altogether, the results point to a model in which auditory prosodic categorization and socio-inferential conceptualization cooperate to translate perceived vocal tone into a coherent representation of the speaker's intent.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Hellbernd, Nele; Sammler, Daniela] Max Planck Inst Human Cognit &amp; Brain
   Sci, Otto Hahn Grp Neural Bases Intonat Speech &amp; Mus, Stephanstr 1a,
   D-04103 Leipzig, Germany.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sammler, D (reprint author), Max Planck Inst Human Cognit &amp; Brain Sci, Stephanstr 1a, D-04103 Leipzig, Germany.</td>
</tr>

<tr>
<td valign="top">EM </td><td>sammler@cbs.mpg.de</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Sammler, Daniela</display_name>&nbsp;</font></td><td><font size="3">0000-0001-7458-0229&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">VL </td><td>13</td>
</tr>

<tr>
<td valign="top">IS </td><td>6</td>
</tr>

<tr>
<td valign="top">BP </td><td>604</td>
</tr>

<tr>
<td valign="top">EP </td><td>615</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1093/scan/nsy034</td>
</tr>

<tr>
<td valign="top">SC </td><td>Neurosciences &amp; Neurology; Psychology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000438330000005</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Dincer, I
   <br>Acar, C</td>
</tr>

<tr>
<td valign="top">AF </td><td>Dincer, Ibrahim
   <br>Acar, Canan</td>
</tr>

<tr>
<td valign="top">TI </td><td>Smart energy solutions with hydrogen options</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERNATIONAL JOURNAL OF HYDROGEN ENERGY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Clean energy; Exergy; Hydrogen; Renewables; Smart energy; Sustainability</td>
</tr>

<tr>
<td valign="top">ID </td><td>ELECTROLYSIS CELLS SOECS; VISIBLE-LIGHT; DARK FERMENTATION; WATER;
   BIOPHOTOLYSIS; GASIFICATION; ELECTRICITY; INTEGRATION; SYSTEMS; WASTE</td>
</tr>

<tr>
<td valign="top">AB </td><td>We are in an era where everything is now requested to be smart. Here are some examples, such as smart materials smart devices, smartphones, smart grid, and smart metering. In regard to energy portfolio, we need to make it in line with these under smart energy solutions. With the developed cutting-edge technologies and artificial intelligence applications, we need to change the course of action in dealing with energy matters by covering the entire energy spectrum under five categories, namely, energy fundamentals and concepts, energy materials, energy production, energy conversion, and energy management. It is important to highlight the importance of a recent event. On 17 January 2017 a total of thirteen leading energy, transport and industry companies in the World Economic Forum in Davos (Switzerland) have launched a global initiative, so-called: Hydrogen Council, to voice a united vision and long-term ambition for hydrogen to foster the energy transition. It has aimed to join the global efforts in promoting hydrogen to help meet climate goals. This is a clear indication that smart solutions are not possible without hydrogen options. This study focuses on introducing and highlighting smart energy solutions under the portfolio pertaining to exergization, greenization, renewabilization, hydrogenization, integration, multigeneration, storagization, and intelligization. Each one of these plays a critical role within the smart energy portfolio and becomes key for a more sustainable future. This study also focuses on the newly developed smart energy systems by combining both renewable energy sources and hydrogen energy systems to provide more efficient, more cost-effective, more environmentally benign and more sustainable solutions for implementation. Furthermore, a wide range of integrated systems is presented to illustrate the feasibility and importance such a coupling to overcome several technical issues. Moreover, numerous studies from the recent literature are presented to highlight the importance of sustainable hydrogen production methods for a carbon-free economy. (C) 2018 Hydrogen Energy Publications LLC. Published by Elsevier Ltd. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Dincer, Ibrahim] Univ Ontario Inst Technol, Fac Engn &amp; Appl Sci, 2000
   Simcoe St North, Oshawa, ON L1H 7K4, Canada.
   <br>[Dincer, Ibrahim] Yildiz Tech Univ, Fac Mech Engn, Istanbul, Turkey.
   <br>[Acar, Canan] Bahcesehir Univ, Fac Engn &amp; Nat Sci, Ciragan Caddesi 4-6
   Besiktas, TR-34353 Istanbul, Turkey.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Acar, C (reprint author), Bahcesehir Univ, Fac Engn &amp; Nat Sci, Ciragan Caddesi 4-6 Besiktas, TR-34353 Istanbul, Turkey.</td>
</tr>

<tr>
<td valign="top">EM </td><td>Canan.Acar@eng.bau.edu.tr</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Dincer, Ibrahim</display_name>&nbsp;</font></td><td><font size="3">A-5379-2012&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>15</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>15</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAY 3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">VL </td><td>43</td>
</tr>

<tr>
<td valign="top">IS </td><td>18</td>
</tr>

<tr>
<td valign="top">BP </td><td>8579</td>
</tr>

<tr>
<td valign="top">EP </td><td>8599</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.ijhydene.2018.03.120</td>
</tr>

<tr>
<td valign="top">SC </td><td>Chemistry; Electrochemistry; Energy &amp; Fuels</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000432769000001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Liu, ZC
   <br>Ling, ZH
   <br>Dai, LR</td>
</tr>

<tr>
<td valign="top">AF </td><td>Liu, Zheng-Chen
   <br>Ling, Zhen-Hua
   <br>Dai, Li-Rong</td>
</tr>

<tr>
<td valign="top">TI </td><td>Articulatory-to-acoustic conversion using BLSTM-RNNs with augmented
   input representation</td>
</tr>

<tr>
<td valign="top">SO </td><td>SPEECH COMMUNICATION</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Articulatory-to-acoustic conversion; Gaussian mixture model; Deep neural
   network; Recurrent neural network; Long short-term memory</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH; FEATURES; MOVEMENTS</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a method to convert articulatory movements into speech waveforms using a data-driven approach. In this method, recorded electromagnetic midsagittal articulography (EMA) measurements are converted into both spectral features (i.e., Mel-cepstra) and excitation features (i.e., power, voiced/unvoiced flag, and FO) from which speech waveforms are then reconstructed. By considering the nonlinear and dynamic dependency relationships between articulatory movements and acoustic signals, this study adopts bidirectional long short-term memory (BLSTM) based recurrent neural networks (RNN) for the articulatory-to-acoustic conversion. Due to the limitations of current data acquisition technology, recorded articulatory movements are inevitably insufficient to completely describe the articulatory configuration during pronunciation. Therefore, this paper proposes to further augment the model input by concatenating EMA vectors with two other representations. First, the posterior probabilities derived from a phoneme classifier are concatenated with EMA features to provide a linguistic description of each frame for acoustic feature prediction. The classifier is trained to determine the phoneme label of each frame based on the observed EMA features. Second, a cascaded prediction strategy is designed to utilize the predicted spectral features as auxiliary input to boost the prediction accuracy of the excitation features. The results of experiment show that BLSTM-RNNs can achieve a better objective and subjective performance than deep neural networks (DNN) and Gaussian mixture models (GMM) in articulatory-to-acoustic conversion. In addition, our results show that the proposed methods for integrating linguistic representation and utilizing a cascaded prediction strategy can further improve the accuracy of acoustic feature prediction.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Liu, Zheng-Chen; Ling, Zhen-Hua; Dai, Li-Rong] Univ Sci &amp; Technol
   China, Natl Engn Lab Speech &amp; Language Informat Proc, Hefei 230027,
   Anhui, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ling, ZH (reprint author), Univ Sci &amp; Technol China, Natl Engn Lab Speech &amp; Language Informat Proc, Hefei 230027, Anhui, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>liuzhch@mail.ustc.edu.cn; zhling@ustc.edu.cn; lrdai@ustc.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAY</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">VL </td><td>99</td>
</tr>

<tr>
<td valign="top">BP </td><td>161</td>
</tr>

<tr>
<td valign="top">EP </td><td>172</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.specom.2018.02.008</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000440877900016</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kobayashi, K
   <br>Toda, T
   <br>Nakamura, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kobayashi, Kazuhiro
   <br>Toda, Tomoki
   <br>Nakamura, Satoshi</td>
</tr>

<tr>
<td valign="top">TI </td><td>Intra-gender statistical singing voice conversion with direct waveform
   modification using log-spectral differential</td>
</tr>

<tr>
<td valign="top">SO </td><td>SPEECH COMMUNICATION</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Statistical singing voice conversion; Direct waveform modification;
   Log-spectral differential; Global variance; Gaussian mixture model</td>
</tr>

<tr>
<td valign="top">ID </td><td>PLUS NOISE MODEL; SPARSE REPRESENTATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a novel intra-gender statistical singing voice conversion (SVC) technique with direct waveform modification based on the log-spectrum differential (DIFFSVC) that can convert the voice timbre of a source singer into that of a target singer without vocoder-based waveform generation of the converted singing voice. SVC makes it possible to convert the singing voice characteristics of an arbitrary source singer into those of an arbitrary target singer by converting some of its acoustic features, such as F-o, aperiodicity, and spectral features based on a statistical conversion function. However, the sound quality of the converted singing voice is typically degraded compared with that of a natural singing voice, owing to various factors, such as analysis and modeling errors in the vocoding process and over-smoothing of the converted feature trajectory. To alleviate sound quality degradation, we propose a statistical conversion process that directly modifies the signal in the waveform domain by estimating the difference in the spectra of the source and target singers' singing voices. Additionally, we propose the following several techniques for the DIFFSVC method: 1) derivation of a differential Gaussian mixture model (DIFFGMM) from a conventional Gaussian mixture model (GMM). and 2) a parameter generation algorithm considering the global variance (GV). The experimental results demonstrate that the proposed DIFFSVC methods enable significant improvements in the sound quality of the converted singing voice, while preserving the conversion accuracy of the singer's identity compared with conventional SVC.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kobayashi, Kazuhiro; Toda, Tomoki] Nagoya Univ, Informat Technol Ctr,
   Nagoya, Aichi, Japan.
   <br>[Nakamura, Satoshi] Nara Inst Sci &amp; Technol NAIST, Grad Sch Informat
   Sci, Ikoma, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kobayashi, K (reprint author), Nagoya Univ, Informat Technol Ctr, Nagoya, Aichi, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>kobayashi.kazuhiro@g.sp.m.is.nagoya-u.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAY</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">VL </td><td>99</td>
</tr>

<tr>
<td valign="top">BP </td><td>211</td>
</tr>

<tr>
<td valign="top">EP </td><td>220</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.specom.2018.03.011</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000440877900020</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Holman, N
   <br>Mossa, A
   <br>Pani, E</td>
</tr>

<tr>
<td valign="top">AF </td><td>Holman, Nancy
   <br>Mossa, Alessandra
   <br>Pani, Erica</td>
</tr>

<tr>
<td valign="top">TI </td><td>Planning, value(s) and the market: An analytic for "what comes next?"</td>
</tr>

<tr>
<td valign="top">SO </td><td>ENVIRONMENT AND PLANNING A-ECONOMY AND SPACE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Neoliberalism; deregulation; London; short-term letting; permitted
   development rights</td>
</tr>

<tr>
<td valign="top">ID </td><td>POLITICS; NEOLIBERALISM; CITY; GEOGRAPHIES; SPACES; URBAN</td>
</tr>

<tr>
<td valign="top">AB </td><td>For 30 years, planning has been attacked both rhetorically and materially in England as governments have sought to promote economic deregulation over landuse planning. Our paper examines two new moments of planning deregulation. These are the loosening of regulation around short-term letting in London and the new permitted development rights, which allow for office to residential conversion without the need for planning permission. Whilst these may be viewed as rather innocuous reforms on the surface, they directly and profoundly illustrate how planners are often trapped between their legal duty to promote public values as dictated by national planning policy and the government's desire to deregulate. We argue that viewing these changes through a value-based approach to economy and regulation illuminates how multiple and complex local values and understandings of value shape planners' strategies and actions and thus vary national policies in practice. In so doing, the paper demonstrates how planners have, at least, the opportunity to develop a critical voice and to advocate for policy interpretations that can help to create better outcomes for local communities.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Holman, Nancy; Mossa, Alessandra] London Sch Econ, London, England.
   <br>[Pani, Erica] Newcastle Univ, Newcastle Upon Tyne, Tyne &amp; Wear, England.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Holman, N (reprint author), London Sch Econ, Dept Geog &amp; Environm, Houghton St, London WC2A 2AE, England.</td>
</tr>

<tr>
<td valign="top">EM </td><td>n.e.holman@lse.ac.uk</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAY</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">VL </td><td>50</td>
</tr>

<tr>
<td valign="top">IS </td><td>3</td>
</tr>

<tr>
<td valign="top">BP </td><td>608</td>
</tr>

<tr>
<td valign="top">EP </td><td>626</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1177/0308518X17749730</td>
</tr>

<tr>
<td valign="top">SC </td><td>Environmental Sciences &amp; Ecology; Geography</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000434472000009</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Jannati, MJ
   <br>Sayadiyan, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Jannati, Mohammad Javad
   <br>Sayadiyan, Abolghasem</td>
</tr>

<tr>
<td valign="top">TI </td><td>Part-Syllable Transformation-Based Voice Conversion with Very Limited
   Training Data</td>
</tr>

<tr>
<td valign="top">SO </td><td>CIRCUITS SYSTEMS AND SIGNAL PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; Very limited training data; Part-syllable</td>
</tr>

<tr>
<td valign="top">ID </td><td>VECTOR QUANTIZATION; SIGNALS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion suffers from two drawbacks: requiring a large number of sentences from target speaker and concatenation error (in concatenative methods). In this research, part-syllable transformation-based voice conversion (PST-VC) method, which performs voice conversion with very limited data from a target speaker and simultaneously reduces concatenation error, is introduced. In this method, every syllable is segmented into three parts: left transition, vowel core, and right transition. Using this new language unit called part-syllable (PS), PST-VC, reduces concatenation error by transferring segmentation and concatenation from the transition points to the relatively stable points of a syllable. Since the greatest amount of information from any speaker is contained in the vowels, PST-VC method uses this information to transform the vowels into all of the language PSs. In this approach, a series of transformations are trained that can generate all of the PSs of a target speaker's voice by receiving one vowel core as the input. Having all of the PSs, any voice of target speaker can be imitated. Therefore, PST-VC reduces the number of training sentences needed to a single-syllable word and also reduces the concatenation error.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Jannati, Mohammad Javad] Iran Univ Sci &amp; Technol, Sch Comp Engn,
   Tehran, Iran.
   <br>[Sayadiyan, Abolghasem] Amirkabir Univ Technol, Dept Elect Engn, 424
   Hafez Ave,POB 15875-4413, Tehran, Iran.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sayadiyan, A (reprint author), Amirkabir Univ Technol, Dept Elect Engn, 424 Hafez Ave,POB 15875-4413, Tehran, Iran.</td>
</tr>

<tr>
<td valign="top">EM </td><td>mjannati@aut.ac.ir; sayadiyan@outlook.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAY</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">VL </td><td>37</td>
</tr>

<tr>
<td valign="top">IS </td><td>5</td>
</tr>

<tr>
<td valign="top">BP </td><td>1935</td>
</tr>

<tr>
<td valign="top">EP </td><td>1957</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/s00034-017-0639-x</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000428236000008</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Vijayalakshmi, P
   <br>Ramani, B
   <br>Jeeva, MPA
   <br>Nagarajan, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Vijayalakshmi, P.
   <br>Ramani, B.
   <br>Jeeva, M. P. Actlin
   <br>Nagarajan, T.</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Multilingual to Polyglot Speech Synthesizer for Indian Languages Using
   a Voice-Converted Polyglot Speech Corpus</td>
</tr>

<tr>
<td valign="top">SO </td><td>CIRCUITS SYSTEMS AND SIGNAL PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Polyglot; Multilingual; HMM; GMM; Voice conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>SELECTION</td>
</tr>

<tr>
<td valign="top">AB </td><td>A multilingual synthesizer synthesizes speech, for any given monolingual or mixed-language text, that is intelligible to human listeners. The necessity for such synthesizer arises in a country like India, where multiple languages coexist. For the current work, multilingual synthesizers are developed using HMM-based speech synthesis technique. However, for a mixed-language text, the synthesized speech shows speaker switching at language switching points which is quite annoying to the listener. This is due to the fact that, speech data used for training is collected for each language from a different (native) speaker. To overcome the speaker switching at language switching points, a polyglot speech synthesizer is developed using polyglot speech corpus (all the speech data in a single speaker's voice). The polyglot speech corpus is obtained using cross-lingual voice conversion (CLVC) technique. In the current work, polyglot synthesizer is developed for five languages namely Tamil, Telugu, Hindi, Malayalam and Indian English. The regional Indian languages considered are acoustically similar, to certain extent, and hence, common phoneset and question set is used to build the synthesizer. Experiments are carried out by developing various bilingual polyglot synthesizers to choose the language (thereby the speaker) that can be considered as target for polyglot synthesizer. The performance of the synthesizers is evaluated subjectively for speaker/language switching using perceptual test and quality using mean opinion score. Speaker identity is evaluated objectively using a GMM-based speaker identification system. Further, the polyglot synthesizer developed using polyglot speech corpus is compared with the adaptation-based polyglot synthesizer, in terms of quality of the synthesized speech and amount of data required for adaptation and voice conversion. It is observed that the performance of the polyglot synthesizer developed using polyglot speech corpus obtained from CLVC technique is better or almost similar to that of the adaptation-based polyglot synthesizer.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Vijayalakshmi, P.; Ramani, B.; Jeeva, M. P. Actlin; Nagarajan, T.] SSN
   Coll Engn, Old Mahabalipuram Rd, Madras, Tamil Nadu, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Vijayalakshmi, P (reprint author), SSN Coll Engn, Old Mahabalipuram Rd, Madras, Tamil Nadu, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>vijayalakshmip@ssn.edu.in; ramanib@ssn.edu.in; actlinjeevamp@ssn.edu.in;
   nagarajant@ssn.edu.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAY</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">VL </td><td>37</td>
</tr>

<tr>
<td valign="top">IS </td><td>5</td>
</tr>

<tr>
<td valign="top">BP </td><td>2142</td>
</tr>

<tr>
<td valign="top">EP </td><td>2163</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/s00034-017-0659-6</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000428236000017</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Singh, JB
   <br>Lehana, P</td>
</tr>

<tr>
<td valign="top">AF </td><td>Singh, Jang Bahadur
   <br>Lehana, Parveen</td>
</tr>

<tr>
<td valign="top">TI </td><td>STRAIGHT-Based Emotion Conversion Using Quadratic Multivariate
   Polynomial</td>
</tr>

<tr>
<td valign="top">SO </td><td>CIRCUITS SYSTEMS AND SIGNAL PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Emotion conversion; STRAIGHT; DTW; Mahalanobis distance; Quadratic
   multivariate polynomial</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION; SPEECH SYNTHESIS; NEURAL-NETWORKS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Speech is the natural mode of communication and the easiest way of expressing human emotions. Emotional speech is expressed in terms of features like f0 contour, intensity, speaking rate, and voice quality. The group of these features is called prosody. Generally, prosody is modified by pitch and time scaling. Emotional speech conversion is more sensitive to prosody unlike voice conversion, where spectral conversion is the main concern. Several techniques, linear as well as nonlinear, have been used for transforming the speech. Our hypothesis is that quality of emotional speech conversion can be improved by estimating nonlinear relationship between the neutral and emotional speech feature vectors. In this research work, quadratic multivariate polynomial (QMP) has been explored for transforming neutral speech to emotional target speech. Both subjective and objective analyses were carried out to evaluate the transformed emotional speech using comparison mean opinion scores (CMOS), mean opinion scores (MOS), identification rate, root-mean-square error, and Mahalanobis distance. For Toronto emotional database, except for neutral/sad conversion, the CMOS analysis indicates that the transformed speech can partly be perceived as target emotion. Moreover, the MOS and spectrogram indicate good quality of transformed speech. For German database except for neutral/boredom conversion, the CMOS value of proposed technique has better score than gross and initial-middle-final methods but less than syllable method. However, QMP technique is simple, is easy to implement, has better quality of transformed speech, and estimates transformation function using limited number of utterances of training set.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Singh, Jang Bahadur; Lehana, Parveen] Univ Jammu, Dept Elect, DSP Lab,
   Jammu 180006, JK, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Singh, JB (reprint author), Univ Jammu, Dept Elect, DSP Lab, Jammu 180006, JK, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>sonajbs@gmail.com; pklehana@gmail.com</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Singh, Jang Bahadur</display_name>&nbsp;</font></td><td><font size="3">0000-0001-7017-1989&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAY</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">VL </td><td>37</td>
</tr>

<tr>
<td valign="top">IS </td><td>5</td>
</tr>

<tr>
<td valign="top">BP </td><td>2179</td>
</tr>

<tr>
<td valign="top">EP </td><td>2193</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/s00034-017-0660-0</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000428236000019</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Pribil, J
   <br>Pribilova, A
   <br>Matousek, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Pribil, Jiri
   <br>Pribilova, Anna
   <br>Matousek, Jindrich</td>
</tr>

<tr>
<td valign="top">TI </td><td>Evaluation of speaker de-identification based on voice gender and age
   conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>JOURNAL OF ELECTRICAL ENGINEERING-ELEKTROTECHNICKY CASOPIS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>GMM classifier; spectral and prosodic features of speech; speaker gender
   and age classification</td>
</tr>

<tr>
<td valign="top">ID </td><td>FORMANT FREQUENCIES; TRANSFORMATION; RECOGNITION; CORPUS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Two basic tasks are covered in this paper. The first one consists in the design and practical testing of a new method for voice de-identification that changes the apparent age and/or gender of a speaker by multi-segmental frequency scale transformation combined with prosody modification. The second task is aimed at verification of applicability of a classifier based on Gaussian mixture models (GMM) to detect the original Czech and Slovak speakers after applied voice de-identification. The performed experiments confirm functionality of the developed gender and age conversion for all selected types of de-identification which can be objectively evaluated by the GMM-based open-set classifier. The original speaker detection accuracy was compared also for sentences uttered by German and English speakers showing language independence of the proposed method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Pribil, Jiri] Slovak Acad Sci, Inst Measurement Sci, Bratislava,
   Slovakia.
   <br>[Pribilova, Anna] Slovak Univ Technol Bratislava, Fac Elect Engn &amp;
   Informat Technol, Bratislava, Slovakia.
   <br>[Matousek, Jindrich] Univ West Bohemia, Fac Appl Sci, Dept Cybernet,
   Plzen, Czech Republic.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Pribil, J (reprint author), Slovak Acad Sci, Inst Measurement Sci, Bratislava, Slovakia.</td>
</tr>

<tr>
<td valign="top">EM </td><td>Jiri.Pribil@savba.sk; Anna.Pribilova@stuba.sk; jmatouse@kky.zcu.cz</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Matousek, Jindrich</display_name>&nbsp;</font></td><td><font size="3">C-2146-2011&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Matousek, Jindrich</display_name>&nbsp;</font></td><td><font size="3">0000-0002-7408-7730&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">VL </td><td>69</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>138</td>
</tr>

<tr>
<td valign="top">EP </td><td>147</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.2478/jee-2018-0017</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000440648400005</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Mozaffari, F
   <br>Sayadian, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Mozaffari, Fatemeh
   <br>Sayadian, Abolghasem</td>
</tr>

<tr>
<td valign="top">TI </td><td>Improvement of time alignment of the speech signals to be used in voice
   conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERNATIONAL JOURNAL OF SPEECH TECHNOLOGY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Dynamic time warping; Parallel corpus; Time alignment; Voice conversion</td>
</tr>

<tr>
<td valign="top">AB </td><td>One of the main applications of time alignment is parallel corpus based voice conversion. In the literature, various methods such as dynamic time warping (DTW) and hidden Markov model have been suggested for time alignment of two speech signals. In this paper, we introduce some modifications to DTW in order to decrease the time alignment error. These modifications are refinement, which is done by exerting a threshold, normalization, and comparisons between the preceding and the following frames to make sound correspondence between two different parallel corpus-based speakers' speeches. Evaluation of this approach which has been done on some corpus sentences indicates a significant improvement of time alignment. At least about 4% and in some cases 15% decrease of error in comparison with DTW has been achieved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Mozaffari, Fatemeh; Sayadian, Abolghasem] Amirkabir Univ Technol, Dept
   Elect Engn, Tehran, Iran.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Mozaffari, F (reprint author), Amirkabir Univ Technol, Dept Elect Engn, Tehran, Iran.</td>
</tr>

<tr>
<td valign="top">EM </td><td>fa_mozaffari@aut.ac.ir; eeas35@aut.ac.ir</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">VL </td><td>21</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>79</td>
</tr>

<tr>
<td valign="top">EP </td><td>84</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/s10772-018-9490-0</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000424668200008</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wang, M
   <br>Zhao, Y
   <br>Liu, L
   <br>Xu, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wang Min
   <br>Zhao Yuan
   <br>Liu Li
   <br>Xu Juan</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice conversion based on quantum particle swarm optimization of
   generalized regression neural network</td>
</tr>

<tr>
<td valign="top">SO </td><td>CHINESE JOURNAL OF LIQUID CRYSTALS AND DISPLAYS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; quantum particle swarm optimization; generalized
   regression neural network; quantum bite; smooth factor</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, a new quantum particle swarm optimization algorithm is used to optimize the voice conversion model of generalized regression neural network in order to solve the problem of slow convergence and premature phenomenon in particle swarm optimization. The quantum particle swarm optimization algorithm changes the position vector by changing the quantum bit phase and uses the quantum non-gate to perform the mutation operation. Therefore, we first use the quantum particle swarm to optimize the network to get the best smooth factor parameters, so as to establish spectrum mapping rules. After that, we use the correlation between the spectral parameters and the fundamental frequency parameters to convert the prosodic characteristic fundamental frequency. Then, the STRAIGHT model is used to synthesize the target voice in conjunction with the converted spectral parameters and the fundamental frequency parameters. Finally, we use the subjective and objective evaluation methods to evaluate. The experimental results show that the natural and similarity of the proposed method for the transformed voice are improved and the spectral distortion rate is reduced by 2.1% compared with the traditional particle swarm optimization algorithm. The proposed method has better voice conversion performance than radial basis function neural network, generalized regression neural network and generalized regression neural network optimized by particle swarm optimization.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Wang Min; Zhao Yuan; Liu Li; Xu Juan] Xian Univ Architecture &amp; Technol,
   Sch Informat &amp; Control Engn, Xian 710055, Shaanxi, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Zhao, Y (reprint author), Xian Univ Architecture &amp; Technol, Sch Informat &amp; Control Engn, Xian 710055, Shaanxi, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>wangmin1329@163.com; 1098234739@qq.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>FEB 5</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">VL </td><td>33</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>165</td>
</tr>

<tr>
<td valign="top">EP </td><td>173</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.3788/YJYXS20183302.0165</td>
</tr>

<tr>
<td valign="top">SC </td><td>Crystallography</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000429810000010</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sisman, BM
   <br>Li, HZ</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sisman, Berrak
   <br>Li, Haizhou</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Wavelet Analysis of Speaker Dependent and Independent Prosody for Voice
   Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>19TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2018), VOLS 1-6: SPEECH RESEARCH FOR EMERGING
   MARKETS IN MULTILINGUAL SOCIETIES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>19th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2018)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 02-SEP 06, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hyderabad, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Wavelet transform; prosody analysis; voice conversion</td>
</tr>

<tr>
<td valign="top">AB </td><td>Thus far, voice conversion studies are mainly focused on the conversion of spectrum. However, speaker identity is also characterized by its prosody features, such as fundamental frequency (F0) and energy contour. We believe that with a better understanding of speaker dependent/independent prosody features, we can devise an analytic approach that addresses voice conversion in a better way. We consider that speaker dependent features reflect speaker's individuality, while speaker independent features reflect the expression of linguistic content. Therefore, the former is to be converted while the latter is to be carried over from source to target during the conversion. To achieve this, we provide an analysis of speaker dependent and speaker independent prosody patterns in different temporal scales by using wavelet transform. The centrepiece of this paper is based on the understanding that a speech utterance can be characterized by speaker dependent and independent features in its prosodic manifestations. Experiments show that the proposed prosody analysis scheme improves the prosody conversion performance consistently under the sparse representation framework.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Sisman, Berrak; Li, Haizhou] Natl Univ Singapore, Singapore, Singapore.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sisman, BM (reprint author), Natl Univ Singapore, Singapore, Singapore.</td>
</tr>

<tr>
<td valign="top">EM </td><td>berraksisman@u.nus.edu; haizhou.li@nus.edu.sg</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>52</td>
</tr>

<tr>
<td valign="top">EP </td><td>56</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000465363900011</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ding, SJ
   <br>Zhao, GL
   <br>Liberatore, C
   <br>Gutierrez-Osuna, R</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ding, Shaojin
   <br>Zhao, Guanlong
   <br>Liberatore, Christopher
   <br>Gutierrez-Osuna, Ricardo</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Improving Sparse Representations in Exemplar-Based Voice Conversion with
   a Phoneme-Selective Objective Function</td>
</tr>

<tr>
<td valign="top">SO </td><td>19TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2018), VOLS 1-6: SPEECH RESEARCH FOR EMERGING
   MARKETS IN MULTILINGUAL SOCIETIES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>19th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2018)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 02-SEP 06, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hyderabad, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; sparse representation; exemplar-based methods</td>
</tr>

<tr>
<td valign="top">AB </td><td>The acoustic quality of exemplar-based voice conversion (VC) degrades whenever the phoneme labels of the selected exemplars do not match the phonetic content of the frame being represented. To address this issue, we propose a Phoneme Selective Objective Function (PSOF) that promotes a sparse representation of each speech frame with exemplars from a few phoneme classes. Namely, PSOF enforces group sparsity on the representation, where each group corresponds to a phoneme class. The sparse representation for exemplars within a phoneme class tends to activate or suppress simultaneously using the proposed objective function. We conducted two sets of experiments on the ARCTIC corpus to evaluate the proposed method. First, we evaluated the ability of PSOF to reduce phoneme mismatches. Then, we assessed its performance on a VC task and compared it against three baseline methods from previous studies. Results from objective measurements and subjective listening tests show that the proposed method effectively reduces phoneme mismatches and significantly improves VC acoustic quality while retaining the voice identity of the target speaker.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ding, Shaojin; Zhao, Guanlong; Liberatore, Christopher;
   Gutierrez-Osuna, Ricardo] Texas A&amp;M Univ, Dept Comp Sci &amp; Engn, College
   Stn, TX 77843 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ding, SJ (reprint author), Texas A&amp;M Univ, Dept Comp Sci &amp; Engn, College Stn, TX 77843 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>shjd@tamu.edu; gzhao@tamu.edu; cliberatore@tamu.edu; rgutier@tamu.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>476</td>
</tr>

<tr>
<td valign="top">EP </td><td>480</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000465363900099</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ding, SJ
   <br>Liberatore, C
   <br>Gutierrez-Osuna, R</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ding, Shaojin
   <br>Liberatore, Christopher
   <br>Gutierrez-Osuna, Ricardo</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Learning Structured Dictionaries for Exemplar-based Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>19TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2018), VOLS 1-6: SPEECH RESEARCH FOR EMERGING
   MARKETS IN MULTILINGUAL SOCIETIES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>19th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2018)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 02-SEP 06, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hyderabad, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; dictionary learning</td>
</tr>

<tr>
<td valign="top">ID </td><td>MATRIX FACTORIZATION; NEURAL-NETWORKS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Incorporating phonetic information has been shown to improve the performance of exemplar-based voice conversion. A standard approach is to build a phonetically structured dictionary, where exemplars are categorized into sub dictionaries according to their phoneme labels. However, acquiring phoneme labels can be expensive, and the phoneme labels can have inaccuracies. The latter problem becomes more salient when the speakers are non-native speakers. This paper presents an iterative dictionary-learning algorithm that avoids the need for phoneme labels, and instead learns the structured dictionaries in an unsupervised fashion. At each iteration, two steps are alternatively performed: cluster update and dictionary update. In the cluster update step, each training frame is assigned to a cluster whose sub-dictionary represents it with the lowest residual. In the dictionary update step, the sub-dictionary for a cluster is updated using all the speech frames in the cluster. We evaluate the proposed algorithm through objective and subjective experiments on a new corpus of non-native English speech. Compared to previous studies, the proposed algorithm improves the acoustic quality of voice-converted speech while retaining the target speaker's identity.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ding, Shaojin; Liberatore, Christopher; Gutierrez-Osuna, Ricardo] Texas
   A&amp;M Univ, Dept Comp Sci &amp; Engn, College Stn, TX 77843 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ding, SJ (reprint author), Texas A&amp;M Univ, Dept Comp Sci &amp; Engn, College Stn, TX 77843 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>shjd@tamu.edu; cliberatore@tamu.edu; rgutier@tamu.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>481</td>
</tr>

<tr>
<td valign="top">EP </td><td>485</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000465363900100</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Peng, YH
   <br>Hwang, HT
   <br>Wu, YC
   <br>Tsao, Y
   <br>Wang, HM</td>
</tr>

<tr>
<td valign="top">AF </td><td>Peng, Yu-Huai
   <br>Hwang, Hsin-Te
   <br>Wu, Yi-Chiao
   <br>Tsao, Yu
   <br>Wang, Hsin-Min</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Exemplar-Based Spectral Detail Compensation for Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>19TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2018), VOLS 1-6: SPEECH RESEARCH FOR EMERGING
   MARKETS IN MULTILINGUAL SOCIETIES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>19th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2018)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 02-SEP 06, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hyderabad, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>locally linear embedding; exemplar; Gaussian mixture model; vocoder;
   voice conversion</td>
</tr>

<tr>
<td valign="top">AB </td><td>Most voice conversion (VC) systems are established under the vocoder-based VC framework. When performing spectral conversion (SC) under this framework, the low-dimensional spectral features, such as mel-ceptral coefficients (MCCs), are often adopted to represent the high-dimensional spectral envelopes. The joint density Gaussian mixture model (GMM)-based SC method with the STRAIGHT vocoder is a wellknown representative. Although it is reasonably effective, the loss of spectral details in the converted spectral envelopes inevitably deteriorates speech quality and similarity. To overcome this problem, we propose a novel exemplar-based spectral detail compensation method for VC. In the offline stage, the paired dictionaries of source spectral envelopes and target spectral details are constructed. In the online stage, the locally linear embedding (LLE) algorithm is applied to predict the target spectral details from the source spectral envelopes, and then, the predicted spectral details are used to compensate the converted spectral envelopes obtained by a baseline GMM-based SC method with the STRAIGHT vocoder. Experimental results show that the proposed method can notably improve the baseline system in terms of objective and subjective tests.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Peng, Yu-Huai; Hwang, Hsin-Te; Wang, Hsin-Min] Acad Sinica, Inst
   Informat Sci, Taipei, Taiwan.
   <br>[Wu, Yi-Chiao] Nagoya Univ, Grad Sch Informat, Nagoya, Aichi, Japan.
   <br>[Tsao, Yu] Acad Sinica, Res Ctr Informat Technol Innovat, Taipei, Taiwan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Peng, YH (reprint author), Acad Sinica, Inst Informat Sci, Taipei, Taiwan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>roland0601@iis.sinica.edu.tw; hwanght@iis.sinica.edu.tw;
   yu.tsao@citi.sinica.edu.tw; whm@iis.sinica.edu.tw</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>486</td>
</tr>

<tr>
<td valign="top">EP </td><td>490</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000465363900101</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Meenakshi, GN
   <br>Ghosh, PK</td>
</tr>

<tr>
<td valign="top">AF </td><td>Meenakshi, G. Nisha
   <br>Ghosh, Prasanta Kumar</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Whispered speech to neutral speech conversion using bidirectional LSTMs</td>
</tr>

<tr>
<td valign="top">SO </td><td>19TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2018), VOLS 1-6: SPEECH RESEARCH FOR EMERGING
   MARKETS IN MULTILINGUAL SOCIETIES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>19th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2018)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 02-SEP 06, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hyderabad, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Whispered speech; LSTM; STRAIGHT</td>
</tr>

<tr>
<td valign="top">AB </td><td>We propose a bidirectional long short-term memory (BLSTM) based whispered speech to neutral speech conversion system that employs the STRAIGHT speech synthesizer. We use a BLSTM to map the spectral features of whispered speech to those of neutral speech. Three other BLSTMs are employed to predict the pitch, periodicity levels and the voiced/unvoiced phoneme decisions from the spectral features of whispered speech. We use objective measures to quantify the quality of the predicted spectral features and excitation parameters, using data recorded from six subjects, in a four fold setup. We find that the temporal smoothness of the spectral features predicted using the proposed BLSTM based system is statistically more compared to that predicted using deep neural network based baseline schemes. We also observe that while the performance of the proposed system is comparable to the baseline scheme for pitch prediction, it is superior in terms of classifying voicing decisions and predicting periodicity levels. From subjective evaluation via listening test, we find that the proposed method is chosen as the best performing scheme 26.61% (absolute) more often than the best baseline scheme. This reveals that the proposed method yields a more natural sounding neutral speech from whispered speech.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Meenakshi, G. Nisha; Ghosh, Prasanta Kumar] Indian Inst Sci, Elect
   Engn, Bangalore 560012, Karnataka, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Meenakshi, GN (reprint author), Indian Inst Sci, Elect Engn, Bangalore 560012, Karnataka, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>nishag@iisc.ac.in; prasantg@iisc.ac.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>491</td>
</tr>

<tr>
<td valign="top">EP </td><td>495</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000465363900102</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Liu, SX
   <br>Zhong, JH
   <br>Sun, LF
   <br>Wu, XX
   <br>Liu, XY
   <br>Meng, HL</td>
</tr>

<tr>
<td valign="top">AF </td><td>Liu, Songxiang
   <br>Zhong, Jinghua
   <br>Sun, Lifa
   <br>Wu, Xixin
   <br>Liu, Xunying
   <br>Meng, Helen</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Conversion Across Arbitrary Speakers based on a Single
   Target-Speaker Utterance</td>
</tr>

<tr>
<td valign="top">SO </td><td>19TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2018), VOLS 1-6: SPEECH RESEARCH FOR EMERGING
   MARKETS IN MULTILINGUAL SOCIETIES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>19th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2018)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 02-SEP 06, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hyderabad, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; i-vector; speaker encoder; low resource deployment</td>
</tr>

<tr>
<td valign="top">ID </td><td>ARTIFICIAL NEURAL-NETWORKS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Developing a voice conversion (VC) system for a particular speaker typically requires considerable data from both the source and target speakers. This paper aims to effectuate VC across arbitrary speakers. which we call any-to-any VC, with only a single target-speaker utterance. Two systems are studied: (1) the i-vector-based VC (IVC) system and (2) the speaker encoder -based VC (SEVC) system. Phonetic PosteriorGrams are adopted as speaker-independent linguistic features extracted from speech samples. Both systems train a multi-speaker deep bidirectional long-short term memory (DBLSTM) VC model, taking in additional inputs that encode speaker identities, in order to generate the outputs. In the IVC system, the speaker identity of a new target speaker is represented by i-vectors. In the SEVC system, the speaker identity is represented by speaker embedding predicted from a separately trained model. Experiments verify the effectiveness of both systems in achieving VC based only on a single target-speaker utterance. Furthermore, the IVC approach is superior to SEVC, in terms of the quality of the converted speech and its similarity to the utterance produced by the genuine target speaker.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Liu, Songxiang; Zhong, Jinghua; Sun, Lifa; Wu, Xixin; Liu, Xunying;
   Meng, Helen] Chinese Univ Hong Kong, Dept Syst Engn &amp; Engn Management,
   Human Comp Commun Lab, Hong Kong, Peoples R China.
   <br>[Sun, Lifa] SpeechX Ltd, Shenzhen, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Liu, SX (reprint author), Chinese Univ Hong Kong, Dept Syst Engn &amp; Engn Management, Human Comp Commun Lab, Hong Kong, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>sxliu@se.cuhk.edu.hk; jhzhong@se.cuhk.edu.hk; lfsun@se.cuhk.edu.hk;
   wuxx@se.cuhk.edu.hk; xyliu@se.cuhk.edu.hk; hmmeng@se.cuhk.edu.hk</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>496</td>
</tr>

<tr>
<td valign="top">EP </td><td>500</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000465363900103</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Chou, JC
   <br>Yeh, CC
   <br>Lee, HY
   <br>Lee, LS</td>
</tr>

<tr>
<td valign="top">AF </td><td>Chou, Ju-chieh
   <br>Yeh, Cheng-chieh
   <br>Lee, Hung-yi
   <br>Lee, Lin-shan</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Multi-target Voice Conversion without Parallel Data by Adversarially
   Learning Disentangled Audio Representations</td>
</tr>

<tr>
<td valign="top">SO </td><td>19TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2018), VOLS 1-6: SPEECH RESEARCH FOR EMERGING
   MARKETS IN MULTILINGUAL SOCIETIES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>19th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2018)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 02-SEP 06, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hyderabad, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; disentangled representation; adversarial training</td>
</tr>

<tr>
<td valign="top">ID </td><td>NEURAL-NETWORKS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Recently, cycle-consistent adversarial network (Cycle-GAN) has been successfully applied to voice conversion to a different speaker without parallel data, although in those approaches an individual model is needed for each target speaker. In this paper, we propose an adversarial learning framework for voice conversion, with which a single model can be trained to convert the voice to many different speakers, all without parallel data, by separating the speaker characteristics from the linguistic content in speech signals. An autoencoder is first trained to extract speaker-independent latent representations and speaker embedding separately using another auxiliary speaker classifier to regularize the latent representation. The decoder then takes the speaker-independent latent representation and the target speaker embedding as the input to generate the voice of the target speaker with the linguistic content of the source utterance. The quality of decoder output is further improved by patching with the residual signal produced by another pair of generator and discriminator. A target speaker set size of 20 was tested in the preliminary experiments, and very good voice quality was obtained. Conventional voice conversion metrics are reported. We also show that the speaker information has been properly reduced from the latent representations.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Chou, Ju-chieh; Yeh, Cheng-chieh; Lee, Hung-yi; Lee, Lin-shan] Natl
   Taiwan Univ, Coll Elect Engn &amp; Comp Sci, Taipei, Taiwan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Chou, JC (reprint author), Natl Taiwan Univ, Coll Elect Engn &amp; Comp Sci, Taipei, Taiwan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>r06922020@ntu.edu.tw; r06942067@ntu.edu.tw; hungyilee@ntu.edu.tw;
   lslee@gate.sinica.edu.tw</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>501</td>
</tr>

<tr>
<td valign="top">EP </td><td>505</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000465363900104</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Shah, NJ
   <br>Patil, HA</td>
</tr>

<tr>
<td valign="top">AF </td><td>Shah, Nirmesh J.
   <br>Patil, Hemant A.</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Effectiveness of Dynamic Features in INCA and Temporal Context-INCA</td>
</tr>

<tr>
<td valign="top">SO </td><td>19TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2018), VOLS 1-6: SPEECH RESEARCH FOR EMERGING
   MARKETS IN MULTILINGUAL SOCIETIES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>19th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2018)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 02-SEP 06, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hyderabad, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>INCA; Temporal Context (TC)-INCA; dynamic features; alignment; Voice
   Conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Non-parallel Voice Conversion (VC) has gained significant attention since last one decade. Obtaining corresponding speech frames from both the source and target speakers before learning the mapping function in the non-parallel VC is a key step in the standalone VC task. Obtaining such corresponding pairs, is more challenging due to the fact that both the speakers may have uttered different utterances from same or the different languages. Iterative combination of a Nearest Neighbor search step and a Conversion step Alignment (INCA) and its variant Temporal Context (TC)-INCA are popular unsupervised alignment algorithms. The INCA and TC-INCA iteratively learn the mapping function after getting the Nearest Neighbor (NN) aligned pairs from the intermediate converted and the target spectral features. In this paper, we propose to use dynamic features along with static features to calculate the NN aligned pairs in both the INCA and TC-INCA algorithms (since the dynamic features are known to play a key role to differentiate major phonetic categories). We obtained on an average relative improvement of 13.75 % and 5.39 % with our proposed Dynamic INCA and Dynamic TC-INCA, respectively. This improvement is also positively reflected in the quality of converted voices.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Shah, Nirmesh J.; Patil, Hemant A.] Dhirubhai Ambani Inst Informat &amp;
   Commun DA IICT, Speech Res Lab, Gandhinagar 382007, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Shah, NJ (reprint author), Dhirubhai Ambani Inst Informat &amp; Commun DA IICT, Speech Res Lab, Gandhinagar 382007, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>nirmesh88_shah@daiict.ac.in; hemant_patil@daiict.ac.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>711</td>
</tr>

<tr>
<td valign="top">EP </td><td>715</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2018-1538</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000465363900149</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ohsugi, Y
   <br>Saito, D
   <br>Minematsu, N</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ohsugi, Yasuhito
   <br>Saito, Daisuke
   <br>Minematsu, Nobuaki</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Comparative Study of Statistical Conversion of Face to Voice Based on
   Their Subjective Impressions</td>
</tr>

<tr>
<td valign="top">SO </td><td>19TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2018), VOLS 1-6: SPEECH RESEARCH FOR EMERGING
   MARKETS IN MULTILINGUAL SOCIETIES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>19th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2018)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 02-SEP 06, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hyderabad, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>subjective impressions; face to voice conversion; conditional
   variational autoencoder; Gaussian mixture model; probabilistic canonical
   correlation analysis</td>
</tr>

<tr>
<td valign="top">ID </td><td>IDENTITY</td>
</tr>

<tr>
<td valign="top">AB </td><td>Recently, various types of Voice-based User Interfaces (VUIs) including smart speakers have been developed to be on the market. However, many of the VUIs use only synthetic voices to provide information for users. To realize a more natural interface, one feasible solution will be personifying VUIs by adding visual features such as face, but what kind of face is suited to a given quality of voice or what kind of voice quality is suited to a given face? In this paper, we test methods of statistical conversion from face to voice based on their subjective impressions. To this end, six combinations of two types of face features, one type of speech features, and three types of conversion models are tested using a parallel corpus developed based on subjective mapping from face features to voice features. The experimental results show that each subject judge one specific and subject dependent voice quality as suited to different faces, and that the optimal number of mixtures of face features is different from the numbers of mixtures of voice features tested.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ohsugi, Yasuhito; Saito, Daisuke; Minematsu, Nobuaki] Univ Tokyo, Grad
   Sch Engn, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ohsugi, Y (reprint author), Univ Tokyo, Grad Sch Engn, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>yasuhito.ohsugi@gavo.t.u-tokyo.ac.jp; dsk_saito@gavo.t.u-tokyo.ac.jp;
   mlne@gavo.t.u-tokyo.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>1001</td>
</tr>

<tr>
<td valign="top">EP </td><td>1005</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000465363900208</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Shah, NJ
   <br>Madhavi, MC
   <br>Patil, HA</td>
</tr>

<tr>
<td valign="top">AF </td><td>Shah, Nirmesh J.
   <br>Madhavi, Maulik C.
   <br>Patil, Hemant A.</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Unsupervised Vocal Tract Length Warped Posterior Features for
   Non-Parallel Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>19TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2018), VOLS 1-6: SPEECH RESEARCH FOR EMERGING
   MARKETS IN MULTILINGUAL SOCIETIES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>19th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2018)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 02-SEP 06, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hyderabad, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Vocal Tract Length Normalization; Posterior-gram; Deep Neural Network;
   Voice Conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>MAXIMUM-LIKELIHOOD</td>
</tr>

<tr>
<td valign="top">AB </td><td>In the non-parallel Voice Conversion (VC) with the Iterative combination of Nearest Neighbor search step and Conversion step Alignment (INCA) algorithm, the occurrence of one-to many and many-to-one pairs in the training data will deteriorate the performance of the stand-alone VC system. The work on handling these pairs during the training is less explored. In this paper, we establish the relationship via intermediate speaker-independent posteriorgram representation, instead of directly mapping the source spectrum to the target spectrum. To that effect, a Deep Neural Network (DNN) is used to map the source spectrum to posteriorgram representation and another DNN is used to map this posteriorgram representation to the target speaker's spectrum. In this paper, we propose to use unsupervised Vocal Tract Length Normalization (VTLN)based warped Gaussian posteriorgram features as the speaker independent representations. We performed experiments on a small subset of publicly available Voice Conversion Challenge (VCC) 2016 database. We obtain the lower Mel Cepstral Distortion (MCD) values with the proposed approach compared to the baseline as well as the supervised phonetic posterior gram feature-based speaker-independent representations. Furthermore, subjective evaluation gave relative improvement of 13.3 % with the proposed approach in terms of Speaker Similarity (SS).</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Shah, Nirmesh J.; Patil, Hemant A.] DA IICT, Speech Res Lab,
   Gandhinagar 382007, India.
   <br>[Madhavi, Maulik C.] Natl Univ Singapore, Elect &amp; Comp Engn Dept,
   Singapore, Singapore.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Shah, NJ (reprint author), DA IICT, Speech Res Lab, Gandhinagar 382007, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>nirmesh88_shah@daiict.ac.in; hemant_patil@daiict.ac.in;
   elemaul@nus.edu.sg</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>1968</td>
</tr>

<tr>
<td valign="top">EP </td><td>1972</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000465363900414</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Zhou, C
   <br>Horgan, M
   <br>Kumar, V
   <br>Vasco, C
   <br>Darcy, D</td>
</tr>

<tr>
<td valign="top">AF </td><td>Zhou, Cong
   <br>Horgan, Michael
   <br>Kumar, Vivek
   <br>Vasco, Cristina
   <br>Darcy, Dan</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Conversion with Conditional SampleRNN</td>
</tr>

<tr>
<td valign="top">SO </td><td>19TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2018), VOLS 1-6: SPEECH RESEARCH FOR EMERGING
   MARKETS IN MULTILINGUAL SOCIETIES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>19th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2018)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 02-SEP 06, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hyderabad, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; SampleRNN; deep neural networks</td>
</tr>

<tr>
<td valign="top">AB </td><td>Here we present a novel approach to conditioning the SampleRNN [1] generative model for voice conversion (VC). Conventional methods for VC modify the perceived speaker identity by converting between source and target acoustic features. Our approach focuses on preserving voice content and depends on the generative network to learn voice style. We first train a multi-speaker SampleRNN model conditioned on linguistic features, pitch contour, and speaker identity using a multi-speaker speech corpus. Voice-converted speech is generated using linguistic features and pitch contour extracted from the source speaker, and the target speaker identity. We demonstrate that our system is capable of many-to-many voice conversion without requiring parallel data, enabling broad applications. Subjective evaluation demonstrates that our approach outperforms conventional VC methods.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Zhou, Cong; Horgan, Michael; Kumar, Vivek; Vasco, Cristina; Darcy, Dan]
   Dolby Labs, San Francisco, CA 94103 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Zhou, C (reprint author), Dolby Labs, San Francisco, CA 94103 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>cong.zhou@dolby.com; mhorg@dolby.com; vzkuma@dolby.com; cvasc@dolby.com;
   dan.darcy@dolby.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>1973</td>
</tr>

<tr>
<td valign="top">EP </td><td>1977</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000465363900415</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sisman, B
   <br>Zhang, MY
   <br>Li, HZ</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sisman, Berrak
   <br>Zhang, Mingyang
   <br>Li, Haizhou</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Voice Conversion Framework with Tandem Feature Sparse Representation
   and Speaker-Adapted WaveNet Vocoder</td>
</tr>

<tr>
<td valign="top">SO </td><td>19TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2018), VOLS 1-6: SPEECH RESEARCH FOR EMERGING
   MARKETS IN MULTILINGUAL SOCIETIES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>19th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2018)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 02-SEP 06, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hyderabad, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Phonetic Sparse Representation; WaveNet Vocoder; Voice Conversion</td>
</tr>

<tr>
<td valign="top">AB </td><td>A voice conversion system typically consists of two modules, the feature conversion module that is followed by a vocoder. The exemplar-based sparse representation marks a success in feature conversion when we only have a very limited amount of training data. While parametric vocoder is generally designed to simulate the mechanics of the human speech generation process under certain simplification assumptions, it doesn't work consistently well for all target applications. In this paper, we study two effective ways to make use of the limited amount of training data for voice conversion. Firstly, we study a novel technique for sparse representation that augments the spectral features with phonetic information, or Tandem Feature. Secondly, we study the use of WaveNet vocoder that can be trained on multi-speaker and target speaker data to improve the vocoding quality. We evaluate that the proposed strategy with Tandem Feature and WaveNet vocoder, and show that it provides performance improvement consistently over the traditional sparse representations framework in objective and subjective evaluations.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Sisman, Berrak; Zhang, Mingyang; Li, Haizhou] Natl Univ Singapore,
   Singapore, Singapore.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sisman, BM (reprint author), Natl Univ Singapore, Singapore, Singapore.</td>
</tr>

<tr>
<td valign="top">EM </td><td>berraksisman@u.nus.edu; mingyang.zhang@u.nus.edu; haizhou.li@nus.edu.sg</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>1978</td>
</tr>

<tr>
<td valign="top">EP </td><td>1982</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000465363900416</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Liu, LJ
   <br>Ling, ZH
   <br>Yuan-Jiang
   <br>Ming-Zhou
   <br>Dai, LR</td>
</tr>

<tr>
<td valign="top">AF </td><td>Liu, Li-Juan
   <br>Ling, Zhen-Hua
   <br>Yuan-Jiang
   <br>Ming-Zhou
   <br>Dai, Li-Rong</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>WaveNet Vocoder with Limited Training Data for Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>19TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2018), VOLS 1-6: SPEECH RESEARCH FOR EMERGING
   MARKETS IN MULTILINGUAL SOCIETIES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>19th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2018)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 02-SEP 06, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hyderabad, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; WaveNet; vocoder; adaptation</td>
</tr>

<tr>
<td valign="top">ID </td><td>NEURAL-NETWORKS</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper investigates the approaches of building WaveNet vocoders with limited training data for voice conversion (VC). Current VC systems using statistical acoustic models always suffer from the quality degradation of converted speech. One of the major causes is the use of hand-crafted vocoders for waveform generation. Recently, with the emergence of WaveNet for waveform modeling, speaker-dependent WaveNet vocoders have been proposed and they can reconstruct speech with better quality than conventional vocoders, such as STRAIGHT. Because training a WaveNet vocoder in the speaker -dependent way requires a relatively large training dataset, it remains a challenge to build a high -quality WaveNet vocoder for VC tasks when the training data of target speakers is limited. In this paper, we propose to build WaveNet vocoders by combining the initialization using a multi -speaker corpus and the adaptation using a small amount of target data, and evaluate this proposed method on the Voice Conversion Challenge (VCC) 2018 dataset which contains approximately 5 minute recordings for each target speaker. Experimental results show that the WaveNet vocoders built using our proposed method outperform conventional STRAIGHT vocoder. Furthermore, our system achieves an average naturalness MOS of 4.13 in VCC 2018, which is the highest among all submitted systems.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Liu, Li-Juan; Yuan-Jiang; Ming-Zhou] iFLYTEK Co Ltd, iFLYTEK Res,
   Hefei, Anhui, Peoples R China.
   <br>[Ling, Zhen-Hua; Dai, Li-Rong] Univ Sci &amp; Technol China, Natl Engn Lab
   Speech &amp; Language Informat Proc, Hefei, Anhui, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Liu, LJ (reprint author), iFLYTEK Co Ltd, iFLYTEK Res, Hefei, Anhui, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ljliu@iflytek.com; zhling@ustc.edu.cn; yuanjiang@iflytek.com;
   mingzhou@iflytek.com; lrdai@ustc.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>1983</td>
</tr>

<tr>
<td valign="top">EP </td><td>1987</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000465363900417</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wu, YC
   <br>Kobayashi, K
   <br>Hayashi, T
   <br>Tobing, PL
   <br>Toda, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wu, Yi-Chiao
   <br>Kobayashi, Kazuhiro
   <br>Hayashi, Tomoki
   <br>Tobing, Patrick Lumban
   <br>Toda, Tomoki</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Collapsed speech segment detection and suppression for WaveNet vocoder</td>
</tr>

<tr>
<td valign="top">SO </td><td>19TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2018), VOLS 1-6: SPEECH RESEARCH FOR EMERGING
   MARKETS IN MULTILINGUAL SOCIETIES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>19th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2018)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 02-SEP 06, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hyderabad, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>speech generation; WaveNet; vocoder; linear predictive coding; collapsed
   speech detection; voice conversion</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we propose a technique to alleviate the quality degradation caused by collapsed speech segments sometimes generated by the WaveNet vocoder. The effectiveness of the WaveNet vocoder for generating natural speech from acoustic features has been proved in recent works. However, it sometimes generates very noisy speech with collapsed speech segments when only a limited amount of training data is, available or significant acoustic mismatches exist between the training and testing data. Such a limitation on the corpus and limited ability of the model can easily occur in some speech generation applications, such as voice conversion and speech enhancement. To address this problem, we propose a technique to automatically detect collapsed speech segments. Moreover, to refine the detected segments, we also propose a waveform generation technique for WaveNet using a linear predictive coding constraint. Verification and subjective tests are conducted to investigate the effectiveness of the proposed techniques. The verification results indicate that the detection technique can detect most collapsed segments. The subjective evaluations of voice conversion demonstrate that the generation technique significantly improves the speech quality while maintaining the same speaker similarity.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Wu, Yi-Chiao; Tobing, Patrick Lumban] Nagoya Univ, Grad Sch Informat,
   Nagoya, Aichi, Japan.
   <br>[Kobayashi, Kazuhiro; Toda, Tomoki] Nagoya Univ, Ctr Informat Technol,
   Nagoya, Aichi, Japan.
   <br>[Hayashi, Tomoki] Nagoya Univ, Grad Sch Informat Sci, Nagoya, Aichi,
   Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Wu, YC (reprint author), Nagoya Univ, Grad Sch Informat, Nagoya, Aichi, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>yichiao.wu@g.sp.m.is.nagoya-u.c.jp; tomoki@icts.nagoya-u.c.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>1988</td>
</tr>

<tr>
<td valign="top">EP </td><td>1992</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000465363900418</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Chen, K
   <br>Chen, B
   <br>Lai, JH
   <br>Yu, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Chen, Kuan
   <br>Chen, Bo
   <br>Lai, Jiahao
   <br>Yu, Kai</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>High-quality Voice Conversion Using Spectrogram-Based WaveNet Vocoder</td>
</tr>

<tr>
<td valign="top">SO </td><td>19TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2018), VOLS 1-6: SPEECH RESEARCH FOR EMERGING
   MARKETS IN MULTILINGUAL SOCIETIES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>19th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2018)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 02-SEP 06, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hyderabad, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; WaveNet vocoder; mel-frequency spectrogram; LSTM-RNN</td>
</tr>

<tr>
<td valign="top">ID </td><td>SYSTEM; TIME</td>
</tr>

<tr>
<td valign="top">AB </td><td>Waveform generator is a key component in voice conversion. Recently, WaveNet waveform generator conditioned on the Mel-cepstrum (Mcep) has shown better quality over standard vocoder. In this paper, an enhanced WaveNet model based on spectrogram is proposed to further improve voice conversion performance. Here, Mel-frequency spectrogram is converted from source speaker to target speaker using an LSTMRNN based frame-to-frame feature mapping. To evaluate the performance, the proposed approach is compared to an Mcep based LSTM-RNN voice conversion system. Both STRAIGHT vocoder and Mcep-based WaveNet vocoder are elected to produce the converted speech for Mcep conversion system. The fundamental frequency (F-0) of the converted speech in different systems is analyzed. The naturalness, similarity and intelligibility are evaluated in subjective measures. Results show that the spectrogram based WaveNet waveform generator can achieve better voice conversion quality compared to traditional WaveNet approaches. The Mel-spectrogram based voice conversion can achieve significant improvement in speaker similarity and inherent F-0 conversion.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Chen, Kuan; Chen, Bo; Lai, Jiahao; Yu, Kai] Shanghai Jiao Tong Univ,
   Key Lab Shanghai Educ Commiss Intelligent Interac, Brain Sci &amp; Technol
   Res Ctr, SpeechLab,Dept Comp Sci &amp; Engn, Shanghai, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Yu, K (reprint author), Shanghai Jiao Tong Univ, Key Lab Shanghai Educ Commiss Intelligent Interac, Brain Sci &amp; Technol Res Ctr, SpeechLab,Dept Comp Sci &amp; Engn, Shanghai, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>azraelkuan@sjtu.edu.cn; bobmilk@sjtu.edu.cn; ljhao1993@sjtu.edu.cn;
   kai.yu@sjtu.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>1993</td>
</tr>

<tr>
<td valign="top">EP </td><td>1997</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000465363900419</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Juvela, L
   <br>Tsiaras, V
   <br>Bollepalli, B
   <br>Airaksinen, M
   <br>Yamagishi, J
   <br>Alku, P</td>
</tr>

<tr>
<td valign="top">AF </td><td>Juvela, Lauri
   <br>Tsiaras, Vassilis
   <br>Bollepalli, Bajibabu
   <br>Airaksinen, Manu
   <br>Yamagishi, Junichi
   <br>Alku, Paavo</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Speaker-independent raw waveform model for glottal excitation</td>
</tr>

<tr>
<td valign="top">SO </td><td>19TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2018), VOLS 1-6: SPEECH RESEARCH FOR EMERGING
   MARKETS IN MULTILINGUAL SOCIETIES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>19th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2018)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 02-SEP 06, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hyderabad, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Glottal source generation; WaveNet; mixture density network</td>
</tr>

<tr>
<td valign="top">ID </td><td>INVERSE FILTERING ANALYSIS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Recent speech technology research has seen a growing interest in using WaveNets as statistical vocoders, i.e., generating speech waveforms from acoustic features. These models have been shown to improve the generated speech quality over classical vocoders in many tasks, such as text-to-speech synthesis and voice conversion. Furthermore, conditioning WaveNets with acoustic features allows sharing the waveform generator model across multiple speakers without additional speaker codes. However, multi-speaker WaveNet models require large amounts of training data and computation to cover the entire acoustic space. This paper proposes leveraging the source-filter model of speech production to more effectively train a speaker independent waveform generator with limited resources. We present a multi-speaker 'GlotNet' vocoder, which utilizes a WaveNet to generate glottal excitation waveforms, which are then used to excite the corresponding vocal tract filter to produce speech. Listening tests show that the proposed model performs favourably to a direct WaveNet vocoder trained with the same model architecture and data.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Juvela, Lauri; Bollepalli, Bajibabu; Airaksinen, Manu; Alku, Paavo]
   Aalto Univ, Espoo, Finland.
   <br>[Tsiaras, Vassilis] Univ Crete, Iraklion, Greece.
   <br>[Yamagishi, Junichi] Natl Inst Informat, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Juvela, L (reprint author), Aalto Univ, Espoo, Finland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>lauri.juvela@aalto.fi; tsiaras@csd.uoc.gr</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>2012</td>
</tr>

<tr>
<td valign="top">EP </td><td>2016</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000465363900423</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Murakami, H
   <br>Hara, S
   <br>Abe, M
   <br>Sato, M
   <br>Minagi, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Murakami, Hiroki
   <br>Hara, Sunao
   <br>Abe, Masanobu
   <br>Sato, Masaaki
   <br>Minagi, Shogo</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Naturalness Improvement Algorithm for Reconstructed Glossectomy
   Patient's Speech Using Spectral Differential Modification in Voice
   Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>19TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2018), VOLS 1-6: SPEECH RESEARCH FOR EMERGING
   MARKETS IN MULTILINGUAL SOCIETIES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>19th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2018)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 02-SEP 06, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hyderabad, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; speech intelligibility; glossectomy; spectral
   differential; neural network</td>
</tr>

<tr>
<td valign="top">ID </td><td>PROSTHESES</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we propose an algorithm to improve the naturalness of the reconstructed glossectomy patient's speech that is generated by voice conversion to enhance the intelligibility of speech uttered by patients with a wide glossectomy. While existing VC algorithms make it possible to improve intelligibility and naturalness, the result is still not satisfying. To solve the continuing problems, we propose to directly modify the speech waveforms using a spectrum differential. The motivation is that glossectomy patients mainly have problems in their vocal tract, not in their vocal cords. The proposed algorithm requires no source parameter extractions for speech synthesis, so there are no errors in source parameter extractions and we are able to make the best use of the original source characteristics. In terms of spectrum conversion, we evaluate with both GMM and DNN. Subjective evaluations show that our algorithm can synthesize more natural speech than the vocoder-based method. Judging from observations of the spectrogram, power in high-frequency bands of fricatives and stops is reconstructed to be similar to that of natural speech.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Murakami, Hiroki; Hara, Sunao; Abe, Masanobu] Okayama Univ, Grad Sch
   Nat Sci &amp; Technol, Okayama, Japan.
   <br>[Sato, Masaaki; Minagi, Shogo] Okayama Univ, Grad Sch Med Dent &amp;
   Pharmaceut Sci, Okayama, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Murakami, H (reprint author), Okayama Univ, Grad Sch Nat Sci &amp; Technol, Okayama, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>h_muraka@a.cs.okayama-u.ac.jp; hara@cs.okayama-u.ac.jp;
   abe@cs.okayama-u.ac.jp; sato.masaaki@s.okayama-u.ac.jp;
   minagi@md.okayama-u.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>2464</td>
</tr>

<tr>
<td valign="top">EP </td><td>2468</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2018-1239</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000465363900518</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Tamura, S
   <br>Horio, K
   <br>Endo, H
   <br>Hayamizu, S
   <br>Toda, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Tamura, Satoshi
   <br>Horio, Kento
   <br>Endo, Hajime
   <br>Hayamizu, Satoru
   <br>Toda, Tomoki</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Audio-visual voice conversion using deep canonical correlation analysis
   for deep bottleneck features</td>
</tr>

<tr>
<td valign="top">SO </td><td>19TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2018), VOLS 1-6: SPEECH RESEARCH FOR EMERGING
   MARKETS IN MULTILINGUAL SOCIETIES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>19th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2018)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 02-SEP 06, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hyderabad, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>statistical speech conversion; audio-visual processing; deep learning;
   bottleneck feature; canonical component analysis</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper proposes Audio-Visual Voice Conversion (AVVC) methods using Deep BottleNeck Features (DBNF) and Deep Canonical Correlation Analysis (DCCA). DBNF has been adopted in several speech applications to obtain better feature representations. DCCA can generate much correlated features in two views, and enhance features in one modality based on another view. In addition, DCCA can make projections from different views ideally to the same vector space. Firstly, in this work, we enhance our conventional AVVC scheme by employing the DBNF technique in the visual modality. Secondly, we apply the DCCA technology to DBNFs for new effective visual features. Thirdly, we build a cross-modal voice conversion model available for both audio and visual DCCA features. In order to clarify effectiveness of these frameworks, we carried out subjective and objective evaluations and compared them with conventional methods. Experimental results show that our DBNF- and DCCA-based AVVC can successfully improve the quality of converted speech waveforms.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Tamura, Satoshi; Horio, Kento; Endo, Hajime; Hayamizu, Satoru] Gifu
   Univ, Gifu, Japan.
   <br>[Toda, Tomoki] Nagoya Univ, Nagoya, Aichi, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Tamura, S (reprint author), Gifu Univ, Gifu, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>tamura@info.gifu-u.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>2469</td>
</tr>

<tr>
<td valign="top">EP </td><td>2473</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2018-2286</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000465363900519</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Zhao, GL
   <br>Sonsaat, S
   <br>Silpachai, A
   <br>Lucic, I
   <br>Chukharev-Hudilainen, E
   <br>Levis, J
   <br>Gutierrez-Osuna, R</td>
</tr>

<tr>
<td valign="top">AF </td><td>Zhao, Guanlong
   <br>Sonsaat, Sinem
   <br>Silpachai, Alif
   <br>Lucic, Ivana
   <br>Chukharev-Hudilainen, Evgeny
   <br>Levis, John
   <br>Gutierrez-Osuna, Ricardo</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>L2-ARCTIC: A Non-Native English Speech Corpus</td>
</tr>

<tr>
<td valign="top">SO </td><td>19TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2018), VOLS 1-6: SPEECH RESEARCH FOR EMERGING
   MARKETS IN MULTILINGUAL SOCIETIES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>19th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2018)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 02-SEP 06, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hyderabad, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>speech corpus; voice conversion; accent conversion; mispronunciation
   detection</td>
</tr>

<tr>
<td valign="top">ID </td><td>STRESS</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we introduce L2-ARCTIC, a speech corpus of non-native English that is intended for research in voice conversion, accent conversion, and mispronunciation detection. This initial release includes recordings from ten non-native speakers of English whose first languages (LIs) are Hindi, Korean, Mandarin, Spanish, and Arabic, each L1 containing recordings from one male and one female speaker. Each speaker recorded approximately one hour of read speech from the Carnegie Mellon University ARCTIC prompts, from which we generated orthographic and forced-aligned phonetic transcriptions. In addition, we manually annotated 150 utterances per speaker to identify three types of mispronunciation errors: substitutions, deletions, and additions, making it a valuable resource not only for research in voice conversion and accent conversion but also in computer-assisted pronunciation training. The corpus is publicly accessible at https://psi.engr.tamu.edu/12-arctic-corpus/.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Zhao, Guanlong; Gutierrez-Osuna, Ricardo] Texas A&amp;M Univ, Dept Comp Sci
   &amp; Engn, College Stn, TX 77843 USA.
   <br>[Sonsaat, Sinem; Silpachai, Alif; Lucic, Ivana; Chukharev-Hudilainen,
   Evgeny; Levis, John] Iowa State Univ, Dept English, Ames, IA USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Zhao, GL (reprint author), Texas A&amp;M Univ, Dept Comp Sci &amp; Engn, College Stn, TX 77843 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>gzhao@tamu.edu; sonsaat@iastate.edu; alif@iastate.edu;
   ilucic@iastate.edu; evgeny@iastate.edu; jlevis@iastate.edu;
   rgutier@tamu.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>2783</td>
</tr>

<tr>
<td valign="top">EP </td><td>2787</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2018-1110</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000465363900582</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Mohammadi, SH
   <br>Kim, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Mohammadi, Seyed Hamidreza
   <br>Kim, Taehwan</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Investigation of using disentangled and interpretable representations
   for one-shot cross-lingual voice conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>19TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2018), VOLS 1-6: SPEECH RESEARCH FOR EMERGING
   MARKETS IN MULTILINGUAL SOCIETIES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>19th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2018)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 02-SEP 06, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hyderabad, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; one-shot learning; cross lingual; variational
   autoencoder</td>
</tr>

<tr>
<td valign="top">AB </td><td>We study the problem of cross-lingual voice conversion in non-parallel speech corpora and one-shot learning setting. Most prior work require either parallel speech corpora or enough amount of training data from a target speaker. However, we convert an arbitrary sentences of an arbitrary source speaker to target speaker's given only one target speaker training utterance. To achieve this, we formulate the problem as learning disentangled speaker-specific and context-specific representations and follow the idea of [1] which uses Factorized Hierarchical Variational Autoencoder (FHVAE). After training FHVAE on multi speaker training data, given arbitrary source and target speakers' utterance, we estimate those latent representations and then reconstruct the desired utterance of converted voice to that of target speaker. We investigate the effectiveness of the approach by conducting voice conversion experiments with varying size of training utterances and it was able to achieve reasonable performance with even just one training utterance. We also examine the speech representation and show that World vocoder outperforms Short-time Fourier Transform (STFT) used in [1]. Finally, in the subjective tests, for one language and cross-lingual voice conversion, our approach achieved significantly better or comparable results compared to VAE-STFT and GMM baselines in speech quality and similarity.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Mohammadi, Seyed Hamidreza; Kim, Taehwan] ObEN Inc, Pasadena, CA 91103
   USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Mohammadi, SH (reprint author), ObEN Inc, Pasadena, CA 91103 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>hamid@oben.com; taehwan@oben.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>2833</td>
</tr>

<tr>
<td valign="top">EP </td><td>2837</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2018-2525</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000465363900592</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>He, XZ
   <br>Liu, LL
   <br>Wan, FC
   <br>Cai, LL
   <br>Zhu, SY</td>
</tr>

<tr>
<td valign="top">AF </td><td>He Xiangzhen
   <br>Liu Lulu
   <br>Wan Fucheng
   <br>Cai Lulu
   <br>Zhu Shengyin</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Tibetan Lhasa Dialect Tone Conversion System Design and Implementation</td>
</tr>

<tr>
<td valign="top">SO </td><td>2018 INTERNATIONAL CONFERENCE ON ROBOTS &amp; INTELLIGENT SYSTEM (ICRIS
   2018)</td>
</tr>

<tr>
<td valign="top">SE </td><td>IEEE International Conference on Intelligent Robots and Systems</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Robotics and Intelligent System (ICRIS)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 26-27, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Changsha, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Tibetan Lhasa Dialect; Tone Conversion; Text-Voice Conversion; Voice
   Transformation</td>
</tr>

<tr>
<td valign="top">AB </td><td>In order to protect and promote the language culture in Tibetan region, deepen the recognition upon Tibetan Lhasa dialect pronunciation mechanism, and further accelerate the technical development for Tibetan voice recognition, conversion, composition, etc., as well as promote Tibetan information construction and the application thereof in multimedia, etc., Tibetan Lhasa dialect tone processing and conversion system was implemented in this paper on the basis of Matlab GUI. Moreover, the system had good test effect.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[He Xiangzhen; Liu Lulu; Wan Fucheng; Cai Lulu; Zhu Shengyin] Northwest
   Minzu Univ, Natl Languages Informat Technol Res Inst, Lanzhou, Gansu,
   Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>He, XZ (reprint author), Northwest Minzu Univ, Natl Languages Informat Technol Res Inst, Lanzhou, Gansu, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>5967148@qq.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>312</td>
</tr>

<tr>
<td valign="top">EP </td><td>315</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/ICRIS.2018.00085</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Robotics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000467261200077</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kameoka, H
   <br>Kaneko, T
   <br>Tanaka, K
   <br>Hojo, N</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kameoka, Hirokazu
   <br>Kaneko, Takuhiro
   <br>Tanaka, Kou
   <br>Hojo, Nobukatsu</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>STARGAN-VC: NON-PARALLEL MANY-TO-MANY VOICE CONVERSION USING STAR
   GENERATIVE ADVERSARIAL NETWORKS</td>
</tr>

<tr>
<td valign="top">SO </td><td>2018 IEEE WORKSHOP ON SPOKEN LANGUAGE TECHNOLOGY (SLT 2018)</td>
</tr>

<tr>
<td valign="top">SE </td><td>IEEE Workshop on Spoken Language Technology</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE Workshop on Spoken Language Technology (SLT)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 18-21, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Athens, GREECE</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion (VC); non-parallel VC; many-to-many VC; generative
   adversarial networks (GANs); CycleGAN-VC; StarGAN-VC</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH SYNTHESIS; SPARSE REPRESENTATION; NEURAL-NETWORKS</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper proposes a method that allows non-parallel many-to-many voice conversion (VC) by using a variant of a generative adversarial network (GAN) called StarGAN. Our method, which we call StarGAN-VC, is noteworthy in that it (1) requires no parallel utterances, transcriptions, or time alignment procedures for speech generator training, (2) simultaneously learns many-to-many mappings across different attribute domains using a single generator network, (3) is able to generate converted speech signals quickly enough to allow real-time implementations and (4) requires only several minutes of training examples to generate reasonably realistic sounding speech. Subjective evaluation experiments on a non-parallel many-to-many speaker identity conversion task revealed that the proposed method obtained higher sound quality and speaker similarity than a state-of-the-art method based on variational autoencoding GANs.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kameoka, Hirokazu; Kaneko, Takuhiro; Tanaka, Kou; Hojo, Nobukatsu] NTT
   Corp, NTT Commun Sci Labs, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kameoka, H (reprint author), NTT Corp, NTT Commun Sci Labs, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>266</td>
</tr>

<tr>
<td valign="top">EP </td><td>273</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000463141800038</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Yeh, CC
   <br>Hsu, PC
   <br>Chou, JC
   <br>Lee, HY
   <br>Lee, LS</td>
</tr>

<tr>
<td valign="top">AF </td><td>Yeh, Cheng-chieh
   <br>Hsu, Po-chun
   <br>Chou, Ju-chieh
   <br>Lee, Hung-yi
   <br>Lee, Lin-shan</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>RHYTHM-FLEXIBLE VOICE CONVERSION WITHOUT PARALLEL DATA USING CYCLE-GAN
   OVER PHONEME POSTERIORGRAM SEQUENCES</td>
</tr>

<tr>
<td valign="top">SO </td><td>2018 IEEE WORKSHOP ON SPOKEN LANGUAGE TECHNOLOGY (SLT 2018)</td>
</tr>

<tr>
<td valign="top">SE </td><td>IEEE Workshop on Spoken Language Technology</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE Workshop on Spoken Language Technology (SLT)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 18-21, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Athens, GREECE</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; sequence-to-sequence learning; unsupervised learning;
   cycle-gan</td>
</tr>

<tr>
<td valign="top">AB </td><td>Speaking rate refers to the average number of phonemes within some unit time, while the rhythmic patterns refer to duration distributions for realizations of different phonemes within different phonetic structures. Both are key components of prosody in speech, which is different for different speakers. Models like cycle-consistent adversarial network (Cycle-GAN) and variational auto-encoder (VAE) have been successfully applied to voice conversion tasks without parallel data. However, due to the neural network architectures and feature vectors chosen for these approaches, the length of the predicted utterance has to be fixed to that of the input utterance, which limits the flexibility in mimicking the speaking rates and rhythmic patterns for the target speaker. On the other hand, sequence-to-sequence learning model was used to remove the above length constraint, but parallel training data are needed. In this paper, we propose an approach utilizing sequence-to-sequence model trained with unsupervised Cycle-GAN to perform the transformation between the phoneme posteriorgram sequences for different speakers. In this way, the length constraint mentioned above is removed to offer rhythm-flexible voice conversion without requiring parallel data. Preliminary evaluation on two datasets showed very encouraging results.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Yeh, Cheng-chieh; Hsu, Po-chun; Chou, Ju-chieh; Lee, Hung-yi; Lee,
   Lin-shan] Natl Taiwan Univ, Coll Elect Engn &amp; Comp Sci, Taipei, Taiwan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Yeh, CC (reprint author), Natl Taiwan Univ, Coll Elect Engn &amp; Comp Sci, Taipei, Taiwan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>r06942067@ntu.edu.tw; b03901071@ntu.edu.tw; r06922020@ntu.edu.tw;
   hungyilee@ntu.edu.tw; lslee@gate.sinica.edu.tw</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>274</td>
</tr>

<tr>
<td valign="top">EP </td><td>281</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000463141800039</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sisman, B
   <br>Zhang, M
   <br>Sakti, S
   <br>Li, HZ
   <br>Nakamura, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sisman, Berrak
   <br>Zhang, Mingyang
   <br>Sakti, Sakriani
   <br>Li, Haizhou
   <br>Nakamura, Satoshi</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>ADAPTIVE WAVENET VOCODER FOR RESIDUAL COMPENSATION IN GAN-BASED VOICE
   CONVERSION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2018 IEEE WORKSHOP ON SPOKEN LANGUAGE TECHNOLOGY (SLT 2018)</td>
</tr>

<tr>
<td valign="top">SE </td><td>IEEE Workshop on Spoken Language Technology</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE Workshop on Spoken Language Technology (SLT)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 18-21, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Athens, GREECE</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; generative adversarial networks; adaptive Wavenet;
   residual compensation</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we propose to use generative adversarial networks (GAN) together with a WaveNet vocoder to address the over-smoothing problem arising from the deep learning approaches to voice conversion, and to improve the vocoding quality over the traditional vocoders. As GAN aims to minimize the divergence between the natural and converted speech parameters, it effectively alleviates the over-smoothing problem in the converted speech. On the other hand, WaveNet vocoder allows us to leverage from the human speech of a large speaker population, thus improving the naturalness of the synthetic voice. Furthermore, for the first time, we study how to use WaveNet vocoder for residual compensation to improve the voice conversion performance. The experiments show that the proposed voice conversion framework consistently outperforms the baselines.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Sisman, Berrak; Zhang, Mingyang; Li, Haizhou] Natl Univ Singapore,
   Singapore, Singapore.
   <br>[Sisman, Berrak; Sakti, Sakriani; Nakamura, Satoshi] Nara Inst Sci &amp;
   Technol, Nara, Japan.
   <br>[Sisman, Berrak; Sakti, Sakriani; Nakamura, Satoshi] RIKEN, Ctr Adv
   Intelligence Project AIP, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sisman, BM (reprint author), Natl Univ Singapore, Singapore, Singapore.; Sisman, BM (reprint author), Nara Inst Sci &amp; Technol, Nara, Japan.; Sisman, BM (reprint author), RIKEN, Ctr Adv Intelligence Project AIP, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>berraksisman@u.nus.edu; mingyang.zhang@u.nus.edu; ssakti@is.naist.jp;
   haizhou.li@nus.edu.sg; s-nakamura@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>282</td>
</tr>

<tr>
<td valign="top">EP </td><td>289</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000463141800040</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kons, Z
   <br>Shechtman, S
   <br>Sorin, A
   <br>Hoory, R
   <br>Rabinovitz, C
   <br>Morais, ED</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kons, Zvi
   <br>Shechtman, Slava
   <br>Sorin, Alex
   <br>Hoory, Ron
   <br>Rabinovitz, Carmel
   <br>Morais, Edmilson Da Silva</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>NEURAL TTS VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2018 IEEE WORKSHOP ON SPOKEN LANGUAGE TECHNOLOGY (SLT 2018)</td>
</tr>

<tr>
<td valign="top">SE </td><td>IEEE Workshop on Spoken Language Technology</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE Workshop on Spoken Language Technology (SLT)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 18-21, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Athens, GREECE</td>
</tr>

<tr>
<td valign="top">DE </td><td>DNN TTS; speech synthesis; voice adaptation; voice conversion</td>
</tr>

<tr>
<td valign="top">AB </td><td>Recently, speaker adaptation of neural TTS models received significant interest, and several studies focusing on this topic have been published. All of them explore an adaptation of an initial multi-speaker model trained on a corpus containing from tens to hundreds of individual speaker voices.
   <br>In this work we focus on a challenging task of TTS voice conversion where an initial system is trained on a single-speaker data and then need to be adapted to a variety of external speaker voices. The TTS voice conversion setup represents a very important use case. Transcribed multi-speaker datasets might be unavailable for many languages while any TTS technology provider is expected to have at least one suitable single-speaker dataset per supported language.
   <br>We present a neural TTS system comprising separate prosody generator and synthesizer DNN models. The system is trained on a high quality proprietary male speaker dataset. We show that the system models can be converted to a variety of external male and female ordinary voices and an extremely expressive artist's voice and present crowd-base subjective evaluation results.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kons, Zvi; Shechtman, Slava; Sorin, Alex; Hoory, Ron; Rabinovitz,
   Carmel; Morais, Edmilson Da Silva] IBM Res, Armonk, NY 10504 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kons, Z (reprint author), IBM Res, Armonk, NY 10504 USA.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>290</td>
</tr>

<tr>
<td valign="top">EP </td><td>296</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000463141800041</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Tobing, PL
   <br>Hayashi, T
   <br>Wu, YC
   <br>Kobayashi, K
   <br>Toda, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Tobing, Patrick Lumban
   <br>Hayashi, Tomoki
   <br>Wu, Yi-Chiao
   <br>Kobayashi, Kazuhiro
   <br>Toda, Tomoki</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>AN EVALUATION OF DEEP SPECTRAL MAPPINGS AND WAVENET VOCODER FOR VOICE
   CONVERSION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2018 IEEE WORKSHOP ON SPOKEN LANGUAGE TECHNOLOGY (SLT 2018)</td>
</tr>

<tr>
<td valign="top">SE </td><td>IEEE Workshop on Spoken Language Technology</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE Workshop on Spoken Language Technology (SLT)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 18-21, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Athens, GREECE</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; WaveNet; deep spectral mappings; oversmoothed features</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents an evaluation of deep spectral mapping and WaveNet vocoder in voice conversion (VC). In our VC framework, spectral features of an input speaker are converted into those of a target speaker using the deep spectral mapping, and then together with the excitation features, the converted waveform is generated using WaveNet vocoder. In this work, we compare three different deep spectral mapping networks, i.e., a deep single density network (DSDN), a deep mixture density network (DMDN), and a long short-term memory recurrent neural network with an autoregressive output layer (LSTM-AR). Moreover, we also investigate several methods for reducing mismatches of spectral features used in WaveNet vocoder between training and conversion processes, such as some methods to alleviate oversmoothing effects of the converted spectral features, and another method to refine WaveNet using the converted spectral features. The experimental results demonstrate that the LSTM-AR yields nearly better spectral mapping accuracy than the others, and the proposed WaveNet refinement method significantly improves the naturalness of the converted waveform.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Tobing, Patrick Lumban; Hayashi, Tomoki; Wu, Yi-Chiao] Nagoya Univ,
   Grad Sch Informat Sci, Nagoya, Aichi, Japan.
   <br>[Kobayashi, Kazuhiro; Toda, Tomoki] Nagoya Univ, Informat Technol Ctr,
   Nagoya, Aichi, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Tobing, PL (reprint author), Nagoya Univ, Grad Sch Informat Sci, Nagoya, Aichi, Japan.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>297</td>
</tr>

<tr>
<td valign="top">EP </td><td>303</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000463141800042</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nariai, T
   <br>Kojima, H
   <br>Obari, H
   <br>Itai, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nariai, Tomoko
   <br>Kojima, Hiroaki
   <br>Obari, Hiroyuki
   <br>Itai, Shiroh</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Method of intonation conversion for facilitating English communication
   between native and non-native speakers</td>
</tr>

<tr>
<td valign="top">SO </td><td>2018 4TH INTERNATIONAL CONFERENCE ON UNIVERSAL VILLAGE (IEEE UV 2018):
   HUMANKIND IN HARMONY WITH NATURE THROUGH WISE USE OF TECHNOLOGY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>4th International Conference on Universal Village (UV) - Humankind in
   Harmony with Nature Through Wise Use of Technology</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 21-24, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>MIT, Boston, MA</td>
</tr>

<tr>
<td valign="top">HO </td><td>MIT</td>
</tr>

<tr>
<td valign="top">DE </td><td>speech analysis; speech synthesis; intonatic voice conversion; STRAIGHT</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION; DEEP; PROSODY; STRESS</td>
</tr>

<tr>
<td valign="top">AB </td><td>With the growth of the Internet and globalization, hardships in English speech communication became more serious for non-native speakers. It would be desirable to develop a speech communication device which can convert English speech into a more intelligible one for listeners. In this study, English speech was hypothesized to be more intelligible to non-native listeners by being modified into the listeners' native language speaking style. First, the characteristics of English between native and Japanese speeches were statistically analyzed. The analysis determined the modification model marked modification of intonation of native speakers' speech so as to include Japanese characteristics. The hypothesis was realized by speech modification generated by a speech analysis, modification and synthesis system, STRAIGHT. Then, the modified speeches were evaluated on the basis of listening experiments involving Japanese listeners. The experiments consisted of dictation tests and preference tests, and resulted in an increase in the scores. The results, therefore, verified the hypothesis. This also revealed the potential feasibility of the speech conversion method which facilitates spoken English communication.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nariai, Tomoko; Itai, Shiroh] Tsukuba Gakuin Univ, Econ &amp; Informat
   Dept, Tsukuba, Ibaraki, Japan.
   <br>[Kojima, Hiroaki; Obari, Hiroyuki] Natl Inst Adv Ind Sci &amp; Technol,
   Intelligent Syst Res Inst, Tsukuba, Ibaraki, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nariai, T (reprint author), Tsukuba Gakuin Univ, Econ &amp; Informat Dept, Tsukuba, Ibaraki, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>tomoko.nariai@gmail.com; h.kojima@aist.go.jp; obari119@gmail.com;
   itai@tsukuba-g.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000462628400020</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Miyamoto, S
   <br>Nose, T
   <br>Ito, S
   <br>Koike, H
   <br>Chiba, Y
   <br>Ito, A
   <br>Shinozaki, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Miyamoto, Sou
   <br>Nose, Takashi
   <br>Ito, Suzunosuke
   <br>Koike, Harunori
   <br>Chiba, Yuya
   <br>Ito, Akinori
   <br>Shinozaki, Takahiro</td>
</tr>

<tr>
<td valign="top">BE </td><td>Pan, JS
   <br>Tsai, PW
   <br>Watada, J
   <br>Jain, LC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Conversion from Arbitrary Speakers Based on Deep Neural Networks
   with Adversarial Learning</td>
</tr>

<tr>
<td valign="top">SO </td><td>ADVANCES IN INTELLIGENT INFORMATION HIDING AND MULTIMEDIA SIGNAL
   PROCESSING, PT II</td>
</tr>

<tr>
<td valign="top">SE </td><td>Smart Innovation Systems and Technologies</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>13th International Conference on Intelligent Information Hiding and
   Multimedia Signal Processing (IIH-MSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 12-15, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Univ Teknologi Petronas, Matsue, JAPAN</td>
</tr>

<tr>
<td valign="top">HO </td><td>Univ Teknologi Petronas</td>
</tr>

<tr>
<td valign="top">DE </td><td>DNN-based voice conversion; Adversarial learning; Spectral differential
   filter; Model training</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this study, we propose a voice conversion technique from arbitrary speakers based on deep neural networks using adversarial learning, which is realized by introducing adversarial learning to the conventional voice conversion. Adversarial learning is expected to enable us more natural voice conversion by using a discriminative model which classifies input speech to natural speech or converted speech in addition to a generative model. Experiments showed that proposed method was effective to enhance global variance (GV) of mel-cepstrum but naturalness of converted speech was a little lower than speech using the conventional variance compensation technique.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Miyamoto, Sou; Nose, Takashi; Ito, Suzunosuke; Koike, Harunori; Chiba,
   Yuya; Ito, Akinori] Tohoku Univ, Grad Sch Engn, Aoba Ku, Aramaki Aza
   Aoba 6-6-05, Sendai, Miyagi 9808579, Japan.
   <br>[Ito, Suzunosuke; Koike, Harunori; Shinozaki, Takahiro] Tokyo Inst
   Technol, Sch Engn, Dept Informat &amp; Commun Engn, Midori Ku, Nagatsuta Cho
   4259, Yokohama, Kanagawa 2268502, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Miyamoto, S (reprint author), Tohoku Univ, Grad Sch Engn, Aoba Ku, Aramaki Aza Aoba 6-6-05, Sendai, Miyagi 9808579, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>sou.miyamoto.s8@dc.tohoku.ac.jp; tnose@m.tohoku.ac.jp;
   yuya@spcom.ecei.tohoku.ac.jp; aito@spcom.ecei.tohoku.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">VL </td><td>82</td>
</tr>

<tr>
<td valign="top">BP </td><td>97</td>
</tr>

<tr>
<td valign="top">EP </td><td>103</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/978-3-319-63859-1_13</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000460570700013</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hasunuma, Y
   <br>Hirayama, C
   <br>Kobayashi, M
   <br>Nagao, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hasunuma, Yuta
   <br>Hirayama, Chiaki
   <br>Kobayashi, Masayuki
   <br>Nagao, Tomoharu</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Non-parallel Voice Conversion using Generative Adversarial Networks</td>
</tr>

<tr>
<td valign="top">SO </td><td>2018 IEEE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN, AND CYBERNETICS
   (SMC)</td>
</tr>

<tr>
<td valign="top">SE </td><td>IEEE International Conference on Systems Man and Cybernetics Conference
   Proceedings</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Systems, Man, and Cybernetics (SMC)</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 07-10, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>IEEE Syst Man &amp; Cybernet Soc, Miyazaki, JAPAN</td>
</tr>

<tr>
<td valign="top">HO </td><td>IEEE Syst Man &amp; Cybernet Soc</td>
</tr>

<tr>
<td valign="top">DE </td><td>non-parallel voice conversion; generative adversarial networks; deep
   neural network</td>
</tr>

<tr>
<td valign="top">AB </td><td>Considering the ease of data collection, it is desirable to build a voice conversion system (VCS) from non-parallel voice data, with different sentences read by the source and target speakers. Previous non-parallel VCSs have used either a melcepstrum or spectral envelope as an input feature. However, these features have different acoustic characteristics that play important roles in speaker recognition. Thus, we propose a non-parallel VCS that efficiently uses both mel-cepstrum and spectral envelopes as input features. Our method is based on three key strategies: 1) we use generative adversarial networks for voice conversion; 2) we add noise to facilitate the training of the formant part; and 3) we integrate the acoustic features to generate high-quality converted voices. Subjective evaluations in the Voice Conversion Challenge 2016 (VCC 2016) revealed that our model outperformed the previous approaches in terms of the naturalness and similarity of the converted voice. Our converted voice result can be watched at https://youtu.be/XLPg8kr3XKE.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Hasunuma, Yuta; Hirayama, Chiaki; Kobayashi, Masayuki; Nagao, Tomoharu]
   Yokohama Natl Univ, Yokohama, Kanagawa, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hasunuma, Y (reprint author), Yokohama Natl Univ, Yokohama, Kanagawa, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>hasunuma-yuta-sd@ynu.jp; hirayama-chiaki-cw@ynu.jp;
   kobayashi-masayuki-xc@ynu.jp; nagao@ynu.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>1635</td>
</tr>

<tr>
<td valign="top">EP </td><td>1640</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/SMC.2018.00283</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000459884801120</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Midtlyng, M
   <br>Sato, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Midtlyng, Mads
   <br>Sato, Yuji</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Adaptation from Mean Dataset Voice Profile with Dynamic Power</td>
</tr>

<tr>
<td valign="top">SO </td><td>2018 IEEE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN, AND CYBERNETICS
   (SMC)</td>
</tr>

<tr>
<td valign="top">SE </td><td>IEEE International Conference on Systems Man and Cybernetics Conference
   Proceedings</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Systems, Man, and Cybernetics (SMC)</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 07-10, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>IEEE Syst Man &amp; Cybernet Soc, Miyazaki, JAPAN</td>
</tr>

<tr>
<td valign="top">HO </td><td>IEEE Syst Man &amp; Cybernet Soc</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice adaptation; speech processing; quantization; sound-index; voice
   profile; prosody</td>
</tr>

<tr>
<td valign="top">ID </td><td>CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper proposes a nonparallel voice adaptation, a form of speech processing, which utilizes a voice profile with a mean dataset, paired with auxiliary sets generated from a stylized quantization from a single recording where only the target voice subject is required to train beforehand aimed towards real-time applications. Conventional methods suffer from complexified training where two or more speakers must be trained in parallel, thus impractical for actual real-time uses. Our quantized data is used to produce sound-indexes, a signature for each sound-frame for comparing speakers. Prosody is sustained by translating the input's power in the current frame into the corresponding output frame real-time. The design was tested with an English voice profile and the rate of matching output and frames were measured to determine a factor of fragmentation, which is the degree of unsuccessful segments. Results showed diverse fragmentation, complementing the input length. Basic input ranged in the very good 5-6% fragmentation range, while convoluted input up to 26%. The average matching time was 176 milliseconds. Future work focuses on improving performance and decreasing fragmentation.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Midtlyng, Mads; Sato, Yuji] Hosei Univ, Dept Comp Sci, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Midtlyng, M (reprint author), Hosei Univ, Dept Comp Sci, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>midtlyng.madsalexander.9c@stu.hosei.ac.jp; yuji@k.hosei.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>2037</td>
</tr>

<tr>
<td valign="top">EP </td><td>2042</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/SMC.2018.00351</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000459884802019</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nanzaka, R
   <br>Kitamura, T
   <br>Adachi, Y
   <br>Tai, K
   <br>Takiguchi, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nanzaka, Ryuka
   <br>Kitamura, Tsuyoshi
   <br>Adachi, Yuji
   <br>Tai, Kiyoto
   <br>Takiguchi, Tetsuya</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Spectrum Enhancement of Singing Voice Using Deep Learning</td>
</tr>

<tr>
<td valign="top">SO </td><td>2018 IEEE INTERNATIONAL SYMPOSIUM ON MULTIMEDIA (ISM 2018)</td>
</tr>

<tr>
<td valign="top">SE </td><td>IEEE International Symposium on Multimedia-ISM</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>20th IEEE International Symposium on Multimedia (ISM)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 10-12, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Taichung, TAIWAN</td>
</tr>

<tr>
<td valign="top">DE </td><td>deep learning; speech enhancement; voice conversion; highway networks</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we propose a novel singing-voice enhancement system that makes the singing voice of amateurs similar to that of professional opera singers, where the singing voice of amateurs is emphasized by using a singing voice of a professional opera singer on a frequency band that represents the remarkable characteristic of the professional singer. Moreover, our proposed singing-voice enhancement based on highway networks is able to convert any song (that a professional opera singer does not sing). As a result of our experiments, the singing voice of the amateur singer at the middle-high frequency range which contains a lot of frequency components that affect glossiness was emphasized while maintaining speaker characteristics.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nanzaka, Ryuka; Kitamura, Tsuyoshi; Takiguchi, Tetsuya] Kobe Univ, Grad
   Sch Syst Informat, Kobe, Hyogo, Japan.
   <br>[Adachi, Yuji; Tai, Kiyoto] MEC Co LTD, Amagasaki, Hyogo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nanzaka, R (reprint author), Kobe Univ, Grad Sch Syst Informat, Kobe, Hyogo, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ryuka.nanzaka@stu.kobe-u.ac.jp; kitamura@stu.kobe-u.ac.jp;
   takigu@kobe-u.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>167</td>
</tr>

<tr>
<td valign="top">EP </td><td>170</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/ISM.2018.00-18</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000459863600029</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hossain, MJ
   <br>Al Amin, SM
   <br>Islam, MS
   <br>Marium-E-Jannat</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hossain, Md. Jakir
   <br>Al Amin, Sayed Mahmud
   <br>Islam, Md. Saiful
   <br>Marium-E-Jannat</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Development of robotic voice conversion for RIBO using text-to-speech
   synthesis</td>
</tr>

<tr>
<td valign="top">SO </td><td>2018 4TH INTERNATIONAL CONFERENCE ON ELECTRICAL ENGINEERING AND
   INFORMATION &amp; COMMUNICATION TECHNOLOGY (ICEEICT)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Electrical Engineering and Information
   Communication Technology</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>4th International Conference on Electrical Engineering and Information
   and Communication Technology (iCEEiCT)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 13-15, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Milit Inst Sci &amp; Technol, Dhaka, BANGLADESH</td>
</tr>

<tr>
<td valign="top">HO </td><td>Milit Inst Sci &amp; Technol</td>
</tr>

<tr>
<td valign="top">DE </td><td>TTS; RIBO; Diode; Ring modulator; VCA; Transformer; RF</td>
</tr>

<tr>
<td valign="top">AB </td><td>RIBO is the first social interaction robot in Bangladesh. This robot is designed and developed by 'ROBO SUST' team of Shahjalal University of Science and Technology. RIBO is able to hands and eyes ups and downs, can walk very slowly and can speak some Bengali recorded sentences. Now the 'ROBO SUST' team is trying to develop the RIBO so that it can communicate with human. One of the parts to communicate with human is convert bengali text to bengali speech in robotic voice. In this article, we propose a method which will convert bengali text to speech in robotic voice using google text to speech system and ring modulator. There are existed some text to speech synthesizer system which can convert bengali text to bengali speech. Among these TTS synthesizer system google TTS system for bengali is better. Hence, we use google text to speech system to produce bengali speech from any bengali written text. Google TTS synthesizer system produces speech as audio object file which can be converted to .mp3 file. Then we modify this .mp3 file using the characteristics of diode and ring modulator concept to get machine voice. After changing pitch and speed of this machine voice we get our final robotic voice which will be used in RIBO as his voice.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Hossain, Md. Jakir; Al Amin, Sayed Mahmud; Islam, Md. Saiful;
   Marium-E-Jannat] Shahjalal Univ Sci &amp; Technol Sylhet, Dept Comp Sci &amp;
   Engn, Sylhet, Bangladesh.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hossain, MJ (reprint author), Shahjalal Univ Sci &amp; Technol Sylhet, Dept Comp Sci &amp; Engn, Sylhet, Bangladesh.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jakirhossainsust54@gmail.com; shapathsust@gmail.com;
   saiful-cse@sust.edu; jannat.16.11@gmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>422</td>
</tr>

<tr>
<td valign="top">EP </td><td>425</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000459239000079</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Yin, HX
   <br>Li, YF
   <br>Xing, FY
   <br>Wu, B
   <br>Zhou, ZH
   <br>Zhang, WB</td>
</tr>

<tr>
<td valign="top">AF </td><td>Yin, Hongxi
   <br>Li, Yanfang
   <br>Xing, Fangyuan
   <br>Wu, Bin
   <br>Zhou, Zhiheng
   <br>Zhang, Wenbo</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Hybrid Acoustic, Wireless Optical and Fiber-optic Underwater Cellular
   Mobile Communication Networks</td>
</tr>

<tr>
<td valign="top">SO </td><td>2018 IEEE 18TH INTERNATIONAL CONFERENCE ON COMMUNICATION TECHNOLOGY
   (ICCT)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>18th IEEE International Conference on Communication Technology (IEEE
   ICCT)</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 08-11, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Chongqing, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>HUCN; network topology; protocol architecture; 100Mbps Ethernet traffic</td>
</tr>

<tr>
<td valign="top">AB </td><td>A novel underwater cellular mobile communication network by incorporating acoustic wave, wireless light and optical-fiber, and its protocol architecture are proposed. A hardware circuit module for the Ethernet protocol conversion is designed besides developed underwater optical wireless transmitter and receiver. A variety of media including data, voice and video, are transmitted on the underwater optical wireless Ethernet established by us.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Yin, Hongxi; Li, Yanfang; Xing, Fangyuan; Wu, Bin; Zhou, Zhiheng;
   Zhang, Wenbo] Dalian Univ Technol, Lab Opt Commun &amp; Photon Technol, Sch
   Informat &amp; Commun Engn, Dalian 116023, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Yin, HX (reprint author), Dalian Univ Technol, Lab Opt Commun &amp; Photon Technol, Sch Informat &amp; Commun Engn, Dalian 116023, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>hxyin@dlut.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>721</td>
</tr>

<tr>
<td valign="top">EP </td><td>726</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000458684600140</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Zheng, CY
   <br>Zhang, XW
   <br>Sun, M
   <br>Xing, YB
   <br>Shi, HW</td>
</tr>

<tr>
<td valign="top">AF </td><td>Zheng, Changyan
   <br>Zhang, Xiongwei
   <br>Sun, Meng
   <br>Xing, Yibo
   <br>Shi, Huawen</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Throat Microphone Speech Enhancement via Progressive Learning of
   Spectral Mapping Based on LSTM-RNN</td>
</tr>

<tr>
<td valign="top">SO </td><td>2018 IEEE 18TH INTERNATIONAL CONFERENCE ON COMMUNICATION TECHNOLOGY
   (ICCT)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>18th IEEE International Conference on Communication Technology (IEEE
   ICCT)</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 08-11, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Chongqing, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>throat microphone; voice conversion; artificial bandwidth extension;
   recurrent neural networks; long short-term memory</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we propose a progressive spectral mapping learning algorithm for throat microphone (TM) speech enhancement. Unlike previous full-band spectra mapping algorithms, this algorithm divides the spectra mapping from TM speech to Air-conducted (AC) speech into two tasks, one is the voice conversion task, and the other is the artificial bandwidth extension task. Long short-term memory recurrent neural network (LSTM-RNN) is further deployed as the mapping model. Objective evaluation results show that the TM speech quality is improved when compared with conventional full-band spectra mapping framework and DNNbased mapping model.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Zheng, Changyan; Zhang, Xiongwei; Sun, Meng; Xing, Yibo; Shi, Huawen]
   Army Engn Univ, Lab Intelligent Informat Proc, Nanjing, Jiangsu, Peoples
   R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Zheng, CY (reprint author), Army Engn Univ, Lab Intelligent Informat Proc, Nanjing, Jiangsu, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>echoaimaomao@163.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>1002</td>
</tr>

<tr>
<td valign="top">EP </td><td>1006</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000458684600194</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ben Othmane, I
   <br>Di Martino, J
   <br>Ouni, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ben Othmane, Imen
   <br>Di Martino, Joseph
   <br>Ouni, Kais</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Improving the computational performance of standard GMM-based voice
   conversion systems used in real-time applications</td>
</tr>

<tr>
<td valign="top">SO </td><td>2018 INTERNATIONAL CONFERENCE ON ELECTRONICS, CONTROL, OPTIMIZATION AND
   COMPUTER SCIENCE (ICECOCS)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Electronics, Control, Optimization and
   Computer Science (ICECOCS)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 05-06, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>IBN TOFAIL Univ Kenitra, Fac Sci, Kenitra, MOROCCO</td>
</tr>

<tr>
<td valign="top">HO </td><td>IBN TOFAIL Univ Kenitra, Fac Sci</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; Gaussian Mixture Models; classification; local mapping
   functions</td>
</tr>

<tr>
<td valign="top">ID </td><td>ALGORITHM; SIGNAL</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion (VC) can be described as finding a mapping function which transforms the features extracted from a source speaker to those of a target speaker. Gaussian mixture model (GMM) based conversion is the most commonly used technique in VC, but is often sensitive to overfitting and oversmoothing. To address these issues, we propose a secondary classification by applying a K-means classification in each class obtained by a primary classification in order to obtain more precise local conversion functions. This proposal avoids the need for complex training algorithms because the local mapping functions are determined at the same time. The proposed approach consists of a Fourier cepstral analysis, followed by a training phase in order to find the local mapping functions which transform the vocal tract characteristics of the source speaker into those of the target speaker. The converted parameters together with excitation and phase extracted from the target training space using a frame index selection are used in the synthesis step to generate a converted speech with target speech characteristics.
   <br>Objective and subjective experiments prove that the proposed technique outperforms the baseline GMM approach while greatly reducing the training and transformation computation times.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ben Othmane, Imen; Ouni, Kais] Univ Carthage, Natl Engn Sch Carthage,
   ENICarthage, Res Unit Signals &amp; Mechatron Syst,SMS,UR13ES49, Tunis,
   Tunisia.
   <br>[Di Martino, Joseph] LORIA Lab Lorrain Rech Informat &amp; Ses Applicat, BP
   239 54506, Vandoeuvre Les Nancy, France.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ben Othmane, I (reprint author), Univ Carthage, Natl Engn Sch Carthage, ENICarthage, Res Unit Signals &amp; Mechatron Syst,SMS,UR13ES49, Tunis, Tunisia.</td>
</tr>

<tr>
<td valign="top">EM </td><td>imen.benothmen@hotmail.fr; joseph.di-martino@loria.fr;
   kais.ouni@enicarthage.rnu.tn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000458420800010</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Shang, JC
   <br>Chen, S
   <br>Wu, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Shang, Jiacheng
   <br>Chen, Si
   <br>Wu, Jie</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Defending Against Voice Spoofing: A Robust Software-based Liveness
   Detection System</td>
</tr>

<tr>
<td valign="top">SO </td><td>2018 IEEE 15TH INTERNATIONAL CONFERENCE ON MOBILE AD HOC AND SENSOR
   SYSTEMS (MASS)</td>
</tr>

<tr>
<td valign="top">SE </td><td>IEEE International Conference on Mobile Ad-hoc and Sensor Systems</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>15th IEEE International Conference on Mobile Ad Hoc and Sensor Systems
   (MASS)</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 09-12, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Chengdu, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice authentication; liveness detection</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEAKER VERIFICATION; ACCURACY</td>
</tr>

<tr>
<td valign="top">AB </td><td>The recent proliferation of smartphones has been the primary driving factor behind the booming of voice-based mobile applications. However, the human voice is often exposed to the public in many different scenarios, and an adversary can easily steal a person's voice and attack voice-based applications with the help of state-of-the-art voice synthesis/conversion softwares. In this paper, we propose a robust software-based voice liveness detection system for defending against voice spoofing attack. The proposed system is tailored for mobile platforms and can be easily integrated with existing mobile applications. We propose three approaches based on leveraging the vibration of human vocal cords, the motion of the human vocal system, and the functionality of vibration motor inside the smartphone. Experimental results show that our system can detect a live speaker with a mean accuracy of 94.38% and detect an attacker with a mean accuracy of 88.89% by combining three approaches we proposed.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Shang, Jiacheng; Wu, Jie] Temple Univ, Ctr Network Comp, Philadelphia,
   PA 19121 USA.
   <br>[Chen, Si] West Chester Univ Penn, Comp Sci Dept, W Chester, PA 19383
   USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Shang, JC (reprint author), Temple Univ, Ctr Network Comp, Philadelphia, PA 19121 USA.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>28</td>
</tr>

<tr>
<td valign="top">EP </td><td>36</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/MASS.2018.00016</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000457600600004</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kaneko, T
   <br>Kameoka, H</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kaneko, Takuhiro
   <br>Kameoka, Hirokazu</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>CycleGAN-VC: Non-parallel Voice Conversion Using Cycle-Consistent
   Adversarial Networks</td>
</tr>

<tr>
<td valign="top">SO </td><td>2018 26TH EUROPEAN SIGNAL PROCESSING CONFERENCE (EUSIPCO)</td>
</tr>

<tr>
<td valign="top">SE </td><td>European Signal Processing Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>European Signal Processing Conference (EUSIPCO)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 03-07, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Rome, ITALY</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; non-parallel conversion; generative adversarial
   networks; CycleGAN; gated CNN</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPARSE REPRESENTATION; SPEECH SYNTHESIS; NEURAL-NETWORKS</td>
</tr>

<tr>
<td valign="top">AB </td><td>We propose a non-parallel voice-conversion (VC) method that can learn a mapping from source to target speech without relying on parallel data. The proposed method is particularly noteworthy in that it is general purpose and high quality and works without any extra data, modules, or alignment procedure. Our method, called CycleGAN-VC, uses a cycle-consistent adversarial network (CycleGAN) with gated convolutional neural networks (CNNs) and an identity-mapping loss. A CycleGAN learns forward and inverse mappings simultaneously using adversarial and cycle-consistency losses. This makes it possible to find an optimal pseudo pair from non-parallel data. Furthermore, the adversarial loss can bring the converted speech close to the target one on the basis of indistinguishability without explicit density estimation. This allows to avoid over-smoothing caused by statistical averaging, which occurs in many conventional statistical model-based VC methods that represent data distribution explicitly. We configure a CycleGAN with gated CNNs and train it with an identity-mapping loss. This allows the mapping function to capture sequential and hierarchical structures while preserving linguistic information. We evaluated our method on a non-parallel VC task. An objective evaluation showed that the converted feature sequence was near natural in terms of global variance and modulation spectra, which are structural indicators highly correlated with subjective evaluation. A subjective evaluation showed that the quality of the converted speech was comparable to that obtained with a Gaussian mixture model-based parallel VC method even though CycleGAN-VC is trained under disadvantageous conditions (non-parallel and half the amount of data).</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kaneko, Takuhiro; Kameoka, Hirokazu] NTT Corp, NTT Commun Sci Labs,
   Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kaneko, T (reprint author), NTT Corp, NTT Commun Sci Labs, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>kaneko.takuhiro@lab.ntt.co.jp; kameoka.hirokazu@lab.ntt.co.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>2100</td>
</tr>

<tr>
<td valign="top">EP </td><td>2104</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000455614900422</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kobayashi, K
   <br>Toda, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kobayashi, Kazuhiro
   <br>Toda, Tomoki</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Electrolaryngeal Speech Enhancement with Statistical Voice Conversion
   based on CLDNN</td>
</tr>

<tr>
<td valign="top">SO </td><td>2018 26TH EUROPEAN SIGNAL PROCESSING CONFERENCE (EUSIPCO)</td>
</tr>

<tr>
<td valign="top">SE </td><td>European Signal Processing Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>European Signal Processing Conference (EUSIPCO)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 03-07, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Rome, ITALY</td>
</tr>

<tr>
<td valign="top">DE </td><td>electrolaryngeal speech; statistical voice conversion; speech
   enhancement; deep neural network</td>
</tr>

<tr>
<td valign="top">ID </td><td>NOISE</td>
</tr>

<tr>
<td valign="top">AB </td><td>An electrolarynx (EL) is a widely used device to mechanically generate excitation signals, making it possible for laryngectomees to produce EL speech without vocal fold vibrations. Although EL speech sounds relatively intelligible, is significantly less natural than normal speech owing to its mechanical excitation signals. To address this issue, a statistical voice conversion (VC) technique based on Gaussian mixture models (GMMs) has been applied to EL speech enhancement. In this technique, input EL speech is converted into target normal speech by converting spectral features of the EL speech into spectral and excitation parameters of normal speech using GMMs. Although this technique makes it possible to significantly improve the naturalness of EL speech, the enhanced EL speech is still far from the target normal speech. To improve the performance of statistical EL speech enhancement, in this paper, we propose an EL-to-speech conversion method based on CLDNNs consisting of convolutional layers, long short-term memory recurrent layers, and fully connected deep neural network layers. Three CLDNNs are trained, one to convert EL speech spectral features into spectral and band-aperiodicity parameters, one to convert them into unvoiced/voiced symbols, and one to convert them into continuous F-0 patterns. The experimental results demonstrate that the proposed method significantly outperforms the conventional method in terms of both objective evaluation metrics and subjective evaluation scores.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kobayashi, Kazuhiro; Toda, Tomoki] Nagoya Univ, Informat Technol Ctr,
   Nagoya, Aichi, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kobayashi, K (reprint author), Nagoya Univ, Informat Technol Ctr, Nagoya, Aichi, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>kobayashi.kazuhiro@g.sp.m.is.nagoya-u.ac.jp; tomoki@icts.nagoya-u.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>2115</td>
</tr>

<tr>
<td valign="top">EP </td><td>2119</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000455614900425</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hojo, N
   <br>Kameoka, H
   <br>Tanaka, K
   <br>Kaneko, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hojo, Nobukatsu
   <br>Kameoka, Hirokazu
   <br>Tanaka, Kou
   <br>Kaneko, Takuhiro</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Automatic speech pronunciation correction with dynamic frequency
   warping-based spectral conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>2018 26TH EUROPEAN SIGNAL PROCESSING CONFERENCE (EUSIPCO)</td>
</tr>

<tr>
<td valign="top">SE </td><td>European Signal Processing Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>European Signal Processing Conference (EUSIPCO)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 03-07, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Rome, ITALY</td>
</tr>

<tr>
<td valign="top">DE </td><td>Accent conversion; dynamic frequency warping; voice conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>MODEL</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper deals with the problem of pronunciation conversion (PC) task, a problem to reduce non-native accents in speech while preserving the original speaker identity. Although PC can be regarded as a special class of voice conversion (VC), a straightforward application of conventional VC methods to a PC task would not be successful since with VC the original speaker identity of input speech may also change. This problem is due to the fact that two functions, namely an accent conversion function and a speaker similarity conversion function, are entangled in an acoustic feature mapping function. This paper proposes dynamic frequency warping (DFW)-based spectral conversion to solve this problem. The proposed DFW-based PC converts the pronunciation of input speech by relocating the formants to the corresponding positions in which native speakers tend to locate their formants. We expect the speaker identity is preserved because other factors such as formant powers are kept unchanged. in a low frequency domain evaluation results confirmed that DFW-based PC with spectral residual modeling showed higher speaker similarity to original speaker while showing a comparable effect of reducing foreign accents to a conventional GMM-based VC method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Hojo, Nobukatsu; Kameoka, Hirokazu; Tanaka, Kou; Kaneko, Takuhiro] NTT
   Corp, NTT Commun Sci Labs, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hojo, N (reprint author), NTT Corp, NTT Commun Sci Labs, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>hojo.nobukatsu@lab.ntt.co.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>2310</td>
</tr>

<tr>
<td valign="top">EP </td><td>2314</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000455614900464</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sutcliffe, B
   <br>Wiggins, L
   <br>Rubin, D
   <br>Aharonson, V</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sutcliffe, Bianca
   <br>Wiggins, Lindzi
   <br>Rubin, David
   <br>Aharonson, Vered</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice quality enhancement for vocal tract rehabilitation</td>
</tr>

<tr>
<td valign="top">SO </td><td>2018 3RD BIENNIAL SOUTH AFRICAN BIOMEDICAL ENGINEERING CONFERENCE
   (SAIBMEC)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>3rd Biennial South African Biomedical Engineering Conference (SAIBMEC)</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 04-06, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Stellenbosch, SOUTH AFRICA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; source-filter model; Gaussian mixture model; linear
   regression; linear predictive coding coefficients; line spectral
   frequencies</td>
</tr>

<tr>
<td valign="top">AB </td><td>Vocal rehabilitation devices used by patients after Laryngectomy produce an unnatural sounding speech. Our study aims at increasing the quality of these synthetically generated voices by implementing human-like characteristics. A simplified source filter model, linear predictive coding coefficients and line spectral frequencies were used to model the vocal tract and manipulate the acoustic features of their resulting speech. Two different mapping functions were employed to convert between the features of synthetically generated voice and those of a human voice: A Gaussian mixture model and a linear regression model. The models were trained on a set of 50 human and 50 synthetic voice utterances. Both mapping functions yielded significant changes in the transformed synthetic voices and their spectra were similar to the human voices. The linear regression model mapping produced slightly better results compared to the Gaussian mixture model mapping. Listeners' tests confirmed this result, but indicated that voices re-synthesized from the transformed model coefficients, improved on the synthetic voice but still sounded unnatural. This may imply that the vocal tract model is lacking in information that produces the subjective perception of "artificial speech". Future work will investigate an elaborate model which will include the speech production excitation and radiation signals and the transformation of their features. These models have the potential to improve the conversion of synthetically generated electrolarynx voice into human sounding one.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Sutcliffe, Bianca; Wiggins, Lindzi; Rubin, David; Aharonson, Vered]
   Univ Witwatersrand, Sch Elect &amp; Informat Engn, Biomed Engn Res Grp,
   Johannesburg, South Africa.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sutcliffe, B (reprint author), Univ Witwatersrand, Sch Elect &amp; Informat Engn, Biomed Engn Res Grp, Johannesburg, South Africa.</td>
</tr>

<tr>
<td valign="top">EM </td><td>biancasutcliffe@gmail.com; lindzi.wiggins@gmail.com;
   david.rubin@wits.ac.za; vered.aharonson@wits.ac.za</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000454456100026</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Dandekar, D</td>
</tr>

<tr>
<td valign="top">AF </td><td>Dandekar, Deepra</td>
</tr>

<tr>
<td valign="top">TI </td><td>Translation and the Christian Conversion of Women in Colonial India:
   Rev. Sheshadri and Bala Sundarabai Thakur</td>
</tr>

<tr>
<td valign="top">SO </td><td>SOUTH ASIA-JOURNAL OF SOUTH ASIAN STUDIES</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Bengal; Christianity; colonialism; conversion; English; Hindu; Marathi;
   mission; translation; women</td>
</tr>

<tr>
<td valign="top">AB </td><td>This article foregrounds the interstitial and hybrid third voice of a nineteenth-century Christian convert in colonial India. Bala Shundoree Tagore, a Bengali woman and wife to the esteemed Gyanendra Mohan Tagore, was declared spiritually Christian by missionaries, even though she died before being baptised. Bala's narrative production by her biographers and translators obfuscated and transformed her voice, writing her into the history of Indian missions as a success story. Refashioned as a gendered symbol for Indian Christian women from the nineteenth century, Bala's narrative was utilised by missionaries by divesting her of the agency she possessed.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Dandekar, Deepra] Max Planck Inst Human Dev, Ctr Hist Emot, Berlin,
   Germany.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Dandekar, D (reprint author), Max Planck Inst Human Dev, Ctr Hist Emot, Berlin, Germany.</td>
</tr>

<tr>
<td valign="top">EM </td><td>dandekar@mpib-berlin.mpg.de</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">VL </td><td>41</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>366</td>
</tr>

<tr>
<td valign="top">EP </td><td>383</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1080/00856401.2018.1443370</td>
</tr>

<tr>
<td valign="top">SC </td><td>Area Studies; History; Asian Studies</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000432639400008</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Chorowski, J
   <br>Weiss, RJ
   <br>Saurous, RA
   <br>Bengio, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Chorowski, Jan
   <br>Weiss, Ron J.
   <br>Saurous, Rif A.
   <br>Bengio, Samy</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>ON USING BACKPROPAGATION FOR SPEECH TEXTURE GENERATION AND VOICE
   CONVERSION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech and Signal Processing
   (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 15-20, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Calgary, CANADA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Texture synthesis; voice conversion; style transfer; deep neural
   networks; convolutional networks; CTC</td>
</tr>

<tr>
<td valign="top">AB </td><td>Inspired by recent work on neural network image generation which rely on backpropagation towards the network inputs, we present a proof-of-concept system for speech texture synthesis and voice conversion based on two mechanisms: approximate inversion of the representation learned by a speech recognition neural network, and on matching statistics of neuron activations between different source and target utterances. Similar to image texture synthesis and neural style transfer, the system works by optimizing a cost function with respect to the input waveform samples. To this end we use a differentiable mel-filterbank feature extraction pipeline and train a convolutional CTC speech recognition network. Our system is able to extract speaker characteristics from very limited amounts of target speaker data, as little as a few seconds, and can be used to generate realistic speech babble or reconstruct an utterance in a different voice.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Chorowski, Jan; Weiss, Ron J.; Saurous, Rif A.; Bengio, Samy] Google
   Brain, Mountain View, CA 94043 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Chorowski, J (reprint author), Google Brain, Mountain View, CA 94043 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>chorowski@google.com; ronw@google.com; rif@google.com; bengio@google.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>2256</td>
</tr>

<tr>
<td valign="top">EP </td><td>2260</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000446384602087</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Gao, Y
   <br>Singh, R
   <br>Raj, B</td>
</tr>

<tr>
<td valign="top">AF </td><td>Gao, Yang
   <br>Singh, Rita
   <br>Raj, Bhiksha</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>VOICE IMPERSONATION USING GENERATIVE ADVERSARIAL NETWORKS</td>
</tr>

<tr>
<td valign="top">SO </td><td>2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech and Signal Processing
   (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 15-20, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Calgary, CANADA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice impersonation; generative adversarial network; style
   transformation; style transfer</td>
</tr>

<tr>
<td valign="top">ID </td><td>CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice impersonation is not the same as voice transformation, although the latter is an essential element of it. In voice impersonation, the resultant voice must convincingly convey the impression of having been naturally produced by the target speaker, mimicking not only the pitch and other perceivable signal qualities, but also the style of the target speaker. In this paper, we propose a novel neural-network based speech quality-and style-mimicry framework for the synthesis of impersonated voices. The framework is built upon a fast and accurate generative adversarial network model. Given spectrographic representations of source and target speakers' voices, the model learns to mimic the target speaker's voice quality and style, regardless of the linguistic content of either's voice, generating a synthetic spectrogram from which the time-domain signal is reconstructed using the Griffin-Lim method. In effect, this model reframes the well-known problem of style-transfer for images as the problem of style-transfer for speech signals, while intrinsically addressing the problem of durational variability of speech sounds. Experiments demonstrate that the model can generate extremely convincing samples of impersonated speech. It is even able to impersonate voices across different genders effectively. Results are qualitatively evaluated using standard procedures for evaluating synthesized voices.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Gao, Yang; Singh, Rita; Raj, Bhiksha] Carnegie Mellon Univ, Elect &amp;
   Comp Engn Dept, Pittsburgh, PA 15213 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Gao, Y (reprint author), Carnegie Mellon Univ, Elect &amp; Comp Engn Dept, Pittsburgh, PA 15213 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>yanggao@cs.cmu.edu; rsingh@cs.cmu.edu; bhiksha@cs.cmu.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>2506</td>
</tr>

<tr>
<td valign="top">EP </td><td>2510</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000446384602136</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Saito, Y
   <br>Ijima, Y
   <br>Nishida, K
   <br>Takamichi, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Saito, Yuki
   <br>Ijima, Yusuke
   <br>Nishida, Kyosuke
   <br>Takamichi, Shinnosuke</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>NON-PARALLEL VOICE CONVERSION USING VARIATIONAL AUTOENCODERS CONDITIONED
   BY PHONETIC POSTERIORGRAMS AND D-VECTORS</td>
</tr>

<tr>
<td valign="top">SO </td><td>2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech and Signal Processing
   (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 15-20, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Calgary, CANADA</td>
</tr>

<tr>
<td valign="top">DE </td><td>VAE-based non-parallel VC; phonetic posteriorgrams; d-vectors;
   many-to-many VC</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH SYNTHESIS</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper proposes novel frameworks for non-parallel voice conversion (VC) using variational autoencoders (VAEs). Although conventional VAE-based VC models can be trained using non-parallel speech corpora with given speaker representations, phonetic contents of the converted speech tend to vanish because of an over-regularization issue often observed in latent variables of the VAEs. To overcome the issue, this paper proposes a VAE-based non-parallel VC conditioned by not only the speaker representations but also phonetic contents of speech represented as phonetic posteriorgrams (PPGs). Since the phonetic contents are given during the training, we can expect that the VC models effectively learn speaker-independent latent features of speech. Focusing on the point, this paper also extends the conventional VAE-based non-parallel VC to many-tomany VC that can convert arbitrary speakers' characteristics into another arbitrary speakers' ones. We investigate two methods to estimate speaker representations for speakers not included in speech corpora used for training VC models: 1) adapting conventional speaker codes, and 2) using d-vectors for the speaker representations. Experimental results demonstrate that 1) PPGs successfully improve both naturalness and speaker similarity of the converted speech, and 2) both speaker codes and d-vectors can be adopted to the VAE-based many-to-many non-parallel VC.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Saito, Yuki; Takamichi, Shinnosuke] Univ Tokyo, Grad Sch Informat Sci &amp;
   Technol, Tokyo, Japan.
   <br>[Saito, Yuki; Ijima, Yusuke; Nishida, Kyosuke] NTT Corp, NTT Media
   Intelligence Labs, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Saito, Y (reprint author), Univ Tokyo, Grad Sch Informat Sci &amp; Technol, Tokyo, Japan.; Saito, Y (reprint author), NTT Corp, NTT Media Intelligence Labs, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>5274</td>
</tr>

<tr>
<td valign="top">EP </td><td>5278</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000446384605089</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Fang, FM
   <br>Yamagishi, J
   <br>Echizen, I
   <br>Lorenzo-Trueba, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Fang, Fuming
   <br>Yamagishi, Junichi
   <br>Echizen, Isao
   <br>Lorenzo-Trueba, Jaime</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>HIGH-QUALITY NONPARALLEL VOICE CONVERSION BASED ON CYCLE-CONSISTENT
   ADVERSARIAL NETWORK</td>
</tr>

<tr>
<td valign="top">SO </td><td>2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech and Signal Processing
   (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 15-20, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Calgary, CANADA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; deep learning; cycle-consistent adversarial network;
   generative adversarial network</td>
</tr>

<tr>
<td valign="top">AB </td><td>Although voice conversion (VC) algorithms have achieved remarkable success along with the development of machine learning, superior performance is still difficult to achieve when using nonparallel data. In this paper, we propose using a cycle-consistent adversarial network (Cyc1eGAN) for nonparallel data-based VC training. A Cyc1eGAN is a generative adversarial network (GAN) originally developed for unpaired image-to-image translation. A subjective evaluation of inter-gender conversion demonstrated that the proposed method significantly outperformed a method based on the Merlin open source neural network speech synthesis system (a parallel VC system adapted for our setup) and a GAN-based parallel VC system. This is the first research to show that the performance of a nonparallel VC method can exceed that of state-of-the-art parallel VC methods.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Fang, Fuming; Yamagishi, Junichi; Echizen, Isao; Lorenzo-Trueba, Jaime]
   Natl Inst Informat, Tokyo, Japan.
   <br>[Yamagishi, Junichi] Univ Edinburgh, Edinburgh, Midlothian, Scotland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Fang, FM (reprint author), Natl Inst Informat, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>fang@nii.ac.jp; jyamagis@nii.ac.jp; iechizen@nii.ac.jp; jaime@nii.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>5279</td>
</tr>

<tr>
<td valign="top">EP </td><td>5283</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000446384605090</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Liberatore, C
   <br>Zhao, GL
   <br>Gutierrez-Osuna, R</td>
</tr>

<tr>
<td valign="top">AF </td><td>Liberatore, Christopher
   <br>Zhao, Guanlong
   <br>Gutierrez-Osuna, Ricardo</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>VOICE CONVERSION THROUGH RESIDUAL WARPING IN A SPARSE, ANCHOR-BASED
   REPRESENTATION OF SPEECH</td>
</tr>

<tr>
<td valign="top">SO </td><td>2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech and Signal Processing
   (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 15-20, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Calgary, CANADA</td>
</tr>

<tr>
<td valign="top">DE </td><td>sparse coding; voice conversion; residual; dynamic frequency warping;
   weighted frequency warping</td>
</tr>

<tr>
<td valign="top">ID </td><td>LINEAR TRANSFORMATION; REGRESSION; LASSO</td>
</tr>

<tr>
<td valign="top">AB </td><td>In previous work we presented a Sparse, Anchor-Based Representation of speech (SABR) that uses phonemic "anchors" to represent an utterance with a set of sparse non-negative weights. SABR is speaker-independent: combining weights from a source speaker with anchors from a target speaker can be used for voice conversion. Here, we present an extension of the original SABR that significantly improves voice conversion synthesis. Namely, we take the residual signal from the SABR decomposition of the source speaker's utterance, and warp it to the target speaker's space using a weighted warping function learned from pairs of source-target anchors. Using subjective and objective evaluations, we examine the performance of adding the warped residual (SABR+Res) to the original synthesis (SABR). Specifically, listeners rated SABR+Res with an average mean opinion score (MOS) of 3.6, a significant improvement compared to 2.2 MOS for SABR alone (p &lt; 0.01) and 2.5 MOS for a baseline GMM method (p &lt; 0.01). In an XAB speaker identity test, listeners correctly identified the identity of SABR+Res (81%) and SABR (84%) as frequently as a GMM method (82%) (p = 0.70, p = 0.35). These results indicate that adding the warped residual can dramatically improve synthesis while retaining the desirable independent qualities of SABR models.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Liberatore, Christopher; Zhao, Guanlong; Gutierrez-Osuna, Ricardo]
   Texas A&amp;M Univ, College Stn, TX 77843 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Liberatore, C (reprint author), Texas A&amp;M Univ, College Stn, TX 77843 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>cliberatore@tamu.edu; gzhao@tamu.edu; rgutier@tamu.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>5284</td>
</tr>

<tr>
<td valign="top">EP </td><td>5288</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000446384605091</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Niwa, J
   <br>Yoshimura, T
   <br>Hashimoto, K
   <br>Oura, K
   <br>Nankaku, Y
   <br>Tokuda, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Niwa, Jumpei
   <br>Yoshimura, Takenori
   <br>Hashimoto, Kei
   <br>Oura, Keiichiro
   <br>Nankaku, Yoshihiko
   <br>Tokuda, Keiichi</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>STATISTICAL VOICE CONVERSION BASED ON WAVENET</td>
</tr>

<tr>
<td valign="top">SO </td><td>2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech and Signal Processing
   (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 15-20, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Calgary, CANADA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; WaveNet; Deep Neural Network; statistical model</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH RECOGNITION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper proposes a voice conversion technique based on WaveNet to directly generate target audio waveforms from acoustic features of a source speaker. In voice conversion based on statistical models, the relation between acoustic features, such as spectral parameters, extracted from source and target audio waveforms is generally modeled using statistical models, such as Gaussian mixture models and neural networks. Although modeling the relation between acoustic features is reasonable and efficient, these models are not optimized for predicting target audio waveforms because the vocoder parameters are used as intermediate representations. To overcome this problem, we developed a voice conversion method to model the relation between target audio waveforms and acoustic features extracted from source audio waveforms using WaveNet, which is a generative model for audio waveforms. The proposed model can directly generate converted audio waveforms without vocoders. Experimental results indicate that the proposed method can generate a more naturally sounding converted speech than that using a conventional DNN method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Niwa, Jumpei; Yoshimura, Takenori; Hashimoto, Kei; Oura, Keiichiro;
   Nankaku, Yoshihiko; Tokuda, Keiichi] Nagoya Inst Technol, Dept Comp Sci
   &amp; Engn, Nagoya, Aichi, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Niwa, J (reprint author), Nagoya Inst Technol, Dept Comp Sci &amp; Engn, Nagoya, Aichi, Japan.</td>
</tr>

<tr>
<td valign="top">TC </td><td>4</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>4</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>5289</td>
</tr>

<tr>
<td valign="top">EP </td><td>5293</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000446384605092</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Takashima, Y
   <br>Yano, H
   <br>Nakashika, T
   <br>Takiguchi, T
   <br>Ariki, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Takashima, Yuki
   <br>Yano, Hajime
   <br>Nakashika, Toru
   <br>Takiguchi, Tetsuya
   <br>Ariki, Yasuo</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>PARALLEL-DATA-FREE DICTIONARY LEARNING FOR VOICE CONVERSION USING
   NON-NEGATIVE TUCKER DECOMPOSITION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech and Signal Processing
   (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 15-20, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Calgary, CANADA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; non-negative Tucker decomposition; non-negative matrix
   factorization; non-parallel training</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion (VC) is a technique where only speaker-specific information in source speech is converted while preserving the associated phonological information. Non-negative Matrix Factorization (NMF)-based VC has been researched because of the natural-sounding voice it produces compared with conventional Gaussian Mixture Model-based VC. In conventional NMF-VC, parallel data are used to train the models; therefore, unnatural pre-processing of speech data to make parallel data is needed. NMF-VC also tends to be a large model because this method has many parallel exemplars for the dictionary matrix; therefore, the computational cost is high. In this paper, we propose a novel parallel dictionary learning method using non-negative Tucker decomposition (NTD) which uses tensor decomposition and decomposes an input observation into a set of mode matrices and one core tensor. Our proposed NTD-based dictionary learning method estimates the dictionary matrix for NMF-VC without using parallel data. Experimental results show that our proposed method outperforms conventional non-parallel VC methods.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Takashima, Yuki; Yano, Hajime; Takiguchi, Tetsuya; Ariki, Yasuo] Kobe
   Univ, Grad Sch Syst Informat, Kobe, Hyogo, Japan.
   <br>[Nakashika, Toru] Univ Electrocommun, Grad Sch Informat &amp; Engn, Chofu,
   Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Takashima, Y (reprint author), Kobe Univ, Grad Sch Syst Informat, Kobe, Hyogo, Japan.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>5294</td>
</tr>

<tr>
<td valign="top">EP </td><td>5298</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000446384605093</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Zhao, GL
   <br>Sonsaat, S
   <br>Levis, J
   <br>Chukharev-Hudilainen, E
   <br>Gutierrez-Osuna, R</td>
</tr>

<tr>
<td valign="top">AF </td><td>Zhao, Guanlong
   <br>Sonsaat, Sinem
   <br>Levis, John
   <br>Chukharev-Hudilainen, Evgeny
   <br>Gutierrez-Osuna, Ricardo</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>ACCENT CONVERSION USING PHONETIC POSTERIORGRAMS</td>
</tr>

<tr>
<td valign="top">SO </td><td>2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech and Signal Processing
   (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 15-20, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Calgary, CANADA</td>
</tr>

<tr>
<td valign="top">DE </td><td>speech synthesis; accent conversion; frame pairing; posteriorgram;
   acoustic model</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION; FOREIGN ACCENT; SPEECH</td>
</tr>

<tr>
<td valign="top">AB </td><td>Accent conversion (AC) aims to transform non-native speech to sound as if the speaker had a native accent. This can be achieved by mapping source spectra from a native speaker into the acoustic space of the non-native speaker. In prior work, we proposed an AC approach that matches frames between the two speakers based on their acoustic similarity after compensating for differences in vocal tract length. In this paper, we propose an approach that matches frames between the two speakers based on their phonetic (rather than acoustic) similarity. Namely, we map frames from the two speakers into a phonetic posteriorgram using speaker-independent acoustic models trained on native speech. We evaluate the proposed algorithm on a corpus containing multiple native and non-native speakers. Compared to the previous AC algorithm, the proposed algorithm improves the ratings of acoustic quality (20% increase in mean opinion score) and native accent (69% preference) while retaining the voice identity of the non-native speaker.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Zhao, Guanlong; Gutierrez-Osuna, Ricardo] Texas A&amp;M Univ, Dept Comp Sci
   &amp; Engn, College Stn, TX 77843 USA.
   <br>[Sonsaat, Sinem; Levis, John; Chukharev-Hudilainen, Evgeny] Iowa State
   Univ, Dept English, Ames, IA USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Zhao, GL (reprint author), Texas A&amp;M Univ, Dept Comp Sci &amp; Engn, College Stn, TX 77843 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>gzhao@tamu.edu; sonsaat@iastate.edu; jlevis@iastate.edu;
   evgeny@iastate.edu; rgutier@tamu.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>5314</td>
</tr>

<tr>
<td valign="top">EP </td><td>5318</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000446384605097</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Adiga, N
   <br>Tsiaras, V
   <br>Stylianou, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Adiga, Nagaraj
   <br>Tsiaras, Vassilis
   <br>Stylianou, Yannis</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>ON THE USE OF WAVENET AS A STATISTICAL VOCODER</td>
</tr>

<tr>
<td valign="top">SO </td><td>2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech and Signal Processing
   (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 15-20, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Calgary, CANADA</td>
</tr>

<tr>
<td valign="top">DE </td><td>WaveNet; filter-bank features; vocoder; autoregressive model; correlated
   conditioning</td>
</tr>

<tr>
<td valign="top">ID </td><td>CONCATENATIVE SPEECH SYNTHESIS; VOICE CONVERSION; TRANSFORM</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we explore the possibility of using the WaveNet architecture as a statistical vocoder. In that case, the generation of speech waveforms is locally conditioned only by acoustic features. Focusing on the single speaker case at the moment, we investigate the impact of the local conditions as well as that of the amount of data available for training Furthermore, variations of the WaveNet architecture are considered and discussed in the context of our work. We compare our work against a very recent work which also used WaveNet architecture as a speech vocoder using the same speech data. More specifically, we used two female and two male speakers from the CMU-ARCTIC database to contrast the use of cepstrum coefficients and filter-bank features as local conditioners with the goal to improve the overall quality for both male and female speakers. In the paper we also discuss the impact of the size of the training data. Objective metrics for quality and intelligibility of the generated by the WaveNet speech as well as subjective tests support our suggestions.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Adiga, Nagaraj; Tsiaras, Vassilis; Stylianou, Yannis] Univ Crete, Dept
   Comp Sci, Iraklion, Crete, Greece.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Adiga, N (reprint author), Univ Crete, Dept Comp Sci, Iraklion, Crete, Greece.</td>
</tr>

<tr>
<td valign="top">EM </td><td>nagaraj@csd.uoc.gr; tsiaras@csd.uoc.gr; yannis@csd.uoc.gr</td>
</tr>

<tr xmlns:date="http://exslt.org/dates-and-times">
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Adiga, Nagaraj</display_name>&nbsp;</font></td><td><font size="3">0000-0002-3438-567X&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>5674</td>
</tr>

<tr>
<td valign="top">EP </td><td>5678</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000446384605167</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wirtz, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wirtz, Kristina</td>
</tr>

<tr>
<td valign="top">TI </td><td>Materializations of oricha voice through divinations in Cuban Santeri</td>
</tr>

<tr>
<td valign="top">SO </td><td>JOURNAL DE LA SOCIETE DES AMERICANISTES</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice; materiality; ritual speech; spirit presence; Santeria; Cuba</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this essay I explore how oricha (deities') voice is produced in and through Cuban Santeria practices that render oricha speech audible, meaningful, and quotable. In the semiotic order governing Cuban popular religious practice, human bodies and other objects can be "activated" as instruments of oricha speech. Divination objects such as cowry shells are understood to be the "tongue" of the oricha, through which the deities and spirits of the dead speak just as surely as when speaking through a possessed devotee or transmitting a message through a human medium. Oricha voices emerge most audibly in divinations and possession speech, although I will argue that the material processes that produce them exceed these specific ritual moments, and that orichas also can speak outside of ritual settings that elicit them. The analysis shows how approaching voice as a material phenomenon, sensible and physical, also activates its potential as a marker of social personhood, agentive force, and even biographical individuality. I trace the qualia of materials themselves and in lows of material substances and interconversions across objects, each contributing their own affordances, to argue for the central importance of transduction or signal conversion across media, and equilibration or the production of equivalences in producing oricha voice.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Wirtz, Kristina] Western Michigan Univ, Kalamazoo, MI 49008 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Wirtz, K (reprint author), Western Michigan Univ, Kalamazoo, MI 49008 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>kristina.wirtz@wmich.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">VL </td><td>104</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>149</td>
</tr>

<tr>
<td valign="top">EP </td><td>177</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.4000/jsa.15808</td>
</tr>

<tr>
<td valign="top">SC </td><td>Anthropology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000444775100014</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wang, XT
   <br>Jin, C
   <br>Zhao, W</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wang, XueTing
   <br>Jin, Cong
   <br>Zhao, Wei</td>
</tr>

<tr>
<td valign="top">TI </td><td>Beijing Opera Synthesis Based on Straight Algorithm and Deep Learning</td>
</tr>

<tr>
<td valign="top">SO </td><td>ADVANCES IN MULTIMEDIA</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">ID </td><td>SINGING VOICE</td>
</tr>

<tr>
<td valign="top">AB </td><td>Speech synthesis is an important research content in the field of human-computer interaction and has a wide range of applications. As one of its branches, singing synthesis plays an important role. Beijing Opera is a famous traditional Chinese opera, and it is called Chinese quintessence. The singing of Beijing Opera carries some features of speech but it has its own unique pronunciation rules and rhythms which differ from ordinary speech and singing. In this paper, we propose three models for the synthesis of Beijing Opera. Firstly, the speech signals of the source speaker and the target speaker are extracted by using the straight algorithm. And then through the training of GMM, we complete the voice control model to input the voice to be converted and output the voice after the voice conversion. Finally, by modeling the fundamental frequency, duration, and frequency separately, a melodic control model is constructed using GAN to realize the synthesis of the Beijing Opera fragment. We connect the fragments and superimpose the background music to achieve the synthesis of Beijing Opera. The experimental results show that the synthesized Beijing Opera has some audibility and can basically complete the composition of Beijing Opera. We also extend our models to human-AI cooperative music generation: given a target voice of human, we can generate a Beijing Opera which is sung by a new target voice.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Wang, XueTing; Zhao, Wei] Commun Univ China, Coll Sci &amp; Technol,
   Beijing, Peoples R China.
   <br>[Jin, Cong] Commun Univ China, Key Lab Media Audio &amp; Video, Beijing,
   Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Jin, C (reprint author), Commun Univ China, Key Lab Media Audio &amp; Video, Beijing, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jincong0623@cuc.edu.cn</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>jin, cong</display_name>&nbsp;</font></td><td><font size="3">0000-0003-0464-9862&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">AR </td><td>5158164</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1155/2018/5158164</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000440496900001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Bonvin, JM
   <br>Laruffa, F</td>
</tr>

<tr>
<td valign="top">AF </td><td>Bonvin, Jean-Michel
   <br>Laruffa, Francesco</td>
</tr>

<tr>
<td valign="top">TI </td><td>Deliberative democracy in the real world, the contribution of the
   capability approach</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERNATIONAL REVIEW OF SOCIOLOGY-REVUE INTERNATIONALE DE SOCIOLOGIE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Capability approach; positional objectivity; conversion; markets;
   deliberative democracy</td>
</tr>

<tr>
<td valign="top">ID </td><td>SOCIETY; MARKET; AGENCY; SEN</td>
</tr>

<tr>
<td valign="top">AB </td><td>Drawing on Amartya Sen's writings, this article presents the capability approach to democracy and shows its relevance for the sociological reflection and research on democratic processes conceived as ways to convert individual preferences into collective norms or decisions. Two moments are key in this respect: the formation of individual preferences and their translation into collective norms in the course of public debates. The initial sections present Sen's conception of democracy, particularly emphasizing its articulation with the notions of 'positional objectivity' and 'conversion'. Then, this conception is compared with two other mechanisms that may be used to coordinate individual decisions or preferences, namely the market and idealistic views on deliberative democracy. The article emphasizes how the capability approach departs from these two conceptions with regard to the two key concepts of capacity to aspire and capability for voice. The final section shows how Sen's notion of democracy may open up a new field for research, namely the sociological investigation of the informational (or knowledge) basis of democracy.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Bonvin, Jean-Michel; Laruffa, Francesco] Univ Geneva, Inst Demog &amp;
   Socioecon, Blvd Pont dArve 40, CH-1211 Geneva 4, Switzerland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Bonvin, JM (reprint author), Univ Geneva, Inst Demog &amp; Socioecon, Blvd Pont dArve 40, CH-1211 Geneva 4, Switzerland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jean-michel.bonvin@unige.ch</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">VL </td><td>28</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>216</td>
</tr>

<tr>
<td valign="top">EP </td><td>233</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1080/03906701.2018.1477101</td>
</tr>

<tr>
<td valign="top">SC </td><td>Sociology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000437107100002</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Vekkot, S
   <br>Tripathi, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Vekkot, Susmitha
   <br>Tripathi, Shikha</td>
</tr>

<tr>
<td valign="top">BE </td><td>Balas, VE
   <br>Jain, LC
   <br>Balas, MM</td>
</tr>

<tr>
<td valign="top">TI </td><td>Significance of Glottal Closure Instants Detection Algorithms in Vocal
   Emotion Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>SOFT COMPUTING APPLICATIONS, SOFA 2016, VOL 1</td>
</tr>

<tr>
<td valign="top">SE </td><td>Advances in Intelligent Systems and Computing</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>7th International Workshop on Soft Computing Applications (SOFA)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 24-26, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Arad, ROMANIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Glottal Closure Instants; DYPSA; SEDREAMS; ZFF; Inter-emotion conversion</td>
</tr>

<tr>
<td valign="top">AB </td><td>The objective of this work is to explore the significance of efficient glottal activity detection for inter-emotion conversion. Performance of popular glottal epoch detection algorithms like Dynamic Projected Phase-Slope Algorithm (DYPSA), Speech Event Detection using Residual Excitation And a Mean-based Signal (SEDREAMS) and Zero Frequency Filtering (ZFF) are compared in the context of vocal emotion conversion. Existing conversion approaches deal with synthesis/conversion from neutral to different emotions. In this work, we have demonstrated the efficacy of determining the conversion parameters based on statistical values derived from multiple emotions and using them for inter-emotion conversion in Indian context. Pitch modification is effected by using transformation scales derived from both male and female speakers in IIT Kharagpur-Simulated Emotion Speech Corpus. Three archetypal emotions viz. anger, fear and happiness were generated using pitch and amplitude modification algorithm. Analysis of statistical parameters for pitch after conversion revealed that anger gives good subjective and objective similarity while characteristics of fear and happiness are most challenging to synthesise. Also, use of male voice for synthesis gave better intelligibility. Glottal activity detection by ZFF gave results with least error for median pitch. The results from this study indicated that for emotions with overlapping characteristics like surprise and happiness, inter-emotion conversion can be a better choice than conversion from neutral.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Vekkot, Susmitha; Tripathi, Shikha] Amrita Univ, Dept Elect &amp; Commun
   Engn, Amrita Sch Engn, Amrita Vishwa Vidyapeetham, Bengaluru, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Tripathi, S (reprint author), Amrita Univ, Dept Elect &amp; Commun Engn, Amrita Sch Engn, Amrita Vishwa Vidyapeetham, Bengaluru, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>v_susmitha@blr.amrita.edu; t_shikha@blr.amrita.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">VL </td><td>633</td>
</tr>

<tr>
<td valign="top">BP </td><td>462</td>
</tr>

<tr>
<td valign="top">EP </td><td>473</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/978-3-319-62521-8_40</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000433138300040</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Saito, Y
   <br>Nakamura, K
   <br>Itani, S
   <br>Tsukahara, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Saito, Yu
   <br>Nakamura, Kazuhiro
   <br>Itani, Shigeto
   <br>Tsukahara, Kiyoaki</td>
</tr>

<tr>
<td valign="top">TI </td><td>Type 3 Thyroplasty for a Patient with Female-to-Male Gender Identity
   Disorder</td>
</tr>

<tr>
<td valign="top">SO </td><td>CASE REPORTS IN OTOLARYNGOLOGY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">ID </td><td>III THYROPLASTY</td>
</tr>

<tr>
<td valign="top">AB </td><td>Objective. In most cases, about the voice of the patient with female-to-male/gender identity disorder (FTM/GID), hormone therapy makes the voice low-pitched. In success cases, there is no need for phonosurgery. However, hormone therapy is not effective in some cases. We perform type 3 thyroplasty in these cases. Method. Hormone therapy was started in 2008 but did not lower the speaking fundamental frequencies (SFFs). We therefore performed TP3 under local anesthesia. Results. In our case, the SFF at the first visit was 146 Hz. The postoperative SFF was 110 Hz. Conclusions. TP3 was performed under local anesthesia in a patient with FTM/GID in whom hormone therapy proved ineffective. With successful conversion to a lower-pitched voice, the patient could begin to live daily life as a male. QOL improved significantly with TP3. If hormone therapy proves ineffective, TP3 may be selected as an optional treatment and appears to show few surgical complications and was, in this case, a very effective treatment.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Saito, Yu; Itani, Shigeto; Tsukahara, Kiyoaki] Tokyo Med Univ, Dept
   Otolaryngol Head &amp; Neck Surg, Shinjuku Ku, 6-7-1 Nisisinnjuku, Tokyo
   1600023, Japan.
   <br>[Nakamura, Kazuhiro] Todachuo Gen Hosp, Dept Otolaryngol, 1-19-3
   Honthou, Toda, Saitama 3350023, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Saito, Y (reprint author), Tokyo Med Univ, Dept Otolaryngol Head &amp; Neck Surg, Shinjuku Ku, 6-7-1 Nisisinnjuku, Tokyo 1600023, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>sysesy20121111117@yahoo.co.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">AR </td><td>4280381</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1155/2018/4280381</td>
</tr>

<tr>
<td valign="top">SC </td><td>Otorhinolaryngology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000430692200001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Jannati, MJ
   <br>Sayadiyan, A
   <br>Razi, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Jannati, Mohammad Javad
   <br>Sayadiyan, Abolghasem
   <br>Razi, Abolfazl</td>
</tr>

<tr>
<td valign="top">TI </td><td>Speech naturalness improvement via epsilon-closed extended vectors sets
   in voice conversion systems</td>
</tr>

<tr>
<td valign="top">SO </td><td>MULTIDIMENSIONAL SYSTEMS AND SIGNAL PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; Spectrum envelope reconstruction; e-Closed set of
   features; Speech synthesis</td>
</tr>

<tr>
<td valign="top">ID </td><td>MAXIMUM-LIKELIHOOD-ESTIMATION; REPRESENTATION; QUANTIZATION; ALGORITHM</td>
</tr>

<tr>
<td valign="top">AB </td><td>In conventional voice conversion methods, some features of a speech signal's spectrum envelope are first extracted. Then, these features are converted so as to best match a target speaker's speech by designing and using a set of conversions. Ultimately, the spectrum envelope of the target speaker's speech signal is reconstructed from the converted features. The spectrum envelope reconstructed from the converted features usually deviates from its natural form. This aberration from the natural form observed in cases such as over-smoothing, over-fitting, and widening of formants is partially caused by two factors: (1) there is an error in the reconstruction of spectrum envelope from the features, and (2) the set of features extracted from the spectrum envelope of the speech signal is not closed. A method is put forward to improve the naturalness of speech by means of epsilon-closed sets of extended vectors in voice conversion systems. In this approach, -closed sets to reconstruct the natural spectrum envelope of a signal in the synthesis phase are introduced. The elements of these sets are generated by forming a group of extended vectors of features and applying a quantization scheme on the features of a speech signal. The use of this method in speech synthesis leads to a noticeable reduction of error in spectrum reconstruction from the features. Furthermore, the final spectrum envelope extracted from voice conversions maintains its natural form and, consequently, the problems arising from the deviation of voice from its natural state are resolved. The above method can be generally used as one phase of speech synthesis. It is independent of the voice conversion technique used and its parallel or non-parallel training method, and can be applied to improve the naturalness of the generated speech signal in all common voice conversion methods. Moreover, this method can be used in other fields of speech processing like texts to speech systems and vocoders to improve the quality of the output signal in the synthesis step.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Jannati, Mohammad Javad; Sayadiyan, Abolghasem] Amirkabir Univ Technol,
   Dept Elect Engn, POB 15875-4413,424 Hafez Ave, Tehran, Iran.
   <br>[Razi, Abolfazl] Northern AZ Univ, Dept Elect Engn &amp; Comp Sci, Room
   253,Engn Bldg,2112 S Huffer Ln, Flagstaff, AZ USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sayadiyan, A (reprint author), Amirkabir Univ Technol, Dept Elect Engn, POB 15875-4413,424 Hafez Ave, Tehran, Iran.</td>
</tr>

<tr>
<td valign="top">EM </td><td>mjannati@aut.ac.ir; sayadiyan@outlook.com; abolfazl.razi@nau.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>JAN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">VL </td><td>29</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>385</td>
</tr>

<tr>
<td valign="top">EP </td><td>403</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/s11045-017-0470-3</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000419920700018</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Saito, Y
   <br>Takamichi, S
   <br>Saruwatari, H</td>
</tr>

<tr>
<td valign="top">AF </td><td>Saito, Yuki
   <br>Takamichi, Shinnosuke
   <br>Saruwatari, Hiroshi</td>
</tr>

<tr>
<td valign="top">TI </td><td>Statistical Parametric Speech Synthesis Incorporating Generative
   Adversarial Networks</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Statistical parametric speech synthesis; text-to-speech synthesis; voice
   conversion; deep neural networks; generative adversarial networks;
   over-smoothing</td>
</tr>

<tr>
<td valign="top">AB </td><td>A method for statistical parametric speech synthesis incorporating generative adversarial networks (GANs) is proposed. Although powerful deep neural networks techniques can be applied to artificially synthesize speech waveform, the synthetic speech quality is low compared with that of natural speech. One of the issues causing the quality degradation is an oversmoothing effect often observed in the generated speech parameters. A GAN introduced in this paper consists of two neural networks: a discriminator to distinguish natural and generated samples, and a generator to deceive the discriminator. In the proposed framework incorporating the GANs, the discriminator is trained to distinguish natural and generated speech parameters, while the acoustic models are trained to minimize the weighted sum of the conventional minimum generation loss and an adversarial loss for deceiving the discriminator. Since the objective of the GANs is to minimize the divergence (i.e., distribution difference) between the natural and generated speech parameters, the proposed method effectively alleviates the oversmoothing effect on the generated speech parameters. We evaluated the effectiveness for text-to-speech and voice conversion, and found that the proposed method can generate more natural spectral parameters and F-0 than conventional minimum generation error training algorithm regardless of its hyperparameter settings. Furthermore, we investigated the effect of the divergence of various GANs, and found that a Wasserstein GAN minimizing the Earth-Mover's distance works the best in terms of improving the synthetic speech quality.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Saito, Yuki; Takamichi, Shinnosuke; Saruwatari, Hiroshi] Univ Tokyo,
   Grad Sch Informat Sci &amp; Technol, Tokyo 1138656, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Saito, Y (reprint author), Univ Tokyo, Grad Sch Informat Sci &amp; Technol, Tokyo 1138656, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>yuuki_saito@ipc.i.u-tokyo.ac.jp;
   shinnosuke_takamichi@ipc.i.u-tokyo.ac.jp;
   hiroshi_saruwatari@ipc.i.u-tokyo.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>13</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>14</td>
</tr>

<tr>
<td valign="top">PD </td><td>JAN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">VL </td><td>26</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>84</td>
</tr>

<tr>
<td valign="top">EP </td><td>96</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASLP.2017.2761547</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000417931800007</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Lee, KS</td>
</tr>

<tr>
<td valign="top">AF </td><td>Lee, Ki-Seung</td>
</tr>

<tr>
<td valign="top">TI </td><td>HMM-Based Maximum Likelihood Frame Alignment for Voice Conversion from a
   Nonparallel Corpus</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEICE TRANSACTIONS ON INFORMATION AND SYSTEMS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>nonparallel speech corpus; voice conversion; hidden Markov model;
   maximum likelihood criterion</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH</td>
</tr>

<tr>
<td valign="top">AB </td><td>One of the problems associated with voice conversion from a nonparallel corpus is how to find the best match or alignment between the source and the target vector sequences without linguistic information. In a previous study, alignment was achieved by minimizing the distance between the source vector and the transformed vector. This method, however, yielded a sequence of feature vectors that were not well matched with the underlying speaker model. In this letter, the vectors were selected from the candidates by maximizing the overall likelihood of the selected vectors with respect to the target model in the HMM context. Both objective and subjective evaluations were carried out using the CMU ARCTIC database to verify the effectiveness of the proposed method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Lee, Ki-Seung] Konkuk Univ, Dept Elect Engn, Seoul 143701, South Korea.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Lee, KS (reprint author), Konkuk Univ, Dept Elect Engn, Seoul 143701, South Korea.</td>
</tr>

<tr>
<td valign="top">EM </td><td>kseung@konkuk.ac.kr</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>DEC</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>E100D</td>
</tr>

<tr>
<td valign="top">IS </td><td>12</td>
</tr>

<tr>
<td valign="top">BP </td><td>3064</td>
</tr>

<tr>
<td valign="top">EP </td><td>3067</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1587/transinf.2017EDL8144</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000417990300041</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Schultz, T
   <br>Wand, M
   <br>Hueber, T
   <br>Krusienski, DJ
   <br>Herff, C
   <br>Brumberg, JS</td>
</tr>

<tr>
<td valign="top">AF </td><td>Schultz, Tanja
   <br>Wand, Michael
   <br>Hueber, Thomas
   <br>Krusienski, Dean J.
   <br>Herff, Christian
   <br>Brumberg, Jonathan S.</td>
</tr>

<tr>
<td valign="top">TI </td><td>Biosignal-Based Spoken Communication: A Survey</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Biosignals; spoken communication; multimodal technologies; speech
   recognition and synthesis; speech rehabilitation; electromyography;
   ultrasound; functional near-infrared spectroscopy;
   electroencephalography; electrocorticography</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH RECOGNITION; CLASSIFICATION; RECORDINGS; VOICE; TIME;
   ARTICULOGRAPHY; CONVERSION; INTERFACES; MOVEMENTS; SIGNALS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Speech is a complex process involving a wide range of biosignals, including but not limited to acoustics. These biosignals-stemming from the articulators, the articulator muscle activities, the neural pathways, and the brain itself-can be used to circumvent limitations of conventional speech processing in particular, and to gain insights into the process of speech production in general. Research on biosignal-based speech processing is a wide and very active field at the intersection of various disciplines, ranging from engineering, computer science, electronics and machine learning to medicine, neuroscience, physiology, and psychology. Consequently, a variety of methods and approaches have been used to investigate the common goal of creating biosignal-based speech processing devices for communication applications in everyday situations and for speech rehabilitation, as well as gaining a deeper understanding of spoken communication. This paper gives an overview of the various modalities, research approaches, and objectives for biosignal-based spoken communication.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Schultz, Tanja; Herff, Christian] Univ Bremen, Cognit Syst Lab, Fac
   Comp Sci &amp; Math, D-28359 Bremen, Germany.
   <br>[Wand, Michael] Ist Dalle Molle Intelligenza Artificiale, Swiss AI Lab,
   CH-6928 Manno, Switzerland.
   <br>[Hueber, Thomas] Grenoble Alpes Univ, CNRS, GIPSA Lab, F-38402 Grenoble,
   France.
   <br>[Krusienski, Dean J.] Old Dominion Univ, Biomed Engn Inst, ASPEN Lab,
   Norfolk, VA 23529 USA.
   <br>[Brumberg, Jonathan S.] Univ Kansas, Speech Language Hearing Dept,
   Speech &amp; Appl Neurosci Lab, Lawrence, KS 66045 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Schultz, T (reprint author), Univ Bremen, Cognit Syst Lab, Fac Comp Sci &amp; Math, D-28359 Bremen, Germany.</td>
</tr>

<tr>
<td valign="top">EM </td><td>tanja.schultz@uni-bremen.de; michael@idsia.ch;
   thomas.hueber@gipsa-lab.fr; deankrusienski@ieee.org;
   christian.herff@uni-bremen.de; brumberg@ku.edu</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Herff, Christian</display_name>&nbsp;</font></td><td><font size="3">0000-0002-5610-2618&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Brumberg, Jonathan</display_name>&nbsp;</font></td><td><font size="3">0000-0001-5739-968X&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>12</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>12</td>
</tr>

<tr>
<td valign="top">PD </td><td>DEC</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>25</td>
</tr>

<tr>
<td valign="top">IS </td><td>12</td>
</tr>

<tr>
<td valign="top">BP </td><td>2257</td>
</tr>

<tr>
<td valign="top">EP </td><td>2271</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASLP.2017.2752365</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000417743800002</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Gonzalez, JA
   <br>Cheah, LA
   <br>Gomez, AM
   <br>Green, PD
   <br>Gilbert, JM
   <br>Ell, SR
   <br>Moore, RK
   <br>Holdsworth, E</td>
</tr>

<tr>
<td valign="top">AF </td><td>Gonzalez, Jose A.
   <br>Cheah, Lam A.
   <br>Gomez, Angel M.
   <br>Green, Phil D.
   <br>Gilbert, James M.
   <br>Ell, Stephen R.
   <br>Moore, Roger K.
   <br>Holdsworth, Ed</td>
</tr>

<tr>
<td valign="top">TI </td><td>Direct Speech Reconstruction From Articulatory Sensor Data by Machine
   Learning</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Silent speech interfaces; articulatory-to-acoustic mapping; speech
   rehabilitation; permanent magnet articulography; speech synthesis</td>
</tr>

<tr>
<td valign="top">ID </td><td>DEEP NEURAL-NETWORKS; VOCAL-TRACT; CONVERSION; RECOGNITION;
   REPRESENTATIONS; TRANSFORMATION; SYSTEM</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper describes a technique that generates speech acoustics from articulator movements. Our motivation is to help people who can no longer speak following laryngectomy, a procedure that is carried out tens of thousands of times per year in the Western world. Our method for sensing articulator movement, permanent magnetic articulography, relies on small, unobtrusive magnets attached to the lips and tongue. Changes in magnetic field caused by magnet movements are sensed and form the input to a process that is trained to estimate speech acoustics. In the experiments reported here this "Direct Synthesis" technique is developed for normal speakers, with glued-on magnets, allowing us to train with parallel sensor and acoustic data. We describe three machine learning techniques for this task, based on Gaussian mixture models, deep neural networks, and recurrent neural networks (RNNs). We evaluate our techniques with objective acoustic distortion measures and subjective listening tests over spoken sentences read from novels (the CMU Arctic corpus). Our results show that the best performing technique is a bidirectional RNN (BiRNN), which employs both past and future contexts to predict the acoustics from the sensor data. BiRNNs are not suitable for synthesis in real time but fixed-lag RNNs give similar results and, because they only look a little way into the future, overcome this problem. Listening tests show that the speech produced by this method has a natural quality that preserves the identity of the speaker. Furthermore, we obtain up to 92% intelligibility on the challenging CMU Arctic material. To our knowledge, these are the best results obtained for a silent-speech system without a restricted vocabulary and with an unobtrusive device that delivers audio in close to real time. This work promises to lead to a technology that truly will give people whose larynx has been removed their voices back.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Gonzalez, Jose A.; Green, Phil D.; Moore, Roger K.] Univ Sheffield,
   Dept Comp Sci, Sheffield S1 4DP, S Yorkshire, England.
   <br>[Cheah, Lam A.; Gilbert, James M.] Univ Hull, Sch Engn, Kingston Upon
   Hull HU6 7RX, N Humberside, England.
   <br>[Gomez, Angel M.] Univ Granada, Dept Signal Theory Telemat &amp; Commun,
   Granada 18010, Spain.
   <br>[Ell, Stephen R.] Hull &amp; East Yorkshire Hosp Trust, Castle Hill Hosp,
   Cottingham HU16 5JQ, England.
   <br>[Holdsworth, Ed] Pract Control Ltd, Sheffield S9 2RS, S Yorkshire,
   England.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Gonzalez, JA (reprint author), Univ Sheffield, Dept Comp Sci, Sheffield S1 4DP, S Yorkshire, England.</td>
</tr>

<tr>
<td valign="top">EM </td><td>j.gonzalez@sheffield.ac.uk; l.cheah@hull.ac.uk; amgg@ugr.es;
   p.green@sheffield.ac.uk; j.m.gilbert@hull.ac.uk; srell@doctors.org.uk;
   r.k.moore@sheffield.ac.uk; ed.holdsworth@practicalcontrol.com</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Gomez Garcia, Angel Manuel</display_name>&nbsp;</font></td><td><font size="3">C-6856-2012&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Gomez Garcia, Angel Manuel</display_name>&nbsp;</font></td><td><font size="3">0000-0002-9995-3068&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Gonzalez Lopez, Jose Andres</display_name>&nbsp;</font></td><td><font size="3">0000-0002-5531-8994&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>5</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>5</td>
</tr>

<tr>
<td valign="top">PD </td><td>DEC</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>25</td>
</tr>

<tr>
<td valign="top">IS </td><td>12</td>
</tr>

<tr>
<td valign="top">BP </td><td>2362</td>
</tr>

<tr>
<td valign="top">EP </td><td>2374</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASLP.2017.2757263</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000417743800011</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Janke, M
   <br>Diener, L</td>
</tr>

<tr>
<td valign="top">AF </td><td>Janke, Matthias
   <br>Diener, Lorenz</td>
</tr>

<tr>
<td valign="top">TI </td><td>EMG-to-Speech: Direct Generation of Speech From Facial Electromyographic
   Signals</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Biosignal; electromyography (EMG); silent speech interface; SSI</td>
</tr>

<tr>
<td valign="top">ID </td><td>MYOELECTRIC SIGNALS; VOICE CONVERSION; CLASSIFICATION; RECOGNITION;
   INFORMATION; SELECTION; SYSTEM; NOISE</td>
</tr>

<tr>
<td valign="top">AB </td><td>Silent speech interfaces are systems that enable speech communication even when an acoustic signal is unavailable. Over the last years, public interest in such interfaces has intensified. They provide solutions for some of the challenges faced by today's speech-driven technologies, such as robustness to noise and usability for people with speech impediments. In this paper, we provide an overview over our silent speech interface. It is based on facial surface electromyography (EMG), which we use to record the electrical signals that control muscle contraction during speech production. These signals are then converted directly to an audible speech waveform, retaining important paralinguistic speech cues for information such as speaker identity and mood. This paper gives an overview over our state-of-the-art direct EMG-to-speech transformation system. This paper describes the characteristics of the speech EMG signal, introduces techniques for extracting relevant features, presents different EMG-to-speech mapping methods, and finally, presents an evaluation of the different methods for real-time capability and conversion quality.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Janke, Matthias; Diener, Lorenz] Univ Bremen, Cognit Syst Lab, D-28359
   Bremen, Germany.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Diener, L (reprint author), Univ Bremen, Cognit Syst Lab, D-28359 Bremen, Germany.</td>
</tr>

<tr>
<td valign="top">EM </td><td>matthias.janke@googlemail.com; lorenz.diener@uni-bremen.de</td>
</tr>

<tr>
<td valign="top">TC </td><td>4</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>4</td>
</tr>

<tr>
<td valign="top">PD </td><td>DEC</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>25</td>
</tr>

<tr>
<td valign="top">IS </td><td>12</td>
</tr>

<tr>
<td valign="top">BP </td><td>2375</td>
</tr>

<tr>
<td valign="top">EP </td><td>2385</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASLP.2017.2738568</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000417743800012</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Harkness, N</td>
</tr>

<tr>
<td valign="top">AF </td><td>Harkness, Nicholas</td>
</tr>

<tr>
<td valign="top">TI </td><td>Transducing a Sermon, Inducing Conversion: Billy Graham, Billy Kim, and
   the 1973 Crusade in Seoul</td>
</tr>

<tr>
<td valign="top">SO </td><td>REPRESENTATIONS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper is an analysis of the final sermon of Billy Graham's 1973 Crusade in Seoul, South Korea, when he preached to a crowd estimated to exceed one million people. Next to Graham at the pulpit was Billy Jang Hwan Kim, a preacher who, in his capacity as interpreter, translated Graham's sermon verbally and peri-verbally-utterance by utterance, tone by tone, gesture by gesture-for the Korean-speaking audience. I examine the dynamic pragmatics (for example, chronotopic formulations, deictic calibrations, voicing and register effects, and indexical dimensions of entextualization) by which a sermonic copy across linguistic codes became an evangelical conduit between Cold War polities. In so doing, I demonstrate how the scope of intertextual analysis can be expanded productively from the narrow translation of denotation across codes to the broader indexical processes of semiotic "transduction'' across domains of cultural semiosis. Representations 137. Winter 2017 (C) The Regents of the University of California.</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PD </td><td>WIN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">IS </td><td>137</td>
</tr>

<tr>
<td valign="top">SI </td><td>SI</td>
</tr>

<tr>
<td valign="top">BP </td><td>112</td>
</tr>

<tr>
<td valign="top">EP </td><td>142</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1525/rep.2017.137.6.112</td>
</tr>

<tr>
<td valign="top">SC </td><td>Cultural Studies</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000398200500006</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Fu, SW
   <br>Li, PC
   <br>Lai, YH
   <br>Yang, CC
   <br>Hsieh, LC
   <br>Tsao, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Fu, Szu-Wei
   <br>Li, Pei-Chun
   <br>Lai, Ying-Hui
   <br>Yang, Cheng-Chien
   <br>Hsieh, Li-Chun
   <br>Tsao, Yu</td>
</tr>

<tr>
<td valign="top">TI </td><td>Joint Dictionary Learning-Based Non-Negative Matrix Factorization for
   Voice Conversion to Improve Speech Intelligibility After Oral Surgery</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Joint dictionary learning; non-negative matrix factorization (NMF);
   sparse representation; voice conversion (VC)</td>
</tr>

<tr>
<td valign="top">AB </td><td>Objective: This paper focuses on machine learning based voice conversion (VC) techniques for improving the speech intelligibility of surgical patients who have had parts of their articulators removed. Because of the removal of parts of the articulator, a patient's speech may be distorted and difficult to understand. To overcome this problem, VC methods can be applied to convert the distorted speech such that it is clear and more intelligible. To design an effective VC method, two key points must be considered: 1) the amount of training data may be limited (because speaking for a long time is usually difficult for postoperative patients); 2) rapid conversion is desirable (for better communication). Methods: We propose a novel joint dictionary learning based non-negative matrix factorization (JD-NMF) algorithm. Compared to conventional VC techniques, JD-NMF can perform VC efficiently and effectively with only a small amount of training data. Results: The experimental results demonstrate that the proposed JD-NMF method not only achieves notably higher short-time objective intelligibility (STOI) scores (a standardized objective intelligibility evaluation metric) than those obtained using the original unconverted speech but is also significantly more efficient and effective than a conventional exemplar-based NMF VC method. Conclusion: The proposed JD-NMF method may outperform the state-of-the-art exemplar-based NMF VC method in terms of STOI scores under the desired scenario. Significance: We confirmed the advantages of the proposed joint training criterion for the NMF-based VC. Moreover, we verified that the proposed JD-NMF can effectively improve the speech intelligibility scores of oral surgery patients.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Fu, Szu-Wei] Natl Taiwan Univ, Dept Comp Sci &amp; Informat Engn, Taipei,
   Taiwan.
   <br>[Fu, Szu-Wei; Tsao, Yu] Acad Sinica, Res Ctr Informat Technol Innovat,
   Taipei 11574, Taiwan.
   <br>[Li, Pei-Chun; Yang, Cheng-Chien; Hsieh, Li-Chun] Mackay Med Coll, Dept
   Audiol &amp; Speech Language Pathol, New Taipei, Taiwan.
   <br>[Lai, Ying-Hui] Yuan Ze Univ, Dept Elect Engn, Taoyuan, Taiwan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Tsao, Y (reprint author), Acad Sinica, Res Ctr Informat Technol Innovat, Taipei 11574, Taiwan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>yu.tsao@citi.sinica.edu.tw</td>
</tr>

<tr>
<td valign="top">TC </td><td>4</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>4</td>
</tr>

<tr>
<td valign="top">PD </td><td>NOV</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>64</td>
</tr>

<tr>
<td valign="top">IS </td><td>11</td>
</tr>

<tr>
<td valign="top">BP </td><td>2584</td>
</tr>

<tr>
<td valign="top">EP </td><td>2594</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TBME.2016.2644258</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000413315000009</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Pal, M
   <br>Saha, G</td>
</tr>

<tr>
<td valign="top">AF </td><td>Pal, Monisankha
   <br>Saha, Goutam</td>
</tr>

<tr>
<td valign="top">TI </td><td>Spectral Mapping Using Prior Re-Estimation of i-Vectors and System
   Fusion for Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Identity vector (i-vector); joint density Gaussian mixture model
   (JDGMM); mixture of factor analyzer (MFA); modified-prior; spectrogram
   fusion; voice conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>LIMITED TRAINING DATA; SPEAKER VERIFICATION; SPEECH SYNTHESIS;
   NEURAL-NETWORKS; TO-MANY; COMBINATION; ROBUST; MODEL</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we propose a new voice conversion (VC) method using i-vectors which consider low-dimensional representation of speech utterances. An attempt is made to restrict the i-vector variability in the intermediate computation of total variability (T) matrix by using a novel approach that uses modified-prior distribution of the intermediate i-vectors. This T-modification improves the speaker individuality conversion. For further improvement of conversion score and to keep a better balance between similarity and quality, band-wise spectrogram fusion between conventional joint density Gaussian mixture model (JDGMM) and i-vector based converted spectrograms is employed. The fused spectrogram retains more spectral details and leverages the complementary merits of each subsystem. Experiments in terms of objective and subjective evaluation are conducted extensively on CMU ARCTIC database. The results show that the proposed technique can produce a better trade-off between similarity and quality score than other state-of-the-art baseline VC methods. Furthermore, it works better than JDGMM in limited VC training data. The proposed VC performs moderately better (both objective and subjective) than mixture of factor analyzer based baseline VC. In addition, the proposed VC provides better quality converted speech as compared to maximum likelihood-GMM VC with dynamic feature constraint.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Pal, Monisankha; Saha, Goutam] Indian Inst Technol, Dept Elect &amp; Elect
   Commun Engn, Kharagpur 721302, W Bengal, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Pal, M (reprint author), Indian Inst Technol, Dept Elect &amp; Elect Commun Engn, Kharagpur 721302, W Bengal, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>monisankhapal@iitkgp.ac.in; gsaha@ece.iitkgp.ernet.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>NOV</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>25</td>
</tr>

<tr>
<td valign="top">IS </td><td>11</td>
</tr>

<tr>
<td valign="top">BP </td><td>2071</td>
</tr>

<tr>
<td valign="top">EP </td><td>2084</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASLP.2017.2743620</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000411749200003</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Magarinos, C
   <br>Lopez-Otero, P
   <br>Docio-Fernandez, L
   <br>Rodriguez-Banga, E
   <br>Erro, D
   <br>Garcia-Mateo, C</td>
</tr>

<tr>
<td valign="top">AF </td><td>Magarinos, Carmen
   <br>Lopez-Otero, Paula
   <br>Docio-Fernandez, Laura
   <br>Rodriguez-Banga, Eduardo
   <br>Erro, Daniel
   <br>Garcia-Mateo, Carmen</td>
</tr>

<tr>
<td valign="top">TI </td><td>Reversible speaker de-identification using pre-trained transformation
   functions</td>
</tr>

<tr>
<td valign="top">SO </td><td>COMPUTER SPEECH AND LANGUAGE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speaker de-identification; Voice transformation; Speaker
   re-identification; Frequency warping; Amplitude scaling; i-vector</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION; FREQUENCY; MODEL</td>
</tr>

<tr>
<td valign="top">AB </td><td>Speaker de-identification approaches must accomplish three main goals: universality, naturalness and reversibility. The main drawback of the traditional approach to speaker de-identification using voice conversion techniques is its lack of universality, since a parallel corpus between the input and target speakers is necessary to train the conversion parameters. It is possible to make use of a synthetic target to overcome this issue, but this harms the naturalness of the resulting de-identified speech. Hence, a technique is proposed in this paper in which a pool of pre-trained transformations between a set of speakers is used as follows: given a new user to de-identify, its most similar speaker in this set of speakers is chosen as the source speaker, and the speaker that is the most dissimilar to the source speaker is chosen as the target speaker. Speaker similarity is measured using the i-vector paradigm, which is usually employed as an objective measure of speaker de-identification performance, leading to a system with high de-identification accuracy. The transformation method is based on frequency warping and amplitude scaling, in order to obtain natural sounding speech while masking the identity of the speaker. In addition, compared to other voice conversion approaches, the proposed method is easily reversible. Experiments were conducted on Albayzin database, and performance was evaluated in terms of objective and subjective measures. These results showed a high success when de-identifying speech, as well as a great naturalness of the transformed voices. In addition, when making the transformation parameters available to a trusted holder, it is possible to invert the de-identification procedure, hence recovering the original speaker identity. The computational cost of the proposed approach is small, making it possible to produce de-identified speech in real-time with a high level of naturalness. (C) 2017 Elsevier Ltd. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Magarinos, Carmen; Lopez-Otero, Paula; Docio-Fernandez, Laura;
   Rodriguez-Banga, Eduardo; Garcia-Mateo, Carmen] AtlantTIC Res Ctr, EE
   Telecomuni, Campus Univ S-N, Vigo 36310, Spain.
   <br>[Erro, Daniel] Univ Basque Country, Ingn Goi Eskola Teknikoa, Aholab,
   Ikerbasque, Urkijo Zum Z-G, Bilbao 48013, Spain.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Lopez-Otero, P (reprint author), AtlantTIC Res Ctr, EE Telecomuni, Campus Univ S-N, Vigo 36310, Spain.</td>
</tr>

<tr>
<td valign="top">EM </td><td>plopez@gts.uvigo.es</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Rodriguez Banga, Eduardo</display_name>&nbsp;</font></td><td><font size="3">C-4296-2011&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Garcia-Mateo, Carmen</display_name>&nbsp;</font></td><td><font size="3">I-4144-2015&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Docio-Fernandez, Laura</display_name>&nbsp;</font></td><td><font size="3">D-3189-2018&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Magarinos, Carmen</display_name>&nbsp;</font></td><td><font size="3">D-1678-2018&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Rodriguez Banga, Eduardo</display_name>&nbsp;</font></td><td><font size="3">0000-0001-7989-4526&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Garcia-Mateo, Carmen</display_name>&nbsp;</font></td><td><font size="3">0000-0001-6856-939X&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Docio-Fernandez, Laura</display_name>&nbsp;</font></td><td><font size="3">0000-0003-3838-2406&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Lopez-Otero, Paula</display_name>&nbsp;</font></td><td><font size="3">0000-0003-2859-099X&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Magarinos, Carmen</display_name>&nbsp;</font></td><td><font size="3">0000-0003-3525-1304&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>4</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>4</td>
</tr>

<tr>
<td valign="top">PD </td><td>NOV</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>46</td>
</tr>

<tr>
<td valign="top">BP </td><td>36</td>
</tr>

<tr>
<td valign="top">EP </td><td>52</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.csl.2017.05.001</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000407609600002</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ghorbandoost, M
   <br>Saba, V</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ghorbandoost, Mostafa
   <br>Saba, Valiallah</td>
</tr>

<tr>
<td valign="top">TI </td><td>Non-parallel training for voice conversion using background-based
   alignment of GMMs and INCA algorithm</td>
</tr>

<tr>
<td valign="top">SO </td><td>IET SIGNAL PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>natural language processing; speech processing; Gaussian processes;
   mixture models; nonparallel training method; voice conversion;
   background-based alignment; INCA algorithm; GMMs; parallel training
   corpora; conversion function; nearest neighbour search step; conversion
   step alignment method algorithm; nonparallel VC; Gaussian mixture
   models; universal background model; frame alignment method; training
   sentences</td>
</tr>

<tr>
<td valign="top">AB </td><td>Most of the voice conversion (VC) researches have used parallel training corpora to train the conversion function. However, in practice it is not always possible to gather parallel corpora, so the need for non-parallel training methods arises. As a successful non-parallel method, nearest neighbour search step and a conversion step alignment method (INCA) algorithm has attracted a lot of attention in recent years. In this study, the authors propose a new method of non-parallel VC which is based on the INCA algorithm. The authors' method effectively solves the initialisation problem of INCA algorithm. Their proposed initialisation for INCA is done with alignment of Gaussian mixture models (GMM) using universal background model. Results of objective and subjective experiments determined that the authors' proposed method improves the INCA algorithm. It is observed that this superiority holds for different sizes of training material from 10 to 50 training sentences. In terms of mean opinion score, the authors' method scores 0.25 higher in the case of quality and 0.2 higher in the case of similarity to the target speaker compared with traditional INCA. It seems that the authors' proposed method is a suitable frame alignment method for non-parallel corpora in VC task.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ghorbandoost, Mostafa; Saba, Valiallah] AJA Univ Med Sci, Fac
   Paramedicine, Ctr Radiat Res, Etemadzade Ave,Fatemi St, Tehran, Iran.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Saba, V (reprint author), AJA Univ Med Sci, Fac Paramedicine, Ctr Radiat Res, Etemadzade Ave,Fatemi St, Tehran, Iran.</td>
</tr>

<tr>
<td valign="top">EM </td><td>vsaba@aut.ac.ir</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>OCT</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>11</td>
</tr>

<tr>
<td valign="top">IS </td><td>8</td>
</tr>

<tr>
<td valign="top">BP </td><td>998</td>
</tr>

<tr>
<td valign="top">EP </td><td>1005</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1049/iet-spr.2016.0693</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000412240100013</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Udvardi, P
   <br>Rado, J
   <br>Straszner, A
   <br>Ferencz, J
   <br>Hajnal, Z
   <br>Soleimani, S
   <br>Schneider, M
   <br>Schmid, U
   <br>Revesz, P
   <br>Volk, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Udvardi, Peter
   <br>Rado, Janos
   <br>Straszner, Andras
   <br>Ferencz, Janos
   <br>Hajnal, Zoltan
   <br>Soleimani, Saeedeh
   <br>Schneider, Michael
   <br>Schmid, Ulrich
   <br>Revesz, Peter
   <br>Volk, Janos</td>
</tr>

<tr>
<td valign="top">TI </td><td>Spiral-Shaped Piezoelectric MEMS Cantilever Array for Fully Implantable
   Hearing Systems</td>
</tr>

<tr>
<td valign="top">SO </td><td>MICROMACHINES</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>artificial basilar membrane; cochlear implant; frequency selectivity;
   Archimedean spiral; aluminum nitride (AlN); piezoelectric cantilever;
   micro-electromechanical system (MEMS); finite element analysis; energy
   harvesting</td>
</tr>

<tr>
<td valign="top">ID </td><td>ARTIFICIAL BASILAR-MEMBRANE; RESIDUAL-STRESS; PHYSICAL MODEL; HUMAN
   COCHLEA; THIN-FILMS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Fully implantable, self-powered hearing aids with no external unit could significantly increase the life quality of patients suffering severe hearing loss. This highly demanding concept, however, requires a strongly miniaturized device which is fully implantable in the middle/inner ear and includes the following components: frequency selective microphone or accelerometer, energy harvesting device, speech processor, and cochlear multielectrode. Here we demonstrate a low volume, piezoelectric micro-electromechanical system (MEMS) cantilever array which is sensitive, even in the lower part of the voice frequency range (300-700 Hz). The test array consisting of 16 cantilevers has been fabricated by standard bulk micromachining using a Si-on-Insulator (SOI) wafer and aluminum nitride (AlN) as a complementary metal-oxide-semiconductor (CMOS) and biocompatible piezoelectric material. The low frequency and low device footprint are ensured by Archimedean spiral geometry and Si seismic mass. Experimentally detected resonance frequencies were validated by an analytical model. The generated open circuit voltage (3-10 mV) is sufficient for the direct analog conversion of the signals for cochlear multielectrode implants.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Udvardi, Peter; Rado, Janos; Straszner, Andras; Ferencz, Janos; Hajnal,
   Zoltan; Soleimani, Saeedeh; Volk, Janos] MTA EK, Inst Tech Phys &amp; Mat
   Sci, 1121 Konkoly Thege M Ut 29-33, H-1121 Budapest, Hungary.
   <br>[Rado, Janos] Obuda Univ, Doctoral Sch Mat Sci &amp; Technol, Becsi Ut 96-B,
   H-1034 Budapest, Hungary.
   <br>[Schneider, Michael; Schmid, Ulrich] TU Wien, Inst Sensor &amp; Actuator
   Syst, A-1040 Vienna, Austria.
   <br>[Revesz, Peter] Univ Pecs, Dept Otorhinolaryngol Head &amp; Neck Surg, Clin
   Ctr, H-7601 Pecs, Hungary.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Volk, J (reprint author), MTA EK, Inst Tech Phys &amp; Mat Sci, 1121 Konkoly Thege M Ut 29-33, H-1121 Budapest, Hungary.</td>
</tr>

<tr>
<td valign="top">EM </td><td>udvardi.peter98@gmail.com; rado@mfa.kfki.hu; straszner@mfa.kfki.hu;
   ferencz@mfa.kfki.hu; hajnal@mfa.kfki.hu;
   soleimani.saeedeh@energia.mta.hu; michael.schneider@tuwien.ac.at;
   ulrich.e366.schmid@tuwien.ac.at; revesz.peter@pte.hu; volk@mfa.kfki.hu</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>HAJNAL, Zoltan</display_name>&nbsp;</font></td><td><font size="3">B-6385-2008&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>HAJNAL, Zoltan</display_name>&nbsp;</font></td><td><font size="3">0000-0003-1758-4754&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Schneider, Michael</display_name>&nbsp;</font></td><td><font size="3">0000-0001-9846-7132&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Rado, Janos</display_name>&nbsp;</font></td><td><font size="3">0000-0001-9475-5356&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PD </td><td>OCT</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>8</td>
</tr>

<tr>
<td valign="top">IS </td><td>10</td>
</tr>

<tr>
<td valign="top">AR </td><td>311</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.3390/mi8100311</td>
</tr>

<tr>
<td valign="top">SC </td><td>Science &amp; Technology - Other Topics; Instruments &amp; Instrumentation</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000414859000025</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Tian, XH
   <br>Lee, SW
   <br>Wu, ZZ
   <br>Chng, ES
   <br>Li, HZ</td>
</tr>

<tr>
<td valign="top">AF </td><td>Tian, Xiaohai
   <br>Lee, Siu Wa
   <br>Wu, Zhizheng
   <br>Chng, Eng Siong
   <br>Li, Haizhou</td>
</tr>

<tr>
<td valign="top">TI </td><td>An Exemplar-Based Approach to Frequency Warping for Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Exemplar; frequency warping; residual compensation; sparse
   representation; voice conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPARSE REPRESENTATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>The voice conversion's task is to modify a source speaker's voice to sound like that of a target speaker. A conversion method is considered successful when the produced speech sounds natural and similar to the target speaker. This paper presents a new voice conversion framework in which we combine frequency warping and exemplar-based method for voice conversion. Our method maintains high-resolution details during conversion by directly applying frequency warping on the high-resolution spectrum to represent the target. The warping function is generated by a sparse interpolation from a dictionary of exemplar warping functions. As the generated warping function is dependent only on a very small set of exemplars, we do away with the statistical averaging effects inherited from Gaussian mixture models. To compensate for the conversion error, we also apply residual exemplars into the conversion process. Both objective and subjective evaluations on the VOICES database validated the effectiveness of the proposed voice conversion framework. We observed a significant improvement in speech quality over the state-of-the-art parametric methods.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Tian, Xiaohai; Lee, Siu Wa; Wu, Zhizheng; Chng, Eng Siong; Li, Haizhou]
   Nanyang Technol Univ, Sch Comp Sci &amp; Engn, Singapore, Singapore.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Tian, XH (reprint author), Nanyang Technol Univ, Sch Comp Sci &amp; Engn, Singapore, Singapore.</td>
</tr>

<tr>
<td valign="top">EM </td><td>xhtian@ntu.edu.sg; swylee@i2r.a-star.edu.sg; zhizheng.wu@ed.ac.uk;
   aseschng@ntu.edu.sg; haizhou.li@nus.edu.sg</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Li, Haizhou</display_name>&nbsp;</font></td><td><font size="3">Q-6438-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PD </td><td>OCT</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>25</td>
</tr>

<tr>
<td valign="top">IS </td><td>10</td>
</tr>

<tr>
<td valign="top">BP </td><td>1863</td>
</tr>

<tr>
<td valign="top">EP </td><td>1876</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASLP.2017.2723721</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000408388700001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nagy, D
   <br>Speelman, G</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nagy, Dorottya
   <br>Speelman, Ge</td>
</tr>

<tr>
<td valign="top">TI </td><td>Conversion controlled: Missiological reflections on assessing
   conversions to Christianity among asylum seekers in the European Union</td>
</tr>

<tr>
<td valign="top">SO </td><td>THEOLOGY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>asylum policy; asylum seekers; Christian organizations; conversion;
   European Union</td>
</tr>

<tr>
<td valign="top">AB </td><td>This article draws on the interactions of multiple voices addressing the issue of conversion-based asylum claims in Europe. It formulates a set of theological and missiological reflections on conversion and asylum. It argues that the complex interactions among immigration services, institutionalized churches, Christian organizations and asylum seekers capture conversion as static. The article proposes moving beyond the credibility discourse, which preserves the model of conversion from one closed faith system into another, and revisiting more complex theologies of conversion theologies.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nagy, Dorottya] Protestant Theol Univ Amsterdam, Missiol, Amsterdam,
   Netherlands.
   <br>[Speelman, Ge] Protestant Theol Univ Amsterdam, Religious Studies,
   Amsterdam, Netherlands.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nagy, D (reprint author), Protestant Theol Univ Amsterdam, Missiol, Amsterdam, Netherlands.</td>
</tr>

<tr>
<td valign="top">EM </td><td>d.nagy@pthu.nl</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>SEP-OCT</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>120</td>
</tr>

<tr>
<td valign="top">IS </td><td>5</td>
</tr>

<tr>
<td valign="top">BP </td><td>355</td>
</tr>

<tr>
<td valign="top">EP </td><td>363</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1177/0040571X17710200</td>
</tr>

<tr>
<td valign="top">SC </td><td>Religion</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000419631000006</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Carmenta, R
   <br>Zabala, A
   <br>Daeli, W
   <br>Phelps, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Carmenta, Rachel
   <br>Zabala, Aiora
   <br>Daeli, Willy
   <br>Phelps, Jacob</td>
</tr>

<tr>
<td valign="top">TI </td><td>Perceptions across scales of governance and the Indonesian peatland
   fires</td>
</tr>

<tr>
<td valign="top">SO </td><td>GLOBAL ENVIRONMENTAL CHANGE-HUMAN AND POLICY DIMENSIONS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Policy; Q method; Haze; Deforestation; Conservation; Transboundary
   governance</td>
</tr>

<tr>
<td valign="top">ID </td><td>TROPICAL PEATLANDS; LAND; CONSERVATION; AGRICULTURE; DYNAMICS; SERVICES;
   SUMATRA; ISSUES; HAZE</td>
</tr>

<tr>
<td valign="top">AB </td><td>Across leading environmental challenges-fire management, climate change, deforestation - there is growing awareness of the need to better account for diverse stakeholder perceptions across complex, multi-level governance arrangements. Perceptions often condition behavior, compliance and engagement in ways that impact environmental outcomes. We illustrate the importance of, and approaches to, examining perceptions across scales of governance (e.g. international, national, local) and sectors (e.g. civil society, government, corporate) through the example of Indonesian peatland fires. Peatlands are crucial global carbon stocks threatened by land use change and fire and subject to a range of policy interventions that affect many different stakeholder groups. Peatland drainage and conversion to plantation agriculture has been associated with severe, uncontrolled peat fires that present significant climate, public health and economic risks. Peatland fire management has become a domestic and international priority, spurring intensely contentious debates, policies and legal proceedings. Previous fire management interventions (FMI) are numerous yet have suffered widespread implementation failures. Against this backdrop, our manuscript provides a thematically and methodologically novel analysis of how diverse stakeholders, from local farmers to international policy makers, perceive peatland fires in terms of, i) how they prioritize the associated benefits and burdens, and ii) the perceived effectiveness of FMI. We adopt an innovative application of Q method to provide needed insights that serve to quantify the areas of contention and consensus that exist among the stakeholders and their multi-dimensional perspectives. We show that many of the contemporary FMI were perceived as among the most effective interventions overall, but were also the most controversial between groups. Clear consensus areas were related to the shared concerns for the local health impacts and the potential of government support for fire-free alternatives as a solution pathway. Improved understanding of stakeholder perceptions has potential to: give voice to marginalized communities; enable transparent mediation of diverse priorities; inform public education campaigns, and shape future policy and governance arrangements.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Carmenta, Rachel; Daeli, Willy; Phelps, Jacob] Ctr Int Forestry Res
   CIFOR, Forests &amp; Governance, Jalan CIFOR, Bogor 16115, Barat, Indonesia.
   <br>[Carmenta, Rachel] Univ Cambridge, Cambridge Conservat Initiat, David
   Attenborough Bldg,Pembroke St, Cambridge CB2 3QZ, England.
   <br>[Zabala, Aiora] Univ Cambridge, Dept Land Econ, 19 Silver St, Cambridge
   CB3 9EP, England.
   <br>[Daeli, Willy] Univ Florida, Sch Forest Resources &amp; Conservat, 136
   Newins Ziegler Hall, Gainesville, FL 32611 USA.
   <br>[Phelps, Jacob] Univ Lancaster, Lancaster Environm Ctr, Lib Ave,
   Lancaster LA1 4YQ, England.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Carmenta, R (reprint author), Univ Cambridge, Cambridge Conservat Initiat, David Attenborough Bldg,Pembroke St, Cambridge CB2 3QZ, England.</td>
</tr>

<tr>
<td valign="top">EM </td><td>rachelcarmenta@gmail.com; aiora.zabala@gmail.com; willydaeli@gmail.com;
   jacop.phelps@gmail.com</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Zabala, Aiora</display_name>&nbsp;</font></td><td><font size="3">B-2633-2010&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Zabala, Aiora</display_name>&nbsp;</font></td><td><font size="3">0000-0001-8534-3325&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>8</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>8</td>
</tr>

<tr>
<td valign="top">PD </td><td>SEP</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>46</td>
</tr>

<tr>
<td valign="top">BP </td><td>50</td>
</tr>

<tr>
<td valign="top">EP </td><td>59</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.gloenvcha.2017.08.001</td>
</tr>

<tr>
<td valign="top">SC </td><td>Environmental Sciences &amp; Ecology; Geography</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000413381500006</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sayadian, A
   <br>Mozaffari, F</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sayadian, Abolghasem
   <br>Mozaffari, Fatemeh</td>
</tr>

<tr>
<td valign="top">TI </td><td>A novel method for voice conversion based on non-parallel corpus</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERNATIONAL JOURNAL OF SPEECH TECHNOLOGY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Demi-syllable; GMM; Non-parallel corpus; Voice conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>FREQUENCY; TRANSFORM; SYSTEM</td>
</tr>

<tr>
<td valign="top">AB </td><td>This article puts forward a new algorithm for voice conversion which not only removes the necessity of parallel corpus in the training phase but also resolves the issue of insufficiency of the target speaker's corpus. The proposed approach is based on one of the new voice conversion models utilizing classical LPC analysis-synthesis model combined with GMM. Through this algorithm, the conversion functions among vowels and demi-syllables are derived. We assumed that these functions are rather the same for different speakers if their genders, accents, and languages are alike. Therefore, we will be able to produce the demi-syllables with just having access to few sentences from the target speaker and forming the GMM for one of his/her vowels. The results from the appraisal of the proposed method for voice conversion clarifies that this method has the ability to efficiently realize the speech features of the target speaker. It can also provide results comparable to the ones obtained through the parallel-corpus-based approaches.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Sayadian, Abolghasem; Mozaffari, Fatemeh] Amirkabir Univ Technol, Dept
   Elect Engn, Tehran, Iran.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Mozaffari, F (reprint author), Amirkabir Univ Technol, Dept Elect Engn, Tehran, Iran.</td>
</tr>

<tr>
<td valign="top">EM </td><td>eeas35@aut.ac.ir; fa_mozaffari@aut.ac.ir</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PD </td><td>SEP</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>20</td>
</tr>

<tr>
<td valign="top">IS </td><td>3</td>
</tr>

<tr>
<td valign="top">BP </td><td>587</td>
</tr>

<tr>
<td valign="top">EP </td><td>592</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/s10772-017-9430-4</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000407464900013</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kumar, VM
   <br>Thipesh, DSH</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kumar, V. Manoj
   <br>Thipesh, D. Srii Hari</td>
</tr>

<tr>
<td valign="top">TI </td><td>Robot Arm Performing Writing through Speech Recognition Using Dynamic
   Time Warping Algorithm</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERNATIONAL JOURNAL OF ENGINEERING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Dynamic Time Warping; Endpoint Detection, Speech To Text; Mel Frequency
   Cel Cepstrum; Link Length; Degree of Freedom; RRRR</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION; WORD RECOGNITION; TO-TEXT; MANIPULATORS;
   REPRESENTATION; MOVEMENT</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper aims to develop a writing robot by recognizing the speech signal from the user. The robot arm constructed mainly for the disabled people who can't perform writing on their own. Here, dynamic time warping (DTW) algorithm is used to recognize the speech signal from the user. The action performed by the robot arm in the environment is done by reducing the redundancy which frequently faced by the robot arm with high accuracy in both velocity and position in its own trajectory.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kumar, V. Manoj; Thipesh, D. Srii Hari] SRM Univ, Dept Mech Engn,
   Madras 603203, Tamil Nadu, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Thipesh, DSH (reprint author), SRM Univ, Dept Mech Engn, Madras 603203, Tamil Nadu, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>thipesh91@gmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PD </td><td>AUG</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>30</td>
</tr>

<tr>
<td valign="top">IS </td><td>8</td>
</tr>

<tr>
<td valign="top">BP </td><td>1238</td>
</tr>

<tr>
<td valign="top">EP </td><td>1245</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.5829/ije.2017.30.08b.17</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000428206800017</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Phung, TN</td>
</tr>

<tr>
<td valign="top">AF </td><td>Trung-Nghia Phung</td>
</tr>

<tr>
<td valign="top">TI </td><td>Multiple emotional voice conversion in Vietnamese HMM-based speech
   synthesis using non-negative matrix factorization</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERNATIONAL JOURNAL OF ADVANCED AND APPLIED SCIENCES</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>HMM-based speech synthesis; Voice adaption; Exemplar-based voice
   conversion; Non-negative matrix factorization; Emotional speech
   synthesis</td>
</tr>

<tr>
<td valign="top">AB </td><td>Most of current text-to-speech (TTS) systems can synthesize only single voice with neutral emotion. If different emotional voices are required to be synthesized, the system has to be trained again with the new emotional voices. The training process normally requires a huge amount of emotional speech data that is usually impractical. The state of the art TTS using Hidden Markov Model (HMM), called as HMM-based TTS, can synthesize speech with various emotions by using speaker adaption methods. However, both of the emotional voices synthesized and adapted by HMM-based TTS are "over-smooth". When these voices are over-smooth, the detail structures clearly linked to speaker emotions may be missing. We can also synthesize multiple voices by using some voice conversion (VC) methods combined with HMM-based TTS. However, current voice conversions still cannot synthesize target speech while keeping the detail information related to speaker emotions of the target voice and just using limited amount data of target voices. In this paper, we proposed to use exemplar-based emotional voice conversion combined with HMM-based TTS to synthesize multiple high-quality emotional voices with a few amount of target data. The evaluation results using the Vietnamese emotional speech data corpus confirmed the merits of the proposed method. (C) 2017 The Authors. Published by IASE. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Trung-Nghia Phung] Thai Nguyen Univ Informat &amp; Commun Technol, Thai
   Nguyen 25000, Vietnam.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Phung, TN (reprint author), Thai Nguyen Univ Informat &amp; Commun Technol, Thai Nguyen 25000, Vietnam.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ptnghia@ictu.edu.vn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>AUG</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>4</td>
</tr>

<tr>
<td valign="top">IS </td><td>8</td>
</tr>

<tr>
<td valign="top">BP </td><td>1</td>
</tr>

<tr>
<td valign="top">EP </td><td>5</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21833/ijaas.2017.08.001</td>
</tr>

<tr>
<td valign="top">SC </td><td>Science &amp; Technology - Other Topics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000411445200001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Luo, ZJ
   <br>Chen, JH
   <br>Takiguchi, T
   <br>Ariki, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Luo, Zhaojie
   <br>Chen, Jinhui
   <br>Takiguchi, Tetsuya
   <br>Ariki, Yasuo</td>
</tr>

<tr>
<td valign="top">TI </td><td>Emotional voice conversion using neural networks with arbitrary scales
   F0 based on wavelet transform</td>
</tr>

<tr>
<td valign="top">SO </td><td>EURASIP JOURNAL ON AUDIO SPEECH AND MUSIC PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>F0 features; Continuous wavelet transform; Neural networks; Deep belief
   networks; Emotional voice conversion</td>
</tr>

<tr>
<td valign="top">AB </td><td>An artificial neural network is an important model for training features of voice conversion (VC) tasks. Typically, neural networks (NNs) are very effective in processing nonlinear features, such as Mel Cepstral Coefficients (MCC), which represent the spectrum features. However, a simple representation of fundamental frequency (F0) is not enough for NNs to deal with emotional voice VC. This is because the time sequence of F0 for an emotional voice changes drastically. Therefore, in our previous method, we used the continuous wavelet transform (CWT) to decompose F0 into 30 discrete scales, each separated by one third of an octave, which can be trained by NNs for prosody modeling in emotional VC. In this study, we propose the arbitrary scales CWT (AS-CWT) method to systematically capture F0 features of different temporal scales, which can represent different prosodic levels ranging from micro-prosody to sentence levels. Meanwhile, the proposed method uses deep belief networks (DBNs) to pre-train the NNs that then convert spectral features. By utilizing these approaches, the proposed method can change the spectrum and the F0 for an emotional voice simultaneously as well as outperform other state-of-the-art methods in terms of emotional VC.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Luo, Zhaojie; Chen, Jinhui; Takiguchi, Tetsuya; Ariki, Yasuo] Kobe
   Univ, Grad Sch Syst Informat, Nada Ku, 1-1 Rokkodai, Kobe, Hyogo
   6578501, Japan.
   <br>[Chen, Jinhui] Kobe Univ, RIEB, Nada Ku, 2-1 Rokkodai, Kobe, Hyogo
   6578501, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Chen, JH (reprint author), Kobe Univ, Grad Sch Syst Informat, Nada Ku, 1-1 Rokkodai, Kobe, Hyogo 6578501, Japan.; Chen, JH (reprint author), Kobe Univ, RIEB, Nada Ku, 2-1 Rokkodai, Kobe, Hyogo 6578501, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ianchen@me.cs.scitec.kobe-u.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PD </td><td>AUG 1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">AR </td><td>18</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1186/s13636-017-0116-2</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000410948600001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Saito, Y
   <br>Takamichi, S
   <br>Saruwatari, H</td>
</tr>

<tr>
<td valign="top">AF </td><td>Saito, Yuki
   <br>Takamichi, Shinnosuke
   <br>Saruwatari, Hiroshi</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Conversion Using Input-to-Output Highway Networks</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEICE TRANSACTIONS ON INFORMATION AND SYSTEMS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>statistical parametric speech synthesis; DNN-based voice conversion;
   highway networks; over-smoothing</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH SYNTHESIS; GENERATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper proposes Deep Neural Network (DNN)-based Voice Conversion (VC) using input-to-output highway networks. VC is a speech synthesis technique that converts input features into output speech parameters, and DNN-based acoustic models for VC are used to estimate the output speech parameters from the input speech parameters. Given that the input and output are often in the same domain (e.g., cepstrum) in VC, this paper proposes a VC using highway networks connected from the input to output. The acoustic models predict the weighted spectral differentials between the input and output spectral parameters. The architecture not only alleviates over-smoothing effects that degrade speech quality, but also effectively represents the characteristics of spectral parameters. The experimental results demonstrate that the proposed architecture outperforms Feed-Forward neural networks in terms of the speech quality and speaker individuality of the converted speech.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Saito, Yuki; Takamichi, Shinnosuke; Saruwatari, Hiroshi] Univ Tokyo,
   Grad Sch Informat Sci &amp; Technol, Tokyo 1138656, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Saito, Y (reprint author), Univ Tokyo, Grad Sch Informat Sci &amp; Technol, Tokyo 1138656, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>yuukisaito@ipc.i.u-tokyo.ac.jp;
   shinnosuke_takamichi@ipc.i.u-tokyo.ac.jp;
   hiroshi_saruwatari@ipc.i.u-tokyo.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PD </td><td>AUG</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>E100D</td>
</tr>

<tr>
<td valign="top">IS </td><td>8</td>
</tr>

<tr>
<td valign="top">BP </td><td>1925</td>
</tr>

<tr>
<td valign="top">EP </td><td>1928</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1587/transinf.2017EDL8034</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000406868400045</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Mashhadi, MB
   <br>Salarieh, N
   <br>Farahani, ES
   <br>Marvasti, F</td>
</tr>

<tr>
<td valign="top">AF </td><td>Mashhadi, Mahdi Boloursaz
   <br>Salarieh, Nikan
   <br>Farahani, Ehsan Shahrabi
   <br>Marvasti, Farokh</td>
</tr>

<tr>
<td valign="top">TI </td><td>Level crossing speech sampling and its sparsity promoting reconstruction
   using an iterative method with adaptive thresholding</td>
</tr>

<tr>
<td valign="top">SO </td><td>IET SIGNAL PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>iterative methods; analogue-digital conversion; speech processing;
   gradient methods; level crossing speech sampling; sparsity promoting
   reconstruction; iterative method; adaptive thresholding; asynchronous
   level crossing; LC A; D converters; redundancy voice sampling; iterative
   methods with adaptive thresholding; adaptive LC; IMAT algorithm;
   gradient projection optimisation techniques; gradient descent
   optimisation techniques; square error minimisation; IMATLC
   reconstruction method; low-pass signal assumption; signal-to-noise ratio
   reconstruction; SNR</td>
</tr>

<tr>
<td valign="top">ID </td><td>SIGNAL RECONSTRUCTION; BANDPASS SIGNALS; ZERO CROSSINGS; TIME;
   QUANTIZATION; CONVERSION; SCHEME</td>
</tr>

<tr>
<td valign="top">AB </td><td>The authors propose asynchronous level crossing (LC) A/D converters for low redundancy voice sampling. They propose to utilise the family of iterative methods with adaptive thresholding (IMAT) for reconstructing voice from non-uniform LC and adaptive LC (ALC) samples thereby promoting sparsity. The authors modify the basic IMAT algorithm and propose the iterative method with adaptive thresholding for level crossing (IMATLC) algorithm for improved reconstruction performance. To this end, the authors analytically derive the basic IMAT algorithm by applying the gradient descent and gradient projection optimisation techniques to the problem of square error minimisation subjected to sparsity. The simulation results indicate that the proposed IMATLC reconstruction method outperforms the conventional reconstruction method based on low-pass signal assumption by 6.56dBs in terms of reconstruction signal-to-noise ratio (SNR) for LC sampling. In this scenario, IMATLC outperforms orthogonal matching pursuit, least absolute shrinkage and selection operator and smoothed L0 sparsity promoting algorithms by average amounts of 12.13, 10.31, and 10.28dBs, respectively. Finally, the authors compare the performance of the proposed LC/ALC-based A/Ds with the conventional uniform sampling-based A/Ds and their random sampling-based counterparts both in terms of perceptual evaluation of speech quality and reconstruction SNR.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Mashhadi, Mahdi Boloursaz] Queens Univ, ECE Dept, Kingston, ON, Canada.
   <br>[Mashhadi, Mahdi Boloursaz; Marvasti, Farokh] SUT, EE Dept, Tehran, Iran.
   <br>[Salarieh, Nikan] Univ Miami, ECE Dept, Miami, FL USA.
   <br>[Farahani, Ehsan Shahrabi] Univ Calgary UoC, ECE Dept, Calgary, AB,
   Canada.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Mashhadi, MB (reprint author), Queens Univ, ECE Dept, Kingston, ON, Canada.; Mashhadi, MB (reprint author), SUT, EE Dept, Tehran, Iran.</td>
</tr>

<tr>
<td valign="top">EM </td><td>mahdi.boloursazmashhadi@queensu.ca</td>
</tr>

<tr>
<td valign="top">TC </td><td>6</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>6</td>
</tr>

<tr>
<td valign="top">PD </td><td>AUG</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>11</td>
</tr>

<tr>
<td valign="top">IS </td><td>6</td>
</tr>

<tr>
<td valign="top">BP </td><td>721</td>
</tr>

<tr>
<td valign="top">EP </td><td>726</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1049/iet-spr.2016.0569</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000407015000009</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Lee, KS</td>
</tr>

<tr>
<td valign="top">AF </td><td>Lee, Ki-Seung</td>
</tr>

<tr>
<td valign="top">TI </td><td>Restricted Boltzmann Machine-Based Voice Conversion for Nonparallel
   Corpus</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE SIGNAL PROCESSING LETTERS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Linear transformation; restricted Boltzmann machine (RBM); voice
   conversion (VC)</td>
</tr>

<tr>
<td valign="top">ID </td><td>ADAPTATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>A large amount of parallel training corpus is necessary for robust, high-quality voice conversion. However, such parallel data may not always be available. This letter presents a new voice conversion method that needs no parallel speech corpus, and adopts a restricted Boltzmann machine (RBM) to represent the distribution of the spectral features derived from a target speaker. A linear transformation was employed to convert the spectral and delta features. A conversion function was obtained by maximizing the conditional probability density function with respect to the target RBM. A feasibility test was carried out on the OGI VOICES corpus. Results from the subjective listening tests and the objective results both showed that the proposed method outperforms the conventional GMM-based method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Lee, Ki-Seung] Konkuk Univ, Dept Elect Engn, Seoul 143701, South Korea.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Lee, KS (reprint author), Konkuk Univ, Dept Elect Engn, Seoul 143701, South Korea.</td>
</tr>

<tr>
<td valign="top">EM </td><td>kseung@konkuk.ac.kr</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PD </td><td>AUG</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>24</td>
</tr>

<tr>
<td valign="top">IS </td><td>8</td>
</tr>

<tr>
<td valign="top">BP </td><td>1103</td>
</tr>

<tr>
<td valign="top">EP </td><td>1107</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/LSP.2017.2713412</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000404291100001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Jin, ZY
   <br>Mysore, GJ
   <br>Diverdi, S
   <br>Lu, JW
   <br>Finkelstein, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Jin, Zeyu
   <br>Mysore, Gautham J.
   <br>Diverdi, Stephen
   <br>Lu, Jingwan
   <br>Finkelstein, Adam</td>
</tr>

<tr>
<td valign="top">TI </td><td>VoCo: Text-based Insertion and Replacement in Audio Narration</td>
</tr>

<tr>
<td valign="top">SO </td><td>ACM TRANSACTIONS ON GRAPHICS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>audio; human computer interaction</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION; SPEECH SYNTHESIS; SPECTRUM; STRAIGHT</td>
</tr>

<tr>
<td valign="top">AB </td><td>Editing audio narration using conventional software typically involves many painstaking low-level manipulations. Some state of the art systems allow the editor to work in a text transcript of the narration, and perform select, cut, copy and paste operations directly in the transcript; these operations are then automatically applied to the waveform in a straightforward manner. However, an obvious gap in the text-based interface is the ability to type new words not appearing in the transcript, for example inserting a new word for emphasis or replacing a misspoken word. While high-quality voice synthesizers exist today, the challenge is to synthesize the new word in a voice that matches the rest of the narration. This paper presents a system that can synthesize a new word or short phrase such that it blends seamlessly in the context of the existing narration. Our approach is to use a text to speech synthesizer to say the word in a generic voice, and then use voice conversion to convert it into a voice that matches the narration. Offering a range of degrees of control to the editor, our interface supports fully automatic synthesis, selection among a candidate set of alternative pronunciations, fine control over edit placements and pitch profiles, and even guidance by the editors own voice. The paper presents studies showing that the output of our method is preferred over baseline methods and often indistinguishable from the original voice.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Jin, Zeyu; Finkelstein, Adam] Princeton Univ, Princeton Comp Sci Dept,
   35 Olden St, Princeton, NJ 08540 USA.
   <br>[Mysore, Gautham J.; Diverdi, Stephen; Lu, Jingwan] Adobe Res, 601
   Townsend St, San Francisco, CA 94103 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Jin, ZY (reprint author), Princeton Univ, Princeton Comp Sci Dept, 35 Olden St, Princeton, NJ 08540 USA.</td>
</tr>

<tr xmlns:date="http://exslt.org/dates-and-times">
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Lu, Jingwan</display_name>&nbsp;</font></td><td><font size="3">0000-0002-3598-9918&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUL</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>36</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">AR </td><td>96</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1145/3072959.3073702</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000406432100064</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nakashika, T
   <br>Minami, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nakashika, Toru
   <br>Minami, Yasuhiro</td>
</tr>

<tr>
<td valign="top">TI </td><td>Speaker-adaptive-trainable Boltzmann machine and its application to
   non-parallel voice conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>EURASIP JOURNAL ON AUDIO SPEECH AND MUSIC PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; Boltzmann machine; Unsupervised training; Energy-based
   model; Speaker adaptation; Non-parallel training; SAT</td>
</tr>

<tr>
<td valign="top">ID </td><td>TRANSFORMATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we present a voice conversion (VC) method that does not use any parallel data while training the model. Voice conversion is a technique where only speaker-specific information in the source speech is converted while keeping the phonological information unchanged. Most of the existing VC methods rely on parallel data-pairs of speech data from the source and target speakers uttering the same sentences. However, the use of parallel data in training causes several problems: (1) the data used for the training is limited to the pre-defined sentences, (2) the trained model is only applied to the speaker pair used in the training, and (3) a mismatch in alignment may occur. Although it is generally preferable in VC to not use parallel data, a non-parallel approach is considered difficult to learn. In our approach, we realize the non-parallel training based on speaker-adaptive training (SAT). Speech signals are represented using a probabilistic model based on the Boltzmann machine that defines phonological information and speaker-related information explicitly. Speaker-independent (SI) and speaker-dependent (SD) parameters are simultaneously trained using SAT. In the conversion stage, a given speech signal is decomposed into phonological and speaker-related information, the speaker-related information is replaced with that of the desired speaker, and then voice-converted speech is obtained by combining the two. Our experimental results showed that our approach outperformed the conventional non-parallel approach regarding objective and subjective criteria.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nakashika, Toru; Minami, Yasuhiro] Univ Electrocommun, Grad Sch
   Informat Syst, 1-5-1 Chofugaoka, Chofu, Tokyo 1828585, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nakashika, T (reprint author), Univ Electrocommun, Grad Sch Informat Syst, 1-5-1 Chofugaoka, Chofu, Tokyo 1828585, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>nakashika@uec.ac.jp; minami.yasuhiro@is.uec.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUN 29</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">AR </td><td>16</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1186/s13636-017-0112-6</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000404653200001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nguyen, GN
   <br>Phung, TN</td>
</tr>

<tr>
<td valign="top">AF </td><td>Gia-Nhu Nguyen
   <br>Trung-Nghia Phung</td>
</tr>

<tr>
<td valign="top">TI </td><td>Reducing over-smoothness in HMM-based speech synthesis using
   exemplar-based voice conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>EURASIP JOURNAL ON AUDIO SPEECH AND MUSIC PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speech synthesis; HMM-based; Voice conversion; Exemplar-based;
   Non-negative matrix factorization; Over-smooth</td>
</tr>

<tr>
<td valign="top">AB </td><td>Speech synthesis has been applied in many kinds of practical applications. Currently, state-of-the-art speech synthesis uses statistical methods based on hidden Markov model (HMM). Speech synthesized by statistical methods can be considered over-smooth caused by the averaging in statistical processing. In the literature, there have been many studies attempting to solve over-smoothness in speech synthesized by an HMM. However, they are still limited. In this paper, a hybrid synthesis between HMM and exemplar-based voice conversion has been proposed. The experimental results show that the proposed method outperforms state-of-the-art HMM synthesis using global variance.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Gia-Nhu Nguyen] Duy Tan Univ, Da Nang, Vietnam.
   <br>[Trung-Nghia Phung] Thai Nguyen Univ Informat &amp; Commun Technol, Thai
   Nguyen, Vietnam.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Phung, TN (reprint author), Thai Nguyen Univ Informat &amp; Commun Technol, Thai Nguyen, Vietnam.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ptnghia@ictu.edu.vn</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Nhu, Nguyen Gia</display_name>&nbsp;</font></td><td><font size="3">E-4331-2016&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Nhu, Nguyen Gia</display_name>&nbsp;</font></td><td><font size="3">0000-0003-4267-3900&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>phung, trung-nghia</display_name>&nbsp;</font></td><td><font size="3">0000-0003-0075-3427&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUN 24</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">AR </td><td>14</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1186/s13636-017-0113-5</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000404393600001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Paul, D
   <br>Pal, M
   <br>Saha, G</td>
</tr>

<tr>
<td valign="top">AF </td><td>Paul, Dipjyoti
   <br>Pal, Monisankha
   <br>Saha, Goutam</td>
</tr>

<tr>
<td valign="top">TI </td><td>Spectral Features for Synthetic Speech Detection</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Anti-spoofing; block-based MFCC; speaker verification;
   speech-signal-based frequency cepstral coefficients (SFCC); spoofing
   attack</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEAKER VERIFICATION; RECOGNITION; INFORMATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Recent advancements in voice conversion (VC) and speech synthesis research make speech-based biometric systems highly prone to spoofing attacks. This can provoke an increase in false acceptance rate in such systems and requires countermeasure to mitigate such spoofing attacks. In this paper, we first study the characteristics of synthetic speech vis-a-vis natural speech and then propose a set of novel short-term spectral features that can efficiently capture the discriminative information between them. The proposed features are computed using inverted frequency warping scale and overlapped block transformation of filter bank log energies. Our study presents a detailed analysis of antispoofing performance with respect to the variations in the warping scale for inverted frequency and block size for the block transform. For performance analysis, Gaussian mixture model (GMM) based synthetic speech detector is used as a classifier on a stand-alone basis and also, integrated with automatic speaker verification (ASV) systems. For ASV systems, standard mel-frequency cepstral coefficients are used as feature while GMM with universal background model and i-vector are used as classifiers. The experiments are conducted on ten different kinds of synthetic data from ASVspoof 2015 corpus. The results show that the countermeasures based on the proposed features outperform other spectral features for both known and unknown attacks. An average equal error rate (EER) of 0.00% has been achieved for nine attacks that use VC or SS speech and the best performance of 7.12% EER is arrived at the remaining natural speech concatenation-based spoofing attack.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Paul, Dipjyoti; Pal, Monisankha; Saha, Goutam] Indian Inst Technol,
   Dept Elect &amp; Elect Commun Engn, Kharagpur 721302, W Bengal, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Paul, D (reprint author), Indian Inst Technol, Dept Elect &amp; Elect Commun Engn, Kharagpur 721302, W Bengal, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>dipjyotipaul@ece.iitkgp.ernet.in; monisankhapal@iitkgp.ac.in;
   gsaha@ece.iitkgp.ernet.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>7</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>7</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>11</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">BP </td><td>605</td>
</tr>

<tr>
<td valign="top">EP </td><td>617</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/JSTSP.2017.2684705</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000401343600003</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sriskandaraja, K
   <br>Sethu, V
   <br>Ambikairajah, E
   <br>Li, HZ</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sriskandaraja, Kaavya
   <br>Sethu, Vidhyasaharan
   <br>Ambikairajah, Eliathamby
   <br>Li, Haizhou</td>
</tr>

<tr>
<td valign="top">TI </td><td>Front-End for Antispoofing Countermeasures in Speaker Verification:
   Scattering Spectral Decomposition</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Anti-spoofing; automatic speaker verification; modulation spectrum;
   scattering spectrum; spoofing countermeasures; spoofing detection;
   wavelet decomposition</td>
</tr>

<tr>
<td valign="top">ID </td><td>SYSTEM</td>
</tr>

<tr>
<td valign="top">AB </td><td>As speaker verification is widely used as a means of verifying personal identity in commercial applications, the study of antispoofing countermeasures has become increasingly important. By choosing appropriate spectral and prosodic feature mapping, spoofing methods based on voice conversion and speech synthesis are both capable of deceiving speaker verification systems that typically rely on these features. Consequently alternative front-ends are required for effective spoofing detection. This paper investigates the use of the recently proposed hierarchical scattering decomposition technique, which can be viewed as a generalization of all constant-Q spectral decompositions, to implement front-ends for stand-alone spoofing detection. The coefficients obtained using this decomposition are converted to a feature vector of Scattering Cepstral Coefficients (SCCs). We evaluate the performance of SCCs on the recent spoofing and Antispoofing (SAS) corpus as well as the ASVspoof 2015 challenge corpus and show that SCCs are superior to all other front-ends that have previously been bench-marked on the ASVspoof corpus.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Sriskandaraja, Kaavya; Sethu, Vidhyasaharan; Ambikairajah, Eliathamby]
   Univ New South Wales, Sch Elect Engn &amp; Telecommun, Sydney, NSW 2052,
   Australia.
   <br>[Ambikairajah, Eliathamby] ATP, Data61, Eveleigh, NSW 2015, Australia.
   <br>[Li, Haizhou] Natl Univ Singapore, Dept Elect &amp; Comp Engn, Singapore
   117583, Singapore.
   <br>[Li, Haizhou] ASTAR, Inst Infocomm Res, Singapore 138632, Singapore.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sriskandaraja, K (reprint author), Univ New South Wales, Sch Elect Engn &amp; Telecommun, Sydney, NSW 2052, Australia.</td>
</tr>

<tr>
<td valign="top">EM </td><td>k.sriskandaraja@unsw.edu.au; v.sethu@unsw.edu.au; ambi@ee.unsw.edu.au;
   eleliha@nus.edu.sg</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Li, Haizhou</display_name>&nbsp;</font></td><td><font size="3">Q-6438-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Sethu, Vidhyasaharan</display_name>&nbsp;</font></td><td><font size="3">0000-0001-8492-1787&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Li, Haizhou</display_name>&nbsp;</font></td><td><font size="3">0000-0001-9158-9401&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>6</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>6</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>11</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">BP </td><td>632</td>
</tr>

<tr>
<td valign="top">EP </td><td>643</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/JSTSP.2016.2647202</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000401343600005</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Patel, TB
   <br>Patil, HA</td>
</tr>

<tr>
<td valign="top">AF </td><td>Patel, Tanvina B.
   <br>Patil, Hemant A.</td>
</tr>

<tr>
<td valign="top">TI </td><td>Significance of Source-Filter Interaction for Classification of Natural
   vs. Spoofed Speech</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Anti-spoofing; source-filter interaction; LF-model; residue; Gaussian
   mixture model</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEAKER VERIFICATION; CONVERSION; PHONATION; ALGORITHM</td>
</tr>

<tr>
<td valign="top">AB </td><td>Countermeasures used to detect synthetic and voice-converted spoofed speech are usually based on excitation source or system features. However, in the natural speech production mechanism, there exists nonlinear source-filter (S-F) interaction as well. This interaction is an attribute of natural speech and is rarely present in synthetic or voice-converted speech. Therefore, we propose features based on the S-F interaction for a spoofed speech detection (SSD) task. To that effect, we estimate the voice excitation source (i.e., differenced glottal flow waveform, (g) over dot(t)) and model it using the well-known Liljencrants-Fant model to get coarse structure, g(c)(t). The residue or difference, g(r)(t), between (g) over dot(t) and g(c)(t) is known to capture the nonlinear S-F interaction. In the time domain, the L-2 norm of g(r)(t) in the closed, open, and return phases of the glottis are considered as features. In the frequency domain, the Mel representation of g(r)(t) showed significant contribution in the SSD task. The proposed features are evaluated on the first ASVspoof 2015 challenge database using a Gaussian mixture model based classification system. On the evaluation set, for vocoder-based spoofs (i.e., S1-S9), the score-level fusion of residual energy features, Mel representation of the residual signal, and Mel frequency cepstral coefficients (MFCC) features gave an equal error rate (EER) of 0.017%, which is much less than the 0.319% obtained with MFCC alone. Furthermore, the residues of the spectrogram (as well as the Mel-warped spectrogram) of estimated (g) over dot(t) and g(c)(t) are also explored as features for the SSD task. The features are evaluated for robustness in the presence of additive white, babble, and car noise at various signal-to-noise-ratio levels on the ASVspoof 2015 database and for channel mismatch condition on the Blizzard Challenge 2012 dataset. For both cases, the proposed features gave significantly less EER than that obtained by MFCC on the evaluation set.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Patel, Tanvina B.; Patil, Hemant A.] Dhirubhai Ambani Inst Informat &amp;
   Commun Technol, Speech Res Lab, Gandhinagar 382007, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Patel, TB (reprint author), Dhirubhai Ambani Inst Informat &amp; Commun Technol, Speech Res Lab, Gandhinagar 382007, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>tanvina_bhupendrabhai_patel@daiict.ac.in; hemant_patil@daiict.ac.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>11</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">BP </td><td>644</td>
</tr>

<tr>
<td valign="top">EP </td><td>659</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/JSTSP.2017.2682788</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000401343600006</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wang, LB
   <br>Nakagawa, S
   <br>Zhang, ZF
   <br>Yoshida, YH
   <br>Kawakami, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wang, Longbiao
   <br>Nakagawa, Seiichi
   <br>Zhang, Zhaofeng
   <br>Yoshida, Yohei
   <br>Kawakami, Yuta</td>
</tr>

<tr>
<td valign="top">TI </td><td>Spoofing Speech Detection Using Modified Relative Phase Information</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Countermeasures; Gaussian mixture model (GMM); group delay; relative
   phase (RP) information; spoofing speech detection</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEAKER VERIFICATION; VOICE CONVERSION; SYNTHETIC SPEECH;
   IDENTIFICATION; RECOGNITION; HMM; ATTACKS; MODELS</td>
</tr>

<tr>
<td valign="top">AB </td><td>The detection of human and spoofing (synthetic or converted) speech has started to receive an increasing amount of attention. In this paper, modified relative phase (MRP) information extracted from a Fourier spectrum is proposed for spoofing speech detection. Because original phase information is almost entirely lost in spoofing speech using current synthesis or conversion techniques, some phase information extraction methods, such as the modified group delay feature and cosine phase feature, have been shown to be effective for detecting human speech and spoofing speech. However, existing phase information-based features cannot obtain very high spoofing speech detection performance because they cannot extract precise phase information from speech. Relative phase (RP) information, which extracts phase information precisely, has been shown to be effective for speaker recognition. In this paper, RP information is applied to spoofing speech detection, and it is expected to achieve better spoofing detection performance. Furthermore, two modified processing techniques of the original RP, that is, pseudo pitch synchronization and linear discriminant analysis based full-band RP extraction, are proposed in this paper. In this study, MRP information is also combined with the Mel-frequency cepstral coefficient (MFCC) and modified group delay. The proposed method was evaluated using the ASVspoof 2015: Automatic Speaker Verification Spoofing and Countermeasures Challenge dataset. The results show that the proposed MRP information significantly outperforms the MFCC, modified group delay, and other phase information based features. For the development dataset, the equal error rate (EER) was reduced from 1.883% of the MFCC, 0.567% of the modified group delay to 0.013% of the MRP. By combining the RP with the MFCC and modified group delay, the EER was reduced to 0.003%. For the evaluation dataset, the MRP obtained much better performance than the magnitude-based feature and other phase-based features, except for S10 spoofing speech.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Wang, Longbiao] Tianjin Univ, Sch Comp Sci &amp; Technol, Tianjin Key Lab
   Cognit Comp &amp; Applicat, Tianjin 300072, Peoples R China.
   <br>[Nakagawa, Seiichi] Toyohashi Univ Technol, Dept Informat &amp; Comp Sci,
   Toyohashi, Aichi 4418580, Japan.
   <br>[Zhang, Zhaofeng; Yoshida, Yohei; Kawakami, Yuta] Nagaoka Univ Technol,
   Nagaoka, Niigata 9402188, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Wang, LB (reprint author), Tianjin Univ, Sch Comp Sci &amp; Technol, Tianjin Key Lab Cognit Comp &amp; Applicat, Tianjin 300072, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>longbiao_wang@tju.edu.cn; nakagawa@tut.jp; phoenixflame11@gmail.com;
   s123182@stn.nagaokaut.ac.jp; s123118@stn.nagaokaut.ac.jp</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>nakagawa, seiichi</display_name>&nbsp;</font></td><td><font size="3">L-5543-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>5</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>5</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>11</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">BP </td><td>660</td>
</tr>

<tr>
<td valign="top">EP </td><td>670</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/JSTSP.2017.2694139</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000401343600007</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Demiroglu, C
   <br>Buyuk, O
   <br>Khodabakhsh, A
   <br>Maia, R</td>
</tr>

<tr>
<td valign="top">AF </td><td>Demiroglu, Cenk
   <br>Buyuk, Osman
   <br>Khodabakhsh, Ali
   <br>Maia, Ranniery</td>
</tr>

<tr>
<td valign="top">TI </td><td>Postprocessing Synthetic Speech With a Complex Cepstrum Vocoder for
   Spoofing Phase-Based Synthetic Speech Detectors</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Spoofing; speaker verification; synthetic speech detection; complex
   cepstrum; speech synthesis; voice conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEAKER VERIFICATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>State-of-the-art speaker verification systems are vulnerable to spoofing attacks. To address the issue, high-performance synthetic speech detectors (SSDs) for existing spoofing methods have been proposed. Phase-based SSDs that exploit the fact that most of the parametric speech coders use minimum-phase filters are particularly successful when synthetic speech is generated with a parametric vocoder. Here, we propose a new attack strategy to spoof phase-based SSDs with the objective of increasing the security of voice verification systems by enabling the development of more generalized SSDs. As opposed to other parametric vocoders, the complex cepstrum approach uses mixed-phase filters, which makes it an ideal candidate for spoofing the phase-based SSDs. We propose using a complex cepstrum vocoder as a postprocessor to existing techniques to spoof the speaker verification system as well as the phase-based SSDs. Once synthetic speech is generated with a speech synthesis or a voice conversion technique, for each synthetic speech frame, a natural frame is selected from a training database using a spectral distance measure. Then, complex cepstrum parameters of the natural frame are used for resynthesizing the synthetic frame. In the proposed method, complex cepstrum-based resynthesis is used as a postprocessor. Hence, it can be used in tandem with any synthetic speech generator. Experimental results showed that the approach is successful at spoofing four phase-based SSDs across nine parametric attack algorithms. Moreover, performance at spoofing the speaker verification system did not substantially degrade compared to the case when no postprocessor is employed.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Demiroglu, Cenk; Khodabakhsh, Ali] Ozyegin Univ, Dept Elect &amp; Comp
   Engn, TR-34794 Istanbul, Turkey.
   <br>[Buyuk, Osman] Kocaeli Univ, Dept Elect &amp; Telecommun Engn, TR-41380
   Kocaeli, Turkey.
   <br>[Maia, Ranniery] Toshiba Res Europe Ltd, Cambridge Res Lab, Cambridge
   CB4 0GZ, England.
   <br>[Maia, Ranniery] Univ Fed Santa Catarina, Dept Elect Engn, BR-88040900
   Florianopolis, SC, Brazil.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Demiroglu, C (reprint author), Ozyegin Univ, Dept Elect &amp; Comp Engn, TR-34794 Istanbul, Turkey.</td>
</tr>

<tr>
<td valign="top">EM </td><td>cenk.demiroglu@ozyegin.edu.tr; osman.buyuk@kocaeli.edu.tr;
   ali.khodabakhsh@gmail.com; rmaia@linse.ufsc.br</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Khodabakhsh, Ali</display_name>&nbsp;</font></td><td><font size="3">0000-0002-2873-4140&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>11</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">BP </td><td>671</td>
</tr>

<tr>
<td valign="top">EP </td><td>683</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/JSTSP.2017.2673807</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000401343600008</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Phung, TN</td>
</tr>

<tr>
<td valign="top">AF </td><td>Phung, Trung-Nghia</td>
</tr>

<tr>
<td valign="top">TI </td><td>HMM-based Speech Synthesis with Multiple Individual Voices using
   Exemplar-based Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERNATIONAL JOURNAL OF COMPUTER SCIENCE AND NETWORK SECURITY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>HMM-based speech synthesis; Speaker adaptation; Exemplar-based voice
   conversion; Non-negative matrix factorization; Speaker individuality</td>
</tr>

<tr>
<td valign="top">AB </td><td>Traditional text-to-speech (TTS) systems can synthesize only single individual voice. When we need to synthesize other individual voices, we have to train the system again with the new voices. The training process normally requires a huge amount of data that is usually available with a few specific voices existed in the database.
   <br>The state of the art TTS using Hidden Markov Model (HMM), called as HMM-based TTS, can synthesize speech with various voice personality characteristics by using speaker adaptation methods. However, both of the voices synthesized and adapted by HMM-based TTS are "over-smooth". When these voices are over-smooth, the detail structures clearly linked to speaker individuality may be missing. We can also synthesize multiple voices by using some voice conversion (VC) methods combined with HMM-based TTS. However, current voice conversions still cannot synthesize target speech while keeping the detail information related to speaker individuality of the target voice and just using limited amount data of target voices. In this paper, we proposed to use exemplar-based voice conversion combined with HMM-based TTS to synthesize multiple high-quality individual voices with a few amount of target data. The evaluation results using the English data corpus CSTR confirmed the advantages of the proposed method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Phung, Trung-Nghia] Thai Nguyen Univ, Thai Nguyen, Vietnam.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Phung, TN (reprint author), Thai Nguyen Univ, Thai Nguyen, Vietnam.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAY 30</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>17</td>
</tr>

<tr>
<td valign="top">IS </td><td>5</td>
</tr>

<tr>
<td valign="top">BP </td><td>192</td>
</tr>

<tr>
<td valign="top">EP </td><td>196</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000412566800025</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nirmal, J
   <br>Zaveri, M
   <br>Patnaik, S
   <br>Kachare, P</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nirmal, Jagannath
   <br>Zaveri, Mukesh
   <br>Patnaik, Suprava
   <br>Kachare, Pramod</td>
</tr>

<tr>
<td valign="top">TI </td><td>Novel approach of MFCC based alignment and WD-residual modification for
   voice conversion using RBF</td>
</tr>

<tr>
<td valign="top">SO </td><td>NEUROCOMPUTING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Dynamic time warping; Gaussian mixture model; LP-residual; Line spectral
   frequencies; Mel frequency cepstrum coefficient; Radial basis function;
   Residual selection method and; Wavelet packet transform</td>
</tr>

<tr>
<td valign="top">ID </td><td>NEURAL-NETWORKS; ALGORITHM; CLASSIFICATION; PREDICTION; FEATURES;
   MIXTURE</td>
</tr>

<tr>
<td valign="top">AB </td><td>The voice conversion system modifies the speaker specific characteristics of the source speaker to that of the target speaker, so it perceives like target speaker. The speaker specific characteristics of the speech signal are reflected at different levels such as the shape of the vocal tract, shape of the glottal excitation and long term prosody. The shape of the vocal tract is represented by Line Spectral Frequency (LSF) and the shape of glottal excitation by Linear Predictive (LP) residuals. In this paper, the fourth level wavelet packet transform is applied to LP-residual to generate the sixteen sub-bands. This approach not only reduces the computational complexity but also presents a genuine transformation model over state of the art statistical prediction methods. In voice conversion, the alignment is an essential process which aligns the features of the source and target speakers. In this paper, the Mel Frequency Cepstrum Coefficients (MFCC) based warping path is proposed to align the LSF and LP-residual sub-bands using proposed constant source and constant target alignment. The conventional alignment technique is compared with two proposed approaches namely, constant source and constant target. Analysis shows that, constant source alignment using MFCC warping path performs slightly better than the constant target alignment and the state-of-the-art alignment approach. Generalized mapping models are developed for each sub-band using Radial Basis Function neural network (RBF) and are compared with Gaussian Mixture mapping model (GMM) and residual selection approach. Various subjective and objective evaluation measures indicate significant performance of RBF based residual mapping approach over the state-of-the-art approaches. (C) 2016 Elsevier B.V. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nirmal, Jagannath] KJ Somaiya Coll Engn, Dept Elect Engn, Bombay
   400077, Maharashtra, India.
   <br>[Zaveri, Mukesh] SV Natl Inst Technol, Dept Comp Engn, Surat 395007,
   India.
   <br>[Patnaik, Suprava] SV Natl Inst Technol, Dept Elect Engn, Surat 395007,
   India.
   <br>[Kachare, Pramod] Veermata Jeejabai Inst Technol, Dept Elect Engn,
   Bombay 400031, Maharashtra, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nirmal, J (reprint author), KJ Somaiya Coll Engn, Dept Elect Engn, Bombay 400077, Maharashtra, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jhnirmal@somaiya.edu; mazaveri@gmail.com; suprava_patnaik@yahoo.com;
   pramod_1991@yahoo.com</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Patnaik, Suprava</display_name>&nbsp;</font></td><td><font size="3">0000-0002-7068-5960&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAY 10</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>237</td>
</tr>

<tr>
<td valign="top">BP </td><td>39</td>
</tr>

<tr>
<td valign="top">EP </td><td>49</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.neucom.2016.07.048</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000397356700004</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Mohammadi, SH
   <br>Kain, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Mohammadi, Seyed Hamidreza
   <br>Kain, Alexander</td>
</tr>

<tr>
<td valign="top">TI </td><td>An overview of voice conversion systems</td>
</tr>

<tr>
<td valign="top">SO </td><td>SPEECH COMMUNICATION</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; Overview; Survey</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH SYNTHESIS; NEURAL-NETWORKS; PROCESSING TECHNIQUES;
   TRANSFORMATION; REPRESENTATION; FREQUENCY; INTELLIGIBILITY; QUALITY; GMM</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice transformation (VT) aims to change one or more aspects of a speech signal while preserving linguistic information. A subset of VT, Voice conversion (VC) specifically aims to change a source speaker's speech in such a way that the generated output is perceived as a sentence uttered by a target speaker. Despite many years of research, VC systems still exhibit deficiencies in accurately mimicking a target speaker spectrally and prosodically, and simultaneously maintaining high speech quality. In this work we provide an overview of real-world applications, extensively study existing systems proposed in the literature, and discuss remaining challenges. (C) 2017 Elsevier B.V. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Mohammadi, Seyed Hamidreza; Kain, Alexander] Oregon Hlth &amp; Sci Univ,
   Ctr Spoken Language Understanding, Portland, OR 97239 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Mohammadi, SH (reprint author), Oregon Hlth &amp; Sci Univ, Ctr Spoken Language Understanding, Portland, OR 97239 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>mohammah@ohsu.edu; kaina@ohsu.edu</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Mohammadi, Seyed Hamidreza</display_name>&nbsp;</font></td><td><font size="3">0000-0002-6892-9241&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>21</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>23</td>
</tr>

<tr>
<td valign="top">PD </td><td>APR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>88</td>
</tr>

<tr>
<td valign="top">BP </td><td>65</td>
</tr>

<tr>
<td valign="top">EP </td><td>82</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.specom.2017.01.008</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000398009600004</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Girin, L
   <br>Hueber, T
   <br>Alameda-Pineda, X</td>
</tr>

<tr>
<td valign="top">AF </td><td>Girin, Laurent
   <br>Hueber, Thomas
   <br>Alameda-Pineda, Xavier</td>
</tr>

<tr>
<td valign="top">TI </td><td>Extending the Cascaded Gaussian Mixture Regression Framework for
   Cross-Speaker Acoustic-Articulatory Mapping</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Acoustic-articulatory inversion; EM; Gaussian mixture regression; GMM;
   missing data; speaker adaptation</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION; MODEL; ADAPTATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper addresses the adaptation of an acousticartic-ulatory inversion model of a reference speaker to the voice of another source speaker, using a limited amount of audio-only data. In this study, the articulatory-acoustic relationship of the reference speaker is modeled by a Gaussian mixture model and inference of articulatory data from acoustic data is made by the as-sociated Gaussian mixture regression (GMR). To address speaker adaptation, we previously proposed a general framework called Cascaded-GMR (C-GMR) which decomposes the adaptation process into two consecutive steps: spectral conversion between source and reference speaker and acoustic-articulatory inversion of con-verted spectral trajectories. In particular, we proposed the inte-grated C-GMR technique (IC-GMR) in which both steps are tied together in the same probabilistic model. In this paper, we extend the C-GMR framework with another model called Joint-GMR (J-GMR). Contrary to the IC-GMR, this model aims at ex-ploiting all potential acoustic-articulatory relationships, including those between the source speaker's acoustics and the reference speaker's articulation. We present the full derivation of the ex-act expectation-maximization (EM) training algorithm for the J-GMR. It exploits the missing data methodology of machine learn-ing to deal with limited adaptation data. We provide an extensive evaluation of the J-GMR on both synthetic acoustic-articulatory data and on the multispeaker MOCHA EMA database. We com-pare the J-GMR performance to other models of the C-GMR framework, notably the IC-GMR, and discuss their respective merits.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Girin, Laurent] Univ Grenoble Alpes, GIPSA Lab, F-38040 Grenoble,
   France.
   <br>[Girin, Laurent; Alameda-Pineda, Xavier] INRIA Grenoble Rhone Alpes,
   F-56521 Montbonnot St Martin, France.
   <br>[Hueber, Thomas] Univ Grenoble Alpes, CNRS, GIPSA Lab, F-38040 Grenoble,
   France.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Girin, L (reprint author), Univ Grenoble Alpes, GIPSA Lab, F-38040 Grenoble, France.; Girin, L (reprint author), INRIA Grenoble Rhone Alpes, F-56521 Montbonnot St Martin, France.</td>
</tr>

<tr>
<td valign="top">EM </td><td>laurent.girin@gipsa-lab.grenoble-inp.fr;
   thomas.hueber@gipsa-lab.grenoble-inp.fr; xavier.alameda-pineda@inria.fr</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Alameda-Pineda, Xavier</display_name>&nbsp;</font></td><td><font size="3">0000-0002-5354-1084&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>25</td>
</tr>

<tr>
<td valign="top">IS </td><td>3</td>
</tr>

<tr>
<td valign="top">BP </td><td>662</td>
</tr>

<tr>
<td valign="top">EP </td><td>673</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASLP.2017.2651398</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000395561200017</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Shah, NJ
   <br>Patil, HA</td>
</tr>

<tr>
<td valign="top">AF </td><td>Shah, Nirmesh J.
   <br>Patil, Hemant A.</td>
</tr>

<tr>
<td valign="top">BE </td><td>Shankar, BU
   <br>Ghosh, K
   <br>Mandal, DP
   <br>Ray, SS
   <br>Zhang, D
   <br>Pal, SK</td>
</tr>

<tr>
<td valign="top">TI </td><td>Analysis of Features and Metrics for Alignment in Text-Dependent Voice
   Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>PATTERN RECOGNITION AND MACHINE INTELLIGENCE, PREMI 2017</td>
</tr>

<tr>
<td valign="top">SE </td><td>Lecture Notes in Computer Science</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>7th International Conference on Pattern Recognition and Machine
   Intelligence (PReMI)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 05-08, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Indian Stat Inst, Kolkata, INDIA</td>
</tr>

<tr>
<td valign="top">HO </td><td>Indian Stat Inst</td>
</tr>

<tr>
<td valign="top">DE </td><td>Gaussian Mixture Model; Spectral features; Posterior features</td>
</tr>

<tr>
<td valign="top">ID </td><td>RECOGNITION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice Conversion (VC) is a technique that convert the perceived speaker identity from a source speaker to a target speaker. Given a source and target speakers' parallel training speech database in the text-dependent VC, first task is to align source and target speakers' spectral features at frame-level before learning the mapping function. The accuracy of alignment will affect the learning of mapping function and hence, the voice quality of converted voice in VC. The impact of alignment is not much explored in the VC literature. Most of the alignment techniques try to align the acoustical features (namely, spectral features, such as Mel Cepstral Coefficients (MCC)). However, spectral features represents both speaker as well as speech-specific information. In this paper, we have done analysis on the use of different speaker-independent features (namely, unsupervised posterior features, such as, Gaussian Mixture Model (GMM)-based and Maximum A Posteriori (MAP) adapted from Universal Background Model (UBM), i.e., GMM-UBM-based posterior features) for the alignment task. In addition, we propose to use different metrics, such as, symmetric Kullback-Leibler (KL) and cosine distances instead of Euclidean distance for the alignment. Our analysis-based on % Phone Accuracy (PA) is correlating with subjective scores of the developed VC systems with 0.98 Pearson correlation coefficient.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Shah, Nirmesh J.; Patil, Hemant A.] DA IICT, Speech Res Lab,
   Gandhinagar, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Shah, NJ; Patil, HA (reprint author), DA IICT, Speech Res Lab, Gandhinagar, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>nirmesh88_shah@daiict.ac.in; hemant_patil@daiict.ac.in</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Shah, Nirmesh</display_name>&nbsp;</font></td><td><font size="3">0000-0002-7294-6757&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>10597</td>
</tr>

<tr>
<td valign="top">BP </td><td>299</td>
</tr>

<tr>
<td valign="top">EP </td><td>307</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/978-3-319-69900-4_38</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000450772100038</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kamble, MR
   <br>Patil, HA</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kamble, Madhu R.
   <br>Patil, Hemant A.</td>
</tr>

<tr>
<td valign="top">BE </td><td>Shankar, BU
   <br>Ghosh, K
   <br>Mandal, DP
   <br>Ray, SS
   <br>Zhang, D
   <br>Pal, SK</td>
</tr>

<tr>
<td valign="top">TI </td><td>Effectiveness of Mel Scale-Based ESA-IFCC Features for Classification of
   Natural vs. Spoofed Speech</td>
</tr>

<tr>
<td valign="top">SO </td><td>PATTERN RECOGNITION AND MACHINE INTELLIGENCE, PREMI 2017</td>
</tr>

<tr>
<td valign="top">SE </td><td>Lecture Notes in Computer Science</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>7th International Conference on Pattern Recognition and Machine
   Intelligence (PReMI)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 05-08, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Indian Stat Inst, Kolkata, INDIA</td>
</tr>

<tr>
<td valign="top">HO </td><td>Indian Stat Inst</td>
</tr>

<tr>
<td valign="top">DE </td><td>Automatic Speaker Verification; Energy Separation Algorithm; Teager
   Energy Operator; Gabor filterbank; Spoofed speech</td>
</tr>

<tr>
<td valign="top">AB </td><td>The performance of biometric systems based on Automatic Speaker Verification (ASV) degrades due to spoofing attacks, generated using different speech synthesis (SS) and voice conversion (VC) techniques. Results of recent ASV spoof 2015 challenge indicate that spoof aware features are a possible solution, rather than focusing on a powerful classifier. In this paper, we investigate the effect of various frequency scales (such as, ERB, Mel and linear) applied on a Gabor filterbank. The output of filterbank was used to exploit the contribution of instantaneous frequency (IF) in each subband energy via Teager Energy Operator-based Energy Separation Algorithm (TEO-ESA) to capture possible changes in spectral envelope of spoofed speech. The IF is computed from narrow band components of the speech signal and Discrete Cosine Transform (DCT) is applied on deviations in IF, which are referred to as Instantaneous Frequency Cosine Coefficients (IFCC). The classification results on static features shows an EER of 1.32% with Mel frequency scale and 1.87% with linear. The results with delta feature of linear frequency scale gets reduced further to 1.39% whereas, with Mel scale, it increased by 0.64% on development set of ASV spoof 2015 challenge database.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kamble, Madhu R.; Patil, Hemant A.] DA IICT, Speech Res Lab,
   Gandhinagar, Gujarat, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kamble, MR; Patil, HA (reprint author), DA IICT, Speech Res Lab, Gandhinagar, Gujarat, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>madhu_kamble@daiict.ac.in; hemant_patil@daiict.ac.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>10597</td>
</tr>

<tr>
<td valign="top">BP </td><td>308</td>
</tr>

<tr>
<td valign="top">EP </td><td>316</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/978-3-319-69900-4_39</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000450772100039</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Liu, JR
   <br>Cheng, YC</td>
</tr>

<tr>
<td valign="top">AF </td><td>Liu, Jiuran
   <br>Cheng, Youchun</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>The Design and Simulation of Real-Time Encryption Algorithm for Mobile
   Terminal Voice Source</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 INTERNATIONAL CONFERENCE ON COMPUTER SYSTEMS, ELECTRONICS AND
   CONTROL (ICCSEC)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Computer Systems, Electronics and Control
   (ICCSEC)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 25-27, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Dalian, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice source encryption; chaos encryption; logistic mapping; real time</td>
</tr>

<tr>
<td valign="top">AB </td><td>Aiming at the characteristics of real-time transmission system of mobile terminal voice source, the paper chooses chaos encryption algorithm based on conversion table with better real-time and security. Because of the security problems of Logistic mapping, the improved Logistic mapping is used as pseudo-random number generator (PRNG) of the algorithm, then the method of generating chaotic binary sequences and the encryption algorithm are improved. The results of simulation show that the security and real-time of the algorithm can meet the real-time transmission system of mobile terminal voice source.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Liu, Jiuran; Cheng, Youchun] Coll Army, Training Ctr Logist, Teaching &amp;
   Res Off Informat Technol, Xiang Yang, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Liu, JR (reprint author), Coll Army, Training Ctr Logist, Teaching &amp; Res Off Informat Technol, Xiang Yang, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>1735088153@qq.com; chengsix@163.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>1016</td>
</tr>

<tr>
<td valign="top">EP </td><td>1021</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000449512500166</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Huang, BY
   <br>Gong, YH
   <br>Sun, JW
   <br>Shen, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Huang, Boyan
   <br>Gong, Yihan
   <br>Sun, Jinwei
   <br>Shen, Yi</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>A wearable bone-conducted speech enhancement system for strong
   background noises</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 18TH INTERNATIONAL CONFERENCE ON ELECTRONIC PACKAGING TECHNOLOGY
   (ICEPT)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>18th International Conference on Electronic Packaging Technology (ICEPT)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 16-19, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>IEEE, Harbin, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">HO </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">DE </td><td>Bone-conducted speech; speech enhancement; deep neural network</td>
</tr>

<tr>
<td valign="top">AB </td><td>Wearable electronic systems have been and will continue to be utilized in both civil and military uses. Strong noise environments derived from large vehicles (e.g. ships, aircrafts, or military tanks) seriously affect the quality of speech communications especially for wearable systems without hermetic packaging. Bone conduction technology through the acquisition of skull vibration is able to obtain voice information, which can effectively avoid the interference of acoustic noise on the speech. In this paper, a high-performance wearable bone-conducted speech enhancement system is developed to reduce the distortion of the environment noises. Both bone-conducted and air-conducted voices are used to train the equalization function of bone-conducted speech to air-conducted speech based on the deep neural network, and spectrum coefficients of linear predictive coding is taken as feature information for conversion model.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Huang, Boyan; Gong, Yihan; Sun, Jinwei] Harbin Inst Technol, Dept
   Automat Testing &amp; Control, Harbin 150001, Heilongjiang, Peoples R China.
   <br>[Shen, Yi] Harbin Inst Technol, Dept Control Sci &amp; Engn, Harbin 150001,
   Heilongjiang, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Huang, BY (reprint author), Harbin Inst Technol, Dept Automat Testing &amp; Control, Harbin 150001, Heilongjiang, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>byhuang@hit.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>1682</td>
</tr>

<tr>
<td valign="top">EP </td><td>1684</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000431392000366</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Yathigiri, A
   <br>Bathula, M
   <br>Kothapalli, S
   <br>Vekkot, S
   <br>Tripathi, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Yathigiri, Anisha
   <br>Bathula, Meenalatha
   <br>Kothapalli, Susmitha
   <br>Vekkot, Susmitha
   <br>Tripathi, Shikha</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Transformation using Pitch and Spectral Mapping</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTING, COMMUNICATIONS
   AND INFORMATICS (ICACCI)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Advances in Computing, Communications and
   Informatics (ICACCI)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 13-16, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Manipal, INDIA</td>
</tr>

<tr>
<td valign="top">ID </td><td>CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper provides a voice transformation model that uses pitch data and Feed-forward Neural Networks on Line Spectral Frequency. The aim of this work is to achieve the transformation of a speech signal produced by a source speaker by modifying voice individuality parameters such that it appears to be spoken by a chosen target speaker, without modifying the message contents. Most of the previous work on voice conversion does not compensate for spectral detail losses, nonlinearities in speech or interaction between excitation and vocal tract. This causes over smoothing and invariably leads to a lack of similarity between desired and converted speech. The key contribution of this paper is a unique choice of the Linear Prediction Coefficients (LPC) converted to Line Spectral Frequencies (LSF). Neural networks is used for mapping. The performances of these Voice Conversion structures are evaluated by subjective and objective measures which validate the efficiency of the conversion style. The result shows an improvement in recognition rates by a factor of 10% in male to female conversion and a factor of 20% in female to male conversion in ABX tests over the conversion using only LPC coefficients. The novelty of this paper lies in the usage of pitch information to facilitate a holistic conversion model. A recognition rate of 90% for female to male conversion and 80% for male to female conversion has been achieved after incorporating pitch details.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Yathigiri, Anisha; Bathula, Meenalatha; Kothapalli, Susmitha; Vekkot,
   Susmitha; Tripathi, Shikha] Amrita Univ, Dept Elect &amp; Commun Engn,
   Bengaluru, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Yathigiri, A (reprint author), Amrita Univ, Dept Elect &amp; Commun Engn, Bengaluru, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>yathigirianisha@gmail.com; meenalathabathula@gmail.com;
   susmitha636@gmail.com; v_susmitha@blr.amrita.edu; shikha.eee@gmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>1540</td>
</tr>

<tr>
<td valign="top">EP </td><td>1544</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000427645500254</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sisman, B
   <br>Lee, GD
   <br>Li, HZ
   <br>Tan, KC</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sisman, Berrak
   <br>Lee, Grandee
   <br>Li, Haizhou
   <br>Tan, Kay Chen</td>
</tr>

<tr>
<td valign="top">BE </td><td>Tong, R
   <br>Zhang, Y
   <br>Lu, Y
   <br>Dong, M</td>
</tr>

<tr>
<td valign="top">TI </td><td>On the analysis and evaluation of prosody conversion techniques</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 INTERNATIONAL CONFERENCE ON ASIAN LANGUAGE PROCESSING (IALP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Asian Language Processing</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Asian Language Processing (IALP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 05-07, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Natl Univ Singapore, Singapore, SINGAPORE</td>
</tr>

<tr>
<td valign="top">HO </td><td>Natl Univ Singapore</td>
</tr>

<tr>
<td valign="top">DE </td><td>Prosody evaluation; prosody transformation; voice conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION; TIME</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion is a process of modifying the characteristics of source speaker such as spectrum or/and prosody, to sound as if it was spoken by another speaker. In this paper, we study the evaluation of prosody transformation, in particular, the evaluation of Fundamental Frequency (F0) conversion. F0 is an essential prosody feature that should be taken care of in a compressive voice conversion framework. So far, the evaluation of the converted prosody features is performed mainly by looking at Pearson Correlation Coefficient and Root Mean Square Error (RMSE). Unfortunately, these techniques do not explicitly measure the F0 alignment between the source and target signals. We believe that an evaluation measure that takes into account the time alignment of In is needed to provide a new perspective. Therefore, in this paper, we study a new technique to assess the accuracy of prosody transformation. In our experiments with different prosody transformation techniques, we report that the proposed evaluation approach achieves consistent results with the baseline evaluation metrics.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Sisman, Berrak; Lee, Grandee; Li, Haizhou] Natl Univ Singapore, Elect &amp;
   Comp Engn Dept, Singapore, Singapore.
   <br>[Tan, Kay Chen] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong
   Kong, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Li, HZ (reprint author), Natl Univ Singapore, Elect &amp; Comp Engn Dept, Singapore, Singapore.</td>
</tr>

<tr>
<td valign="top">EM </td><td>haizhou.li@nus.edu.sg; kaytan@cityu.edu.hk</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Li, Haizhou</display_name>&nbsp;</font></td><td><font size="3">Q-6438-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>44</td>
</tr>

<tr>
<td valign="top">EP </td><td>47</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000428370700011</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Li, YP
   <br>Zuo, YT
   <br>Yang, Z
   <br>Shao, X</td>
</tr>

<tr>
<td valign="top">AF </td><td>Li, Yanping
   <br>Zuo, Yutao
   <br>Yang, Zhen
   <br>Shao, Xi</td>
</tr>

<tr>
<td valign="top">BE </td><td>Li, T
   <br>Lopez, LM
   <br>Li, Y</td>
</tr>

<tr>
<td valign="top">TI </td><td>High Quality Voice Conversion based on ISODATA Clustering Algorithm</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 12TH INTERNATIONAL CONFERENCE ON INTELLIGENT SYSTEMS AND KNOWLEDGE
   ENGINEERING (IEEE ISKE)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>12th International Conference on Intelligent Systems and Knowledge
   Engineering (IEEE ISKE)</td>
</tr>

<tr>
<td valign="top">CY </td><td>NOV 24-26, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>NanJing, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; ISODATA; similarity; quality; bilinear frequency;
   Gaussian mixture model</td>
</tr>

<tr>
<td valign="top">AB </td><td>Two main challenges introduced in current voice conversion are the dependence on parallel training data and the trade-off between speaker similarity and speech quality. To tackle the latter problem, this paper proposes a novel conversion method based on Iterative Self-organizing DATA Analysis Techniques Algorithm (ISODATA) clustering algorithm. Specially, we use ISODATA during the training of Gaussian mixture model, the optimized mixture number can guarantee the validity and accuracy of the GMM model, which can acquire speaker's identity effectively related to speaker similarity between original target speech and converted speech, Next, we combine improved GMM and bilinear frequency warping for the conversion stage, which can get a good balance between speaker similarity and speech quality. Theory analysis and experimental results demonstrate that the proposed algorithm can achieve higher quality and similarity compared with other two methods.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Li, Yanping; Zuo, Yutao; Yang, Zhen; Shao, Xi] Nanjing Univ Posts &amp;
   Telecommun, Coll Telecommun &amp; Informat Engn, Nanjing, Jiangsu, Peoples R
   China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Li, YP (reprint author), Nanjing Univ Posts &amp; Telecommun, Coll Telecommun &amp; Informat Engn, Nanjing, Jiangsu, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>liyp@njupt.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000427969500107</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hanilci, C</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hanilci, Cemal</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Features and Classifiers for Replay Spoofing Attack Detection</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 10TH INTERNATIONAL CONFERENCE ON ELECTRICAL AND ELECTRONICS
   ENGINEERING (ELECO)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>10th International Conference on Electrical and Electronics Engineering
   (ELECO)</td>
</tr>

<tr>
<td valign="top">CY </td><td>NOV 30-DEC 02, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Bursa, TURKEY</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEAKER VERIFICATION; TRANSFORM; MODELS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Automatic speaker verification (ASV) systems are known to be highly vulnerable against spoofing attacks. Various successful countermeasures have recently been proposed to detect spoofing attacks originating from speech synthesis (SS) and voice conversion (VC). However, detecting replay attacks, the most easily implementable spoofing attacks against ASV systems, has gained less attention. Thus, in this paper we present an experimental comparison of various feature extraction techniques and classifiers for replay attack detection. In total, six magnitude spectrum and three phase spectrum based features are used for feature extraction. For classification in turn, four different techniques are utilized. Experiments are conducted on recently released ASVspoof 2017 replay attack detection challenge. Experimental results reveals that magnitude spectrum features considerably outperform phase based features independent of the classifier. Comparative results using four different classifiers indicate that i-vector cosine scoring yields lower equal error rates (EERs) than other methods.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Hanilci, Cemal] Bursa Tech Univ, Dept Elect &amp; Elect Engn, Bursa, Turkey.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hanilci, C (reprint author), Bursa Tech Univ, Dept Elect &amp; Elect Engn, Bursa, Turkey.</td>
</tr>

<tr>
<td valign="top">EM </td><td>cemal.hanilci@btu.edu.tr</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Hanilci, Cemal</display_name>&nbsp;</font></td><td><font size="3">S-4967-2016&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>1187</td>
</tr>

<tr>
<td valign="top">EP </td><td>1191</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000426978800209</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kamble, MR
   <br>Patil, HA</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kamble, Madhu R.
   <br>Patil, Hemant A.</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Novel Energy Separation Based Instantaneous Frequency Features for Spoof
   Speech Detection</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 25TH EUROPEAN SIGNAL PROCESSING CONFERENCE (EUSIPCO)</td>
</tr>

<tr>
<td valign="top">SE </td><td>European Signal Processing Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>25th European Signal Processing Conference (EUSIPCO)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 28-SEP 02, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>GREECE</td>
</tr>

<tr>
<td valign="top">DE </td><td>Teager Energy Operator; Energy Separation Algorithm; Instantaneous
   Frequency Cosine Coefficients; SSD; GMM; EER</td>
</tr>

<tr>
<td valign="top">ID </td><td>HUMAN LISTENING TESTS; TIME PHASE SPECTRUM; RECOGNITION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Speech Synthesis (SS) and Voice Conversion (VC) presents a genuine risk of attacks for Automatic Speaker Verification (ASV) technology. In this paper, we evaluate front-end anti-spoofing technique to protect ASV system for SS and VC attack using a standard benchmarking database. In particular, we propose a novel feature set, namely, Energy Separation Algorithm-based Instantaneous Frequency Cosine Coefficients (ESA-IFCC) to detect the genuine and impostor speech. The experiments are carried out on ASV Spoof 2015 Challenge database. On the development set, the score-level fusion of proposed ESA-IFCC feature set with Mel Frequency Cepstral Coefficients (MFCC) gave an EER of 3.45 %, which reduced significantly from MFCC (6.98 %) and ESA-IFCC (5.43 %) with 13-D static features. The EER decreases further to 2.01 % and 1.89 % for Delta and Delta Delta features derived from proposed ESAIFCC features, respectively. The overall average error rate for known and unknown attacks in evaluation set was 6.79 % for ESA-IFCC and was significantly better than the MFCC (9.15 %) features.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kamble, Madhu R.; Patil, Hemant A.] DA IICT, Gandhinagar 382007,
   Gujarat, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kamble, MR (reprint author), DA IICT, Gandhinagar 382007, Gujarat, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>madhu_kamble@daiict.ac.in; hemant_patil@daiict.ac.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>106</td>
</tr>

<tr>
<td valign="top">EP </td><td>110</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000426986000022</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kishore, KK
   <br>Prudhvi, G
   <br>Naveen, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kishore, K. Krishna
   <br>Prudhvi, G.
   <br>Naveen, M.</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>BRAILLE SCRIPT TO VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 INTERNATIONAL CONFERENCE ON COMPUTING METHODOLOGIES AND
   COMMUNICATION (ICCMC)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Computing Methodologies and Communication
   (ICCMC)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUL 18-19, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Surya Engn Coll, INDIA</td>
</tr>

<tr>
<td valign="top">HO </td><td>Surya Engn Coll</td>
</tr>

<tr>
<td valign="top">DE </td><td>Braille; Wearable; Impaired; Communication</td>
</tr>

<tr>
<td valign="top">AB </td><td>The Braille script to voice conversion explores a wearable messaging device that uses embedded technology for visually impaired person. The main objective of this project is to establish a means of communication for specially abled people. It has a special keyboard that is integrated with the braille for the use of blind. It converts the message that the person wants to communicate to other person into a voice and text, so that he understands what the blind and dumb intend to say. It is the first of its kind that would be of great help to the specially abled, as blindness is considered to be the highest of all other disabilities. One can build communication between blind and dumb people and the community by using this Braille system. Moreover, the system is implemented on the open source Arduino platform which makes it more efficient and also cost effective.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kishore, K. Krishna; Prudhvi, G.; Naveen, M.] GMR Inst Technol, Dept
   ECE, Rajam, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kishore, KK (reprint author), GMR Inst Technol, Dept ECE, Rajam, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>Krishnakishore.k@gmail.com; prudhvigajjarapu@gmail.com;
   m.naveennani1997@gmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>1080</td>
</tr>

<tr>
<td valign="top">EP </td><td>1082</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000426980700201</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Dong, MH
   <br>Yang, CY
   <br>Ehnes, JW
   <br>Lu, YF
   <br>Ming, HP
   <br>Huang, DY</td>
</tr>

<tr>
<td valign="top">AF </td><td>Dong, Minghui
   <br>Yang, Chenyu
   <br>Ehnes, Jochen Walter
   <br>Lu, Yanfeng
   <br>Ming, Huaiping
   <br>Huang, Dongyan</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Frame Labeling and Mapping for Non-parallel Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 IEEE 2ND INTERNATIONAL CONFERENCE ON SIGNAL AND IMAGE PROCESSING
   (ICSIP)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>2nd IEEE International Conference on Signal and Image Processing (ICSIP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 04-06, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Singapore, SINGAPORE</td>
</tr>

<tr>
<td valign="top">DE </td><td>non-parallel voice conversion; frame mapping; dnn-hmm recognizer;
   clustering</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion is to convert one person's voice into another person's voice. Depending on whether the contents of the speech data from both source and target speakers are the same, there are two types of conversion, namely, parallel or non-parallel voice conversions. For parallel voice conversion, since the contents of the speech data from the two speakers are the same, alignment methods can be easily used to establish the correspondence between the speech data of the two speakers. When applying the same methods from parallel voice conversion to non-parallel voice conversion, the mapping of corresponding signal segments is not straightforward. Recently, we proposed to use a DNN-HMM (Hybrid Deep Neural Network - Hidden Markov Model) recognizer to label each frame of the speech data from both source and target speakers, and establish mapping by clustering the vector of pseudo-likelihood of each frame. The experiments showed that the method generates results that are comparable to parallel voice conversion method. In this work, we further study how the method works for different settings in the frame mapping process. Using an exemplar-based parallel method conversion method for testing, we compare our method with the state-of-the-art method INCA (An Iterative combination of a Nearest Neighbor search step and a Conversion step Alignment method). The experiments show that the proposed method generates results similar to those generated by INCA-based voice conversion.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Dong, Minghui; Yang, Chenyu; Ehnes, Jochen Walter; Lu, Yanfeng; Ming,
   Huaiping; Huang, Dongyan] ASTAR, Inst Infocomm Res, Human Language
   Technol Dept, Singapore, Singapore.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Dong, MH (reprint author), ASTAR, Inst Infocomm Res, Human Language Technol Dept, Singapore, Singapore.</td>
</tr>

<tr>
<td valign="top">EM </td><td>mhdong@i2r.a-star.edu.sg; yangc@i2r.a-star.edu.sg;
   jwehnes@i2r.a-star.edu.sg; luyf@i2r.a-star.edu.sg;
   minghp@i2r.a-star.edu.sg; huang@i2r.a-star.edu.sg</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>361</td>
</tr>

<tr>
<td valign="top">EP </td><td>365</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000427487200072</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ezzine, K
   <br>Frikha, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ezzine, Kadria
   <br>Frikha, Mondher</td>
</tr>

<tr>
<td valign="top">BE </td><td>ElHassouni, M
   <br>Karim, M
   <br>BenHamida, A
   <br>BenSlima, A
   <br>Solaiman, B</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Comparative Study of Voice Conversion Techniques: A review</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 3RD INTERNATIONAL CONFERENCE ON ADVANCED TECHNOLOGIES FOR SIGNAL
   AND IMAGE PROCESSING (ATSIP)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>3rd International Conference on Advanced Technologies for Signal and
   Image Processing (ATSIP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 22-24, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Fez, MOROCCO</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice Conversion; Speech synthesis; Voice quality; Similarity</td>
</tr>

<tr>
<td valign="top">ID </td><td>MAXIMUM-LIKELIHOOD-ESTIMATION; NEURAL-NETWORKS; PREDICTION; FREQUENCY;
   GMM</td>
</tr>

<tr>
<td valign="top">AB </td><td>Speaker identity, the sound of a person's voice, is one of the most important characteristics in human communication. Voice conversion (VC) is an emergent problem in voice and speech processing that deals with the process of modifying a speaker's identity. More particularly, the speech signal spoken by the source speaker is modified to sound as if it had been pronounced by another speaker, referred to as the target speaker. A variety of VC techniques has been proposed since the first appearance of the voice conversion problem. The choice among those techniques represents a compromise between the similarity of the converted voice to the target voice and the quality of the output speech signal,both rated by the used technique. In this paper, we review a comprehensive state-of-the-art of voice conversion techniques while pointing out their advantages and disadvantages. These techniques will be applied in significant and most versatile areas of speech technology; applications that are far beyond speech synthesis.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ezzine, Kadria; Frikha, Mondher] Sfax Univ, ATISP, ENET COM, Sfax,
   Tunisia.
   <br>[Ezzine, Kadria] Carthage Univ, ENICARTHAGE, Tunis, Tunisia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ezzine, K (reprint author), Sfax Univ, ATISP, ENET COM, Sfax, Tunisia.; Ezzine, K (reprint author), Carthage Univ, ENICARTHAGE, Tunis, Tunisia.</td>
</tr>

<tr>
<td valign="top">EM </td><td>kadria.ezzine@gmail.com; mondher_frikha05@yahoo.fr</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>361</td>
</tr>

<tr>
<td valign="top">EP </td><td>366</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000427293600064</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ye, YH
   <br>Lawlor, B</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ye, Yuhang
   <br>Lawlor, Bob</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Conversion based on Continuous Frequency Warping and Magnitude
   Scaling</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 28TH IRISH SIGNALS AND SYSTEMS CONFERENCE (ISSC)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>28th Irish Signals and Systems Conference (ISSC)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUN 20-21, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Killarney, IRELAND</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice Conversion; Analysis by Synthesis framework; Clustering;
   Freqeuency Warping; Regression</td>
</tr>

<tr>
<td valign="top">ID </td><td>REGRESSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>in this paper, we present a novel spectrum mapping method - Continuous Frequency Warping and Magnitude Scaling (CFWMS) for voice conversion under the Joint Density Gaussian Mixture Model (JDGMM) framework. JDGMM is a mature clustering technique that models the joint probability density of speech signals from paired speakers. The conventional JDGMM-based approaches morph the spectral features via least square optimization. However, the speech quality is degenerated as the converted features are blurred by statistical smoothing and the uncorrelated conversion functions between adjacent frames cause noticeable distortion. To this end, CFWMS proposes a twofold frame-level conversion method - Frequency Warping and Magnitude Scaling (FWMS). FWMS directly operates on signals in the frequency domain without statistical smoothing. Moreover, a trajectory limitation strategy is introduced to renovate the discontinuities between adjacent frames. Note that the proposed solution does not require global information of sentences, making it feasible for low latency (e.g. real-time) applications. The experimental results show significantly improvements in terms of the speech quality and the perceptual identity.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ye, Yuhang] Athlone Inst Technol, Athlone, Co Westmeath, Ireland.
   <br>[Lawlor, Bob] Maynooth Univ, Maynooth, Kildare, Ireland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ye, YH (reprint author), Athlone Inst Technol, Athlone, Co Westmeath, Ireland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>yye@research.ait.ie; bob.lawlor@nuim.ie</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000427034300002</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Rajan, BK
   <br>Anjitha, V</td>
</tr>

<tr>
<td valign="top">AF </td><td>Rajan, Bindhu K.
   <br>Anjitha, V</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Braille Code Conversion to Voice in Malayalam</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 INTERNATIONAL CONFERENCE ON COMMUNICATION AND SIGNAL PROCESSING
   (ICCSP)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Communication and Signal Processing
   (ICCSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 06-08, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Melmaruvathur, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Braille Character Recognition; extraction; filtering; segmentation; text
   enhancement; translation</td>
</tr>

<tr>
<td valign="top">AB </td><td>Braille Coding System is a method that is widely used by visually impaired people to read and write. Braille Code generally consists of cells of raised dots arranged in a grid to inscribe characters on paper. Blind People can sense the presence and absence of dots using their fingertips, giving them the code for symbol. Here a method is proposed for converting Braille codes to Malayalam voice message implemented using MATLAB which can be read out to many through the computer. In this paper Braille code is extracted from input image and it is mapped to the Malayalam database and spoken out.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Rajan, Bindhu K.; Anjitha, V] Jyothi Engn Coll, Jyothi Hills, Trichur
   67953, Kerala, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Rajan, BK (reprint author), Jyothi Engn Coll, Jyothi Hills, Trichur 67953, Kerala, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>bindhukrajan09@gmail.com; anjitha.gkrishnan@gmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>710</td>
</tr>

<tr>
<td valign="top">EP </td><td>714</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000426990500101</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Shivakumar, KM
   <br>Jain, VV
   <br>Priya, PK</td>
</tr>

<tr>
<td valign="top">AF </td><td>Shivakumar, K. M.
   <br>Jain, Varsha V.
   <br>Priya, Krishna P.</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>A study on impact of Language Model in improving the accuracy of Speech
   to Text Conversion System</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 INTERNATIONAL CONFERENCE ON COMMUNICATION AND SIGNAL PROCESSING
   (ICCSP)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Communication and Signal Processing
   (ICCSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 06-08, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Melmaruvathur, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speech recognition; CMUSphinx; Language model; speech corpus; acoustic
   model</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Speech to text conversion comprises of small, medium and large vocabulary conversions. such systems process or accepts the voice which then gets converted to their respective text. This paper gives a comparative analysis of the technologies used in small, medium, and large vocabulary Speech Recognition System. The comparative study determines the benefits and liabilities of all the approaches so far. The experiment shows the role of language model in improving the accuracy of speech to text conversion system. We experimented the speech data with noisy sentences and incomplete words. The results shows a prominent result for randomly chosen sentences compared to sequential set of sentences.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Shivakumar, K. M.; Jain, Varsha V.; Priya, Krishna P.] Amrita Univ,
   Amrita Vishwa Vidyapeetham, Dept Comp Sci, Mysuru Campus, Mysuru,
   Karnataka, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Shivakumar, KM (reprint author), Amrita Univ, Amrita Vishwa Vidyapeetham, Dept Comp Sci, Mysuru Campus, Mysuru, Karnataka, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>shivadvg19@yahoo.com; varshajainp@gmail.com; krishna.twelf@gmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>1148</td>
</tr>

<tr>
<td valign="top">EP </td><td>1151</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000426990500164</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ra, R
   <br>Aihara, R
   <br>Takiguchi, T
   <br>Ariki, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ra, Rina
   <br>Aihara, Ryo
   <br>Takiguchi, Tesuya
   <br>Ariki, Yasuo</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Visual-to-Speech Conversion Based on Maximum Likelihood Estimation</td>
</tr>

<tr>
<td valign="top">SO </td><td>PROCEEDINGS OF THE FIFTEENTH IAPR INTERNATIONAL CONFERENCE ON MACHINE
   VISION APPLICATIONS - MVA2017</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>15th IAPR International Conference on Machine Vision Applications (MVA)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 08-12, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Nagoya Univ, Nagoya, JAPAN</td>
</tr>

<tr>
<td valign="top">HO </td><td>Nagoya Univ</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper proposes a visual-to-speech conversion method that converts voiceless lip movements into voiced utterances without recognizing text information. Inspired by a Gaussian Mixture Model (GMM)-based voice conversion method, GMM is estimated from jointed visual and audio features and input visual features are converted to audio features using maximum likelihood estimation. In order to capture lip movements whose frame rate data is smaller than the audio data, we construct long-term image features. The proposed method has been evaluated using large-vocabulary continuous speech and experimental results show that our proposed method effectively estimates spectral envelopes and fundamental frequencies of audio speech from voiceless lip movements.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ra, Rina; Aihara, Ryo; Takiguchi, Tesuya; Ariki, Yasuo] Kobe Univ, Grad
   Sch Syst Informat, Nada Ku, 1-1 Rokkodai, Kobe, Hyogo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ra, R (reprint author), Kobe Univ, Grad Sch Syst Informat, Nada Ku, 1-1 Rokkodai, Kobe, Hyogo, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>rinara@me.cs.scitec.kobe-u.ac.jp; aihara@me.cs.scitec.kobe-u.ac.jp;
   takigu@kobe-u.ac.jp; ariki@kobe-u.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>518</td>
</tr>

<tr>
<td valign="top">EP </td><td>521</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000426950300126</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ahangar, M
   <br>Ghorbandoost, M
   <br>Sharma, S
   <br>Smith, MJT</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ahangar, Mohsen
   <br>Ghorbandoost, Mostafa
   <br>Sharma, Sudhendu
   <br>Smith, Mark J. T.</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>VOICE CONVERSION BASED ON A MIXTURE DENSITY NETWORK</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 IEEE WORKSHOP ON APPLICATIONS OF SIGNAL PROCESSING TO AUDIO AND
   ACOUSTICS (WASPAA)</td>
</tr>

<tr>
<td valign="top">SE </td><td>IEEE Workshop on Applications of Signal Processing to Audio and
   Acoustics</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE Workshop on Applications of Signal Processing to Audio and
   Acoustics (WASPAA)</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 15-18, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>New Paltz, NY</td>
</tr>

<tr>
<td valign="top">DE </td><td>Mixture density network; voice conversion</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a new voice conversion (VC) algorithm based on a Mixture Density Network (MDN). MDN is the combination of a Gaussian Mixture Model (GMM) and an Artificial Neural Network (ANN), where the parameters of the GMM are estimated by using the ANN method instead of the Expectation Maximization (EM) algorithm. This characteristic helps the MDN estimate GMM parameters more accurately, which results in lower distortion in the converted speech. To apply the MDN to VC, we combine the MDN with Maximum Likelihood Estimation, employing a Global Variance modification (MLE-GV) method. Objective results show better performance for the proposed MDN method compared with MLE and Joint Density GMM (JDGMM) methods. Subjective experiments demonstrate that the proposed method outperforms the MLE-GV and JDGMM-GV in terms of speech quality and speaker individuality.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ahangar, Mohsen; Sharma, Sudhendu; Smith, Mark J. T.] Purdue Univ, Sch
   Elect &amp; Comp Engn, 465 Northwestern Ave, W Lafayette, IN 47907 USA.
   <br>[Ghorbandoost, Mostafa] Amirkabir Univ Technol, Sch Elect Engn, POB
   15875-4413,424 Hafez Ave, Tehran, Iran.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ahangar, M (reprint author), Purdue Univ, Sch Elect &amp; Comp Engn, 465 Northwestern Ave, W Lafayette, IN 47907 USA.</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>329</td>
</tr>

<tr>
<td valign="top">EP </td><td>333</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000426939000067</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Oyamada, K
   <br>Kameoka, H
   <br>Kaneko, T
   <br>Ando, H
   <br>Hiramatsu, K
   <br>Kashino, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Oyamada, Keisuke
   <br>Kameoka, Hirokazu
   <br>Kaneko, Takuhiro
   <br>Ando, Hiroyasu
   <br>Hiramatsu, Kaoru
   <br>Kashino, Kunio</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Non-native speech conversion with consistency-aware recursive network
   and generative adversarial network</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 ASIA-PACIFIC SIGNAL AND INFORMATION PROCESSING ASSOCIATION ANNUAL
   SUMMIT AND CONFERENCE (APSIPA ASC 2017)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Asia-Pacific Signal and Information Processing Association Annual Summit
   and Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th Annual Summit and Conference of the
   Asia-Pacific-Signal-and-Information-Processing-Association (APSIPA ASC)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 12-15, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Kuala Lumpur, MALAYSIA</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPARSE REPRESENTATION; NEURAL-NETWORKS; VOICE</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper deals with the problem of automatically correcting the pronunciation of non-native speakers. Since the pronunciation characteristics of non-native speakers depend heavily on the context (such as words), conversion rules for correcting pronunciation should be learned from a sequence of features rather than a single-frame feature. For the on-line conversion of local sequences of features, we construct a neural network (NN) that takes a sequence of features as an input/output, generates a sequence of features in a segment-by-segment fashion and guarantees the consistency of the generated features within overlapped segments. Futhermore, we apply a recently proposed generative adversarial network (GAN)-based postfilter to the generated feature sequence with the aim of synthesizing natural-sounding speech. Through subjective and quantitative evaluations, we confirmed the superiority of our proposed method over a conventional NN approach in terms of conversion quality.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Oyamada, Keisuke; Ando, Hiroyasu] Univ Tsukuba, Fac Engn Informat &amp;
   Syst, Div Policy &amp; Planning Sci, Tsukuba, Ibaraki, Japan.
   <br>[Kameoka, Hirokazu; Kaneko, Takuhiro; Hiramatsu, Kaoru; Kashino, Kunio]
   NTT Corp, NTT Commun Sci Labs, Tokyo, Japan.
   <br>[Ando, Hiroyasu] Univ Tsukuba, Ctr Artificial Intelligent Res, Tsukuba,
   Ibaraki, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Oyamada, K (reprint author), Univ Tsukuba, Fac Engn Informat &amp; Syst, Div Policy &amp; Planning Sci, Tsukuba, Ibaraki, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>s1720584@s.tsukuba.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>182</td>
</tr>

<tr>
<td valign="top">EP </td><td>188</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000425879400033</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Shah, NJ
   <br>Patil, HA</td>
</tr>

<tr>
<td valign="top">AF </td><td>Shah, Nirmesh J.
   <br>Patil, Hemant A.</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>On the Convergence of INCA Algorithm</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 ASIA-PACIFIC SIGNAL AND INFORMATION PROCESSING ASSOCIATION ANNUAL
   SUMMIT AND CONFERENCE (APSIPA ASC 2017)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Asia-Pacific Signal and Information Processing Association Annual Summit
   and Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th Annual Summit and Conference of the
   Asia-Pacific-Signal-and-Information-Processing-Association (APSIPA ASC)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 12-15, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Kuala Lumpur, MALAYSIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; INCA; Mean Square Error</td>
</tr>

<tr>
<td valign="top">ID </td><td>MAXIMUM-LIKELIHOOD; VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Development of text-independent Voice Conversion (VC) has gained more research interest for last one decade. Alignment of the source and target speakers' spectral features before learning the mapping function is the challenging step for the development of the text-independent VC as both the speakers have uttered different utterances from the same or different languages. State-of-the-art alignment technique is an Iterative combination of a Nearest Neighbor search step and a Conversion step Alignment (INCA) algorithm that iteratively learns the mapping function after getting the nearest neighbor aligned feature pairs from intermediate converted spectral features and target spectral features. To the best of authors' knowledge, this algorithm was shown to converge empirically, however, its theoretical proof has not been discussed in detail in the VC literature. In this paper, we have presented that the INCA algorithm will converge monotonically to a local minimum in mean square error (MSE) sense. In addition, we also present the reason of convergence in MSE sense in the context of VC task.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Shah, Nirmesh J.; Patil, Hemant A.] DA IICT, Speech Res Lab,
   Gandhinagar 382007, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Shah, NJ (reprint author), DA IICT, Speech Res Lab, Gandhinagar 382007, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>nirmesh88_shah@daiict.ac.in; hemant_patil@daiict.ac.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>559</td>
</tr>

<tr>
<td valign="top">EP </td><td>562</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000425879400102</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Peng, YH
   <br>Hsu, CC
   <br>Wu, YC
   <br>Hwang, HT
   <br>Liu, YW
   <br>Tsao, Y
   <br>Wang, HM</td>
</tr>

<tr>
<td valign="top">AF </td><td>Peng, Yu-Huai
   <br>Hsu, Chin-Cheng
   <br>Wu, Yi-Chiao
   <br>Hwang, Hsin-Te
   <br>Liu, Yi-Wen
   <br>Tsao, Yu
   <br>Wang, Hsin-Min</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Fast Locally Linear Embedding Algorithm for Exemplar-based Voice
   Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 ASIA-PACIFIC SIGNAL AND INFORMATION PROCESSING ASSOCIATION ANNUAL
   SUMMIT AND CONFERENCE (APSIPA ASC 2017)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Asia-Pacific Signal and Information Processing Association Annual Summit
   and Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th Annual Summit and Conference of the
   Asia-Pacific-Signal-and-Information-Processing-Association (APSIPA ASC)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 12-15, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Kuala Lumpur, MALAYSIA</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH PARAMETER GENERATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>The locally linear embedding (LLE) algorithm has been proven to have high output quality and applicability for voice conversion (VC) tasks. However, the major shortcoming of the LLE-based VC approach is the time complexity (especially in the matrix inversion process) during the conversion phase. In this paper, we propose a fast version of the LLE algorithm that significantly reduces the complexity. In the proposed method, each locally linear patch on the data manifold is described by a pre-computed cluster of exemplars, and thus the major part of on-line computation can be carried out beforehand in the off-line phase. Experimental results demonstrate that the VC performance of the proposed fast LLE algorithm is comparable to that of the original LLE algorithm and that a real-time VC system becomes possible because of the highly reduced time complexity.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Peng, Yu-Huai; Liu, Yi-Wen; Wang, Hsin-Min] Natl Tsing Hua Univ,
   Hsinchu, Taiwan.
   <br>[Hsu, Chin-Cheng; Wu, Yi-Chiao; Hwang, Hsin-Te] Acad Sinica, Inst
   Informat Sci, Taipei, Taiwan.
   <br>[Tsao, Yu] Acad Sinica, Res Ctr Informat Technol Innovat, Taipei, Taiwan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Peng, YH (reprint author), Natl Tsing Hua Univ, Hsinchu, Taiwan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>roland19930601@gmail.com; ywliu@ee.nthu.edu.tw;
   yu.tsao@citi.sinica.edu.tw; whm@iis.sinica.edu.tw</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>591</td>
</tr>

<tr>
<td valign="top">EP </td><td>595</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000425879400119</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Morikawa, K
   <br>Toda, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Morikawa, Kazuho
   <br>Toda, Tomoki</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Electrolaryngeal Speech Modification towards Singing Aid System for
   Laryngectomees</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 ASIA-PACIFIC SIGNAL AND INFORMATION PROCESSING ASSOCIATION ANNUAL
   SUMMIT AND CONFERENCE (APSIPA ASC 2017)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Asia-Pacific Signal and Information Processing Association Annual Summit
   and Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th Annual Summit and Conference of the
   Asia-Pacific-Signal-and-Information-Processing-Association (APSIPA ASC)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 12-15, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Kuala Lumpur, MALAYSIA</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION; F0 CONTROL; ENHANCEMENT; NOISE</td>
</tr>

<tr>
<td valign="top">AB </td><td>Towards the development of a singing aid system for laryngectomees, we propose a method for converting electrolaryngeal (EL) speech produced by using an electrolarynx into more naturally sounding singing voices. Singing by using the electrolarynx is less flexible because the pitch of EL speech is determined by the source excitation signal mechanically produced by the electrolarynx, and therefore, it is necessary to embed melodies of songs to be sung in advance to the electrolarynx. In addition, sound quality of singing voices produced by the electrolarynx is severely degraded by an adverse effect of its mechanical excitation sounds emitted outside as noise. To address these problems, the proposed conversion method uses 1) pitch control by playing a musical instrument and 2) noise suppression. In the pitch control, pitch patterns of music sounds played simultaneously in singing with the electrolaryx are modified so that they have specific characteristics usually observed in singing voices, and then, the modified pitch patterns are used as the target pitch patterns in the conversion from EL speech into singing voices. In the noise suppression, spectral subtraction is used to suppress the leaked excitation sounds. The experimental results demonstrate that 1) naturalness of singing voices is significantly improved by the noise suppression and 2) the pitch pattern modification is not necessarily effective in the conversion from EL speech into singing voices.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Morikawa, Kazuho] Nagoya Univ, Grad Sch Informat, Nagoya, Aichi, Japan.
   <br>[Toda, Tomoki] Nagoya Univ, Informat Technol Ctr, Nagoya, Aichi, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Morikawa, K (reprint author), Nagoya Univ, Grad Sch Informat, Nagoya, Aichi, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>morikawa.kazuho@h.mbox.nagoya-u.ac.jp; tomoki@icts.nagoya-u.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>610</td>
</tr>

<tr>
<td valign="top">EP </td><td>613</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000425879400104</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wu, XL
   <br>Gao, SH
   <br>Huang, DY
   <br>Xiang, C</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wu, Xiaoling
   <br>Gao, Shuhua
   <br>Huang, Dong-Yan
   <br>Xiang, Cheng</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>&amp;ITVoichap&amp;IT: a standalone real-time voice change application on iOS
   platform</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 ASIA-PACIFIC SIGNAL AND INFORMATION PROCESSING ASSOCIATION ANNUAL
   SUMMIT AND CONFERENCE (APSIPA ASC 2017)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Asia-Pacific Signal and Information Processing Association Annual Summit
   and Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th Annual Summit and Conference of the
   Asia-Pacific-Signal-and-Information-Processing-Association (APSIPA ASC)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 12-15, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Kuala Lumpur, MALAYSIA</td>
</tr>

<tr>
<td valign="top">ID </td><td>NEURAL-NETWORKS; CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>High-quality voice mimicry is appealing to everyone. However, only few vocal geniuses are endowed with the talent for vivid mimicry. Professional mimics have to be trained and practice over many years for various vocal skills, such as vocal control, precision in pitch, sense of rhythm and personal style, etc. To help achieve our dream for fascinating voice mimicry, such as speaking in a celebrity's voice, we have developed a real-time voice conversion technology for the general users. You can specify any target (like your friend or a celebrity) for your voice conversion as long as the target's training utterances are available. To facilitate easy use, we have implemented it efficiently as a mobile application on the iOS platform, called Voichap, which can generate a desired natural target voice. Notably, the complete training and conversion process is performed locally in a reasonable time, with no need for on-line server service, to improve the user experience. Just three steps are enough to use this application: choose a target, record your voice and then have fun listening to your converted voice.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Wu, Xiaoling; Gao, Shuhua; Xiang, Cheng] Natl Univ Singapore, Dept
   Elect &amp; Comp Engn, Singapore, Singapore.
   <br>[Huang, Dong-Yan] ASTAR, Inst Infocomm Res, Human Language Technol Dept,
   Singapore, Singapore.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Wu, XL (reprint author), Natl Univ Singapore, Dept Elect &amp; Comp Engn, Singapore, Singapore.</td>
</tr>

<tr>
<td valign="top">EM </td><td>a0115281@u.nus.edu; shuhua_gao@u.nus.edu; huang@i2r.a-star.edu.sg;
   elexc@nus.edu.sg</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>728</td>
</tr>

<tr>
<td valign="top">EP </td><td>732</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000425879400136</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sriskandaraja, K
   <br>Suthokumar, G
   <br>Sethu, V
   <br>Ambikairajah, E</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sriskandaraja, Kaavya
   <br>Suthokumar, Gajan
   <br>Sethu, Vidhyasaharan
   <br>Ambikairajah, Eliathamby</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Investigating the use of Scattering Coefficients for Replay Attack
   Detection</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 ASIA-PACIFIC SIGNAL AND INFORMATION PROCESSING ASSOCIATION ANNUAL
   SUMMIT AND CONFERENCE (APSIPA ASC 2017)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Asia-Pacific Signal and Information Processing Association Annual Summit
   and Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th Annual Summit and Conference of the
   Asia-Pacific-Signal-and-Information-Processing-Association (APSIPA ASC)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 12-15, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Kuala Lumpur, MALAYSIA</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEAKER VERIFICATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Widespread adoption of speaker verification for security relies on the existence of effective anti-spoofing countermeasures. This paper presents a countermeasure based on spectral features to detect replay spoofing attacks on automatic speaker verification systems. In particular, the use of hierarchical scattering decomposition coefficients and inversemel frequency cepstral coefficients are explored. Our best system achieved a relative improvement of around 70% in terms of equal error rate on the development set and 20% on the evaluation set, when compared to the baseline on the ASVspoof 2017 database. In addition, we show that features with a shorter window can be beneficial to detecting replayed speech, in contrast to speech synthesis and voice conversion attack.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Sriskandaraja, Kaavya; Suthokumar, Gajan; Sethu, Vidhyasaharan;
   Ambikairajah, Eliathamby] UNSW, Sch Elect Engn &amp; Telecommun, Sydney,
   NSW, Australia.
   <br>[Sriskandaraja, Kaavya; Suthokumar, Gajan; Ambikairajah, Eliathamby]
   CSIRO, DATA61, Sydney, NSW, Australia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sriskandaraja, K (reprint author), UNSW, Sch Elect Engn &amp; Telecommun, Sydney, NSW, Australia.; Sriskandaraja, K (reprint author), CSIRO, DATA61, Sydney, NSW, Australia.</td>
</tr>

<tr>
<td valign="top">EM </td><td>k.sriskandaraja@unsw.edu.au; g.suthokumar@unsw.edu.au;
   v.sethu@unsw.edu.au; e.ambikairajah@unsw.edu.au</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>1195</td>
</tr>

<tr>
<td valign="top">EP </td><td>1198</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000425879400216</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kotani, G
   <br>Saito, D
   <br>Minematsu, N</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kotani, Gaku
   <br>Saito, Daisuke
   <br>Minematsu, Nobuaki</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice conversion based on deep neural networks for time-variant linear
   transformations</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 ASIA-PACIFIC SIGNAL AND INFORMATION PROCESSING ASSOCIATION ANNUAL
   SUMMIT AND CONFERENCE (APSIPA ASC 2017)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Asia-Pacific Signal and Information Processing Association Annual Summit
   and Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th Annual Summit and Conference of the
   Asia-Pacific-Signal-and-Information-Processing-Association (APSIPA ASC)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 12-15, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Kuala Lumpur, MALAYSIA</td>
</tr>

<tr>
<td valign="top">AB </td><td>In voice conversion, deep neural networks are now being used as conversion models that map source features to target features. In this framework, it generally needs a larger amount of data to train more accurate conversion models. This condition, however, will reduce usability of voice conversion because a text-to-speech synthesizer can be built when a large amount of training data are available. We argue that we should take advantage of top-down knowledge that we have instead of preparing a large amount of data. This paper proposes a novel architecture using deep neural networks which can achieve superior performance of voice conversion. Our proposal is a network-based conversion that realizes only linear conversion but in even a time-variant way. Experiments show that naturalness improvement was observed in subjective assessments. It is considered that linear constraints at each time step prevent trained models from converting input features to unrealistic features.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kotani, Gaku; Saito, Daisuke; Minematsu, Nobuaki] Univ Tokyo, Grad Sch
   Engn, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kotani, G (reprint author), Univ Tokyo, Grad Sch Engn, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>kotani@gavo.t.u-tokyo.ac.jp; dsk_saito@gavo.t.u-tokyo.ac.jp;
   mine@gavo.t.u-tokyo.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>1218</td>
</tr>

<tr>
<td valign="top">EP </td><td>1221</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000425879400221</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Li, M
   <br>Wang, LT
   <br>Xu, ZC
   <br>Cai, DW</td>
</tr>

<tr>
<td valign="top">AF </td><td>Li, Ming
   <br>Wang, Luting
   <br>Xu, Zhicheng
   <br>Cai, Danwei</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Mandarin Electrolaryngeal Voice Conversion with Combination of Gaussian
   Mixture Model and Non-negative Matrix Factorization</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 ASIA-PACIFIC SIGNAL AND INFORMATION PROCESSING ASSOCIATION ANNUAL
   SUMMIT AND CONFERENCE (APSIPA ASC 2017)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Asia-Pacific Signal and Information Processing Association Annual Summit
   and Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th Annual Summit and Conference of the
   Asia-Pacific-Signal-and-Information-Processing-Association (APSIPA ASC)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 12-15, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Kuala Lumpur, MALAYSIA</td>
</tr>

<tr>
<td valign="top">ID </td><td>ENHANCEMENT</td>
</tr>

<tr>
<td valign="top">AB </td><td>Electrolarynx (EL) is a speaking-aid device that helps laryngectomees who have their larynx removed to generate voice. However, the voice generated by EL is unnatural and unintelligible due to its flat pitch and strong vibration noise. Targeting these challenges, previous works show that the elec-trolaryngeal speech can be enhanced using Gaussian Mixture Model (GMM) based voice conversion (VC). Although effective in improving the naturalness, it degrades the intelligibility of the converted speech. To address this issue, we propose a hybrid approach using both Non-negative Matrix Factorization (NMF) and GMM methods. For better intelligibility, we apply the NMF to estimate the high quality spectral features. For better naturalness, we use the GMM with dynamic trajectory constraint to recover a smoothed F-0. Additionally, to suppress the EL vibration noise, we include the 0th MCC coefficient in the GMM-based VC. The proposed method significantly increases the F-0 dynamic range, reduces vibration noise, and improves both speech naturalness and intelligibility. One hundred pairs of the normal and electrolaryngeal speech in daily mandarin are recorded as our evaluation data. Experimental results show that our proposed hybrid method reduces the mel-cepstral distortion by(1)7.1 dB and increases the F-0 correlation coefficient to 0.54.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Li, Ming; Wang, Luting; Xu, Zhicheng; Cai, Danwei] Sun Yat Sen Univ,
   SYSU CMU Joint Inst Engn, Sch Elect &amp; Informat Technol, Guangzhou,
   Guangdong, Peoples R China.
   <br>[Li, Ming; Cai, Danwei] SYSU CMU Shunde Int Joint Res Inst, Shunde,
   Guangdong, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Li, M (reprint author), Sun Yat Sen Univ, SYSU CMU Joint Inst Engn, Sch Elect &amp; Informat Technol, Guangzhou, Guangdong, Peoples R China.; Li, M (reprint author), SYSU CMU Shunde Int Joint Res Inst, Shunde, Guangdong, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>liming46@mail.sysu.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>1360</td>
</tr>

<tr>
<td valign="top">EP </td><td>1363</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000425879400249</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kubo, K
   <br>Kobayashi, K
   <br>Toda, T
   <br>Neubig, G
   <br>Sakti, S
   <br>Nakamura, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kubo, Kazutaka
   <br>Kobayashi, Kazuhiro
   <br>Toda, Tomoki
   <br>Neubig, Graham
   <br>Sakti, Sakriani
   <br>Nakamura, Satoshi</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>An Investigation of How to Design Control Parameters for Statistical
   Voice Timbre Control</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 ASIA-PACIFIC SIGNAL AND INFORMATION PROCESSING ASSOCIATION ANNUAL
   SUMMIT AND CONFERENCE (APSIPA ASC 2017)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Asia-Pacific Signal and Information Processing Association Annual Summit
   and Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th Annual Summit and Conference of the
   Asia-Pacific-Signal-and-Information-Processing-Association (APSIPA ASC)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 12-15, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Kuala Lumpur, MALAYSIA</td>
</tr>

<tr>
<td valign="top">ID </td><td>CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Multiple-regression Gaussian mixture models (MR-GMM) allow for control of voice timbre along several axes each described by a voice timbre expression word. To create these axes, perceptual scores corresponding to multiple voice timbre expression words are manually assigned to individual pre-stored target speakers as the voice timbre control parameters, and then acoustic basis vectors corresponding to the individual control parameters are learned. The voice timbre expression words are usually selected from various words using factor analysis so that the voice timbre control parameters are independent of each other. However., the resulting basis vectors are not often orthogonal to each other, and they practically cause difficulties in intuitively controlling the converted voice timbre. Towards the development of the MR-GMM capable of intuitively controlling converted voice timbre, we investigate how to design the voice timbre control parameters so that not only the voice timbre control parameters but also the corresponding acoustic basis vectors are independent of each other. Experimental results demonstrate that 1) a method for annotation of the voice timbre control parameters using the converted voices rather than natural voices is effective, and 2) the independences of the voice timbre control parameters and acoustic basis vectors is helpful for improving the converted voice timbre controllability of the MR-GMM.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kubo, Kazutaka; Sakti, Sakriani; Nakamura, Satoshi] Nara Inst Sci &amp;
   Technol NAIST, Nara, Japan.
   <br>[Kobayashi, Kazuhiro; Toda, Tomoki] Nagoya Univ, Ctr Informat Technol,
   Nagoya, Aichi, Japan.
   <br>[Neubig, Graham] Carnegie Mellon Univ, Language Technol Inst,
   Pittsburgh, PA 15213 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kobayashi, K (reprint author), Nagoya Univ, Ctr Informat Technol, Nagoya, Aichi, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>kobayashi.kazuhiro@g.sp.m.is.nagoya-u.ac.jp; tomoki@icts.nagoya-u.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>1567</td>
</tr>

<tr>
<td valign="top">EP </td><td>1570</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000425879400287</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Shah, NJ
   <br>Bachhav, PB
   <br>Patil, HA</td>
</tr>

<tr>
<td valign="top">AF </td><td>Shah, Nirmesh J.
   <br>Bachhav, Pramod B.
   <br>Patil, Hemant A.</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Novel Filtering-based F-0 Estimation Algorithm with an Application to
   Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 ASIA-PACIFIC SIGNAL AND INFORMATION PROCESSING ASSOCIATION ANNUAL
   SUMMIT AND CONFERENCE (APSIPA ASC 2017)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Asia-Pacific Signal and Information Processing Association Annual Summit
   and Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th Annual Summit and Conference of the
   Asia-Pacific-Signal-and-Information-Processing-Association (APSIPA ASC)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 12-15, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Kuala Lumpur, MALAYSIA</td>
</tr>

<tr>
<td valign="top">AB </td><td>We propose a novel F-0 estimation algorithm that initially estimates the glottal closure instants (GCIs) or pitch and then computes the corresponding fundamental frequency (F0). The proposed method eliminates the assumption that F-0 is constant over a segment of short duration (i.e., 20-30 ms). We use our previously proposed novel filtering-based approach for GCI estimation. As the proposed method directly operates on the entire speech signal, it does not require to set the window length and thus, it is free from the problem of spectral leakage. Measuring the effectiveness of proposed F-0 estimation algorithm w.r.t. three state-of-the-art methods, namely, Yet Another Algorithm for Pitch tracking (YAAPT), Speech Transformation and Representation using Adaptive Interpolation of weiGHT spectrum (STRAIGHT) and Pitch Detection Algorithm (PDA), is challenging in the absence of the ground truth. Since accurate estimation of F-0 will impact the quality of converted voice in Voice Conversion (VC) task. Hence, we measure the effectiveness F-0 estimation algorithm in the application of VC task. The quality and speaker similarity of the converted voice have been evaluated using two subjective measures, namely, Mean Opinion Scores (MOS) and ABX test, respectively.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Shah, Nirmesh J.; Patil, Hemant A.] DA IICT, Speech Res Lab,
   Gandhinagar 382007, India.
   <br>[Bachhav, Pramod B.] EURECOM, Biot, France.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Shah, NJ (reprint author), DA IICT, Speech Res Lab, Gandhinagar 382007, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>nirmesh88_shah@daiict.ac.in; pramod.bachhav@eurecom.fr;
   hemant_patil@daiict.ac.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>1579</td>
</tr>

<tr>
<td valign="top">EP </td><td>1582</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000425879400290</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sisman, B
   <br>Li, HZ
   <br>Tan, KC</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sisman, Berrak
   <br>Li, Haizhou
   <br>Tan, Kay Chen</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Transformation of Prosody in Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 ASIA-PACIFIC SIGNAL AND INFORMATION PROCESSING ASSOCIATION ANNUAL
   SUMMIT AND CONFERENCE (APSIPA ASC 2017)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Asia-Pacific Signal and Information Processing Association Annual Summit
   and Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th Annual Summit and Conference of the
   Asia-Pacific-Signal-and-Information-Processing-Association (APSIPA ASC)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 12-15, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Kuala Lumpur, MALAYSIA</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice Conversion (VC) aims to convert one's voice to sound like that of another. So far, most of the voice conversion frameworks mainly focus only on the conversion of spectrum. We note that speaker identity is also characterized by the prosody features such as fundamental frequency (F0), energy contour and duration. Motivated by this, we propose a framework that can perform F0, energy contour and duration conversion. In the traditional exemplar-based sparse representation approach to voice conversion, a general source-target dictionary of exemplars is constructed to establish the correspondence between source and target speakers. In this work, we propose a Phonetically Aware Sparse Representation of fundamental frequency and energy contour by using Continuous Wavelet Transform (CWT). Our idea is motivated by the facts that CWT decompositions of F0 and energy contours describe prosody patterns in different temporal scales and allow for effective prosody manipulation in speech synthesis. Furthermore, phonetically aware exemplars lead to better estimation of activation matrix, therefore, possibly better conversion of prosody. We also propose a phonetically aware duration conversion framework which takes into account both phone-level and sentence-level speaking rates. We report that the proposed prosody conversion outperforms the traditional prosody conversion techniques in both objective and subjective evaluations.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Sisman, Berrak; Li, Haizhou] Natl Univ Singapore, Singapore, Singapore.
   <br>[Tan, Kay Chen] City Univ Hong Kong, Hong Kong, Hong Kong, Peoples R
   China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sisman, B (reprint author), Natl Univ Singapore, Singapore, Singapore.</td>
</tr>

<tr>
<td valign="top">EM </td><td>berraksisman@u.nus.edu; haizhou.li@nus.edu.sg; kaytan@cityu.edu.hk</td>
</tr>

<tr xmlns:date="http://exslt.org/dates-and-times">
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Li, Haizhou</display_name>&nbsp;</font></td><td><font size="3">Q-6438-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>1588</td>
</tr>

<tr>
<td valign="top">EP </td><td>1597</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000425879400292</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Vijayan, K
   <br>Dong, MH
   <br>Li, HZ</td>
</tr>

<tr>
<td valign="top">AF </td><td>Vijayan, Karthika
   <br>Dong, Minghui
   <br>Li, Haizhou</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Dual Alignment Scheme for Improved Speech-to-Singing Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 ASIA-PACIFIC SIGNAL AND INFORMATION PROCESSING ASSOCIATION ANNUAL
   SUMMIT AND CONFERENCE (APSIPA ASC 2017)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Asia-Pacific Signal and Information Processing Association Annual Summit
   and Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th Annual Summit and Conference of the
   Asia-Pacific-Signal-and-Information-Processing-Association (APSIPA ASC)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 12-15, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Kuala Lumpur, MALAYSIA</td>
</tr>

<tr>
<td valign="top">ID </td><td>LIKELIHOOD; FREQUENCY</td>
</tr>

<tr>
<td valign="top">AB </td><td>In speech-to-singing (STS) voice conversion, the source speech signals from a speaker are used to generate his/her singing voice. Such a process requires accurate detection of boundaries between phonemes and words in the speech signal. The computation and modification of analysis parameters of speech signals with respect to the target musical scores or singing templates, largely depend upon estimation of phoneme durations. In this paper, an improved dual alignment scheme for speech and singing voices in template-based STS (TSTS) systems is proposed. The subsequence dynamic time warping (subDTW) is employed to match source speech to singer's speech in the first pass of dual alignment. We assume that an accurate correspondence between singer's speech and target singing vocals has been established as part of the singing template development. Therefore, once the source speech is aligned with the singer's speech, it is automatically aligned with singing template, that we call the second pass of dual alignment. The proposed scheme delivers a relative reduction of 95.8% in word alignment error, over the baseline dynamic time warping (DTW) approach. Also, it provides a relative improvement of 38.7% in mean opinion scores of synthesized singing voices in subjective studies, over the same baseline. We demonstrate that the proposed dual alignment with the subDTW is effective in STS conversion applications.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Vijayan, Karthika; Li, Haizhou] Natl Univ Singapore, Dept Elect &amp; Comp
   Engn, Singapore, Singapore.
   <br>[Dong, Minghui; Li, Haizhou] ASTAR, Inst Infocomm Res, Human Language
   Technol Dept, Singapore, Singapore.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Vijayan, K (reprint author), Natl Univ Singapore, Dept Elect &amp; Comp Engn, Singapore, Singapore.</td>
</tr>

<tr>
<td valign="top">EM </td><td>vijayan.karthika@nus.edu.sg; mhdong@i2r.a-star.edu.sg;
   hli@i2r.a-star.edu.sg</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Li, Haizhou</display_name>&nbsp;</font></td><td><font size="3">Q-6438-2019&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Vijayan, Karthika</display_name>&nbsp;</font></td><td><font size="3">Y-4001-2018&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Vijayan, Karthika</display_name>&nbsp;</font></td><td><font size="3">0000-0001-7281-1329&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>1598</td>
</tr>

<tr>
<td valign="top">EP </td><td>1606</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000425879400293</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Meng, L
   <br>Nguyen, QH
   <br>Tian, XH
   <br>Shen, ZQ
   <br>Chng, ES
   <br>Guan, FYQ
   <br>Miao, CY
   <br>Leung, C</td>
</tr>

<tr>
<td valign="top">AF </td><td>Meng, Lei
   <br>Quy Hy Nguyen
   <br>Tian, Xiaohai
   <br>Shen, Zhiqi
   <br>Chng, Eng Siong
   <br>Guan, Frank Yunqing
   <br>Miao, Chunyan
   <br>Leung, Cyril</td>
</tr>

<tr>
<td valign="top">GP </td><td>ACM</td>
</tr>

<tr>
<td valign="top">TI </td><td>Towards Age-friendly E-commerce Through Crowd-Improved Speech
   Recognition, Multimodal Search, and Personalized Speech Feedback</td>
</tr>

<tr>
<td valign="top">SO </td><td>PROCEEDINGS OF 2017 2ND INTERNATIONAL CONFERENCE ON CROWD SCIENCE AND
   ENGINEERING ICCSE 2017</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>2nd International Conference on Crowd Science and Engineering (ICCSE) -
   Smart and Crowd</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUL 06-09, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Tsinghua Univ, Beijing, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">HO </td><td>Tsinghua Univ</td>
</tr>

<tr>
<td valign="top">DE </td><td>Age-friendly E-commerce; enhanced user browsing; Crowd-improved speech
   recognition; Multimodal search; Personalized speech feedback</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents an age-friendly system for improving the elderly's online shopping experience. Different from most related studies focusing on website design and content organization, we propose to integrate three assistive techniques to facilitate the elderly's browsing of products in E-commerce platforms, including the crowd-improved speech recognition, the multimodal search, and the personalized speech feedback. The first two techniques, namely, the crowd-improved speech recognition and the multimodal search, work together to allow the elderly search for desired products flexibly using either speech, an image, text, or any combination of them whichever are convenient for the elderly. The personalized speech feedback provides a speech summary of search result in a personalized voice. That is, the elderly are allowed to choose or even create their desired voices, and also can customize the voices in terms of pitch, speaking speed, and loudness. As a whole, the proposed system is expected to help and engage the elderly's E-commerce adoption. Testing on real-world E-commerce product datasets demonstrated the usability of the proposed system.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Meng, Lei; Quy Hy Nguyen; Tian, Xiaohai; Shen, Zhiqi; Guan, Frank
   Yunqing; Miao, Chunyan; Leung, Cyril] Nanyang Technol Univ, LILY Res
   Ctr, Singapore, Singapore.
   <br>[Chng, Eng Siong] Nanyang Technol Univ, Sch Comp Sci &amp; Engn, Singapore,
   Singapore.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Meng, L (reprint author), Nanyang Technol Univ, LILY Res Ctr, Singapore, Singapore.</td>
</tr>

<tr>
<td valign="top">EM </td><td>lmeng@ntu.edu.sg; Nguyen.QH@ntu.edu.sg; XHTian@ntu.edu.sg;
   ZQShen@ntu.edu.sg; aseschng@ntu.edu.sg; yunqing.guan@ntu.edu.sg;
   ASCYMiao@ntu.edu.sg; CLeung@ntu.edu.sg</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>127</td>
</tr>

<tr>
<td valign="top">EP </td><td>135</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1145/3126973.3129306</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000426574900022</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sisman, B
   <br>Li, HZ
   <br>Tan, KC</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sisman, Berrak
   <br>Li, Haizhou
   <br>Tan, Kay Chen</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>SPARSE REPRESENTATION OF PHONETIC FEATURES FOR VOICE CONVERSION WITH AND
   WITHOUT PARALLEL DATA</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 IEEE AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING WORKSHOP (ASRU)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 16-20, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Okinawa, JAPAN</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; sparse representation; phonetic exemplars; Phonetic
   PosteriorGrams</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a voice conversion framework that uses phonetic information in an exemplar-based voice conversion approach. The proposed idea is motivated by the fact that phone-dependent exemplars lead to better estimation of activation matrix, therefore, possibly better conversion. We propose to use the phone segmentation results from automatic speech recognition (ASR) to construct a sub-dictionary for each phone. The proposed framework can work with or without parallel training data. With parallel training data, we found that phonetic sub-dictionary outperforms the state-of-the-art baseline in objective and subjective evaluations. Without parallel training data, we use Phonetic PosteriorGrams (PPGs) as the speaker-independent exemplars in the phonetic sub-dictionary to serve as a bridge between speakers. We report that such technique achieves a competitive performance without the need of parallel training data.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Sisman, Berrak; Li, Haizhou] Natl Univ Singapore, Singapore, Singapore.
   <br>[Tan, Kay Chen] City Univ Hong Kong, Hong Kong, Hong Kong, Peoples R
   China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sisman, B (reprint author), Natl Univ Singapore, Singapore, Singapore.</td>
</tr>

<tr>
<td valign="top">EM </td><td>berraksisman@u.nus.edu; haizhou.li@nus.edu.sg; kaytan@cityu.edu.hk</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Li, Haizhou</display_name>&nbsp;</font></td><td><font size="3">Q-6438-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>677</td>
</tr>

<tr>
<td valign="top">EP </td><td>684</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000426066100094</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Vythelingum, K
   <br>Esteve, Y
   <br>Rosec, O</td>
</tr>

<tr>
<td valign="top">AF </td><td>Vythelingum, Kevin
   <br>Esteve, Yannick
   <br>Rosec, Olivier</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>ERROR DETECTION OF GRAPHEME-TO-PHONEME CONVERSION IN TEXT-TO-SPEECH
   SYNTHESIS USING SPEECH SIGNAL AND LEXICAL CONTEXT</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 IEEE AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING WORKSHOP (ASRU)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 16-20, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Okinawa, JAPAN</td>
</tr>

<tr>
<td valign="top">DE </td><td>automatic error detection; graphemeto-phoneme conversion; forced
   alignment; sequence-tosequence neural networks; speech synthesis</td>
</tr>

<tr>
<td valign="top">AB </td><td>In unit selection text-to-speech synthesis, voice creation involved a phonemic transcription of read speech. This is produced by an automatic grapheme-to-phoneme conversion of the text read, followed by a manual correction. Although grapheme-to-phoneme conversion makes few errors, the manual correction is time consuming as every generated phoneme should be checked. We propose a method to automatically detect grapheme-to-phoneme conversion errors by comparing contrastives phonemisation hypothesis. A lattice-based forced alignment system is implemented, allowing for signaldependent phonemisation. We implement also a sequence-tosequence neural network model to obtain a context-dependent grapheme-to-phoneme conversion. On a French dataset, we show that we can detect to 86.3% of the errors made by a commercial grapheme-to-phoneme system. Moreover, the amount of data annotated as erroneous is kept under 10% of the total evaluation data. The time spent for phoneme manual checking can thus been drastically reduced without decreasing significantly the phonemic transcription quality.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Vythelingum, Kevin; Rosec, Olivier] Voxygen, Pleumeur Bodou, France.
   <br>[Vythelingum, Kevin; Esteve, Yannick] Le Mans Univ, LIUM, Le Mans,
   France.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Vythelingum, K (reprint author), Voxygen, Pleumeur Bodou, France.; Vythelingum, K (reprint author), Le Mans Univ, LIUM, Le Mans, France.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>692</td>
</tr>

<tr>
<td valign="top">EP </td><td>697</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000426066100096</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nan, E
   <br>Radosavac, U
   <br>Papp, I
   <br>Antic, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nan, Eleonora
   <br>Radosavac, Una
   <br>Papp, Istvan
   <br>Antic, Marija</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Architecture of Voice Control Module for Smart Home Automation Cloud</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 IEEE 7TH INTERNATIONAL CONFERENCE ON CONSUMER ELECTRONICS - BERLIN
   (ICCE-BERLIN)</td>
</tr>

<tr>
<td valign="top">SE </td><td>IEEE International Conference on Consumer Electronics-Berlin</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE 7th International Conference on Consumer Electronics - Berlin
   (ICCE-Berlin)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 03-06, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Berlin, GERMANY</td>
</tr>

<tr>
<td valign="top">DE </td><td>home automation cloud; parser generator; voice control; ANTLR</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents one implementation of the voice control module within the smart home automation cloud. The implemented voice control module can process the textual result of arbitrary speech-to-text conversion engine, to detect patterns corresponding to the device names and desired actions. Based on the detected command semantics, control messages for smart home automation cloud are generated, which result in the desired action within the home automation system. The proposed voice control module is implemented within the existing home automation cloud, and its performance is tested.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nan, Eleonora; Radosavac, Una; Papp, Istvan] Univ Novi Sad, Fac Tech
   Sci, Novi Sad, Serbia.
   <br>[Antic, Marija] RT RK Inst Comp Based Syst, Novi Sad, Serbia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nan, E (reprint author), Univ Novi Sad, Fac Tech Sci, Novi Sad, Serbia.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>97</td>
</tr>

<tr>
<td valign="top">EP </td><td>98</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000425845400030</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Lenarczyk, M
   <br>Janicki, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Lenarczyk, Michal
   <br>Janicki, Artur</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice conversion with pitch alteration using phase vocoder</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 SIGNAL PROCESSING: ALGORITHMS, ARCHITECTURES, ARRANGEMENTS, AND
   APPLICATIONS (SPA 2017)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Signal Processing Algorithms Architectures Arrangements and Applications</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Conference on Signal Processing: Algorithms, Architectures,
   Arrangements, and Applications (SPA)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 20-22, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Poznan, POLAND</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; nonparametric transformation; phase vocoder</td>
</tr>

<tr>
<td valign="top">AB </td><td>A novel approach to pitch transposition is presented, which allows to preserve the spectral envelope or to modify it in a voice conversion task. Pitch-shifted output is synthesized directly from the input signal using the phase vocoder, without explicitly estimating the fundamental frequency, which is often prone to errors. Frequency band extension technique is proposed to address the problem of bandwidth reduction when scaling pitch down. Artificial neural network was applied to transform the signal's spectral envelope in the full voice conversion task. Listening tests showed that 55% of listeners preferred the quality of the pitch modification using the phase vocoder over the one offered by a parametric approach.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Lenarczyk, Michal] Polish Acad Sci, IPI PAN Inst Comp Sci, Warsaw,
   Poland.
   <br>[Janicki, Artur] Warsaw Univ Technol, Inst Telecommun, Warsaw, Poland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Lenarczyk, M (reprint author), Polish Acad Sci, IPI PAN Inst Comp Sci, Warsaw, Poland.</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Janicki, Artur</display_name>&nbsp;</font></td><td><font size="3">0000-0002-9937-4402&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>73</td>
</tr>

<tr>
<td valign="top">EP </td><td>77</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000425864300016</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Paul, D
   <br>Sahidullah, M
   <br>Saha, G</td>
</tr>

<tr>
<td valign="top">AF </td><td>Paul, Dipjyoti
   <br>Sahidullah, Md
   <br>Saha, Goutam</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>GENERALIZATION OF SPOOFING COUNTERMEASURES: A CASE STUDY WITH ASVSPOOF
   2015 AND BTAS 2016 CORPORA</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 05-09, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>New Orleans, LA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Spoofing Attack; Replay Attack; ASVspoof 2015; BTAS 2016; Generalized
   countermeasure.</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEAKER VERIFICATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice-based biometric systems are highly prone to spoofing attacks. Recently, various countermeasures have been developed for detecting different kinds of attacks such as replay, speech synthesis (SS) and voice conversion (VC). Most of the existing studies are conducted with a specific training set defined by the evaluation protocol. However, for realistic scenarios, selecting appropriate training data is an open challenge for the system administrator. Motivated by this practical concern, this work investigates the generalization capability of spoofing countermeasures in restricted training conditions where speech from a broad attack types are left out in the training database. We demonstrate that different spoofing types have considerably different generalization capabilities. For this study, we analyze the performance using two kinds of features, mel-frequency cepstral coefficients (MFCCs) which are considered as baseline and recently proposed constant Q cepstral coefficients (CQCCs). The experiments are conducted with standard Gaussian mixture model -maximum likelihood (GMM-ML) classifier on two recently released spoofing corpora: ASVspoof 2015 and BTAS 2016 that includes cross-corpora performance analysis. Featurelevel analysis suggests that static and dynamic coefficients of spectral features, both are important for detecting spoofing attacks in the real-life condition.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Paul, Dipjyoti; Saha, Goutam] Indian Inst Technol Kharagpur, Dept E &amp;
   ECE, Kharagpur, W Bengal, India.
   <br>[Sahidullah, Md] Univ Eastern Finland, Sch Comp, Joensuu, Finland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Paul, D (reprint author), Indian Inst Technol Kharagpur, Dept E &amp; ECE, Kharagpur, W Bengal, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>dipjyotipaul@ece.iitkgp.ernet.in; sahid@cs.uef.fi;
   gsaha@ece.iitkgp.ernet.in</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Sahidullah, Md</display_name>&nbsp;</font></td><td><font size="3">0000-0002-0624-2903&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>7</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>7</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>2047</td>
</tr>

<tr>
<td valign="top">EP </td><td>2051</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000414286202045</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Tajiri, Y
   <br>Kameoka, H
   <br>Toda, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Tajiri, Yusuke
   <br>Kameoka, Hirokazu
   <br>Toda, Tomoki</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>A NOISE SUPPRESSION METHOD FOR BODY-CONDUCTED SOFT SPEECH BASED ON
   NON-NEGATIVE TENSOR FACTORIZATION OF AIR- AND BODY-CONDUCTED SIGNALS</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 05-09, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>New Orleans, LA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Silent speech communication; nonaudible murmur; noise suppression;
   external noise monitoring; non-negative matrix factorization</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a novel noise suppression method to enhance soft speech recorded with a special body-conductive microphone called nonaudible murmur (NAM) microphone. NAM microphone is capable of detecting extremely soft speech, but the recorded soft speech easily suffers from external noise due to its faint volume. To effectively suppress noise on the body-conducted signals, an external noise monitoring framework using an air-conducive microphone has been proposed. In this study, we propose a noise suppression method for this framework based on a probabilistic observation model robust against phase variations. In the proposed method, noise suppression process is formulated as a special case of non-negative tensor factorization of the observed air- and body-conducted signals. Experimental results demonstrate that 1) the proposed method consistently outperforms the conventional method under real noisy environments and 2) the proposed method effectively deals with speech acoustic changes caused by the Lombard reflex.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Tajiri, Yusuke; Toda, Tomoki] Nagoya Univ, Grad Sch Informat Sci,
   Nagoya, Aichi, Japan.
   <br>[Kameoka, Hirokazu] NTT Commun Sci Labs, Media Informat Lab, Kyoto,
   Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Tajiri, Y (reprint author), Nagoya Univ, Grad Sch Informat Sci, Nagoya, Aichi, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>tajiri.yusuke@g.m.sp.is.nagoya-u.ac.jp; kameoka.hirokazu@lab.ntt.co.jp;
   tomoki@icts.nagoya-u.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>4960</td>
</tr>

<tr>
<td valign="top">EP </td><td>4964</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000414286205024</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Rajpal, A
   <br>Shah, NJ
   <br>Zaki, M
   <br>Patil, HA</td>
</tr>

<tr>
<td valign="top">AF </td><td>Rajpal, Avni
   <br>Shah, Nirmesh J.
   <br>Zaki, Mohammadi
   <br>Patil, Hemant A.</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>QUALITY ASSESSMENT OF VOICE CONVERTED SPEECH USING ARTICULATORY FEATURES</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 05-09, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>New Orleans, LA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; acoustic-to-articulatory inversion; articulatory
   features</td>
</tr>

<tr>
<td valign="top">AB </td><td>We propose a novel application of the acoustic-to-articulatory inversion (AAI) towards a quality assessment of the voice converted speech. The ability of humans to speak effortlessly requires the coordinated movements of various articulators, muscles, etc. This effortless movement contributes towards a naturalness, intelligibility and speaker's identity (which is partially present in voice converted speech). Hence, during voice conversion (VC), the information related to the speech production is lost. In this paper, this loss is quantified for a male voice, by showing an increase in RMSE error (up to 12.7 % in tongue tip) for voice converted speech followed by showing a decrease in mutual information (I) (by 8.7 %). Similar results are obtained in the case of a female voice. This observation is extended by showing that the articulatory features can be used as an objective measure. The effectiveness of the proposed measure over MCD is illustrated by comparing their correlation with a Mean Opinion Score (MOS). Moreover, the preference score of MCD contradicted ABX test by 100 %, whereas the proposed measure supported ABX test by 45.8 % and 16.7 % in the case of female-to-male and male-to-female VC, respectively.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Rajpal, Avni; Shah, Nirmesh J.; Patil, Hemant A.] DA IICT, Gandhinagar
   382007, India.
   <br>[Zaki, Mohammadi] IISc, Bangalore 560012, Karnataka, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Rajpal, A (reprint author), DA IICT, Gandhinagar 382007, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>avni_rajpal@daiict.ac.in; nirmesh88_shah@daiict.ac.in;
   zaki@ece.iisc.ernet.in; hemant_patil@daiict.ac.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>5515</td>
</tr>

<tr>
<td valign="top">EP </td><td>5519</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000414286205135</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Shah, NJ
   <br>Patil, HA</td>
</tr>

<tr>
<td valign="top">AF </td><td>Shah, Nirmesh J.
   <br>Patil, Hemant A.</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>NOVEL AMPLITUDE SCALING METHOD FOR BILINEAR FREQUENCY WARPING-BASED
   VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 05-09, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>New Orleans, LA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; frequency warping; amplitude scaling; Gaussian mixture
   model</td>
</tr>

<tr>
<td valign="top">ID </td><td>MAXIMUM-LIKELIHOOD</td>
</tr>

<tr>
<td valign="top">AB </td><td>In Frequency Warping (FW)-based Voice Conversion (VC), the source spectrum is modified to match the frequency-axis of the target spectrum followed by an Amplitude Scaling (AS) to compensate the amplitude differences between the warped spectrum and the actual target spectrum. In this paper, we propose a novel AS technique which linearly transfers the amplitude of the frequency-warped spectrum using the knowledge of a Gaussian Mixture Model (GMM)-based converted spectrum without adding any spurious peaks. The novelty of the proposed approach lies in avoiding a perceptual impression of wrong formant location (due to perfect match assumption between the warped spectrum and the actual target spectrum in state-of-the-art AS method) leading to deterioration in converted voice quality. From subjective analysis, it is evident that the proposed system has been preferred 33.81 % and 12.37 % times more compared to the GMM and state-of-the-art AS method for voice quality, respectively. Similar to the quality conversion trade-offs observed by other studies in the literature, speaker identity conversion was 0.73 % times more and 9.09 % times less preferred over GMM and state-of-the-art AS-based method, respectively.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Shah, Nirmesh J.; Patil, Hemant A.] DA IICT, Gandhinagar, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Shah, NJ (reprint author), DA IICT, Gandhinagar, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>nirmesh88_shah@daiict.ac.in; hemant_patil@daiict.ac.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>6</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>6</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>5520</td>
</tr>

<tr>
<td valign="top">EP </td><td>5524</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000414286205136</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Zhao, GL
   <br>Gutierrez-Osuna, R</td>
</tr>

<tr>
<td valign="top">AF </td><td>Zhao, Guanlong
   <br>Gutierrez-Osuna, Ricardo</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>EXEMPLAR SELECTION METHODS IN VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 05-09, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>New Orleans, LA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; sparse reconstruction; exemplar selection</td>
</tr>

<tr>
<td valign="top">AB </td><td>Exemplar-based methods for voice conversion often use a large number of randomly-selected exemplars to ensure good coverage. As a result, the factorization step can be costly. This paper presents two algorithms that can be used to construct compact sets of exemplars. The first algorithm uses a forward selection procedure to build the exemplar set sequentially, selecting exemplar pairs that minimize the joint reconstruction error on source and target frames. The second algorithm uses a backward elimination procedure to remove exemplars that contribute the least to the factorization. We evaluate both selection strategies on voice conversion tasks using the ARCTIC corpus. Our results using objective measures and subjective listening tests show that both strategies can significantly reduce the size of the exemplar set (five-fold, in our experiments) while achieving the same performance on voice conversion.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Zhao, Guanlong; Gutierrez-Osuna, Ricardo] Texas A&amp;M Univ, Dept Comp Sci
   &amp; Engn, College Stn, TX 77843 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Zhao, GL (reprint author), Texas A&amp;M Univ, Dept Comp Sci &amp; Engn, College Stn, TX 77843 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>gzhao@tamu.edu; rgutier@tamu.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>5525</td>
</tr>

<tr>
<td valign="top">EP </td><td>5529</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000414286205137</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kinnunen, T
   <br>Juvela, L
   <br>Alku, P
   <br>Yamagishi, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kinnunen, Tomi
   <br>Juvela, Lauri
   <br>Alku, Paavo
   <br>Yamagishi, Junichi</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>NON-PARALLEL VOICE CONVERSION USING I-VECTOR PLDA: TOWARDS UNIFYING
   SPEAKER VERIFICATION AND TRANSFORMATION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 05-09, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>New Orleans, LA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; i-vector; non-parallel training</td>
</tr>

<tr>
<td valign="top">AB </td><td>Text-independent speaker verification (recognizing speakers regardless of content) and non-parallel voice conversion (transforming voice identities without requiring content-matched training utterances) are related problems. We adopt i-vector method to voice conversion. An i-vector is a fixed-dimensional representation of a speech utterance that enables treating voice conversion in utterance domain, as opposed to frame domain. The high dimensionality (800) and small number of training utterances (24) necessitates using prior information of speakers. We adopt probabilistic linear discriminant analysis (PLDA) for voice conversion. The proposed approach requires neither parallel utterances, transcriptions nor time alignment procedures at any stage.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kinnunen, Tomi] Univ Eastern Finland, Sch Comp, Kuopio, Finland.
   <br>[Juvela, Lauri; Alku, Paavo] Aalto Univ, Dept Signal Proc &amp; Acoust,
   Espoo, Finland.
   <br>[Yamagishi, Junichi] Natl Inst Informat, Tokyo, Japan.
   <br>[Yamagishi, Junichi] Univ Edinburgh, Ctr Speech Technol Res, Edinburgh,
   Midlothian, Scotland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kinnunen, T (reprint author), Univ Eastern Finland, Sch Comp, Kuopio, Finland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>tkinnu@cs.uef.fi; lauri.juvela@aalto.fi; jyamagis@nii.ac.jp</td>
</tr>

<tr xmlns:date="http://exslt.org/dates-and-times">
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Alku, Paavo</display_name>&nbsp;</font></td><td><font size="3">E-2400-2012&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>12</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>12</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>5535</td>
</tr>

<tr>
<td valign="top">EP </td><td>5539</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000414286205139</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Tombaloglu, B
   <br>Erdem, H</td>
</tr>

<tr>
<td valign="top">AF </td><td>Tombaloglu, Burak
   <br>Erdem, Hamit</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>A SVM Based Speech to Text Converter for Turkish Language</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 25TH SIGNAL PROCESSING AND COMMUNICATIONS APPLICATIONS CONFERENCE
   (SIU)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Signal Processing and Communications Applications Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>25th Signal Processing and Communications Applications Conference (SIU)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 15-18, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Antalya, TURKEY</td>
</tr>

<tr>
<td valign="top">DE </td><td>Turkish Language; Support Vector Machines; Speech to text converter;
   MFCC</td>
</tr>

<tr>
<td valign="top">AB </td><td>In proposed speech to text conversion, a Support Vector Machines (SVM) based Turkish speech to text converter system has been developed. In the recognition system, Mel Frequency Cepstral Coefficients (MFCC) has been applied to extract features of Turkish speech and SVM based classifier has been used to classify the phonemes. The morphological structure of Turkish, a language based on phonemes, has been taken into consideration in the devoloped person-dependent voice recognition system. Unlike the multiclass classifiers which are used in the SVM-MFCC based voice recognition system, a new SVM classifier system has been developed that uses fewer classes in layers, increasing the number of multiclass layers. A new Text Comparison Algorithm is proposed, which also uses phoneme sequence to measure similarity in word similarity measurement. Along with these enhancements, as the training period becomes higher, performance of voice recognition is improved and word recognition performance is increased. The performance of the proposed structure is compared with similar systems.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Tombaloglu, Burak; Erdem, Hamit] Baskent Univ, Elekt &amp; Elekt
   Muhendisligi Bolumu, Ankara, Turkey.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Tombaloglu, B (reprint author), Baskent Univ, Elekt &amp; Elekt Muhendisligi Bolumu, Ankara, Turkey.</td>
</tr>

<tr>
<td valign="top">EM </td><td>buraktombaloglu@hotmail.com; herdem@baskent.edu.tr</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science; Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000413813100349</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Dimauro, G
   <br>Di Nicola, V
   <br>Bevilacqua, V
   <br>Caivano, D
   <br>Girardi, F</td>
</tr>

<tr>
<td valign="top">AF </td><td>Dimauro, Giovanni
   <br>Di Nicola, Vincenzo
   <br>Bevilacqua, Vitoantonio
   <br>Caivano, Danilo
   <br>Girardi, Francesco</td>
</tr>

<tr>
<td valign="top">TI </td><td>Assessment of Speech Intelligibility in Parkinson's Disease Using a
   Speech-To-Text System</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE ACCESS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Parkinson's disease; speech analysis; automatic speech recognition;
   human voice; speech to text</td>
</tr>

<tr>
<td valign="top">ID </td><td>LARGE-SAMPLE; DIADOCHOKINESIS; ARTICULATION; PROGRESSION; INTENSITY</td>
</tr>

<tr>
<td valign="top">AB </td><td>Patients with Parkinson's disease (PD) may have difficulties in speaking because of reduced coordination of the muscles that control breathing, phonation, articulation, and prosody. Symptoms that may occur are weakening of the volume of the voice, voice monotony, changes in the quality of the voice, the speed of speech, uncontrolled repetition of words, and difficult speech intelligibility. To date, evaluation of the speech intelligibility is performed based on the unified PD rating scale. Specifically, section 3.1 (eloquence) of the cited scale provides the specialist with some tips to evaluate the patient's speech ability. With the aim of evaluating the speech intelligibility by measuring the variation in parameters in an objective manner, we show that a speech-to-text (STT) system could help specialists to obtain an accurate and objective measure of speech, phrase, and word intelligibility in PD. STT systems are based on methodologies and technologies that enable the recognition and translation of spoken language into text by computers and computerized devices. We decided to base our study on Google STT conversion. We expand Voxtester, a software system for digital assessment of voice and speech changes in PD, in order to perform this study. No previous studies have been presented to address the mentioned challenges based on STT. The experiments here presented are related with detection/classification between pathological speech from patients with PD and regular speech from healthy control group. The results are very interesting and are an important step toward assessing the intelligibility of the speech of PD patients.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Dimauro, Giovanni; Caivano, Danilo] Univ Bari Aldo Moro, Dipartimento
   Informat, I-70125 Bari, Italy.
   <br>[Di Nicola, Vincenzo] Univ Bari Aldo Moro, Dipartimento Sci Med Base
   Neurosci &amp; Organi Senso, I-70124 Bari, Italy.
   <br>[Bevilacqua, Vitoantonio] Politecn Bari, Dipartimento Ingn Elettr &amp;
   Informaz, I-70125 Bari, Italy.
   <br>[Girardi, Francesco] ASL Bari, I-70123 Bari, Italy.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Dimauro, G (reprint author), Univ Bari Aldo Moro, Dipartimento Informat, I-70125 Bari, Italy.</td>
</tr>

<tr>
<td valign="top">EM </td><td>giovanni.dimauro@uniba.it</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Bevilacqua, Vitoantonio</display_name>&nbsp;</font></td><td><font size="3">0000-0002-3088-0788&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Dimauro, Giovanni</display_name>&nbsp;</font></td><td><font size="3">0000-0002-4120-5876&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>5</td>
</tr>

<tr>
<td valign="top">BP </td><td>22199</td>
</tr>

<tr>
<td valign="top">EP </td><td>22208</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/ACCESS.2017.2762475</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000414735700001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sekii, Y
   <br>Orihara, R
   <br>Kojima, K
   <br>Sei, Y
   <br>Tahara, Y
   <br>Ohsuga, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sekii, Yusuke
   <br>Orihara, Ryohei
   <br>Kojima, Keisuke
   <br>Sei, Yuichi
   <br>Tahara, Yasuyuki
   <br>Ohsuga, Akihiko</td>
</tr>

<tr>
<td valign="top">BE </td><td>VanDenHerik, J
   <br>Rocha, AP
   <br>Filipe, J</td>
</tr>

<tr>
<td valign="top">TI </td><td>Fast Many-to-One Voice Conversion using Autoencoders</td>
</tr>

<tr>
<td valign="top">SO </td><td>ICAART: PROCEEDINGS OF THE 9TH INTERNATIONAL CONFERENCE ON AGENTS AND
   ARTIFICIAL INTELLIGENCE, VOL 2</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th International Conference on Agents and Artificial Intelligence
   (ICAART)</td>
</tr>

<tr>
<td valign="top">CY </td><td>FEB 24-26, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Porto, PORTUGAL</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice Conversion; Autoencoder; Deep Learning; Deep Neural Network;
   Spectral Envelope</td>
</tr>

<tr>
<td valign="top">AB </td><td>Most of voice conversion (VC) methods were dealing with a one-to-one VC issue and there were few studies that tackled many-to-one / many-to-many cases. It is difficult to prepare the training data for an application with the methods because they require a lot of parallel data. Furthermore, the length of time required to convert a speech by Deep Neural Network (DNN) gets longer than pre-DNN methods because the DNN-based methods use complicated networks. In this study, we propose a VC method using autoencoders in order to reduce the amount of the training data and to shorten the converting time. In the method, higher-order features are extracted from acoustic features of source speakers by an autoencoder trained with source speakers' data. Then they are converted to higher-order features of a target speaker by DNN. The converted higher-order features are restored to the acoustic features by an autoencoder trained with data drawn from the target speaker. In the evaluation experiment, the proposed method outperforms the conventional VC methods that use Gaussian Mixture Models (GMM) and DNNs in both one-to-one conversion and many-to-one conversion with a small training set in terms of the conversion accuracy and the converting time.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Sekii, Yusuke; Orihara, Ryohei; Sei, Yuichi; Tahara, Yasuyuki; Ohsuga,
   Akihiko] Univ Electrocommun, Grad Sch Informat Syst, Chofu, Tokyo, Japan.
   <br>[Kojima, Keisuke] Solid Sphere Inc, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sekii, Y (reprint author), Univ Electrocommun, Grad Sch Informat Syst, Chofu, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Orihara, Ryohei</display_name>&nbsp;</font></td><td><font size="3">Q-5926-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Orihara, Ryohei</display_name>&nbsp;</font></td><td><font size="3">0000-0002-9039-7704&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>164</td>
</tr>

<tr>
<td valign="top">EP </td><td>174</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.5220/0006193301640174</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000413244200015</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Baseer, I
   <br>Basir, R</td>
</tr>

<tr>
<td valign="top">AF </td><td>Baseer, Irum
   <br>Basir, Rabeea</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Cross Gender Voice Morphing using Canonical Correlation Analysis</td>
</tr>

<tr>
<td valign="top">SO </td><td>PROCEEDINGS OF 2017 INTERNATIONAL CONFERENCE ON COMMUNICATION, COMPUTING
   AND DIGITAL SYSTEMS (C-CODE)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Communication, Computing and Digital Systems
   (C-CODE)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 08-09, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Bahria Univ, Islamabad, PAKISTAN</td>
</tr>

<tr>
<td valign="top">HO </td><td>Bahria Univ</td>
</tr>

<tr>
<td valign="top">DE </td><td>Canonical Correlation Analysis; DTW; LSF; Voice morphing</td>
</tr>

<tr>
<td valign="top">ID </td><td>CONVERSION; TRANSFORMATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice morphing one of the speech synthesis frameworks, in simplest term aim to transforms speaker's identity from source to target speaker while preserving the original content of message. This paper presents a novel spectral envelope mapping algorithm based on Canonical Correlation Analysis(CCA) that find the association between spectral envelope characteristics of source speaker and target speaker in terms of correlation as a similarity metric. Moreover, the speech also undergoes to prosodic modification using PSOLA as pitch frequency is also an important parameter for varying identity. This morphing algorithm is evaluated by taking the utterances from freely available CMU-ARCTIC speech dataset. The subjective experiment shows that the proposed method successfully transforms speaker identity and produced high-quality morphed signal.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Baseer, Irum] Univ Engn &amp; Technol Taxila, Dept Comp Engn, Taxila,
   Pakistan.
   <br>[Basir, Rabeea] Univ Engn &amp; Technol Taxila, Dept Telecommun Engn,
   Taxila, Pakistan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Baseer, I (reprint author), Univ Engn &amp; Technol Taxila, Dept Comp Engn, Taxila, Pakistan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>irum.baseer@gmail.com; rabeea.basir@gmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>304</td>
</tr>

<tr>
<td valign="top">EP </td><td>309</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000408982800055</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Chern, A
   <br>Lai, YH
   <br>Chang, YP
   <br>Tsao, Y
   <br>Chang, RY
   <br>Chang, HW</td>
</tr>

<tr>
<td valign="top">AF </td><td>Chern, Alan
   <br>Lai, Ying-Hui
   <br>Chang, Yi-Ping
   <br>Tsao, Yu
   <br>Chang, Ronald Y.
   <br>Chang, Hsiu-Wen</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Smartphone-Based Multi-Functional Hearing Assistive System to
   Facilitate Speech Recognition in the Classroom</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE ACCESS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Wireless assistive technologies; smartphone technologies; hearing
   assistive systems; speech recognition; classroom; e-health; m-health</td>
</tr>

<tr>
<td valign="top">ID </td><td>LISTENING EFFORT; FM SYSTEMS; CHILDREN; NOISE; FATIGUE; ADULTS</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we propose a smartphone-based hearing assistive system (termed SmartHear) to facilitate speech recognition for various target users, who could benefit from enhanced listening clarity in the classroom. The SmartHear system consists of transmitter and receiver devices (e.g., smartphone and Bluetooth headset) for voice transmission, and an Android mobile application that controls and connects the different devices via Bluetooth or WiFi technology. The wireless transmission of voice signals between devices overcomes the reverberation and ambient noise effects in the classroom. The main functionalities of SmartHear include: 1) configurable transmitter/receiver assignment, to allow flexible designation of transmitter/receiver roles; 2) advanced noise-reduction techniques; 3) audio recording; and 4) voice-to-text conversion, to give students visual text aid. All the functions are implemented as a mobile application with an easy-to-navigate user interface. Experiments show the effectiveness of the noise-reduction schemes at low signal-to-noise ratios in terms of standard speech perception and quality indices, and show the effectiveness of SmartHear in maintaining voice-to-text conversion accuracy regardless of the distance between the speaker and listener. Future applications of SmartHear are also discussed.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Chern, Alan] Georgia Inst Technol, Sch Comp Sci, Atlanta, GA 30332 USA.
   <br>[Lai, Ying-Hui] Yuan Ze Univ, Dept Elect Engn, Taoyuan 320, Taiwan.
   <br>[Chang, Yi-Ping] Childrens Hearing Fdn, Speech &amp; Hearing Sci Res Inst,
   Taipei 114, Taiwan.
   <br>[Chang, Yi-Ping; Chang, Hsiu-Wen] Mackay Med Coll, Dept Audiol &amp; Speech
   Language Pathol, New Taipei 252, Taiwan.
   <br>[Tsao, Yu; Chang, Ronald Y.] Acad Sinica, Res Ctr Informat Technol
   Innovat, Taipei 115, Taiwan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Chang, HW (reprint author), Mackay Med Coll, Dept Audiol &amp; Speech Language Pathol, New Taipei 252, Taiwan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>iamaudie@hotmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>5</td>
</tr>

<tr>
<td valign="top">BP </td><td>10339</td>
</tr>

<tr>
<td valign="top">EP </td><td>10351</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/ACCESS.2017.2711489</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000404360000029</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Poornima, S
   <br>Sripriya, N
   <br>Vijayalakshmi, B
   <br>Vishnupriya, P</td>
</tr>

<tr>
<td valign="top">AF </td><td>Poornima, S.
   <br>Sripriya, N.
   <br>Vijayalakshmi, B.
   <br>Vishnupriya, P.</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Attendance Monitoring System using Facial Recognition with Audio Output
   and Gender Classification</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 INTERNATIONAL CONFERENCE ON COMPUTER, COMMUNICATION AND SIGNAL
   PROCESSING (ICCCSP)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Computer, Communication and Signal
   Processing (ICCCSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JAN 10-11, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Chennai, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Face Recognition; Principal Component Analysis; Voice Conversion; Gender
   classification</td>
</tr>

<tr>
<td valign="top">ID </td><td>FACES</td>
</tr>

<tr>
<td valign="top">AB </td><td>Maintaining and taking log of attendance in a class is not much effective through manual process. Since bunking the classes or giving proxies for the absentees become fun and fantasy among the current generation students. Manual entering of attendance in logbooks becomes a difficult task and it can be easily manipulated. Therefore, this paper aims in presenting an automated attendance System - AUDACE. This system automatically detects the student in the class room and marks the attendance by recognizing their face.. This system is developed by capturing real time human faces in the class. The detected faces are matched against the reference faces in the dataset and marked the attendance for the attendees. Finally the absentee lists are said aloud through voice conversion system for confirmation. Secondly, the system is trained to classify the gender of the students present in the class.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Poornima, S.; Sripriya, N.; Vijayalakshmi, B.; Vishnupriya, P.] SSN
   Coll Engn, Madras, Tamil Nadu, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Poornima, S (reprint author), SSN Coll Engn, Madras, Tamil Nadu, India.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>235</td>
</tr>

<tr>
<td valign="top">EP </td><td>239</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000403399900044</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Koolagudi, SG
   <br>Vishwanath, BK
   <br>Akshatha, M
   <br>Murthy, YVS</td>
</tr>

<tr>
<td valign="top">AF </td><td>Koolagudi, Shashidhar G.
   <br>Vishwanath, B. Kavya
   <br>Akshatha, M.
   <br>Murthy, Yarlagadda V. S.</td>
</tr>

<tr>
<td valign="top">BE </td><td>Satapathy, SC
   <br>Bhateja, V
   <br>Joshi, A</td>
</tr>

<tr>
<td valign="top">TI </td><td>Performance Analysis of LPC and MFCC Features in Voice Conversion Using
   Artificial Neural Networks</td>
</tr>

<tr>
<td valign="top">SO </td><td>PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON DATA ENGINEERING AND
   COMMUNICATION TECHNOLOGY, ICDECT 2016, VOL 2</td>
</tr>

<tr>
<td valign="top">SE </td><td>Advances in Intelligent Systems and Computing</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>1st International Conference on Data Engineering and Communication
   Technology (ICDECT)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 10-11, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Christ Inst Management, Lavasa, INDIA</td>
</tr>

<tr>
<td valign="top">HO </td><td>Christ Inst Management</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; Morphing; Mel-frequency cepstral coefficients; Linear
   predictive coding and neural networks</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice Conversion is a technique in which source speakers voice is morphed to a target speakers voice by learning source-target relationship from a number of utterances from source and the target. There are many applications which may benefit from this sort of technology for example dubbing movies, TV-shows, TTS systems and so on. In this paper, analysis on the performance of ANN-based Voice Conversion system is done using linear predictive coding (LPC) and mel-frequency cepstral coefficients (MFCCs). Experimental results show that Voice Conversion system based on LPC features is better than the ones based on MFCC features.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Koolagudi, Shashidhar G.; Vishwanath, B. Kavya; Akshatha, M.; Murthy,
   Yarlagadda V. S.] Natl Inst Technol Karnataka, Surathkal 575025,
   Karnataka, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Koolagudi, SG (reprint author), Natl Inst Technol Karnataka, Surathkal 575025, Karnataka, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>koolagudi@yahoo.com; kavyabvishwanath25@gmail.com; akshatham@gmail.com;
   urvishnu@gmail.com</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Yarlagadda, Vishnu</display_name>&nbsp;</font></td><td><font size="3">B-4085-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Yarlagadda, Vishnu</display_name>&nbsp;</font></td><td><font size="3">0000-0001-6146-5272&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>469</td>
</tr>

<tr>
<td valign="top">BP </td><td>275</td>
</tr>

<tr>
<td valign="top">EP </td><td>280</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/978-981-10-1678-3_27</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Robotics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000399004300027</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Merilainen, E</td>
</tr>

<tr>
<td valign="top">AF </td><td>Merilainen, Esa</td>
</tr>

<tr>
<td valign="top">TI </td><td>Comparative Measurements on Loudspeaker Distortion: Current vs. Voltage
   Control</td>
</tr>

<tr>
<td valign="top">SO </td><td>ARCHIVES OF ACOUSTICS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>current driving; current control; loudspeaker distortion; current
   distortion; inductance non-linearity; voice coil former</td>
</tr>

<tr>
<td valign="top">AB </td><td>Modulation and harmonic distortion produced by ordinary moving-coil direct-radiator drive units are measured under current control and voltage control. A realistic two-tone test signal is used to investigate the detrimental effect of the voice coil electromotive forces on the voltage-to-current conversion that is critical with voltage control. Dramatic improvements in distortion performance are obtained with current control. Current nonlinearities in voltage-controlled speakers are shown to be the dominant source of modulation and odd harmonic distortions at signal frequencies above the fundamental resonance region. Currents in conductive voice coil formers also seem to be significant sources of distortion; with current control and non-conductive formers, odd harmonics and related modulation products virtually vanished in the upper midrange.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Merilainen, Esa] Eerikinkallio 4 A 8, FI-02400 Kyrkslatt, Finland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Merilainen, E (reprint author), Eerikinkallio 4 A 8, FI-02400 Kyrkslatt, Finland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>merilainenesa@hotmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>42</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>71</td>
</tr>

<tr>
<td valign="top">EP </td><td>81</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1515/aoa-2017-0008</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000395358100008</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wu, HJ
   <br>Wang, Y
   <br>Huang, JW</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wu, Haojun
   <br>Wang, Yong
   <br>Huang, Jiwu</td>
</tr>

<tr>
<td valign="top">TI </td><td>Identification of Reconstructed Speech</td>
</tr>

<tr>
<td valign="top">SO </td><td>ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Audio forensics; reconstructed speech; identification; speaker
   verification; MFCC; GMM supervectors; LDA-ensemble classification</td>
</tr>

<tr>
<td valign="top">ID </td><td>SUPPORT VECTOR MACHINES; SPEAKER VERIFICATION; VOICE CONVERSION;
   ALGORITHM; MODEL; RECOGNITION; ADAPTATION; SPECTRUM; PHASE</td>
</tr>

<tr>
<td valign="top">AB </td><td>Both voice conversion and hidden Markey model (HAIM) based speech synthesis can be used to produce artificial voices of a target speaker. They have shown great negative impacts on speaker verification (SV) systems. In order to enhance the security of SV systems, the techniques to detect converted/synthesized speech should be taken into consideration. During voice conversion and IIMM-based synthesis, speech reconstruction is applied to transform a set of acoustic parameters to reconstructed speech. Hence, the identification of reconstructed speech can be used to distinguish converted/synthesized speech from human speech. Several related works on such identification have been reported. The equal error rates (EERs) lower than 5% of detecting' reconstructed speech have been achieved. However, through the cross-database evaluations on different speech databases, we find that the EERs of several testing cases are higher than 10%. The robustness of detection algorithms to different speech databases needs to be improved. In this article, we propose an algorithm to identify the reconstructed speech. Three different speech databases and two different reconstruction methods are considered in our work, which has not been addressed in the reported works. The high-dimensional data visualization approach is used to analyze the effect of speech reconstruction on Mel-frequency cepstral coefficients (MFCC) of speech signals. The Gaussian mixture model supervectors of MFCC are used as acoustic features. Furthermore, a set of commonly used classification algorithms are applied to identify reconstructed speech. According to the comparison among different classification methods, linear discriminant analysis-ensemble classifiers are chosen in our algorithm. Extensive experimental results show that the EERs lower than 1% can be achieved by the proposed algorithm in most cases, outperforming the reported state-of-the-art identification techniques.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Wu, Haojun; Huang, Jiwu] Shenzhen Univ, Shenzhen, Peoples R China.
   <br>[Wu, Haojun] Sun Yat Sen Univ, Guangzhou, Peoples R China.
   <br>[Wang, Yong] Guangdong Polytechn Normal Univ, Guangzhou, Peoples R China.
   <br>[Wu, Haojun; Huang, Jiwu] Shenzhen Univ, Coll Informat Engn, Shenzhen
   Key Lab Media Secur, Nanhai Ave 3688, Shenzhen, Peoples R China.
   <br>[Wang, Yong] Guangdong Polytech Normal Univ, Sch Elect &amp; Informat, West
   Zhongshan Ave 293, Guangzhou, Guangdong, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Wu, HJ; Huang, JW (reprint author), Shenzhen Univ, Shenzhen, Peoples R China.; Wu, HJ (reprint author), Sun Yat Sen Univ, Guangzhou, Peoples R China.; Wang, Y (reprint author), Guangdong Polytechn Normal Univ, Guangzhou, Peoples R China.; Wu, HJ; Huang, JW (reprint author), Shenzhen Univ, Coll Informat Engn, Shenzhen Key Lab Media Secur, Nanhai Ave 3688, Shenzhen, Peoples R China.; Wang, Y (reprint author), Guangdong Polytech Normal Univ, Sch Elect &amp; Informat, West Zhongshan Ave 293, Guangzhou, Guangdong, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>wuhaojun@mail2.sysu.edu.cn; isswy@mail.sysu.edu.cn; jwhuang@szu.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PD </td><td>JAN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>13</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">AR </td><td>10</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1145/3004055</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000395522700010</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kinnunen, T
   <br>Sahidullah, M
   <br>Delgado, H
   <br>Todisco, M
   <br>Evans, N
   <br>Yamagishi, J
   <br>Lee, KA</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kinnunen, Tomi
   <br>Sahidullah, Md
   <br>Delgado, Hector
   <br>Todisco, Massimiliano
   <br>Evans, Nicholas
   <br>Yamagishi, Junichi
   <br>Lee, Kong Aik</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>The ASVspoof 2017 Challenge: Assessing the Limits of Replay Spoofing
   Attack Detection</td>
</tr>

<tr>
<td valign="top">SO </td><td>18TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2017), VOLS 1-6: SITUATED INTERACTION</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>18th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2017)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 20-24, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Stockholm, SWEDEN</td>
</tr>

<tr>
<td valign="top">DE </td><td>automatic speaker verification; spoofing; countermeasure; replay
   attacks; ASVspoof</td>
</tr>

<tr>
<td valign="top">ID </td><td>DEPENDENT SPEAKER VERIFICATION; RECOGNITION</td>
</tr>

<tr>
<td valign="top">AB </td><td>The ASVspoof initiative was created to promote the development of countermeasures which aim to protect automatic speaker verification (ASV) from spoofing attacks. The first community-led, common evaluation held in 2015 focused on countermeasures for speech synthesis and voice conversion spoofing attacks. Arguably, however, it is replay attacks which pose the greatest threat. Such attacks involve the replay of recordings collected from enrolled speakers in order to provoke false alarms and can be mounted with greater ease using everyday consumer devices. ASVspoof 2017, the second in the series, hence focused on the development of replay attack countermeasures. This paper describes the database, protocols and initial findings. The evaluation entailed highly heterogeneous acoustic recording and replay conditions which increased the equal error rate (EER) of a baseline ASV system from 1.76% to 31.46%. Submissions were received from 49 research teams. 20 of which improved upon a baseline replay spoofing detector EER of 24.77%, in terms of replay/non-replay discrimination. While largely successful, the evaluation indicates that the quest for countermeasures which are resilient in the face of variable replay attacks remains very much alive.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kinnunen, Tomi; Sahidullah, Md] Univ Eastern Finland, Joensuu, Finland.
   <br>[Delgado, Hector; Todisco, Massimiliano; Evans, Nicholas] EURECOM, Biot,
   France.
   <br>[Yamagishi, Junichi] Natl Inst Informat, Tokyo, Japan.
   <br>[Yamagishi, Junichi] Univ Edinburgh, Edinburgh, Midlothian, Scotland.
   <br>[Lee, Kong Aik] Inst Infocomm Res, Singapore, Singapore.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kinnunen, T (reprint author), Univ Eastern Finland, Joensuu, Finland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>info@asvspoof.org</td>
</tr>

<tr>
<td valign="top">TC </td><td>16</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>17</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>2</td>
</tr>

<tr>
<td valign="top">EP </td><td>6</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2017-1111</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000457505000001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Lavrentyeva, G
   <br>Novoselov, S
   <br>Malykh, E
   <br>Kozlov, A
   <br>Kudashev, O
   <br>Shchemelinin, V</td>
</tr>

<tr>
<td valign="top">AF </td><td>Lavrentyeva, Galina
   <br>Novoselov, Sergey
   <br>Malykh, Egor
   <br>Kozlov, Alexander
   <br>Kudashev, Oleg
   <br>Shchemelinin, Vadim</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Audio replay attack detection with deep learning frameworks</td>
</tr>

<tr>
<td valign="top">SO </td><td>18TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2017), VOLS 1-6: SITUATED INTERACTION</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>18th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2017)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 20-24, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Stockholm, SWEDEN</td>
</tr>

<tr>
<td valign="top">DE </td><td>spoofing; anti-spoofing; speaker recognition; replay attack detection;
   CNN; RNN; ASVspoof</td>
</tr>

<tr>
<td valign="top">AB </td><td>Nowadays spoofing detection is one of the priority research areas in the field of automatic speaker verification. The success of Automatic Speaker Verification Spoofing and Countermeasures (ASVspoof) Challenge 2015 confirmed the impressive perspective in detection of unforeseen spoofing trials based on speech synthesis and voice conversion techniques. However, there is a small number of researches addressed to replay spoofing attacks which arc more likely to be used by non-professional impersonators. This paper describes the Speech Technology Center (STC) anti-spoofing system submitted for ASVspoof 2017 which is focused on replay attacks detection. Here we investigate the efficiency of a deep learning approach for solution of the mentioned-above task. Experimental results obtained on the Challenge corpora demonstrate that the selected approach outperforms current state-of-the-art baseline systems in terms of spoofing detection quality. Our primary system produced an EER of 6.73% on the evaluation part of the corpora which is 72% relative improvement over the ASVspoof 2017 baseline system.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Lavrentyeva, Galina; Novoselov, Sergey; Malykh, Egor; Kudashev, Oleg;
   Shchemelinin, Vadim] ITMO Univ, St Petersburg, Russia.
   <br>[Kozlov, Alexander; Kudashev, Oleg] STC Innovat Ltd, St Petersburg,
   Russia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Lavrentyeva, G (reprint author), ITMO Univ, St Petersburg, Russia.</td>
</tr>

<tr>
<td valign="top">EM </td><td>lavrentyeva@speechpro.com; novoselov@speechpro.com;
   malykh@speechpro.com; kozlov-a@speechpro.com; kudashev@speechpro.com;
   shchemelinin@speechpro.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>20</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>22</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>82</td>
</tr>

<tr>
<td valign="top">EP </td><td>86</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2017-360</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000457505000017</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Chen, ZX
   <br>Xie, ZF
   <br>Zhang, WB
   <br>Xu, XM</td>
</tr>

<tr>
<td valign="top">AF </td><td>Chen, Zhuxin
   <br>Xie, Zhifeng
   <br>Zhang, Weibin
   <br>Xu, Xiangmin</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>ResNet and Model Fusion for Automatic Spoofing Detection</td>
</tr>

<tr>
<td valign="top">SO </td><td>18TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2017), VOLS 1-6: SITUATED INTERACTION</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>18th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2017)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 20-24, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Stockholm, SWEDEN</td>
</tr>

<tr>
<td valign="top">DE </td><td>Replay attacks; Residual neural network; Model fusion; ASVspoof2017</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEAKER VERIFICATION; NEURAL-NETWORKS; RECOGNITION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Speaker verification systems have achieved great progress in recent years. Unfortunately, they are still highly prone to different kinds of spoofing attacks such as speech synthesis, voice conversion, and fake audio recordings etc. Inspired by the success of ResNet in image recognition, we investigated the effectiveness of using ResNet for automatic spoofing detection. Experimental results on the ASVspoof2017 data set show that ResNet performs the best among all the single-model systems. Model fusion is a good way to further improve the system performance. Nevertheless, we found that if the same feature is used for different fused models, the resulting system can hardly be improved. By using different features and models, our best fused model further reduced the Equal Error Rate (EER) by 18% relatively, compared with the best single-model system.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Chen, Zhuxin; Xie, Zhifeng; Zhang, Weibin; Xu, Xiangmin] South China
   Univ Technol, Sch Elect &amp; Informat Engn, Guangzhou, Guangdong, Peoples R
   China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Chen, ZX (reprint author), South China Univ Technol, Sch Elect &amp; Informat Engn, Guangzhou, Guangdong, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>chen.zhuxin@mail.scut.edu.cn; xie.zhifeng@mail.scut.edu.cn;
   eeweibin@scut.edu.cn; xmxu@scut.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>9</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>9</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>102</td>
</tr>

<tr>
<td valign="top">EP </td><td>106</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2017-1085</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000457505000021</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kobayashi, K
   <br>Hayashi, T
   <br>Tamamori, A
   <br>Toda, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kobayashi, Kazuhiro
   <br>Hayashi, Tomoki
   <br>Tamamori, Akira
   <br>Toda, Tomoki</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Statistical voice conversion with WaveNet-based waveform generation</td>
</tr>

<tr>
<td valign="top">SO </td><td>18TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2017), VOLS 1-6: SITUATED INTERACTION</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>18th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2017)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 20-24, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Stockholm, SWEDEN</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; WaveNet; vocoder; Gaussian mixture model; deep neural
   networks</td>
</tr>

<tr>
<td valign="top">ID </td><td>PLUS NOISE MODEL; SPARSE REPRESENTATION; VOCODER</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a statistical voice conversion (VC) technique with the WaveNet-based waveform generation. VC based on a Gaussian mixture model (GMM) makes it possible to convert the speaker identity of a source speaker into that of a target speaker. However, in the conventional vocoding process, various factors such as Fs extraction errors. parameterization errors and over -smoothing effects of converted feature trajectory cause the modeling errors of the speech waveform, which usually bring about sound quality degradation of the converted voice. To address this issue, we apply a direct waveform generation technique based on a WaveNet vocoder to VC. In the proposed method, first. the acoustic features of the source speaker arc converted into those of the target speaker based on the GMM. Then, the waveform samples of the converted voice are generated based on the WaveNet vocoder conditioned on the converted acoustic features. In this paper. to investigate the modeling accuracies of the converted speech waveform, we compare several types of the acoustic features for training and synthesizing based on the WaveNet vocoder. The experimental results confirmed that the proposed VC technique achieves higher conversion accuracy on speaker individuality with comparable sound quality compared to the conventional VC technique.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kobayashi, Kazuhiro; Toda, Tomoki] Nagoya Univ, Informat Technol Ctr,
   Nagoya, Aichi, Japan.
   <br>[Hayashi, Tomoki] Nagoya Univ, Grad Sch Informat Sci, Nagoya, Aichi,
   Japan.
   <br>[Tamamori, Akira] Nagoya Univ, Inst Innovat Future Soc, Nagoya, Aichi,
   Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kobayashi, K (reprint author), Nagoya Univ, Informat Technol Ctr, Nagoya, Aichi, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>fkobayashi.kazuhiro@g.sp.m.is.nagoya-u.ac.jp;
   hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp;
   tamamori@g.sp.m.is.nagoya-u.ac.jp; tomoki@icts.nagoya-u.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>16</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>16</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>1138</td>
</tr>

<tr>
<td valign="top">EP </td><td>1142</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2017-986</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000457505000238</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Miyoshi, H
   <br>Saito, Y
   <br>Takamichi, S
   <br>Saruwatari, H</td>
</tr>

<tr>
<td valign="top">AF </td><td>Miyoshi, Hiroyuki
   <br>Saito, Yuki
   <br>Takamichi, Shinnosuke
   <br>Saruwatari, Hiroshi</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Conversion Using Sequence-to-Sequence Learning of Context
   Posterior Probabilities</td>
</tr>

<tr>
<td valign="top">SO </td><td>18TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2017), VOLS 1-6: SITUATED INTERACTION</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>18th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2017)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 20-24, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Stockholm, SWEDEN</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; context posterior probabilities; sequence-to-sequence
   learning</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion (VC) using sequence-to-sequence learning of context posterior probabilities is proposed. Conventional VC using shared context posterior probabilities predicts target speech parameters from the context posterior probabilities estimated from the source speech parameters. Although conventional VC can be built from non-parallel data, it is difficult to convert speaker individuality such as phonetic property and speaking rate contained in the posterior probabilities because the source posterior probabilities are directly used for predicting target speech parameters. In this work, we assume that the training data partly include parallel speech data and propose sequence-to-sequence learning between the source and target posterior probabilities. The conversion models perform non-linear and variable-length transformation from the source probability sequence to the target one. Further, we propose a joint training algorithm for the modules. In contrast to conventional VC, which separately trains the speech recognition that estimates posterior probabilities and the speech synthesis that predicts target speech parameters, our proposed method jointly trains these modules along with the proposed probability conversion modules. Experimental results demonstrate that our approach outperforms the conventional VC.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Miyoshi, Hiroyuki; Saito, Yuki; Takamichi, Shinnosuke; Saruwatari,
   Hiroshi] Univ Tokyo, Grad Sch Informat Sci &amp; Technol, Bunkyo Ku, 7-3-1
   Hongo, Tokyo 1138656, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Miyoshi, H (reprint author), Univ Tokyo, Grad Sch Informat Sci &amp; Technol, Bunkyo Ku, 7-3-1 Hongo, Tokyo 1138656, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>mathma1306@gmail.com; yuuki_saito@ipc.i.u.-tokyo.ac.jp;
   shinnosuke_takamichi@ipc.i.u.-tokyo.ac.jp;
   hiroshi_sarwatari@ipc.i.u.-tokyo.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>1268</td>
</tr>

<tr>
<td valign="top">EP </td><td>1272</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2017-247</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000457505000264</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hsu, WN
   <br>Zhang, Y
   <br>Glass, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hsu, Wei-Ning
   <br>Zhang, Yu
   <br>Glass, James</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Learning Latent Representations for Speech Generation and Transformation</td>
</tr>

<tr>
<td valign="top">SO </td><td>18TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2017), VOLS 1-6: SITUATED INTERACTION</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>18th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2017)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 20-24, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Stockholm, SWEDEN</td>
</tr>

<tr>
<td valign="top">DE </td><td>unsupervised learning; variational autoencoder; speech generation;
   speech transformation; voice conversion</td>
</tr>

<tr>
<td valign="top">AB </td><td>An ability to model a generative process and learn a latent representation for speech in an unsupervised fashion will be crucial to process vast quantities of unlabelled speech data. Recently, deep probabilistic generative models such as Variational Autoencoders (VAEs) have achieved tremendous success in modeling natural images. In this paper, we apply a convolutional VAE to model the generative process of natural speech. We derive latent space arithmetic operations to disentangle learned latent representations. We demonstrate the capability of our model to modify the phonetic content or the speaker identity for speech segments using the derived operations, without the need for parallel supervisory data.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Hsu, Wei-Ning; Zhang, Yu; Glass, James] MIT, Comp Sci &amp; Artificial
   Intelligence Lab, 77 Massachusetts Ave, Cambridge, MA 02139 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hsu, WN (reprint author), MIT, Comp Sci &amp; Artificial Intelligence Lab, 77 Massachusetts Ave, Cambridge, MA 02139 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>wnhsu@csail.mit.edu; yzhang87@csail.mit.edu; jrg@csail.mit.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>10</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>10</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>1273</td>
</tr>

<tr>
<td valign="top">EP </td><td>1277</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2017-349</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000457505000265</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hashimoto, T
   <br>Uchida, H
   <br>Saito, D
   <br>Minematsu, N</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hashimoto, Tetsuya
   <br>Uchida, Hidetsugu
   <br>Saito, Daisuke
   <br>Minematsu, Nobuaki</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Parallel-data-free Many-to-many Voice Conversion based on DNN Integrated
   with Eigenspace Using a Non-parallel Speech Corpus</td>
</tr>

<tr>
<td valign="top">SO </td><td>18TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2017), VOLS 1-6: SITUATED INTERACTION</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>18th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2017)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 20-24, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Stockholm, SWEDEN</td>
</tr>

<tr>
<td valign="top">DE </td><td>parallel-data-free; many-to-many voice conversion; DNN; EVGMM;
   eigenspace</td>
</tr>

<tr>
<td valign="top">ID </td><td>RECOGNITION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper proposes a novel approach to parallel-data-free and many-to-many voice conversion (VC). As 1-to-1 conversion has less flexibility, researchers focus on many-to-many conversion, where speaker identity is often represented using speaker space bases. In this case, utterances of the same sentences have to be collected from many speakers. This study aims at overcoming this constraint to realize a parallel-data-free and many-to-many conversion. This is made possible by integrating deep neural networks (DNNs) with eigenspace using a non-parallel speech corpus. In our previous study, many-to-many conversion was implemented using DNN, whose training was assisted by EVGMM conversion. By realizing the function of EVGMM equivalently by constructing eigenspace with a non-paralell speech corpus, the desired conversion is made possible. A key technique here is to estimate covariance terms without given parallel data between source and target speakers. Experiments show that objective assessment scores are comparable to those of the baseline system trained with parallel data.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Hashimoto, Tetsuya; Uchida, Hidetsugu; Saito, Daisuke; Minematsu,
   Nobuaki] Univ Tokyo, Grad Sch Engn, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hashimoto, T (reprint author), Univ Tokyo, Grad Sch Engn, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>hashib@gavo.t.u-tokyo.ac.jp; uchida@gavo.t.u-tokyo.ac.jp;
   dsk_saito@gavo.t.u-tokyo.ac.jp; mine@gavo.t.u-tokyo.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>1278</td>
</tr>

<tr>
<td valign="top">EP </td><td>1282</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2017-961</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000457505000266</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kaneko, T
   <br>Kameoka, H
   <br>Hiramatsu, K
   <br>Kashino, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kaneko, Takuhiro
   <br>Kameoka, Hirokazu
   <br>Hiramatsu, Kaoru
   <br>Kashino, Kunio</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Sequence-to-Sequence Voice Conversion with Similarity Metric Learned
   Using Generative Adversarial Networks</td>
</tr>

<tr>
<td valign="top">SO </td><td>18TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2017), VOLS 1-6: SITUATED INTERACTION</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>18th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2017)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 20-24, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Stockholm, SWEDEN</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; deep neural network; generative adversarial network;
   similarity metric learning</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPARSE REPRESENTATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>We propose a training framework for sequence-to-sequence voice conversion (SVC). A well-known problem regarding a conventional VC framework is that acoustic-feature sequences generated from a converter tend to be over-smoothed, resulting in buzzy-sounding speech. This is because a particular form of similarity metric or distribution for parameter training of the acoustic model is assumed so that the generated feature sequence that averagely fits the training target example is considered optimal. This over-smoothing occurs as long as a manually constructed similarity metric is used. To overcome this limitation. our proposed SVC framework uses a similarity metric implicitly derived from a generative adversarial network, enabling the measurement of the distance in the high-level abstract space. This would enable the model to mitigate the over smoothing problem caused in the low-level data space. Furthermore. we use convolutional neural networks to model the long-range context-dependencies. This also enables the similarity metric to have a shift-invariant property: thus, making the model robust against misalignment errors involved in the parallel data. We tested our framework on a non-native-to-native VC task. The experimental results revealed that the use of the proposed framework had a certain effect in improving naturalness, clarity, and speaker individuality.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kaneko, Takuhiro; Kameoka, Hirokazu; Hiramatsu, Kaoru; Kashino, Kunio]
   NTT Corp, NTT Commun Sci Lab, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kaneko, T (reprint author), NTT Corp, NTT Commun Sci Lab, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>kaneko.takuhiro@lab.ntt.co.jp; kameoka.hirokazu@lab.ntt.co.jp;
   hiramatsu.kaoru@lab.ntt.co.jp; kashino.knuio@lab.ntt.co.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>11</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>11</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>1283</td>
</tr>

<tr>
<td valign="top">EP </td><td>1287</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2017-970</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000457505000267</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Mohammadi, SH
   <br>Kain, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Mohammadi, Seyed Hamidreza
   <br>Kain, Alexander</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Siamese Autoencoders for Speech Style Extraction and Switching Applied
   to Voice Identification and Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>18TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2017), VOLS 1-6: SITUATED INTERACTION</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>18th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2017)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 20-24, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Stockholm, SWEDEN</td>
</tr>

<tr>
<td valign="top">DE </td><td>siamese autoencoders; style extraction; style switching; voice
   conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>NEURAL-NETWORKS</td>
</tr>

<tr>
<td valign="top">AB </td><td>We propose an architecture called siamese autoencoders for extracting and switching pre-determined styles of speech signals while retaining the content. We apply this architecture to a voice conversion task in which we define the content to be the linguistic message and the style to be the speaker's voice. We assume two or more data streams with the same content but unique styles. The architecture is composed of two or more separate but shared-weight autoencoders that are joined by loss functions at the hidden layers. A hidden vector is composed of style and content sub-vectors and the loss functions constrain the encodings to decompose style and content. We can select an intended target speaker either by supplying the associated style vector, or by extracting a new style vector from a new utterance. using a proposed style extraction algorithm. We focus on in-training speakers but perform some initial experiments for out-of-training speakers as well. We propose and study several types of loss functions. The experiment results show that the proposed many-to-many model is able to convert voices successfully; however, its performance does not surpass that of the state-of-the-art one-to-one model's.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Mohammadi, Seyed Hamidreza; Kain, Alexander] Oregon Hlth &amp; Sci Univ,
   Ctr Spoken Language Understanding, Portland, OR 97201 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Mohammadi, SH (reprint author), Oregon Hlth &amp; Sci Univ, Ctr Spoken Language Understanding, Portland, OR 97201 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>mohammah@ohsu.edu; kaina@ohsu.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>1293</td>
</tr>

<tr>
<td valign="top">EP </td><td>1297</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2017-1434</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000457505000269</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Alluri, KNRKR
   <br>Achanta, S
   <br>Kadiri, SR
   <br>Gangashetty, SV
   <br>Vuppala, AK</td>
</tr>

<tr>
<td valign="top">AF </td><td>Alluri, K. N. R. K. Raju
   <br>Achanta, Sivanand
   <br>Kadiri, Sudarsana Reddy
   <br>Gangashetty, Suryakanth V.
   <br>Vuppala, Anil Kumar</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Detection of Replay Attacks using Single Frequency Filtering Cepstral
   Coefficients</td>
</tr>

<tr>
<td valign="top">SO </td><td>18TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2017), VOLS 1-6: SITUATED INTERACTION</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>18th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2017)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 20-24, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Stockholm, SWEDEN</td>
</tr>

<tr>
<td valign="top">DE </td><td>Replay attack; countermeasures; Gaussian mixture model; single frequency
   filtering cepstral coefficients</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEAKER; SPEECH</td>
</tr>

<tr>
<td valign="top">AB </td><td>Automatic speaker verification systems are vulnerable to spoofing attacks. Recently, various countermeasures have been developed for detecting high technology attacks such as speech synthesis and voice conversion. However, there is a wide gap in dealing with replay attacks. In this paper. we propose a new feature for replay attack detection based on single frequency filtering (SFF), which provides high temporal and spectral resolution at each instant. Single frequency filtering cepstral coefficients (SFFCC) with Gaussian mixture model classifier are used for the experimentation on the standard BTAS-2016 corpus. The previously reported best result, which is based on constant Q cepstral coefficients (CQCC) achieved a half total error rate of 0.67 % on this data-set. Our proposed method outperforms the state of the art (CQCC) with a half total error rate of 0.0002 %.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Alluri, K. N. R. K. Raju; Achanta, Sivanand; Kadiri, Sudarsana Reddy;
   Gangashetty, Suryakanth V.; Vuppala, Anil Kumar] Int Inst Informat
   Technol, KCIS, Speech Proc Lab, Hyderabad, Telangana, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Alluri, KNRKR (reprint author), Int Inst Informat Technol, KCIS, Speech Proc Lab, Hyderabad, Telangana, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>raju.alluri@research.iiit.ac.in; sivanand.a@research.iiit.ac.in;
   sudarsanareddy.kadiri@research.iiit.ac.in; svg@iiit.ac.in;
   anil.vuppala@iiit.ac.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>2596</td>
</tr>

<tr>
<td valign="top">EP </td><td>2600</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2017-256</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000457505000539</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sailor, HB
   <br>Kamble, MR
   <br>Patil, HA</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sailor, Hardik B.
   <br>Kamble, Madhu R.
   <br>Patil, Hemant A.</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Unsupervised Representation Learning Using Convolutional Restricted
   Boltzmann Machine for Spoof Speech Detection</td>
</tr>

<tr>
<td valign="top">SO </td><td>18TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2017), VOLS 1-6: SITUATED INTERACTION</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>18th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2017)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 20-24, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Stockholm, SWEDEN</td>
</tr>

<tr>
<td valign="top">DE </td><td>Automatic Speaker Verification; spoofing; countermeasures; ConvRBM;
   filterbank</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEAKER VERIFICATION; COUNTERMEASURES</td>
</tr>

<tr>
<td valign="top">AB </td><td>Speech Synthesis (SS) and Voice Conversion (VC) presents a genuine risk of attacks for Automatic Speaker Verification (ASV) technology. In this paper, we use our recently proposed unsupervised filterbank learning technique using Convolutional Restricted Boltzmann Machine (ConvRBM) as a front-end feature representation. ConvRBM is trained on training subset of ASV spoof 2015 challenge database. Analyzing the filterbank trained on this dataset shows that ConvRBM learned more low-frequency subband filters compared to training on natural speech database such as TIMIT. The spoofing detection experiments were performed using Gaussian Mixture Models (GMM) as a back-end classifier. ConvRBM-based cepstral coefficients (ConvRBM-CC) perform better than hand crafted Mel Frequency Cepstral Coefficients (MFCC). On the evaluation set. ConvRBM-CC features give an absolute reduction of 4.76 % in Equal Error Rate (EER) compared to MFCC features. Specifically, ConvRBM-CC features significantly perform better in both known attacks (1.93 %) and unknown attacks (5.87 %) compared to MFCC features.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Sailor, Hardik B.; Kamble, Madhu R.; Patil, Hemant A.] DA IICT, Speech
   Res Lab, Gandhinagar 382007, Gujarat, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sailor, HB (reprint author), DA IICT, Speech Res Lab, Gandhinagar 382007, Gujarat, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>sailor_hardik@daiict.ac.in; madhu_kamble@daiict.ac.in;
   hemant_patil@daiict.ac.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>2601</td>
</tr>

<tr>
<td valign="top">EP </td><td>2605</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2017-1393</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000457505000540</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Lopez-Otero, P
   <br>Docio-Fernandez, L
   <br>Garcia-Mateo, C</td>
</tr>

<tr>
<td valign="top">AF </td><td>Lopez-Otero, Paula
   <br>Docio-Fernandez, Laura
   <br>Garcia-Mateo, Carmen</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Compensating Gender Variability in Query-by-Example Search on Speech
   using Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>18TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2017), VOLS 1-6: SITUATED INTERACTION</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>18th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2017)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 20-24, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Stockholm, SWEDEN</td>
</tr>

<tr>
<td valign="top">DE </td><td>query-by-example search on speech; dynamic time warping; voice
   conversion; variability compensation</td>
</tr>

<tr>
<td valign="top">AB </td><td>The huge amount of available spoken documents has raised the need for tools to perform automatic searches within large audio databases. These collections usually consist of documents with a great variability regarding speaker, language or recording channel, among others. Reducing this variability would boost the performance of query-by-example search on speech systems, especially in zero-resource systems that use acoustic features for audio representation. Hence, in this work. a technique to compensate the variability caused by speaker gender is proposed. Given a data collection composed of documents spoken by both male and female voices, every time a spoken query has to be searched, an alternative version of the query on its opposite gender is generated using voice conversion. After that. the female version of the query is used to search within documents spoken by females and vice versa. Experimental validation of the proposed strategy shows an improvement of search on speech performance caused by the reduction of gender variability.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Lopez-Otero, Paula; Docio-Fernandez, Laura; Garcia-Mateo, Carmen] Univ
   Vigo, AtlantTIC Res Ctr, Multimedia Technol Grp, Vigo, Spain.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Lopez-Otero, P (reprint author), Univ Vigo, AtlantTIC Res Ctr, Multimedia Technol Grp, Vigo, Spain.</td>
</tr>

<tr>
<td valign="top">EM </td><td>plopez@gts.uvigo.es; ldocio@gts.uvigo.es; carmen@gts.uvigo.es</td>
</tr>

<tr xmlns:date="http://exslt.org/dates-and-times">
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Docio-Fernandez, Laura</display_name>&nbsp;</font></td><td><font size="3">D-3189-2018&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Docio-Fernandez, Laura</display_name>&nbsp;</font></td><td><font size="3">0000-0003-3838-2406&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>2909</td>
</tr>

<tr>
<td valign="top">EP </td><td>2913</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2017-1183</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000457505000602</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hsu, CC
   <br>Hwang, HT
   <br>Wu, YC
   <br>Tsao, Y
   <br>Wang, HM</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hsu, Chin-Cheng
   <br>Hwang, Hsin-Te
   <br>Wu, Yi-Chiao
   <br>Tsao, Yu
   <br>Wang, Hsin-Min</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Conversion from Unaligned Corpora using Variational Autoencoding
   Wasserstein Generative Adversarial Networks</td>
</tr>

<tr>
<td valign="top">SO </td><td>18TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2017), VOLS 1-6: SITUATED INTERACTION</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>18th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2017)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 20-24, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Stockholm, SWEDEN</td>
</tr>

<tr>
<td valign="top">DE </td><td>non-parallel voice conversion; Wasserstein generative; adversarial
   network; GAN; variational autoencoder; VAE</td>
</tr>

<tr>
<td valign="top">AB </td><td>Building a voice conversion (VC) system from non-parallel speech corpora is challenging but highly valuable in real application scenarios. In most situations, the source and the target speakers do not repeat the same texts or they may even speak different languages. In this case, one possible, although indirect, solution is to build a generative model for speech. Generative models focus on explaining the observations with latent variables instead of learning a pairwise transformation function, thereby bypassing the requirement of speech frame alignment. In this paper, we propose a non-parallel VC framework with a variational autoencoding Wasserstein generative adversarial network (VAW-GAN) that explicitly considers a VC objective when building the speech model. Experimental results corroborate the capability of our framework for building a VC system from unaligned data, and demonstrate improved conversion quality.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Hsu, Chin-Cheng; Hwang, Hsin-Te; Wu, Yi-Chiao; Wang, Hsin-Min] Acad
   Sinica, Inst Informat Sci, Taipei, Taiwan.
   <br>[Tsao, Yu] Acad Sinica, Res Ctr Informat Technol Innovat, Taipei, Taiwan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hsu, CC (reprint author), Acad Sinica, Inst Informat Sci, Taipei, Taiwan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jeremyechsu@iis.sinica.edu.tw; hwanght@iis.sinica.edu.tw;
   tedwu@iis.sinica.edu.tw; yu.tsao@citi.sinica.edu.tw;
   whm@iis.sinica.edu.tw</td>
</tr>

<tr>
<td valign="top">TC </td><td>8</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>8</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>3364</td>
</tr>

<tr>
<td valign="top">EP </td><td>3368</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2017-63</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000457505000694</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nakashika, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nakashika, Toru</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>CAB: An Energy-Based Speaker Clustering Model for Rapid Adaptation in
   Non-Parallel Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>18TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2017), VOLS 1-6: SITUATED INTERACTION</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>18th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2017)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 20-24, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Stockholm, SWEDEN</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; non-parallel training; cluster adaptive training
   (CAT); rapid adaptation; energy-based model</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, a new energy-based probabilistic model, called CAB (Cluster Adaptive restricted Boltzmann machine), is proposed for voice conversion (VC) that does not require parallel data during the training and requires only a small amount of speech data during the adaptation. Most of the existing VC methods require parallel data for training. Recently, VC methods that do not require parallel data (called non-parallel VCs) have been also proposed and are attracting much attention because they do not require prepared or recorded parallel speech data, unlike conventional approaches. The proposed CAB model is aimed at statistical non -parallel VC based on cluster adaptive training (CAT). This extends the VC method used in our previous model. ARBM (adaptive restricted Boltzmann machine). The ARBM approach assumes that any speech signals can be decomposed into speaker-invariant phonetic information and speaker-identity information using the ARBM adaptation matrices of each speaker. VC is achieved by switching the source speaker's identity into those of the target speaker while retaining the phonetic information obtained by decomposition of the source speaker's speech. In contrast, CAB speaker identities are represented as cluster vectors that determine the adaptation matrices. As the number of clusters is generally smaller than the number of speakers, the number of model parameters can be reduced compared to ARBM, which enables rapid adaptation of a new speaker. Our experimental results show that the proposed method especially performed better than the ARBM approach, particularly in adaptation.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nakashika, Toru] Univ Electrocommun, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nakashika, T (reprint author), Univ Electrocommun, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>nakashika@ec.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>3369</td>
</tr>

<tr>
<td valign="top">EP </td><td>3373</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2017-133</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000457505000695</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Aihara, R
   <br>Takiguchi, T
   <br>Ariki, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Aihara, Ryo
   <br>Takiguchi, Tetsuya
   <br>Ariki, Yasuo</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Phoneme-Discriminative Features for Dysarthric Speech Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>18TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2017), VOLS 1-6: SITUATED INTERACTION</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>18th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2017)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 20-24, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Stockholm, SWEDEN</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice Conversion; Speech Synthesis; Partial Least Square; Assistive
   Technology; Manifold Learning</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>We present in this paper a Voice Conversion (VC) method for a person with dysarthric resulting from athetoid cerebral palsy. VC is being widely researched in the field of speech processing because of increased interest in using such processing in applications such as personalized Text-To-Speech systems. A Gaussian Mixture Model (GMM)-based VC method has been widely researched and Partial Least Square (PLS)-based VC has been proposed to prevent the over-fitting problems associated with the GMM-based VC method. In this paper, we present phoneme-discriminative features, which are associated with PLS-based VC. Conventional VC methods do not consider the phonetic structure of spectral features although phonetic structures are important for speech analysis. Especially for dysarthric speech, their phonetic structures are difficult to discriminate and discriminative learning will improve the conversion accuracy. This paper employs discriminative manifold learning. Spectral features are projected into a subspace in which a near point with the same phoneme label is close to another and a near point with a different phoneme label is apart. Our proposed method was evaluated on dysarthric speaker conversion task which converts dysarthric voice into non-dysarthric speech.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Aihara, Ryo; Takiguchi, Tetsuya; Ariki, Yasuo] Kobe Univ, Grad Sch Syst
   Informat, 1-1 Rokkodai, Kobe, Hyogo, Japan.
   <br>[Aihara, Ryo] Mitsubishi Electr Corp, Informat Technol R&amp;D Ctr, Tokyo,
   Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Aihara, R (reprint author), Kobe Univ, Grad Sch Syst Informat, 1-1 Rokkodai, Kobe, Hyogo, Japan.; Aihara, R (reprint author), Mitsubishi Electr Corp, Informat Technol R&amp;D Ctr, Tokyo, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>aihara@me.cs.scitec.kobe-u.ac.jp; takigu@kobe-u.ac.jp;
   ariki@kobe-u.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>3374</td>
</tr>

<tr>
<td valign="top">EP </td><td>3378</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2017-664</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000457505000696</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wu, J
   <br>Huang, DY
   <br>Xie, L
   <br>Li, HZ</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wu, Jie
   <br>Huang, Dongyan
   <br>Xie, Lei
   <br>Li, Haizhou</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Denoising Recurrent Neural Network for Deep Bidirectional LSTM based
   Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>18TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2017), VOLS 1-6: SITUATED INTERACTION</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>18th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2017)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 20-24, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Stockholm, SWEDEN</td>
</tr>

<tr>
<td valign="top">DE </td><td>residual error; Gaussian noise; denoising; recurrent neural network;
   voice conversion</td>
</tr>

<tr>
<td valign="top">AB </td><td>The paper studies the post processing in deep bidirectional Long Short-Term Memory (DBLSTM) based voice conversion, where the statistical parameters are optimized to generate speech that exhibits similar properties to target speech. However, there always exists residual error between converted speech and target one. We reformulate the residual error problem as speech restoration, which aims to recover the target speech samples from the converted ones. Specifically, we propose a denoising recurrent neural network (DeRNN) by introducing regularization during training to shape the distribution of the converted data in latent space. We compare the proposed approach with global variance (GV), modulation spectrum (MS) and recurrent neural network (RNN) based postfilters, which serve a similar purpose. The subjective test results show that the proposed approach significantly outperforms these conventional approaches in terms of quality and similarity.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Wu, Jie; Xie, Lei] Northwestern Polytech Univ, Sch Comp Sci, Xian,
   Shaanxi, Peoples R China.
   <br>[Huang, Dongyan; Li, Haizhou] ASTAR, Inst Infocomm Res, Singapore,
   Singapore.
   <br>[Li, Haizhou] Natl Univ Singapore, Dept Elect &amp; Comp Engn, Singapore,
   Singapore.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Xie, L (reprint author), Northwestern Polytech Univ, Sch Comp Sci, Xian, Shaanxi, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jiewu@nwpu-aslp.org; huar.g@i2r.a-star.edu.sg; lxie@nwpu.edu.cn;
   eleliha@nus.edu.sg</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Li, Haizhou</display_name>&nbsp;</font></td><td><font size="3">Q-6438-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>3379</td>
</tr>

<tr>
<td valign="top">EP </td><td>3383</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interepeech.2017-694</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000457505000697</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Tanaka, K
   <br>Hara, S
   <br>Abe, M
   <br>Sato, M
   <br>Minagi, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Tanaka, Kei
   <br>Hara, Sunao
   <br>Abe, Masanobu
   <br>Sato, Masaaki
   <br>Minagi, Shogo</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Speaker Dependent Approach for Enhancing a Glossectomy Patient's Speech
   via GMM-based Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>18TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2017), VOLS 1-6: SITUATED INTERACTION</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>18th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2017)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 20-24, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Stockholm, SWEDEN</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; speech intelligibility; glossectomy</td>
</tr>

<tr>
<td valign="top">ID </td><td>PROSTHESES</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, using GMM-based voice conversion algorithm, we propose to generate speaker-dependent mapping functions to improve the intelligibility of speech uttered by patients with a wide glossectomy. The speaker-dependent approach enables to generate the mapping functions that reconstruct missing spectrum features of speech uttered by a patient without having influences of a speaker's factor. The proposed idea is simple, i.e., to collect speech uttered by a patient before and after the glossectomy, but in practice it is hard to ask patients to utter speech just for developing algorithms. To confirm the performance of the proposed approach, in this paper, in order to simulate glossectomy patients, we fabricated an intraoral appliance which covers lower dental arch and tongue surface to restrain tongue movements. In terms of the Mel-frequency cepstrum (MFC) distance, by applying the voice conversion, the distances were reduced by 25% and 42% for speaker dependent case and speaker-independent case, respectively. In terms of phoneme intelligibility, dictation tests revealed that speech reconstructed by speaker-dependent approach almost always showed better performance than the original speech uttered by simulated patients, while speaker-independent approach did not.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Tanaka, Kei; Hara, Sunao; Abe, Masanobu] Okayama Univ, Grad Sch Nat Sci
   &amp; Technol, Okayama, Japan.
   <br>[Sato, Masaaki; Minagi, Shogo] Okayama Univ, Grad Sch Med Dent &amp;
   Pharmaceut Sci, Okayama, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Tanaka, K (reprint author), Okayama Univ, Grad Sch Nat Sci &amp; Technol, Okayama, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>Piot7wfu@s.okayama-u.a.c.jp; hara@cs.okayama-u.ac.jp;
   abe@cs.okayama-u.ac.jp; sato.masaaki@s.okayama-u.ac.jp;
   minagi@md.okayama-u.ac.jp</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>ABE, Masanobu</display_name>&nbsp;</font></td><td><font size="3">B-2626-2011&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>ABE, Masanobu</display_name>&nbsp;</font></td><td><font size="3">0000-0002-3731-6354&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>3384</td>
</tr>

<tr>
<td valign="top">EP </td><td>3388</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2017-841</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000457505000698</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Luo, ZJ
   <br>Chen, JH
   <br>Takiguchi, T
   <br>Ariki, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Luo, Zhaojie
   <br>Chen, Jinhui
   <br>Takiguchi, Tetsuya
   <br>Ariki, Yasuo</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Emotional Voice Conversion with Adaptive Scales F0 based on Wavelet
   Transform using Limited Amount of Emotional Data</td>
</tr>

<tr>
<td valign="top">SO </td><td>18TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2017), VOLS 1-6: SITUATED INTERACTION</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>18th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2017)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 20-24, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Stockholm, SWEDEN</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; F0 features; emotion; continuous wavelet transform;
   deep learning</td>
</tr>

<tr>
<td valign="top">AB </td><td>Deep learning techniques have been successfully applied to speech processing. Typically, neural networks (NNs) are very effective in processing nonlinear features, such as mel cepstral coefficients (MCC), which represent the spectrum features in voice conversion (VC) tasks. Despite these successes, the approach is restricted to problems with moderate dimension and sufficient data. Thus, in emotional VC tasks, it is hard to deal with a simple representation of fundamental frequency (F0), which is the most important feature in emotional voice representation, Another problem is that there are insufficient emotional data for training. To deal with these two problems, in this paper, we propose the adaptive scales continuous wavelet transform (AS-CWT) method to systematically capture the F0 features of different temporal scales. which can represent different prosodic levels ranging from micro-prosody to sentence levels. Meanwhile. we also use the pre-trained conversion functions obtained from other emotional datasets to synthesize new emotional data as additional training samples for target emotional voice conversion. Experimental results indicate that our proposed method achieves the best performance in both objective and subjective evaluations.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Luo, Zhaojie; Chen, Jinhui; Takiguchi, Tetsuya; Ariki, Yasuo] Kobe
   Univ, Grad Sch Syst Informat, Kobe, Hyogo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Luo, ZJ (reprint author), Kobe Univ, Grad Sch Syst Informat, Kobe, Hyogo, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>luozhaojie@me.cs.scitec.kohe-u.ac.jp; ianchen@me.cs.scitec.kohe-u.ac.jp;
   takigu@kobe-u.ac.jp; ariki@kobe-u.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>3399</td>
</tr>

<tr>
<td valign="top">EP </td><td>3403</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2017-984</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000457505000701</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Li, RN
   <br>Wu, ZY
   <br>Ning, YS
   <br>Sun, LF
   <br>Meng, H
   <br>Cai, LH</td>
</tr>

<tr>
<td valign="top">AF </td><td>Li, Runnan
   <br>Wu, Zhiyong
   <br>Ning, Yishuang
   <br>Sun, Lifa
   <br>Meng, Helen
   <br>Cai, Lianhong</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Spectro-Temporal Modelling with Time-Frequency LSTM and Structured
   Output Layer for Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>18TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2017), VOLS 1-6: SITUATED INTERACTION</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>18th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2017)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 20-24, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Stockholm, SWEDEN</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; time-frequency long short term memory (TFLSTM);
   structured output layer (SOL)</td>
</tr>

<tr>
<td valign="top">ID </td><td>NETWORKS</td>
</tr>

<tr>
<td valign="top">AB </td><td>From speech, speaker identity can be, mostly characterized by the spectro-temporal structures of spectrum. Although recent researches have demonstrated the effectiveness of employing long short-term memory (LSTM) recurrent neural network (RNN) in voice conversion, traditional LSTM-RNN based approaches usually focus on temporal evolutions of speech features only. In this paper, we improve the conventional LSTM-RNN method for voice conversion by employing the two-dimensional time-frequency LSTM (TFLSTM) to model spectro-temporal warping along both time and frequency axes. A multi-task learned structured output layer (SOL) is afterward adopted to capture the dependencies between spectral and pitch parameters for further improvement, where spectral parameter targets are conditioned upon pitch parameters prediction. Experimental results show the proposed approach outperforms conventional systems in speech quality and speaker similarity.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Li, Runnan; Wu, Zhiyong; Ning, Yishuang; Meng, Helen; Cai, Lianhong]
   Tsinghua Univ, MJRC, Grad Sch Shenzhen, Beijing, Peoples R China.
   <br>[Wu, Zhiyong; Sun, Lifa; Meng, Helen] CUHK, Dept Syst Engn &amp; Engn
   Management, Hong Kong, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Li, RN (reprint author), Tsinghua Univ, MJRC, Grad Sch Shenzhen, Beijing, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>lirn15@mails.tsinghua.edu.cn; zywu@se.cuhk.edu.hk;
   ningys13@mails.tsinghua.edu.cn; lfsun@se.cuhk.edu.hk;
   hmmeng@se.cuhk.edu.hk; clh-dcs@tsinghua.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>3409</td>
</tr>

<tr>
<td valign="top">EP </td><td>3413</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2017-1122</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000457505000703</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ramos, MV
   <br>Black, AW
   <br>Astudillo, RF
   <br>Trancoso, I
   <br>Fonseca, N</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ramos, Miguel Varela
   <br>Black, Alan W.
   <br>Astudillo, Ramon Fernandez
   <br>Trancoso, Isabel
   <br>Fonseca, Nuno</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Segment Level Voice Conversion with Recurrent Neural Networks</td>
</tr>

<tr>
<td valign="top">SO </td><td>18TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2017), VOLS 1-6: SITUATED INTERACTION</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>18th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2017)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 20-24, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Stockholm, SWEDEN</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; recurrent neural networks; deep learning; spectral
   mapping</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion techniques aim to modify a subject's voice characteristics in order to mimic the one's of another person. Due to the difference in utterance length between source and target speaker, state of the art voice conversion systems often rely on a frame alignment pre-processing step. This step aligns the entire utterances with algorithms such as dynamic time warping (DTW) that introduce errors. hindering system performance. In this paper we present a new technique that avoids the alignment of entire utterances at frame level, while keeping the local context during training. For this purpose, we combine an RNN model with the use of phoneme or syllable level information, obtained from a speech recognition system. This system segments the utterances into segments which then can be grouped into overlapping windows, providing the needed context for the model to learn the temporal dependencies. We show that with this approach, notable improvements can be attained over a state of the art RNN voice conversion system on the CMU ARCTIC database. It is also worth noting that with this technique it is possible to halve the training data size and still outperform the baseline.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ramos, Miguel Varela; Astudillo, Ramon Fernandez; Trancoso, Isabel]
   INESC ID Lisboa, Spoken Languages Syst Lab, Lisbon, Portugal.
   <br>[Black, Alan W.] Carnegie Mellon Univ, Language Technol Inst,
   Pittsburgh, PA 15213 USA.
   <br>[Trancoso, Isabel] Inst Super Tecn, Lisbon, Portugal.
   <br>[Fonseca, Nuno] Polytech Inst Leiria, ESTG, Leiria, Portugal.
   <br>[Astudillo, Ramon Fernandez] Unbabel Lda, Lisbon, Portugal.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ramos, MV (reprint author), INESC ID Lisboa, Spoken Languages Syst Lab, Lisbon, Portugal.</td>
</tr>

<tr>
<td valign="top">EM </td><td>miguelvramos@ist.utl.pt; awb@cs.cmu.edu; ramon@astudillo.com;
   isabel.trancoso@inesc-id.pt; nuno.fonseca@ipleiria.pt</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>3414</td>
</tr>

<tr>
<td valign="top">EP </td><td>3418</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2017-1538</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000457505000704</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Adhikary, P
   <br>Biswas, A
   <br>Mandal, D</td>
</tr>

<tr>
<td valign="top">AF </td><td>Adhikary, Prakriti
   <br>Biswas, Anirban
   <br>Mandal, Dipankar</td>
</tr>

<tr>
<td valign="top">TI </td><td>Improved sensitivity of wearable nanogenerators made of electrospun Eu3+
   doped P(VDF-HFP)/graphene composite nanofibers for self-powered voice
   recognition</td>
</tr>

<tr>
<td valign="top">SO </td><td>NANOTECHNOLOGY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Eu3+ doped P(VDF-HFP)/graphene nanofiber; acoustic energy harvester;
   voice recognition; beta-phase; wearable piezoelectric nanogenerator</td>
</tr>

<tr>
<td valign="top">ID </td><td>ENERGY HARVESTER; COPOLYMER FILMS; GRAPHENE; PERFORMANCE; FABRICATION;
   SENSORS; FIBERS; CONVERSION; SKIN</td>
</tr>

<tr>
<td valign="top">AB </td><td>Composite nanofibers of Eu3+ doped poly(vinylidene fluoride-co-hexafluoropropylene) (P(VDF-HFP))/graphene are prepared by the electrospinning technique for the fabrication of ultrasensitive wearable piezoelectric nanogenerators (WPNGs) where the post-poling technique is not necessary. It is found that the complete conversion of the piezoelectric beta-phase and the improvement of the degree of crystallinity is governed by the incorporation of Eu3+ and graphene sheets into P(VDF-HFP) nanofibers. The flexible nanocomposite fibers are associated with. a hypersensitive electronic transition that results. in. an intense red light emission, and WPNGs also have the capability of detecting external pressure as low as similar to 23 Pa with a higher degree of acoustic sensitivity, similar to 11 V Pa-1,. than has. ever. been previously reported. This means that. ultrasensitive WPNGs can be utilized to recognize human voices, which suggests they could be a potential tool in the biomedical and national security sectors. The capacitor's. ability to charge. from abundant environmental vibrations, such as music, wind, body motion, etc, drives WPNGs as a power source for portable electronics. This fact may open up the prospect of using the. Eu3+ doped P(VDF-HFP)/graphene composite electrospun nanofibers, with their multifunctional properties such as vibration sensitivity, wearability, red light emission capability and piezoelectric energy harvesting, for various promising applications in portable electronics, health care monitoring, noise detection. and security monitoring.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Adhikary, Prakriti; Biswas, Anirban; Mandal, Dipankar] Jadavpur Univ,
   Dept Phys, Organ Nanopiezoelect Device Lab, Kolkata 700032, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Mandal, D (reprint author), Jadavpur Univ, Dept Phys, Organ Nanopiezoelect Device Lab, Kolkata 700032, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>dipankar@phys.jdvu.ac.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>5</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>5</td>
</tr>

<tr>
<td valign="top">PD </td><td>DEC 9</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>27</td>
</tr>

<tr>
<td valign="top">IS </td><td>49</td>
</tr>

<tr>
<td valign="top">AR </td><td>495501</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1088/0957-4484/27/49/495501</td>
</tr>

<tr>
<td valign="top">SC </td><td>Science &amp; Technology - Other Topics; Materials Science; Physics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000412633300001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nirmal, J
   <br>Zaveri, M
   <br>Patnaik, S
   <br>Kachare, P</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nirmal, Jagannath
   <br>Zaveri, Mukesh
   <br>Patnaik, Suprava
   <br>Kachare, Pramod</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice conversion system using salient sub-bands and radial basis
   function</td>
</tr>

<tr>
<td valign="top">SO </td><td>NEURAL COMPUTING &amp; APPLICATIONS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Discrete wavelet transforms; Dynamic time warping; Wavelet packet
   transform; Radial basis function; Sub-bands; Voice conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>ARTIFICIAL NEURAL-NETWORKS; TRANSFORMATION; FEATURES; INDIVIDUALITY</td>
</tr>

<tr>
<td valign="top">AB </td><td>The objective of voice conversion is to replace the speaker-dependent characteristics of the source speaker so that it is perceptually similar to that of the target speaker. The speaker-dependent spectral parameters are characterized using single-scale interpolation techniques such as linear predictive coefficients, formant frequencies, mel cepstrum envelope and line spectral frequencies. These features provide a good approximation of the vocal tract, but produce artifacts at the frame boundaries which result in inaccurate parameter estimation and distortion in re-synthesis of the speech signal. This paper presents a novel approach of voice conversion based on multi-scale wavelet packet transform in the framework of radial basis neural network. The basic idea is to split the signal acoustic space into different salient frequency sub-bands, which are finely tuned to capture the speaker identity, conveyed by the speech signal. Characteristics of different wavelet filters are studied to determine the best filter for the proposed voice conversion system. A relative performance of the proposed algorithm is compared with the state-of-the-art wavelet-based voice morphing using various subjective and objective measures. The results reveal that the proposed algorithm performs better than the conventional wavelet-based voice morphing.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nirmal, Jagannath] KJSCE, Dept Elect Engn, Bombay, Maharashtra, India.
   <br>[Zaveri, Mukesh] SVNIT, Dept Comp Engn, Surat, India.
   <br>[Patnaik, Suprava] SVNIT, Dept Elect Engn, Surat, India.
   <br>[Kachare, Pramod] VJTI, Dept Elect Engn, Bombay, Maharashtra, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nirmal, J (reprint author), KJSCE, Dept Elect Engn, Bombay, Maharashtra, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jhnirmal1975@gmail.com; mazaveri@coed.svnit.ac.in; ssp@eced.svnit.ac.in;
   pramod_1991@yahoo.com</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Patnaik, Suprava</display_name>&nbsp;</font></td><td><font size="3">0000-0002-7068-5960&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>DEC</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>27</td>
</tr>

<tr>
<td valign="top">IS </td><td>8</td>
</tr>

<tr>
<td valign="top">SI </td><td>SI</td>
</tr>

<tr>
<td valign="top">BP </td><td>2615</td>
</tr>

<tr>
<td valign="top">EP </td><td>2628</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/s00521-015-2030-9</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000392418300034</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kim, SE
   <br>Kim, E</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kim, Se Eun
   <br>Kim, Eugene</td>
</tr>

<tr>
<td valign="top">TI </td><td>Local anesthesia with monitored anesthesia care for patients undergoing
   thyroidectomy -a case series-</td>
</tr>

<tr>
<td valign="top">SO </td><td>KOREAN JOURNAL OF ANESTHESIOLOGY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Local anesthesia; Monitored anesthesia care; Thyroidectomy</td>
</tr>

<tr>
<td valign="top">ID </td><td>REGIONAL ANESTHESIA; GENERAL-ANESTHESIA; SEDATION; SURGERY; PROPOFOL</td>
</tr>

<tr>
<td valign="top">AB </td><td>Because the current trend favors minimally invasive surgery for thyroid disease, increasing interest has developed for thyroidectomy under local anesthesia with monitored anesthesia care (MAC). Here, we retrospectively reviewed 18 cases of thyroidectomy performed under local anesthesia with MAC in a single center. All of the procedures were performed by a single surgeon, using local lidocaine infiltration around the incisional site and propofol plus remifentanil target-controlled infusion. Sore throat (4/18), hypocalcemia (1/18), and transient voice color change (1/18) were observed, but the patients recovered during the follow-up period. No cases of postoperative nausea and vomiting, hematoma, wound problems, or vocal cord paralysis were observed. Local anesthesia with MAC provided satisfactory sedation in most patients without conversion to general anesthesia.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kim, Se Eun; Kim, Eugene] Seoul Natl Univ Hosp, Dept Anesthesiol &amp; Pain
   Med, 101 Daehak Ro, Seoul 03080, South Korea.
   <br>[Kim, Eugene] Catholic Univ Daegu, Catholic Univ Hosp Daegu, Dept
   Anesthesiol &amp; Pain Med, Sch Med, Daegu, South Korea.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kim, E (reprint author), Seoul Natl Univ Hosp, Dept Anesthesiol &amp; Pain Med, 101 Daehak Ro, Seoul 03080, South Korea.; Kim, E (reprint author), Catholic Univ Daegu, Catholic Univ Hosp Daegu, Dept Anesthesiol &amp; Pain Med, Sch Med, Daegu, South Korea.</td>
</tr>

<tr>
<td valign="top">EM </td><td>tomomie7@hotmail.com</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Kim, Eugene</display_name>&nbsp;</font></td><td><font size="3">E-2446-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Kim, Eugene</display_name>&nbsp;</font></td><td><font size="3">0000-0002-1926-4191&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PD </td><td>DEC</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>69</td>
</tr>

<tr>
<td valign="top">IS </td><td>6</td>
</tr>

<tr>
<td valign="top">BP </td><td>635</td>
</tr>

<tr>
<td valign="top">EP </td><td>639</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.4097/kjae.2016.69.6.635</td>
</tr>

<tr>
<td valign="top">SC </td><td>Anesthesiology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000390243400017</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hanilci, C
   <br>Kinnunen, T
   <br>Sahidullah, M
   <br>Sizov, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hanilci, Cemal
   <br>Kinnunen, Tomi
   <br>Sahidullah, Md
   <br>Sizov, Aleksandr</td>
</tr>

<tr>
<td valign="top">TI </td><td>Spoofing detection goes noisy: An analysis of synthetic speech detection
   in the presence of additive noise</td>
</tr>

<tr>
<td valign="top">SO </td><td>SPEECH COMMUNICATION</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speaker recognition; Anti spoofing; Countermeasures; Additive noise</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEAKER VERIFICATION; ENHANCEMENT; RECOGNITION; IDENTIFICATION;
   TRANSFORMATION; BIOMETRICS; DATABASE</td>
</tr>

<tr>
<td valign="top">AB </td><td>Automatic speaker verification (ASV) technology is recently finding its way to end-user applications for secure access to personal data, smart services or physical facilities. Similar to other bioinatric technologies, speaker verification is vulnerable to spoofing attacks where an attacker masquerades as a particular target speaker via impersonation, replay, text-to-speech (TTS) or voice conversion (VC) techniques to gain illegitimate access to the system. We focus on TTS and VC that represent the most flexible, high-end spoofing attacks. Most of the prior studies on synthesized or converted speech detection report their findings using high-quality clean recordings. Meanwhile, the performance of spoofing detectors in the presence of additive noise, an important consideration in practical ASV implementations, remains largely unknown. To this end, our study provides a comparative analysis of existing state-of-the-art, off-the-shelf synthetic speech detectors under additive noise contamination with a special focus on front-end processing that has been found critical. Our comparison includes eight acoustic feature sets, five related to spectral magnitude and three to spectral phase information. All the methods contain a number of internal control parameters. Except for feature post-processing steps (deltas and cepstral mean normalization) that we optimized for each method, we fix the internal control parameters to their default values based on literature, and compare all the variants using the exact same dimensionality and back-end system. In addition to the eight feature sets, we consider two alternative classifier back-ends: Gaussian mixture model (GMM) and i-vector, the latter with both cosine scoring and probabilistic linear discriminant analysis (PLDA) scoring. Our extensive analysis on the recent ASVspoof 2015 challenge provides new insights to the robustness of the spoofing detectors. Firstly, unlike in most other speech processing tasks, all the compared spoofing detectors break down even at relatively high signal-to-noise ratios (SNRs) and fail to generalize to noisy conditions even if performing excellently on clean data. This indicates both difficulty of the task, as well as potential to over-fit the methods easily. Secondly, speech enhancement preprocessing is not found helpful. Thirdly, GMM back-end generally outperforms the more involved i-vector back-end. Fourthly, concerning the compared features, the Mel-frequency cepstral coefficient (MFCC) and subband spectral centroid magnitude coefficient (SCMC) features perform the best on average though the winner method depends on SNR and noise type. Finally, a study with two score fusion strategies shows that combining different feature based systems improves recognition accuracy for known and unknown attacks in both clean and noisy conditions. In particular, simple score averaging fusion, as opposed to weighted fusion with logistic loss weight optimization, was found to work better, on average. For clean speech, it provides 88% and 28% relative improvements over the best standalone features for known and unknown spoofing techniques, respectively. If we consider the best score fusion of just two features, then RPS serves as a complementary agent to one of the magnitude features. To sum up, our study reveals a significant gap between the performance of state-of-the-art spoofing detectors between clean and noisy conditions. (C) 2016 Elsevier B.V. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Hanilci, Cemal; Kinnunen, Tomi; Sahidullah, Md; Sizov, Aleksandr] Univ
   Eastern Finland, Sch Comp, Joensuu, Finland.
   <br>[Hanilci, Cemal] Bursa Tech Univ, Dept Elect Elect Engn, Bursa, Turkey.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hanilci, C (reprint author), Univ Eastern Finland, Sch Comp, Joensuu, Finland.; Hanilci, C (reprint author), Bursa Tech Univ, Dept Elect Elect Engn, Bursa, Turkey.</td>
</tr>

<tr>
<td valign="top">EM </td><td>cemal.hanilci@btu.edu.tr; tkinnu@uef.fi; sahid@uef.fi; sizov@uef.fi</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Hanilci, Cemal</display_name>&nbsp;</font></td><td><font size="3">S-4967-2016&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Sahidullah, Md</display_name>&nbsp;</font></td><td><font size="3">0000-0002-0624-2903&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>11</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>11</td>
</tr>

<tr>
<td valign="top">PD </td><td>DEC</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>85</td>
</tr>

<tr>
<td valign="top">BP </td><td>83</td>
</tr>

<tr>
<td valign="top">EP </td><td>97</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.specom.2016.10.002</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000390507000008</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Silva, S
   <br>Reis, A
   <br>Casaca, L
   <br>Petersson, KM
   <br>Faisca, L</td>
</tr>

<tr>
<td valign="top">AF </td><td>Silva, Susana
   <br>Reis, Alexandra
   <br>Casaca, Luis
   <br>Petersson, Karl M.
   <br>Faisca, Luis</td>
</tr>

<tr>
<td valign="top">TI </td><td>When the Eyes No Longer Lead: Familiarity and Length Effects on
   Eye-Voice Span</td>
</tr>

<tr>
<td valign="top">SO </td><td>FRONTIERS IN PSYCHOLOGY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>eye-voice span; eye-tracking; reading aloud; dual-route; sublexical
   processing</td>
</tr>

<tr>
<td valign="top">ID </td><td>READING ALOUD; DEVELOPMENTAL DYSLEXIA; MOVEMENT CONTROL; WORD;
   AUTOMATICITY; FLUENCY; MODEL; TRACKING; CHILDREN; READERS</td>
</tr>

<tr>
<td valign="top">AB </td><td>During oral reading, the eyes tend to be ahead of the voice (eye-voice span, EVS). It has been hypothesized that the extent to which this happens depends on the automaticity of reading processes, namely on the speed of print-to-sound conversion. We tested whether EVS is affected by another automaticity component - immunity from interference. To that end, we manipulated word familiarity (high-frequency, low-frequency, and pseudowords, PW) and word length as proxies of immunity from interference, and we used linear mixed effects models to measure the effects of both variables on the time interval at which readers do parallel processing by gazing at word N + 1 while not having articulated word N yet (offset EVS). Parallel processing was enhanced by automaticity, as shown by familiarity x length interactions on offset EVS, and it was impeded by lack of automaticity, as shown by the transformation of offset EVS into voice eye span (voice ahead of the offset of the eyes) in PWs. The relation between parallel processing and automaticity was strengthened by the fact that offset EVS predicted reading velocity. Our findings contribute to understand how the offset EVS, an index that is obtained in oral reading, may tap into different components of automaticity that underlie reading ability, oral or silent. In addition, we compared the duration of the offset EVS with the average reference duration of stages in word production, and we saw that the offset EVS may accommodate for more than the articulatory programming stage of word N.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Silva, Susana] Univ Porto, Ctr Psychol, Oporto, Portugal.
   <br>[Silva, Susana; Reis, Alexandra; Casaca, Luis; Petersson, Karl M.;
   Faisca, Luis] Univ Algarve, Ctr Biomed Res, Faro, Portugal.
   <br>[Petersson, Karl M.] Max Planck Inst Psycholinguist, Neurobiol Language,
   Nijmegen, Netherlands.
   <br>[Petersson, Karl M.] Donders Inst Brain Cognit &amp; Behav, Language
   Interact, Ngmegen, Netherlands.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Silva, S (reprint author), Univ Porto, Ctr Psychol, Oporto, Portugal.; Silva, S (reprint author), Univ Algarve, Ctr Biomed Res, Faro, Portugal.</td>
</tr>

<tr>
<td valign="top">EM </td><td>zanasilva@gmail.com</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Reis, Alexandra</display_name>&nbsp;</font></td><td><font size="3">A-3888-2013&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Faisca, Luis</display_name>&nbsp;</font></td><td><font size="3">A-4633-2013&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Petersson, Karl Magnus</display_name>&nbsp;</font></td><td><font size="3">E-8188-2012&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Reis, Alexandra</display_name>&nbsp;</font></td><td><font size="3">0000-0001-5598-0999&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Faisca, Luis</display_name>&nbsp;</font></td><td><font size="3">0000-0003-4859-8817&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Petersson, Karl Magnus</display_name>&nbsp;</font></td><td><font size="3">0000-0002-8245-0392&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Silva, Susana</display_name>&nbsp;</font></td><td><font size="3">0000-0003-2240-1828&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PD </td><td>NOV 2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>7</td>
</tr>

<tr>
<td valign="top">AR </td><td>1720</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.3389/fpsyg.2016.01720</td>
</tr>

<tr>
<td valign="top">SC </td><td>Psychology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000386724900001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kobayashi, K
   <br>Toda, T
   <br>Nakano, T
   <br>Goto, M
   <br>Nakamura, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kobayashi, Kazuhiro
   <br>Toda, Tomoki
   <br>Nakano, Tomoyasu
   <br>Goto, Masataka
   <br>Nakamura, Satoshi</td>
</tr>

<tr>
<td valign="top">TI </td><td>Improvements of Voice Timbre Control Based on Perceived Age in Singing
   Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEICE TRANSACTIONS ON INFORMATION AND SYSTEMS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>statistical singing voice conversion; perceived age; gender-dependent
   modeling; direct waveform modification; unsupervised adaptation</td>
</tr>

<tr>
<td valign="top">ID </td><td>PLUS NOISE MODEL; SPEECH SYNTHESIS</td>
</tr>

<tr>
<td valign="top">AB </td><td>As one of the techniques enabling individual singers to produce the varieties of voice timbre beyond their own physical constraints, a statistical voice timbre control technique based on the perceived age has been developed. In this technique, the perceived age of a singing voice, which is the age of the singer as perceived by the listener, is used as one of the intuitively understandable measures to describe voice characteristics of the singing voice. The use of statistical voice conversion (SVC) with a singer-dependent multiple-regression Gaussian mixture model (MR-GMM), which effectively models the voice timbre variations caused by a change of the perceived age, makes it possible for individual singers to manipulate the perceived ages of their own singing voices while retaining their own singer identities. However, there still remain several issues; e.g., 1) a controllable range of the perceived age is limited; 2) quality of the converted singing voice is significantly degraded compared to that of a natural singing voice; and 3) each singer needs to sing the same phrase set as sung by a reference singer to develop the singer-dependent MR-GMM. To address these issues, we propose the following three methods; 1) a method using gender-dependent modeling to expand the controllable range of the perceived age; 2) a method using direct waveform modification based on spectrum differential to improve quality of the converted singing voice; and 3) a rapid unsupervised adaptation method based on maximum a posteriori (MAP) estimation to easily develop the singer-dependent MR-GMM. The experimental results show that the proposed methods achieve a wider controllable range of the perceived age, a significant quality improvement of the converted singing voice, and the development of the singer-dependnet MR-GMM using only a few arbitrary phrases as adaptation data.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kobayashi, Kazuhiro; Nakamura, Satoshi] Nara Inst Sci &amp; Technol NAIST,
   Ikoma 6300192, Japan.
   <br>[Toda, Tomoki] Nagoya Univ, Ctr Informat Technol, Nagoya, Aichi 4648601,
   Japan.
   <br>[Nakano, Tomoyasu; Goto, Masataka] Natl Inst Adv Ind Sci &amp; Technol,
   Tsukuba, Ibaraki 3058568, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kobayashi, K (reprint author), Nara Inst Sci &amp; Technol NAIST, Ikoma 6300192, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>kazuhiro-k@is.naist.jp</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Goto, Masataka</display_name>&nbsp;</font></td><td><font size="3">K-8205-2012&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Nakano, Tomoyasu</display_name>&nbsp;</font></td><td><font size="3">A-8670-2013&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Goto, Masataka</display_name>&nbsp;</font></td><td><font size="3">0000-0003-1167-0977&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Nakano, Tomoyasu</display_name>&nbsp;</font></td><td><font size="3">0000-0001-8014-2209&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>NOV</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>E99D</td>
</tr>

<tr>
<td valign="top">IS </td><td>11</td>
</tr>

<tr>
<td valign="top">BP </td><td>2767</td>
</tr>

<tr>
<td valign="top">EP </td><td>2777</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1587/transinf.2016EDP7234</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000389692000010</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nakashika, T
   <br>Takiguchi, T
   <br>Minami, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nakashika, Toru
   <br>Takiguchi, Tetsuya
   <br>Minami, Yasuhiro</td>
</tr>

<tr>
<td valign="top">TI </td><td>Non-Parallel Training in Voice Conversion Using an Adaptive Restricted
   Boltzmann Machine</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Restricted Boltzmann machine; speaker adaptation; unsupervised training;
   voice conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>NEURAL-NETWORKS; SPEECH; TRANSFORMATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we present a voice conversion (VC) method that does not use any parallel data while training the model. VC is a technique where only speaker-specific information in source speech is converted while keeping the phonological information unchanged. Most of the existing VC methods rely on parallel data-pairs of speech data from the source and target speakers uttering the same sentences. However, the use of parallel data in training causes several problems: 1) the data used for the training are limited to the predefined sentences, 2) the trained model is only applied to the speaker pair used in the training, and 3) mismatches in alignment may occur. Although it is, thus, fairly preferable in VC not to use parallel data, a nonparallel approach is considered difficult to learn. In our approach, we achieve nonparallel training based on a speaker adaptation technique and capturing latent phonological information. This approach assumes that speech signals are produced from a restricted Boltzmann machine-based probabilistic model, where phonological information and speaker-related information are defined explicitly. Speaker-independent and speaker-dependent parameters are simultaneously trained under speaker adaptive training. In the conversion stage, a given speech signal is decomposed into phonological and speaker-related information, the speaker-related information is replaced with that of the desired speaker, and then voice-converted speech is obtained by mixing the two. Our experimental results showed that our approach outperformed another nonparallel approach, and produced results similar to those of the popular conventional Gaussian mixture models-based method that used parallel data in subjective and objective criteria.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nakashika, Toru; Minami, Yasuhiro] Univ Electrocommun, Grad Sch
   Informat Syst, Tokyo 1828585, Japan.
   <br>[Takiguchi, Tetsuya] Kobe Univ, Org Adv Sci &amp; Technol, Kobe, Hyogo
   6578501, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nakashika, T (reprint author), Univ Electrocommun, Grad Sch Informat Syst, Tokyo 1828585, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>nakashika@uec.ac.jp; takigu@kobe-u.ac.jp; minami.yasuhiro@is.uec.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>17</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>17</td>
</tr>

<tr>
<td valign="top">PD </td><td>NOV</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>24</td>
</tr>

<tr>
<td valign="top">IS </td><td>11</td>
</tr>

<tr>
<td valign="top">BP </td><td>2032</td>
</tr>

<tr>
<td valign="top">EP </td><td>2045</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASLP.2016.2593263</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000382677800013</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Takamichi, S
   <br>Toda, T
   <br>Neubig, G
   <br>Sakti, S
   <br>Nakamura, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Takamichi, Shinnosuke
   <br>Toda, Tomoki
   <br>Neubig, Graham
   <br>Sakti, Sakriani
   <br>Nakamura, Satoshi</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Statistical Sample-Based Approach to GMM-Based Voice Conversion Using
   Tied-Covariance Acoustic Models</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEICE TRANSACTIONS ON INFORMATION AND SYSTEMS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>GMM-based voice conversion; sample-based speech synthesis; speech
   parameter conversion; rich context model</td>
</tr>

<tr>
<td valign="top">ID </td><td>SEGMENT SELECTION; SPEECH</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a novel statistical sample-based approach for Gaussian MixtureModel (GMM)-based Voice Conversion (VC). Although GMM-based VC has the promising flexibility of model adaptation, quality in converted speech is significantly worse than that of natural speech. This paper addresses the problem of inaccurate modeling, which is one of the main reasons causing the quality degradation. Recently, we have proposed statistical sample-based speech synthesis using rich context models for high-quality and flexible Hidden Markov Model (HMM)-based Text-To-Speech (TTS) synthesis. This method makes it possible not only to produce high-quality speech by introducing ideas from unit selection synthesis, but also to preserve flexibility of the original HMM-based TTS. In this paper, we apply this idea to GMM-based VC. The rich context models are first trained for individual joint speech feature vectors, and then we gather them mixture by mixture to form a Rich context-GMM (R-GMM). In conversion, an iterative generation algorithm using R-GMMs is used to convert speech parameters, after initialization using over-trained probability distributions. Because the proposed method utilizes individual speech features, and its formulation is the same as that of conventional GMMbased VC, it makes it possible to produce high-quality speech while keeping flexibility of the original GMM-based VC. The experimental results demonstrate that the proposed method yields significant improvements in term of speech quality and speaker individuality in converted speech.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Takamichi, Shinnosuke; Neubig, Graham; Sakti, Sakriani; Nakamura,
   Satoshi] Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Ikoma 6300192,
   Japan.
   <br>[Toda, Tomoki] Nagoya Univ, Ctr Informat Technol, Nagoya, Aichi 4648601,
   Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Takamichi, S (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Ikoma 6300192, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>shinnosuketakamichi@ipc.i.u-tokyo.ac; tomoki@icts.nagoya-u.ac.jp;
   neubig@is.naist.jp; ssakti@is.naist.jp; s-nakamura@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>OCT</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>E99D</td>
</tr>

<tr>
<td valign="top">IS </td><td>10</td>
</tr>

<tr>
<td valign="top">BP </td><td>2490</td>
</tr>

<tr>
<td valign="top">EP </td><td>2498</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1587/transinf.2016SLP0020</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000388743500009</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Janicki, A
   <br>Alegre, F
   <br>Evans, N</td>
</tr>

<tr>
<td valign="top">AF </td><td>Janicki, Artur
   <br>Alegre, Federico
   <br>Evans, Nicholas</td>
</tr>

<tr>
<td valign="top">TI </td><td>An assessment of automatic speaker verification vulnerabilities to
   replay spoofing attacks</td>
</tr>

<tr>
<td valign="top">SO </td><td>SECURITY AND COMMUNICATION NETWORKS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>speaker verification; spoofing; presentation attack; replay;
   countermeasures; local binary patterns</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH SYNTHESIS; ALGORITHMS</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper analyses the threat of replay spoofing or presentation attacks in the context of automatic speaker verification. As relatively high-technology attacks, speech synthesis and voice conversion, which have thus far received far greater attention in the literature, are probably beyond the means of the average fraudster. The implementation of replay attacks, in contrast, requires no specific expertise nor sophisticated equipment. Replay attacks are thus likely to be the most prolific in practice, while their impact is relatively under-researched. The work presented here aims to compare at a high level the threat of replay attacks with those of speech synthesis and voice conversion. The comparison is performed using strictly controlled protocols and with six different automatic speaker verification systems including a state-of-the-art iVector/probabilistic linear discriminant analysis system. Experiments show that low-effort replay attacks present at least a comparable threat to speech synthesis and voice conversion. The paper also describes and assesses two replay attack countermeasures. A relatively new approach based on the local binary pattern analysis of speech spectrograms is shown to outperform a competing approach based on the detection of far-field recordings. Copyright (c) 2016 John Wiley &amp; Sons, Ltd.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Janicki, Artur] Warsaw Univ Technol, Warsaw, Poland.
   <br>[Alegre, Federico] ValidSoft, London, England.
   <br>[Alegre, Federico] EURECOM, Biot, France.
   <br>[Evans, Nicholas] Sophia Antipolis, EURECOM, Nice, France.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Janicki, A (reprint author), Warsaw Univ Technol, Inst Telecommun, Nowowiejska 15-19, PL-00665 Warsaw, Poland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>a.janicki@tele.pw.edu.pl</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Janicki, Artur</display_name>&nbsp;</font></td><td><font size="3">0000-0002-9937-4402&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>8</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>8</td>
</tr>

<tr>
<td valign="top">PD </td><td>OCT</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>9</td>
</tr>

<tr>
<td valign="top">IS </td><td>15</td>
</tr>

<tr>
<td valign="top">BP </td><td>3030</td>
</tr>

<tr>
<td valign="top">EP </td><td>3044</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1002/sec.1499</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000384530900037</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Gonzalez, JA
   <br>Cheah, LA
   <br>Gilbert, JM
   <br>Bai, J
   <br>Ell, SR
   <br>Green, PD
   <br>Moore, RK</td>
</tr>

<tr>
<td valign="top">AF </td><td>Gonzalez, Jose A.
   <br>Cheah, Lam A.
   <br>Gilbert, James M.
   <br>Bai, Jie
   <br>Ell, Stephen R.
   <br>Green, Phil D.
   <br>Moore, Roger K.</td>
</tr>

<tr>
<td valign="top">TI </td><td>A silent speech system based on permanent magnet articulography and
   direct synthesis</td>
</tr>

<tr>
<td valign="top">SO </td><td>COMPUTER SPEECH AND LANGUAGE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Silent speech interfaces; Speech rehabilitation; Speech synthesis;
   Permanent magnet articulography; Augmentative and alternative
   communication</td>
</tr>

<tr>
<td valign="top">ID </td><td>MAXIMUM-LIKELIHOOD-ESTIMATION; VOICE CONVERSION; VOCAL-TRACT;
   RECOGNITION; EXTRACTION</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper we present a silent speech interface (SSI) system aimed at restoring speech communication for individuals who have lost their voice due to laryngectomy or diseases affecting the vocal folds. In the proposed system, articulatory data captured from the lips and tongue using permanent magnet articulography (PMA) are converted into audible speech using a speaker-dependent transformation learned from simultaneous recordings of PMA and audio signals acquired before laryngectomy. The transformation is represented using a mixture of factor analysers, which is a generative model that allows us to efficiently model non-linear behaviour and perform dimensionality reduction at the same time. The learned transformation is then deployed during normal usage of the SSI to restore the acoustic speech signal associated with the captured PMA data. The proposed system is evaluated using objective quality measures and listening tests on two databases containing PMA and audio recordings for normal speakers. Results show that it is possible to reconstruct speech from articulator movements captured by an unobtrusive technique without an intermediate recognition step. The SSI is capable of producing speech of sufficient intelligibility and naturalness that the speaker is clearly identifiable, but problems remain in scaling up the process to function consistently for phonetically rich vocabularies. (C) 2016 Elsevier Ltd. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Gonzalez, Jose A.; Green, Phil D.; Moore, Roger K.] Univ Sheffield,
   Dept Comp Sci, Sheffield S10 2TN, S Yorkshire, England.
   <br>[Cheah, Lam A.; Gilbert, James M.; Bai, Jie] Univ Hull, Sch Engn,
   Kingston Upon Hull, Yorks, England.
   <br>[Ell, Stephen R.] Hull &amp; East Yorkshire Hosp Trust, Castle Hill Hosp,
   Cottingham, England.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Gonzalez, JA (reprint author), Univ Sheffield, Dept Comp Sci, Sheffield S10 2TN, S Yorkshire, England.</td>
</tr>

<tr>
<td valign="top">EM </td><td>j.gonzalez@sheffield.ac.uk</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Gonzalez Lopez, Jose Andres</display_name>&nbsp;</font></td><td><font size="3">0000-0002-5531-8994&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>14</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>14</td>
</tr>

<tr>
<td valign="top">PD </td><td>SEP</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>39</td>
</tr>

<tr>
<td valign="top">BP </td><td>67</td>
</tr>

<tr>
<td valign="top">EP </td><td>87</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.csl.2016.02.002</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000378180400004</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Aihara, R
   <br>Takiguchi, T
   <br>Ariki, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Aihara, Ryo
   <br>Takiguchi, Tetsuya
   <br>Ariki, Yasuo</td>
</tr>

<tr>
<td valign="top">TI </td><td>Multiple Non-Negative Matrix Factorization for Many-to-Many Voice
   Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; Speech synthesis; Exemplar-based; Many-to-many;
   Non-negative matrix factorization (NMF)</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPARSE REPRESENTATION; ALGORITHMS</td>
</tr>

<tr>
<td valign="top">AB </td><td>A novel voice conversion (VC) method for arbitrary speakers is proposed. Non-negative matrix factorization (NMF) has recently been applied to exemplar-based VC. It offers noise robustness and naturalness of the converted voice, compared with widely used Gaussian mixture model-based VC. However, because NMF-based VC requires parallel training data from source and target speakers, the voice of arbitrary speakers cannot be converted in this framework. In this study, we propose the multiple non-negative matrix factorization (Multi-NMF) to allow the implementation of many-to-many, exemplar-based VC. Our experimental results demonstrate that the conversion quality of the proposed method is close to that of conventional one-to-one VC, even though the proposed method requires neither the source speakers' spectra, nor the target speakers' spectra, to be included in the training set.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Aihara, Ryo] Kobe Univ, Grad Sch Syst Informat, Kobe, Hyogo 6578501,
   Japan.
   <br>[Takiguchi, Tetsuya; Ariki, Yasuo] Kobe Univ, Org Adv Sci &amp; Technol,
   Kobe, Hyogo 6578501, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Aihara, R (reprint author), Kobe Univ, Grad Sch Syst Informat, Kobe, Hyogo 6578501, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>aihara@me.cs.scitec.kobe-u.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>5</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>5</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUL</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>24</td>
</tr>

<tr>
<td valign="top">IS </td><td>7</td>
</tr>

<tr>
<td valign="top">BP </td><td>1175</td>
</tr>

<tr>
<td valign="top">EP </td><td>1184</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASLP.2016.2522643</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000393870800002</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Degottex, G
   <br>Ardaillon, L
   <br>Roebel, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Degottex, Gilles
   <br>Ardaillon, Luc
   <br>Roebel, Axel</td>
</tr>

<tr>
<td valign="top">TI </td><td>Multi-Frame Amplitude Envelope Estimation for Modification of Singing
   Voice</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Multi frame analysis; spectral envelope; singing voice; voice analysis
   and modeling; voice synthesis</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPECTRAL ENVELOPE; FREQUENCY; MODEL</td>
</tr>

<tr>
<td valign="top">AB </td><td>Singing voice synthesis benefits from very high quality estimation of the resonances and anti-resonances of the vocal tract filter (VTF), i.e., an amplitude spectral envelope. In the state of the art, a single frame of DFT transform is commonly used as a basis for building spectral envelopes. Even though multiple frame analysis (MFA) has already been suggested for envelope estimation, it is not yet used in concrete applications. Indeed, even though existing attempts have shown very interesting results, we will demonstrate that they are either over complicated or fail to satisfy the high accuracy that is necessary for singing voice. In order to allow future applications of MFA, this article aims to improve the theoretical understanding and advantages of MFA-based methods. The use of singing voice signals is very beneficial for studying MFA methods due to the fact that the VTF configuration can be relatively stable and, at the same time, the vibrato creates a regular variation that is easy to model. By simplifying and extending previous works, we also suggest and describe two MFA-based methods. To better understand the behaviors of the envelope estimates, we designed numerical measurements to assess single frame analysis and MFA methods using synthetic signals. With listening tests, we also designed two proofs of concept using pitch scaling and conversion of timbre. Both evaluations show clear and positive results for MFA-based methods, thus, encouraging this research direction for future applications.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Degottex, Gilles; Ardaillon, Luc; Roebel, Axel] Sorbonne Univ, UPMC,
   CNRS, UMR 9912,STMS,IRCAM, F-75004 Paris, France.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Degottex, G (reprint author), Sorbonne Univ, UPMC, CNRS, UMR 9912,STMS,IRCAM, F-75004 Paris, France.</td>
</tr>

<tr>
<td valign="top">EM </td><td>gilles.degottex@ircam.fr; luc.ardaillon@ircam.fr; Axel.Roebel@ircam.fr</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Degottex, Gilles</display_name>&nbsp;</font></td><td><font size="3">0000-0003-1346-9919&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUL</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>24</td>
</tr>

<tr>
<td valign="top">IS </td><td>7</td>
</tr>

<tr>
<td valign="top">BP </td><td>1242</td>
</tr>

<tr>
<td valign="top">EP </td><td>1254</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASLP.2016.2551863</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000393870800008</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Heldt, S
   <br>Budryte, P
   <br>Ingensiep, HW
   <br>Teichgraber, B
   <br>Schneider, U
   <br>Denecke, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Heldt, Sonja
   <br>Budryte, Paulina
   <br>Ingensiep, Hans Werner
   <br>Teichgraeber, Burkhard
   <br>Schneider, Ute
   <br>Denecke, Martin</td>
</tr>

<tr>
<td valign="top">TI </td><td>Social pitfalls for river restoration: How public participation uncovers
   problems with public acceptance</td>
</tr>

<tr>
<td valign="top">SO </td><td>ENVIRONMENTAL EARTH SCIENCES</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Emscher Conversion; EU-WFD; River basin management; River restoration;
   Acceptance; Public participation; Water infrastructure</td>
</tr>

<tr>
<td valign="top">ID </td><td>FRAMEWORK; MANAGEMENT; ETHICS</td>
</tr>

<tr>
<td valign="top">AB </td><td>As in several other infrastructure sectorshighly popular German examples are the protests concerning "Stuttgart 21" or Munich airport-the people's "new voice" is severely inhibiting the enforcement progress of common legislation in the water management sector, in particular the European Water Framework Directive (2000; EU-WFD). With the launch of the EU-WFD, the European Union is forcing serious changes in watersheds to reach a "good ecological status". However, although affirmatively described by experts, not all of these changes are appreciated by local communities. According to Connif (2014), 75 % of river restoration projects did not reach their minimal goals due to the lack of active stakeholder involvement. To prevent this, a comprehensive consideration of social aspects is essential for a sustainable implementation success of river restoration projects in the German water management sector. In this paper, local stakeholders' individual acceptance and the overall public acceptance of the project to ecologically improve the Emscher River's mouth in the context of the Emscher Conversion ("Emscherumbau") and its relation to certain steps of action in the project (including public participation measures) will be discussed as a case study. To our knowledge, no other research has been conducted so far combining the advantages of qualitative stakeholder interviews and a comprehensive media analysis to get an individual insight into the attitude of different stakeholder groups and to consistently identify snapshots of the public attitude during the course of the project. At first sight the project has high potential for conflicts because of drastic alterations of the current environment, intense construction works and soil transport activities, a relatively dense settlement in close proximity as well as a community that is experienced in asserting their rights. But although public participation was basically limited to information and formal consultation, the local attitude towards the ecological improvement of the Emscher River's mouth is overall positive.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Heldt, Sonja; Budryte, Paulina; Denecke, Martin] Univ Duisburg Essen,
   Dept Water &amp; Waste Management, Univ Str 15, D-45141 Essen, Germany.
   <br>[Ingensiep, Hans Werner] Univ Duisburg Essen, Fac Philosophy, Univ Str
   12D, D-45117 Essen, Germany.
   <br>[Teichgraeber, Burkhard] Emschergenossenschaft Lippeverband,
   Kronprinzenstr 24, D-45128 Essen, Germany.
   <br>[Schneider, Ute] Univ Duisburg Essen, Hist Inst, Univ Str 12, D-45141
   Essen, Germany.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Heldt, S; Budryte, P (reprint author), Univ Duisburg Essen, Dept Water &amp; Waste Management, Univ Str 15, D-45141 Essen, Germany.</td>
</tr>

<tr>
<td valign="top">EM </td><td>sonja.heldt@uni-due.de; paulina.budryte@stud.uni-due.de</td>
</tr>

<tr>
<td valign="top">TC </td><td>8</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>8</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUL</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>75</td>
</tr>

<tr>
<td valign="top">IS </td><td>13</td>
</tr>

<tr>
<td valign="top">AR </td><td>1053</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/s12665-016-5787-y</td>
</tr>

<tr>
<td valign="top">SC </td><td>Environmental Sciences &amp; Ecology; Geology; Water Resources</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000379222800006</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Titze, IR
   <br>Maxfield, L
   <br>Palaparthi, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Titze, Ingo R.
   <br>Maxfield, Lynn
   <br>Palaparthi, Anil</td>
</tr>

<tr>
<td valign="top">TI </td><td>An Oral Pressure Conversion Ratio as a Predictor of Vocal Efficiency</td>
</tr>

<tr>
<td valign="top">SO </td><td>JOURNAL OF VOICE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>AC/DC ratio; Vocal efficiency; Oral pressure; Vocal effort</td>
</tr>

<tr>
<td valign="top">ID </td><td>SUB-GLOTTAL PRESSURE; AIR-FLOW; INTRAORAL PRESSURE; IMPACT STRESS;
   PHONATION; VOICE; RESISTANCE; WAVEFORM; OUTPUT; TRACT</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice production is an inefficient process in terms of energy expended versus acoustic energy produced. A traditional efficiency measure, glottal efficiency, relates acoustic power radiated from the mouth to aerodynamic power produced in the trachea. This efficiency ranges between 0.0001% and 1.0%. It involves lung pressure and hence would appear to be a useful effort measure for a given acoustic output. Difficulty in the combined measurement of lung pressure and tracheal airflow, however, has impeded clinical application of glottal efficiency. This article uses the large data base from Schutte (1980) and a few new measurements to validate a pressure conversion ratio (PCR) as a substitute for glottal efficiency. PCR has the potential for wide application because of low cost and ease of use in clinics and vocal studios.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Titze, Ingo R.; Maxfield, Lynn; Palaparthi, Anil] Univ Utah, Natl Ctr
   Voice &amp; Speech, 136 South Main St,Suite 320, Salt Lake City, UT 84101
   USA.
   <br>[Titze, Ingo R.] Univ Iowa, Dept Commun Sci &amp; Disorders, Iowa City, IA
   USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Titze, IR (reprint author), Univ Utah, Natl Ctr Voice &amp; Speech, 136 South Main St,Suite 320, Salt Lake City, UT 84101 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ingo.titze@ncvs2.org</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Palaparthi, Anil Kumar</display_name>&nbsp;</font></td><td><font size="3">I-1926-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Palaparthi, Anil Kumar</display_name>&nbsp;</font></td><td><font size="3">0000-0003-4273-9113&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>5</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>5</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUL</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>30</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">BP </td><td>398</td>
</tr>

<tr>
<td valign="top">EP </td><td>406</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.jvoice.2015.06.002</td>
</tr>

<tr>
<td valign="top">SC </td><td>Audiology &amp; Speech-Language Pathology; Otorhinolaryngology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000379526100004</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Long, GL</td>
</tr>

<tr>
<td valign="top">AF </td><td>Long, Guangli</td>
</tr>

<tr>
<td valign="top">TI </td><td>DESIGN OF A NON-CONTACT INFRARED THERMOMETER</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERNATIONAL JOURNAL ON SMART SENSING AND INTELLIGENT SYSTEMS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>infrared temperature sensor; MCU; LCD; voice broadcast</td>
</tr>

<tr>
<td valign="top">AB </td><td>In order to realize the human body temperature fast and non-contact measurement, an infrared thermometer is designed. The infrared human body temperature sensor is mainly used to convert the human body's infrared into voltage signal, an operational amplifier to amplify the signal, filter circuit to filter the signal, the analog signal into digital signal by the A/D conversion circuit, data processing by the MCU, LCD display and voice reporting body temperature and time, so the human body non-contact measurement is realized. The experimental results show that: the device can realize the temperature and time of acquisition, the measurement error is not more than 0.5 degrees C, voice broadcast and liquid crystal display the temperature and time, overrun alarm and other functions.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Long, Guangli] Shaanxi Univ Technol, Sch Phys &amp; Telecommun Engn,
   Hanzhong 723001, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Long, GL (reprint author), Shaanxi Univ Technol, Sch Phys &amp; Telecommun Engn, Hanzhong 723001, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>lgl20088@163.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>9</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>1110</td>
</tr>

<tr>
<td valign="top">EP </td><td>1129</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000384840200035</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Tejwani, R
   <br>Kumar, G
   <br>Solanki, CS</td>
</tr>

<tr>
<td valign="top">AF </td><td>Tejwani, R.
   <br>Kumar, G.
   <br>Solanki, C. S.</td>
</tr>

<tr>
<td valign="top">TI </td><td>Remote monitoring of solar PV system for rural areas using GSM, V-F &amp;
   F-V converters</td>
</tr>

<tr>
<td valign="top">SO </td><td>JOURNAL OF INSTRUMENTATION</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Real-time monitoring; Data analysis; Detector control systems (detector
   and experiment monitoring and slow-control systems, architecture;
   hardware, algorithms, databases)</td>
</tr>

<tr>
<td valign="top">AB </td><td>The Small capacity photovoltaic (PV) systems like solar lantern and home lighting systems installed in remote rural area often fail without any prior warning due to lack of monitoring and maintenance. This paper describes implementation of remote monitoring for small capacity solar PV system that uses GSM voice channel for communication. Through GSM analog signal of sine wave with frequency range 300-3500 Hz and amplitude range 2.5-4V is transmitted. Receiver is designed to work in the same frequency range. The voltage from solar PV system in range of 2 to 7.5V can be converted to frequency directly at the transmitting end. The frequency range from 300-6000 Hz can be sensed and directly converted to voltage signal at receiving end. Testing of transmission and reception of analog signal through GSM voice channel is done for voltage to frequency (V-F) and frequency to voltage (F-V) conversions.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Tejwani, R.; Solanki, C. S.] Indian Inst Technol, DESE, Bombay 400076,
   Maharashtra, India.
   <br>[Kumar, G.] Indian Inst Technol, EE, Bombay 400076, Maharashtra, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Tejwani, R (reprint author), Indian Inst Technol, DESE, Bombay 400076, Maharashtra, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ravitejwani@iitb.ac.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAY</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>11</td>
</tr>

<tr>
<td valign="top">AR </td><td>P05001</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1088/1748-0221/11/05/P05001</td>
</tr>

<tr>
<td valign="top">SC </td><td>Instruments &amp; Instrumentation</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000377851700024</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nguyen, HQ
   <br>Lee, SW
   <br>Tian, XH
   <br>Dong, MH
   <br>Chng, ES</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hy Quy Nguyen
   <br>Lee, Siu Wa
   <br>Tian, Xiaohai
   <br>Dong, Minghui
   <br>Chng, Eng Siong</td>
</tr>

<tr>
<td valign="top">TI </td><td>High quality voice conversion using prosodic and high-resolution
   spectral features</td>
</tr>

<tr>
<td valign="top">SO </td><td>MULTIMEDIA TOOLS AND APPLICATIONS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; Deep neural network (DNN); Spectral transformation;
   Fundamental frequency (F0); Duration modeling; Pretraining</td>
</tr>

<tr>
<td valign="top">ID </td><td>DEEP NEURAL-NETWORKS; SPARSE REPRESENTATION; F0</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion methods have advanced rapidly over the last decade. Studies have shown that speaker characteristics are captured by spectral feature as well as various prosodic features. Most existing conversion methods focus on the spectral feature as it directly represents the timbre characteristics, while some conversion methods have focused only on the prosodic feature represented by the fundamental frequency. In this paper, a comprehensive framework using deep neural networks to convert both timbre and prosodic features is proposed. The timbre feature is represented by a high-resolution spectral feature. The prosodic features include F0, intensity and duration. It is well known that DNN is useful as a tool to model high-dimensional features. In this work, we show that DNN initialized by our proposed autoencoder pretraining yields good quality DNN conversion models. This pretraining is tailor-made for voice conversion and leverages on autoencoder to capture the generic spectral shape of source speech. Additionally, our framework uses segmental DNN models to capture the evolution of the prosodic features over time. To reconstruct the converted speech, the spectral feature produced by the DNN model is combined with the three prosodic features produced by the DNN segmental models. Our experimental results show that the application of both prosodic and high-resolution spectral features leads to quality converted speech as measured by objective evaluation and subjective listening tests.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Hy Quy Nguyen; Tian, Xiaohai; Chng, Eng Siong] Nanyang Technol Univ,
   Sch Comp Engn, Singapore 639798, Singapore.
   <br>[Hy Quy Nguyen; Tian, Xiaohai; Chng, Eng Siong] Nanyang Technol Univ,
   Joint NTU UBC Res Ctr Excellence Act Living Elder, Singapore, Singapore.
   <br>[Lee, Siu Wa; Dong, Minghui] ASTAR, Inst Infocomm Res, Human Language
   Technol Dept, Singapore, Singapore.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nguyen, HQ (reprint author), Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.; Nguyen, HQ (reprint author), Nanyang Technol Univ, Joint NTU UBC Res Ctr Excellence Act Living Elder, Singapore, Singapore.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ng0002hy@e.ntu.edu.sg; swylee@i2r.a-star.edu.sg; xhtian@ntu.edu.sg;
   mhdong@i2r.a-star.edu.sg; aseschng@ntu.edu.sg</td>
</tr>

<tr>
<td valign="top">TC </td><td>11</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>12</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAY</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>75</td>
</tr>

<tr>
<td valign="top">IS </td><td>9</td>
</tr>

<tr>
<td valign="top">BP </td><td>5265</td>
</tr>

<tr>
<td valign="top">EP </td><td>5285</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/s11042-015-3039-x</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000376601700025</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wu, ZZ
   <br>Li, HZ</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wu, Zhizheng
   <br>Li, Haizhou</td>
</tr>

<tr>
<td valign="top">TI </td><td>On the study of replay and voice conversion attacks to text-dependent
   speaker verification</td>
</tr>

<tr>
<td valign="top">SO </td><td>MULTIMEDIA TOOLS AND APPLICATIONS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speaker verification; Spoofing attack; Replay; Voice conversion;
   Security</td>
</tr>

<tr>
<td valign="top">ID </td><td>RECOGNITION; SPEECH; SECURITY; ADAPTATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Automatic speaker verification (ASV) is to automatically accept or reject a claimed identity based on a speech sample. Recently, individual studies have confirmed the vulnerability of state-of-the-art text-independent ASV systems under replay, speech synthesis and voice conversion attacks on various databases. However, the behaviours of text-dependent ASV systems have not been systematically assessed in the face of various spoofing attacks. In this work, we first conduct a systematic analysis of text-dependent ASV systems to replay and voice conversion attacks using the same protocol and database, in particular the RSR2015 database which represents mobile device quality speech. We then analyse the interplay of voice conversion and speaker verification by linking the voice conversion objective evaluation measures with the speaker verification error rates to take a look at the vulnerabilities from the perspective of voice conversion.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Wu, Zhizheng] Univ Edinburgh, CSTR, Edinburgh, Midlothian, Scotland.
   <br>[Li, Haizhou] Inst Infocomm Res, Human Language Technol Dept, Singapore,
   Singapore.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Wu, ZZ (reprint author), Univ Edinburgh, CSTR, Edinburgh, Midlothian, Scotland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>zhizheng.wu@ed.ac.uk; hli@i2r.a-star.edu.sg</td>
</tr>

<tr xmlns:date="http://exslt.org/dates-and-times">
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Li, Haizhou</display_name>&nbsp;</font></td><td><font size="3">Q-6438-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Li, Haizhou</display_name>&nbsp;</font></td><td><font size="3">0000-0001-9158-9401&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>9</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>9</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAY</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>75</td>
</tr>

<tr>
<td valign="top">IS </td><td>9</td>
</tr>

<tr>
<td valign="top">BP </td><td>5311</td>
</tr>

<tr>
<td valign="top">EP </td><td>5327</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/s11042-015-3080-9</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000376601700027</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Chakraborty, S
   <br>Bodhinayake, I
   <br>Chiluwal, A
   <br>Langer, DJ
   <br>Ruggieri, R
   <br>Symons, M
   <br>Boockvar, JA</td>
</tr>

<tr>
<td valign="top">AF </td><td>Chakraborty, Shamik
   <br>Bodhinayake, Imithri
   <br>Chiluwal, Amrit
   <br>Langer, David J.
   <br>Ruggieri, Rosamaria
   <br>Symons, Marc
   <br>Boockvar, John A.</td>
</tr>

<tr>
<td valign="top">TI </td><td>Neuro-oncology biotech industry progress report</td>
</tr>

<tr>
<td valign="top">SO </td><td>JOURNAL OF NEURO-ONCOLOGY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Glioblastoma; Brain tumors; Biotechnology</td>
</tr>

<tr>
<td valign="top">ID </td><td>PEPTIDE COMPLEX-96 HSPPC-96; SMALL-MOLECULE ONC201/TIC10;
   RADIATION-THERAPY SYSTEM; RENAL-CELL CARCINOMA; PHASE-II; INTRATUMORAL
   CONVERSION; RINDOPEPIMUT CDX-110; ANTICANCER AGENT; BRAIN;
   5-FLUOROCYTOSINE</td>
</tr>

<tr>
<td valign="top">AB </td><td>The Brain Tumor Biotech Center at the Feinstein Institute for Medical Research, in collaboration with Voices Against Brain Cancer hosted The Brain Tumor Biotech Summit at in New York City in June 2015. The focus was once again on fostering collaboration between neuro-oncologist, neurosurgeons, scientists, leaders from biotechnology and pharmaceutical industries, and members of the financial community. The summit highlighted the recent advances in the treatment of brain tumor, and specifically focused on targeting of stem cells and EGFR, use of prophage and immunostimulatory vaccines, retroviral vectors for drug delivery, biologic prodrug, Cesium brachytherapy, and use of electric field to disrupt tumor cell proliferation. This article summarizes the current progress in brain tumor research as presented at 2015 The Brain Tumor Biotech Summit.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Chakraborty, Shamik; Bodhinayake, Imithri; Ruggieri, Rosamaria; Symons,
   Marc; Boockvar, John A.] Hofstra Northwell Sch Med, Feinstein Inst Med
   Res, Brain Tumor Biotech Ctr, Manhasset, NY USA.
   <br>[Chakraborty, Shamik; Chiluwal, Amrit; Langer, David J.; Boockvar, John
   A.] Hofstra Northwell Sch Med, Dept Neurol Surg, Manhasset, NY USA.
   <br>[Langer, David J.; Boockvar, John A.] Lenox Hill Hosp, Dept Neurol Surg
   &amp; Otolaryngol Head &amp; Neck Surg, Lenox Hill Brain Tumor Ctr, New York, NY
   10021 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Chakraborty, S (reprint author), Hofstra Northwell Sch Med, Feinstein Inst Med Res, Brain Tumor Biotech Ctr, Manhasset, NY USA.; Chakraborty, S (reprint author), Hofstra Northwell Sch Med, Dept Neurol Surg, Manhasset, NY USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>chakrabs@gmail.com; jboockvar@northwell.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAY</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>128</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>175</td>
</tr>

<tr>
<td valign="top">EP </td><td>182</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/s11060-016-2087-z</td>
</tr>

<tr>
<td valign="top">SC </td><td>Oncology; Neurosciences &amp; Neurology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000376095600021</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Crumpton, J
   <br>Bethel, CL</td>
</tr>

<tr>
<td valign="top">AF </td><td>Crumpton, Joe
   <br>Bethel, Cindy L.</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Survey of Using Vocal Prosody to Convey Emotion in Robot Speech</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERNATIONAL JOURNAL OF SOCIAL ROBOTICS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Synthesized speech; Emotional robot speech; Human-robot interaction;
   Vocal prosody</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE; CONVERSION; EXPRESSION; MODEL; PITCH</td>
</tr>

<tr>
<td valign="top">AB </td><td>The use of speech for robots to communicate with their human users has been facilitated by improvements in speech synthesis technology. Now that the intelligibility of synthetic speech has advanced to the point that speech synthesizers are a widely accepted and used technology, what are other aspects of speech synthesis that can be used to improve the quality of human-robot interaction? The communication of emotions through changes in vocal prosody is one way to make synthesized speech sound more natural. This article reviews the use of vocal prosody to convey emotions between humans, the use of vocal prosody by agents and avatars to convey emotions to their human users, and previous work within the human-robot interaction (HRI) community addressing the use of vocal prosody in robot speech. The goals of this article are (1) to highlight the ability and importance of using vocal prosody to convey emotions within robot speech and (2) to identify experimental design issues when using emotional robot speech in user studies.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Crumpton, Joe] Mississippi State Univ, Distributed Analyt &amp; Secur Inst,
   Starkville, MS 39762 USA.
   <br>[Bethel, Cindy L.] Mississippi State Univ, Social Therapeut &amp; Robot Syst
   Lab, Dept Comp Sci &amp; Engn, Starkville, MS USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Crumpton, J (reprint author), Mississippi State Univ, Distributed Analyt &amp; Secur Inst, Starkville, MS 39762 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>crumpton@dasi.msstate.edu; cbethel@cse.msstate.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>11</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>11</td>
</tr>

<tr>
<td valign="top">PD </td><td>APR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>8</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>271</td>
</tr>

<tr>
<td valign="top">EP </td><td>285</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/s12369-015-0329-4</td>
</tr>

<tr>
<td valign="top">SC </td><td>Robotics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000374390500008</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Lee, DY
   <br>Lim, S
   <br>Kang, SH
   <br>Oh, KH
   <br>Cho, JG
   <br>Baek, SK
   <br>Woo, JS
   <br>Kwon, SY
   <br>Jung, KY</td>
</tr>

<tr>
<td valign="top">AF </td><td>Lee, Doh Young
   <br>Lim, Saehee
   <br>Kang, Sung Hoon
   <br>Oh, Kyoung Ho
   <br>Cho, Jae-Gu
   <br>Baek, Seung-Kuk
   <br>Woo, Jeong-Soo
   <br>Kwon, Soon-Young
   <br>Jung, Kwang-Yoon</td>
</tr>

<tr>
<td valign="top">TI </td><td>A prospective 1-year comparative study of transaxillary total
   thyroidectomy regarding functional outcomes: Is it really promising?</td>
</tr>

<tr>
<td valign="top">SO </td><td>SURGICAL ENDOSCOPY AND OTHER INTERVENTIONAL TECHNIQUES</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Total thyroidectomy; Transaxillary approach; Conventional thyroidectomy;
   Voice; Swallowing</td>
</tr>

<tr>
<td valign="top">ID </td><td>ROBOTIC THYROIDECTOMY; CONVENTIONAL OPEN; SURGERY; VALIDATION;
   CARCINOMA; QUALITY</td>
</tr>

<tr>
<td valign="top">AB </td><td>The purpose of this study was to evaluate postoperative voice outcomes and functional parameters in total thyroidectomy via a transaxillary (TA) approach.
   <br>Seventy-six patients who underwent total thyroidectomy via a TA approach (TA group) were included. A total of 204 patients who underwent conventional open total thyroidectomy (conventional group) in the same time period were analyzed as a control group. All patients underwent prospective functional evaluations before surgery and at 1 week and 1, 3, 6, and 12 months postoperatively using a comprehensive battery of functional assessments.
   <br>There was no conversion to conventional open thyroidectomy in the TA group. Operation time and the amount of drainage were significantly higher in the TA group than in the conventional group (p &lt; 0.001 and p = 0.033, respectively), while vocal cord paralysis, hypoparathyroidism, and hematoma were not different among two groups (p = 0.215, 0.290, and 0.385, respectively). Regarding GRBAS, the TA group showed a more aggravated tendency postoperatively, although statistical significance was attained only at postoperative 6 months (p = 0.043). The voice handicap index abruptly increased postoperatively in the TA group, showing significant differences with the conventional group at postoperative 1 week and 1 month (p &lt; 0.001 and p = 0.001, respectively). Fundamental frequency and maximal vocal pitch did not significantly change postoperatively in either group. The conventional group showed a more rapid decline in pain than the TA group, and paresthesias on the neck and chest were more aggravated in the TA group during the early postoperative period. The dysphagia handicap index was higher in the TA group, while cosmesis was better in the TA group at all postoperative periods.
   <br>Although cosmetic outcome was better with the TA approach, the longer operation time, aggravated subjective voice outcomes, paresthesia, and swallowing function need to be considered in selecting the operative approach.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Lee, Doh Young; Lim, Saehee; Kang, Sung Hoon; Oh, Kyoung Ho; Cho,
   Jae-Gu; Baek, Seung-Kuk; Woo, Jeong-Soo; Kwon, Soon-Young; Jung,
   Kwang-Yoon] Korea Univ, Coll Med, Dept Otorhinolaryngol Head &amp; Neck
   Surg, 126-1 Am Dong 5Ka, Seoul 136705, South Korea.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Jung, KY (reprint author), Korea Univ, Coll Med, Dept Otorhinolaryngol Head &amp; Neck Surg, 126-1 Am Dong 5Ka, Seoul 136705, South Korea.</td>
</tr>

<tr>
<td valign="top">EM </td><td>kyjungmd@gmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PD </td><td>APR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>30</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">BP </td><td>1599</td>
</tr>

<tr>
<td valign="top">EP </td><td>1606</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/s00464-015-4386-4</td>
</tr>

<tr>
<td valign="top">SC </td><td>Surgery</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000373022200049</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Takamichi, S
   <br>Toda, T
   <br>Black, AW
   <br>Neubig, G
   <br>Sakti, S
   <br>Nakamura, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Takamichi, Shinnosuke
   <br>Toda, Tomoki
   <br>Black, Alan W.
   <br>Neubig, Graham
   <br>Sakti, Sakriani
   <br>Nakamura, Satoshi</td>
</tr>

<tr>
<td valign="top">TI </td><td>Postfilters to Modify the Modulation Spectrum for Statistical Parametric
   Speech Synthesis</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Statistical parametric speech synthesis; over-smoothing; post-filter;
   global variance; modulation spectrum; HMM-based text-to-speech;
   GMM-based voice conversion; CLUSTERGEN</td>
</tr>

<tr>
<td valign="top">ID </td><td>GENERATION; ENVELOPE</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents novel approaches based on modulation spectrum (MS) for high-quality statistical parametric speech synthesis, including text-to-speech (TTS) and voice conversion (VC). Although statistical parametric speech synthesis offers various advantages over concatenative speech synthesis, the synthetic speech quality is still not as good as that of concatenative speech synthesis or the quality of natural speech. One of the biggest issues causing the quality degradation is the over-smoothing effect often observed in the generated speech parameter trajectories. Global variance (GV) is known as a feature well correlated with the over-smoothing effect, and the effectiveness of keeping the GV of the generated speech parameter trajectories similar to those of natural speech has been confirmed. However, the quality gap between natural speech and synthetic speech is still large. In this paper, we propose using the MS of the generated speech parameter trajectories as a new feature to effectively quantify the over-smoothing effect. Moreover, we propose postfilters to modify the MS utterance by utterance or segment by segment to make the MS of synthetic speech close to that of natural speech. The proposed postfilters are applicable to various synthesizers based on statistical parametric speech synthesis. We first perform an evaluation of the proposed method in the framework of hidden Markov model (HMM)-based TTS, examining its properties from different perspectives. Furthermore, effectiveness of the proposed postfilters are also evaluated in Gaussian mixture model (GMM)-based VC and classification and regression trees (CART)-based TTS (a.k.a., CLUSTERGEN). The experimental results demonstrate that 1) the proposed utterance-level postfilter achieves quality comparable to the conventional generation algorithm considering the GV, and yields significant improvements by applying to the GV-based generation algorithm in HMM-based TTS, 2) the proposed segment-level postfilter capable of achieving low-delay synthesis also yields significant improvements in synthetic speech quality, and 3) the proposed postfilters are also effective in not only HMM-based TTS but also GMM-based VC and CLUSTERGEN.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Takamichi, Shinnosuke; Neubig, Graham; Sakti, Sakriani; Nakamura,
   Satoshi] Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Ikoma 6300192,
   Japan.
   <br>[Toda, Tomoki] Nagoya Univ, Ctr Informat Technol, Nagoya, Aichi 4648601,
   Japan.
   <br>[Black, Alan W.] Carnegie Mellon Univ, Language Technol Inst,
   Pittsburgh, PA 15213 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Takamichi, S (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Ikoma 6300192, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>shinnosuke-t@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>18</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>18</td>
</tr>

<tr>
<td valign="top">PD </td><td>APR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>24</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">BP </td><td>755</td>
</tr>

<tr>
<td valign="top">EP </td><td>767</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASLP.2016.2522655</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000372620600002</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wu, ZZ
   <br>De Leon, PL
   <br>Demiroglu, C
   <br>Khodabakhsh, A
   <br>King, S
   <br>Ling, ZH
   <br>Saito, D
   <br>Stewart, B
   <br>Toda, T
   <br>Wester, M
   <br>Yamagishi, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wu, Zhizheng
   <br>De Leon, Phillip L.
   <br>Demiroglu, Cenk
   <br>Khodabakhsh, Ali
   <br>King, Simon
   <br>Ling, Zhen-Hua
   <br>Saito, Daisuke
   <br>Stewart, Bryan
   <br>Toda, Tomoki
   <br>Wester, Mirjam
   <br>Yamagishi, Junichi</td>
</tr>

<tr>
<td valign="top">TI </td><td>Anti-Spoofing for Text-Independent Speaker Verification: An Initial
   Database, Comparison of Countermeasures, and Human Performance</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speaker verification; speech synthesis; voice conversion; spoofing
   attack; anti-spoofing; countermeasure; security</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH RECOGNITION; SYNTHETIC SPEECH; MODELS; FREQUENCY; FUSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we present a systematic study of the vulnerability of automatic speaker verification to a diverse range of spoofing attacks. We start with a thorough analysis of the spoofing effects of five speech synthesis and eight voice conversion systems, and the vulnerability of three speaker verification systems under those attacks. We then introduce a number of countermeasures to prevent spoofing attacks from both known and unknown attackers. Known attackers are spoofing systems whose output was used to train the countermeasures, while an unknown attacker is a spoofing system whose output was not available to the countermeasures during training. Finally, we benchmark automatic systems against human performance on both speaker verification and spoofing detection tasks.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Wu, Zhizheng; King, Simon; Wester, Mirjam; Yamagishi, Junichi] Univ
   Edinburgh, Ctr Speech Technol Res, Edinburgh EH8 9AB, Midlothian,
   Scotland.
   <br>[De Leon, Phillip L.; Stewart, Bryan] New Mexico State Univ, Klipsch Sch
   Elect &amp; Comp Engn, Las Cruces, NM 88003 USA.
   <br>[Demiroglu, Cenk; Khodabakhsh, Ali] Ozyegin Univ, TR-34662 Istanbul,
   Turkey.
   <br>[Ling, Zhen-Hua] Univ Sci &amp; Technol China, Hefei 230027, Peoples R China.
   <br>[Saito, Daisuke] Univ Tokyo, Tokyo 1130033, Japan.
   <br>[Toda, Tomoki] Nagoya Univ, Ctr Informat Technol, Nagoya, Aichi 4648601,
   Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Wu, ZZ (reprint author), Univ Edinburgh, Ctr Speech Technol Res, Edinburgh EH8 9AB, Midlothian, Scotland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>zhizheng.wu@ed.ac.uk; pdeleon@nmsu.edu; cenk.demiroglu@ozyegin.edu.tr;
   alikhodabakhsh@gmail.com; simon.king@ed.ac.uk; zhling@ustc.edu;
   dsk_saito@gavo.t.u-tokyo.ac.jp; brystewa@nmsu.edu;
   tomoki@icts.nagoya-u.ac.jp; mwester@inf.ed.ac.uk; jyamagis@inf.ed.ac.uk</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>De Leon, Phillip</display_name>&nbsp;</font></td><td><font size="3">N-8884-2014&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Khodabakhsh, Ali</display_name>&nbsp;</font></td><td><font size="3">0000-0002-2873-4140&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>De Leon, Phillip</display_name>&nbsp;</font></td><td><font size="3">0000-0002-7665-9632&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>21</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>21</td>
</tr>

<tr>
<td valign="top">PD </td><td>APR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>24</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">BP </td><td>768</td>
</tr>

<tr>
<td valign="top">EP </td><td>783</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASLP.2016.2526653</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000372620600003</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ramani, B
   <br>Jeeva, MPA
   <br>Vijayalakshmi, P
   <br>Nagarajan, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ramani, B.
   <br>Jeeva, M. P. Actlin
   <br>Vijayalakshmi, P.
   <br>Nagarajan, T.</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Multi-level GMM-Based Cross-Lingual Voice Conversion Using
   Language-Specific Mixture Weights for Polyglot Synthesis</td>
</tr>

<tr>
<td valign="top">SO </td><td>CIRCUITS SYSTEMS AND SIGNAL PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>GMM; Multilingual; Polyglot; Cross-lingual voice conversion;
   Oversmoothing; ABX listening test</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH SYNTHESIS; IDENTIFICATION; TRANSFORMATION; ALGORITHM; SELECTION</td>
</tr>

<tr>
<td valign="top">AB </td><td>For any given mixed-language text, a multilingual synthesizer synthesizes speech that is intelligible to human listener. However, as speech data are usually collected from native speakers to avoid foreign accent, synthesized speech shows speaker switching at language switching points. To overcome this, the multilingual speech corpus can be converted to a polyglot speech corpus using cross-lingual voice conversion, and a polyglot synthesizer can be developed. Cross-lingual voice conversion is a technique to produce utterances in target speaker's voice from source speaker's utterance irrespective of the language and text spoken by the source and the target speakers. Conventional voice conversion technique based on GMM tokenization suffer from degradation in speech quality as the spectrum is oversmoothed due to statistical averaging. The current work focuses on alleviating the oversmoothing effect in GMM-based voice conversion technique, using (source) language-specific mixture weights in a multi-level GMM followed by selective pole focusing in the unvoiced speech segments. The continuity between the frames of the converted speech is ensured by performing fifth-order mean filtering in the cepstral domain. For the current work, cross-lingual voice conversion is performed for four regional Indian languages and a foreign language namely, Tamil, Telugu, Malayalam, Hindi, and Indian English. The performance of the system is evaluated subjectively using ABX listening test for speaker identity and using mean opinion score for quality. Experimental results demonstrate that the proposed method effectively improves the quality and intelligibility mitigating the oversmoothing effect in the voice-converted speech. A hidden Markov model-based polyglot text-to-speech system is also developed, using this converted speech corpus, to further make the system suitable for unrestricted vocabulary.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ramani, B.; Jeeva, M. P. Actlin; Vijayalakshmi, P.; Nagarajan, T.] SSN
   Coll Engn, Old Mahabalipuram Rd, Madras, Tamil Nadu, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ramani, B (reprint author), SSN Coll Engn, Old Mahabalipuram Rd, Madras, Tamil Nadu, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ramanib@ssn.edu.in; actlinjeevamp@ssn.edu.in; vijayalakshmip@ssn.edu.in;
   nagarajant@ssn.edu.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>4</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>4</td>
</tr>

<tr>
<td valign="top">PD </td><td>APR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>35</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">BP </td><td>1283</td>
</tr>

<tr>
<td valign="top">EP </td><td>1311</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/s00034-015-0118-1</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000371384600010</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Arora, A
   <br>Garas, G
   <br>Sharma, S
   <br>Muthuswamy, K
   <br>Budge, J
   <br>Palazzo, F
   <br>Darzi, A
   <br>Tolley, N</td>
</tr>

<tr>
<td valign="top">AF </td><td>Arora, Asit
   <br>Garas, George
   <br>Sharma, Sunil
   <br>Muthuswamy, Keerthini
   <br>Budge, James
   <br>Palazzo, Fausto
   <br>Darzi, Ara
   <br>Tolley, Neil</td>
</tr>

<tr>
<td valign="top">TI </td><td>Comparing transaxillary robotic thyroidectomy with conventional surgery
   in a UK population: A case control study</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERNATIONAL JOURNAL OF SURGERY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Robotic; Transaxillary; Thyroid; Endocrine surgery; Scar; Cosmesis;
   Anthropometry; Follow-up; Outcomes</td>
</tr>

<tr>
<td valign="top">ID </td><td>RANDOMIZED CONTROLLED-TRIAL; LATERAL DIRECT APPROACH; CANCER;
   PARATHYROIDECTOMY; HEMITHYROIDECTOMY; METAANALYSIS; PAIN</td>
</tr>

<tr>
<td valign="top">AB </td><td>Introduction: Transaxillary robotic thyroid surgery was pioneered in South Korea where cultural factors, anthropometry and remuneration favour this. Small thyroid nodules account for the majority of cases due to a national thyroid cancer screening programme. However, the technique has not been evaluated in the United Kingdom where larger thyroid nodules tend to undergo surgery in a patient population with a higher body mass index (BMI).
   <br>Methods: Long term prospective non-randomised study. Sixteen consecutive robotic hemithyroidectomy patients were compared to 16 open controls.
   <br>Results: There were no robotic conversions to open and no significant difference regarding pain, voice, or quality of life (QoL). In the robotic group, long term. scar cosmesis at 3 years was superior (p = 0.02) although the operative time was significantly longer (228 min vs. 85 min, p = 0.01). One patient had a transient recurrent laryngeal nerve paresis and another had temporary shoulder dysfunction. Both resolved in 4 weeks. Discussion: This study highlights the considerable difference between a Western patient population compared to South East Asia. Despite this robotic thyroidectomy is feasible and safe in a UK population.
   <br>Conclusions: Despite a low uptake in the UK compared to the Far East, robotic thyroidectomy represents a viable option for selected patients, achieving superior cosmesis compared with conventional thyroidectomy at the expense of time and cost. The key is careful patient selection. A randomised study is needed to establish the clinical efficacy compared to conventional surgery in this population. (C) 2016 IJS Publishing Group Limited. Published by Elsevier Ltd. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Arora, Asit; Garas, George; Sharma, Sunil; Muthuswamy, Keerthini;
   Budge, James; Tolley, Neil] Imperial Coll Healthcare NHS Trust, St Marys
   Hosp, Dept Otorhinolaryngol &amp; Head &amp; Neck Surg, Praed St, London W2 1NY,
   England.
   <br>[Palazzo, Fausto] Imperial Coll Healthcare NHS Trust, Hammersmith Hosp,
   Dept Endocrine &amp; Thyroid Surg, London W2 1NY, England.
   <br>[Darzi, Ara] Univ London Imperial Coll Sci Technol &amp; Med, St Marys Hosp,
   Dept Biosurg &amp; Surg Technol, London, England.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Arora, A (reprint author), Imperial Coll Healthcare NHS Trust, St Marys Hosp, Dept Otorhinolaryngol &amp; Head &amp; Neck Surg, Praed St, London W2 1NY, England.</td>
</tr>

<tr>
<td valign="top">EM </td><td>asitarora@doctors.org.uk</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Garas, George</display_name>&nbsp;</font></td><td><font size="3">0000-0001-7468-3287&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>18</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>20</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>27</td>
</tr>

<tr>
<td valign="top">BP </td><td>110</td>
</tr>

<tr>
<td valign="top">EP </td><td>117</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.ijsu.2016.01.071</td>
</tr>

<tr>
<td valign="top">SC </td><td>Surgery</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000371783400019</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Beckstead, Z</td>
</tr>

<tr>
<td valign="top">AF </td><td>Beckstead, Zachary</td>
</tr>

<tr>
<td valign="top">TI </td><td>Approaching the ineffable: Synthesizing discursive and nondiscursive
   approaches in the study of religion</td>
</tr>

<tr>
<td valign="top">SO </td><td>CULTURE &amp; PSYCHOLOGY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Religion; spirituality; generalization; discourse; ritual; pilgrimage</td>
</tr>

<tr>
<td valign="top">ID </td><td>PSYCHOLOGY; PILGRIMAGE</td>
</tr>

<tr>
<td valign="top">AB </td><td>Religion is a fascinating, complex, and relatively neglected topic in the field of psychology. It is a topic that is challenging for psychologists, who have historically tended to be less religious than the world's population on average, and the wider public to feel neutral about. The resurgence of interest in religion in the field of psychology not coincidently matches the interest of religion and spirituality in wider society, especially in the United States and even in secular Europe. Yet in spite of the need to study religious phenomena because of their relevance to the wider population and its centrality to human experience, the study of religion has been hampered and remains a tricky enterprise for many reasons. Scardigno, Manuti, and Mininni offer a rich cultural psychological approach for examining how the elderly make sense and give voice to their religious experiences and how they articulate and construct their stories of commitment and conversion. Yet cultural psychology must also understand the nondiscursive and difficult-to-articulate facets of religious experience and belief. Following Jacob Belzen (1999), this commentary argues for a plurality of approaches and one that synthesizes both discursive and nondiscursive approaches and examines rituals and, in particular, the phenomenon of pilgrimage to elucidate this argument.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Beckstead, Zachary] Grand Valley State Univ, Psychol, Allendale, MI
   49401 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Beckstead, Z (reprint author), Clark Univ, 950 Main St, Worcester, MA 01610 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ZBeckstead@gmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>22</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>35</td>
</tr>

<tr>
<td valign="top">EP </td><td>43</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1177/1354067X15622203</td>
</tr>

<tr>
<td valign="top">SC </td><td>Psychology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000370949500002</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hueber, T
   <br>Bailly, G</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hueber, Thomas
   <br>Bailly, Gerard</td>
</tr>

<tr>
<td valign="top">TI </td><td>Statistical conversion of silent articulation into audible speech using
   full-covariance HMM</td>
</tr>

<tr>
<td valign="top">SO </td><td>COMPUTER SPEECH AND LANGUAGE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Silent speech interface; GMM; HMM; Ultrasound; Articulatory-acoustic
   mapping</td>
</tr>

<tr>
<td valign="top">ID </td><td>RECOGNITION; MODEL; INTERFACE; MOVEMENTS</td>
</tr>

<tr>
<td valign="top">AB </td><td>This article investigates the use of statistical mapping techniques for the conversion of articulatory movements into audible speech with no restriction on the vocabulary, in the context of a silent speech interface driven by ultrasound and video imaging. As a baseline, we first evaluated the GMM-based mapping considering dynamic features, proposed by Toda et al. (2007) for voice conversion. Then, we proposed a 'phonetically-informed' version of this technique, based on full-covariance HMM. This approach aims (1) at modeling explicitly the articulatory timing for each phonetic class, and (2) at exploiting linguistic knowledge to regularize the problem of silent speech conversion. Both techniques were compared on continuous speech, for two French speakers (one male, one female). For modal speech, the HMM-based technique showed a lower spectral distortion (objective evaluation). However, perceptual tests (transcription and XAB discrimination tests) showed a better intelligibility of the GMM-based technique, probably related to its less fluctuant quality. For silent speech, a perceptual identification test revealed a better segmental intelligibility for the HMM-based technique on consonants. (C) 2015 Elsevier Ltd. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Hueber, Thomas; Bailly, Gerard] Univ Grenoble Alpes, GIPSA Lab, F-38000
   Grenoble, France.
   <br>[Hueber, Thomas; Bailly, Gerard] CNRS, GIPSA Lab, F-38000 Grenoble,
   France.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hueber, T (reprint author), GIPSA Lab, 11 Rue Math, F-38402 St Martin Dheres, France.</td>
</tr>

<tr>
<td valign="top">EM </td><td>thomas.hueber@gipsa-lab.fr</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>bailly, gerard</display_name>&nbsp;</font></td><td><font size="3">0000-0002-6053-0818&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>19</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>19</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>36</td>
</tr>

<tr>
<td valign="top">BP </td><td>274</td>
</tr>

<tr>
<td valign="top">EP </td><td>293</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.csl.2015.03.005</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000367123000016</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Turan, MAT
   <br>Erzin, E</td>
</tr>

<tr>
<td valign="top">AF </td><td>Turan, M. A. Tugtekin
   <br>Erzin, Engin</td>
</tr>

<tr>
<td valign="top">TI </td><td>Source and Filter Estimation for Throat-Microphone Speech Enhancement</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speech enhancement; throat microphone; Gaussian mixture model;
   statistical mapping</td>
</tr>

<tr>
<td valign="top">ID </td><td>ARTIFICIAL BANDWIDTH EXTENSION; VOICE CONVERSION; MAXIMUM-LIKELIHOOD;
   RECOGNITION; ALGORITHM</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we propose a new statistical enhancement system for throat microphone recordings through source and filter separation. Throat microphones (TM) are skin-attached piezoelectric sensors that can capture speech sound signals in the form of tissue vibrations. Due to their limited bandwidth, TM recorded speech suffers from intelligibility and naturalness. In this paper, we investigate learning phone-dependent Gaussian mixture model (GMM)-based statistical mappings using parallel recordings of acoustic microphone (AM) and TM for enhancement of the spectral envelope and excitation signals of the TM speech. The proposed mappings address the phone-dependent variability of tissue conduction with TM recordings. While the spectral envelope mapping estimates the line spectral frequency (LSF) representation of AM from TM recordings, the excitation mapping is constructed based on the spectral energy difference (SED) of AM and TM excitation signals. The excitation enhancement is modeled as an estimation of the SED features from the TM signal. The proposed enhancement system is evaluated using both objective and subjective tests. Objective evaluations are performed with the log-spectral distortion (LSD), the wideband perceptual evaluation of speech quality (PESQ) and mean-squared error (MSE) metrics. Subjective evaluations are performed with an A/B comparison test. Experimental results indicate that the proposed phone-dependent mappings exhibit enhancements over phone-independent mappings. Furthermore enhancement of the TM excitation through statistical mappings of the SED features introduces significant objective and subjective performance improvements to the enhancement of TM recordings.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Turan, M. A. Tugtekin; Erzin, Engin] Koc Univ, Coll Engn, TR-34450
   Istanbul, Turkey.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Turan, MAT (reprint author), Koc Univ, Coll Engn, TR-34450 Istanbul, Turkey.</td>
</tr>

<tr>
<td valign="top">EM </td><td>mturan@ku.edu.tr; eerzin@ku.edu.tr</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Erzin, Engin</display_name>&nbsp;</font></td><td><font size="3">H-1716-2011&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Erzin, Engin</display_name>&nbsp;</font></td><td><font size="3">0000-0002-2715-2368&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>14</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>14</td>
</tr>

<tr>
<td valign="top">PD </td><td>FEB</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>24</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>265</td>
</tr>

<tr>
<td valign="top">EP </td><td>275</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASLP.2015.2499040</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000367950900001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Benisty, H
   <br>Malah, D
   <br>Crammer, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Benisty, Hadas
   <br>Malah, David
   <br>Crammer, Koby</td>
</tr>

<tr>
<td valign="top">TI </td><td>Grid-based approximation for voice conversion in low resource
   environments</td>
</tr>

<tr>
<td valign="top">SO </td><td>EURASIP JOURNAL ON AUDIO SPEECH AND MUSIC PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Bayesian tracking; Global variance (GV); Mel cepstral distortion (MCD);
   Grid-based approximation; Spectral conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>MAXIMUM-LIKELIHOOD</td>
</tr>

<tr>
<td valign="top">AB </td><td>The goal of voice conversion is to modify a source speaker's speech to sound as if spoken by a target speaker. Common conversion methods are based on Gaussian mixture modeling (GMM). They aim to statistically model the spectral structure of the source and target signals and require relatively large training sets (typically dozens of sentences) to avoid over-fitting. Moreover, they often lead to muffled synthesized output signals, due to excessive smoothing of the spectral envelopes.
   <br>Mobile applications are characterized with low resources in terms of training data, memory footprint, and computational complexity. As technology advances, computational and memory requirements become less limiting; however, the amount of available training data still presents a great challenge, as a typical mobile user is willing to record himself saying just few sentences. In this paper, we propose the grid-based (GB) conversion method for such low resource environments, which is successfully trained using very few sentences (5-10). The GB approach is based on sequential Bayesian tracking, by which the conversion process is expressed as a sequential estimation problem of tracking the target spectrum based on the observed source spectrum. The converted Mel frequency cepstrum coefficient (MFCC) vectors are sequentially evaluated using a weighted sum of the target training vectors used as grid points. The training process includes simple computations of Euclidian distances between the training vectors and is easily performed even in cases of very small training sets.
   <br>We use global variance (GV) enhancement to improve the perceived quality of the synthesized signals obtained by the proposed and the GMM-based methods. Using just 10 training sentences, our enhanced GB method leads to converted sentences having closer GV values to those of the target and to lower spectral distances at the same time, compared to enhanced version of the GMM-based conversion method. Furthermore, subjective evaluations show that signals produced by the enhanced GB method are perceived as more similar to the target speaker than the enhanced GMM signals, at the expense of a small degradation in the perceived quality.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Benisty, Hadas; Malah, David; Crammer, Koby] Technion Israel Inst
   Technol, Dept Elect Engn, Technion, Haifa, Israel.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Malah, D (reprint author), Technion Israel Inst Technol, Dept Elect Engn, Technion, Haifa, Israel.</td>
</tr>

<tr>
<td valign="top">EM </td><td>malah@ee.technion.ac.il</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>JAN 21</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">AR </td><td>3</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1186/s13636-016-0081-1</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000391581400002</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Pribil, J
   <br>Pribilova, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Pribil, Jiri
   <br>Pribilova, Anna</td>
</tr>

<tr>
<td valign="top">BE </td><td>Esposito, A
   <br>FaundezZanuy, M
   <br>Esposito, AM
   <br>Cordasco, G
   <br>Drugman, T
   <br>SoleCasals, J
   <br>Morabito, FC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Comparison of Text-Independent Original Speaker Recognition from
   Emotionally Converted Speech</td>
</tr>

<tr>
<td valign="top">SO </td><td>RECENT ADVANCES IN NONLINEAR SPEECH PROCESSING</td>
</tr>

<tr>
<td valign="top">SE </td><td>Smart Innovation Systems and Technologies</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>7th International Workshop on Nonlinear Speech Processing (NOLISP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 18-20, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Vietri sul Mare, ITALY</td>
</tr>

<tr>
<td valign="top">DE </td><td>Emotional voice conversion; Speech spectral features; Speech prosodic
   features; Gaussian mixture model; Original speaker identification</td>
</tr>

<tr>
<td valign="top">ID </td><td>IDENTIFICATION; ENVIRONMENTS; FEATURES; PROSODY; SYSTEM</td>
</tr>

<tr>
<td valign="top">AB </td><td>The paper describes an application of the classifier based on the Gaussian mixture models (GMM) for reverse identification of the original speaker from the emotionally transformed speech in Czech and Slovak. We investigate whether the identification score given by the GMM classifier depends on the type and the structure of used speech features. Comparison of the results obtained with the sentences in German and English has shown that the structure and the balance of the speech database have influence on the identification accuracy but the used language is not practically important. The evaluation experiments confirmed that the developed text-independent GMM original speaker identifier is functional for the closed-set classification tasks.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Pribil, Jiri] SAS, Inst Measurement Sci, Dubravska Cesta 9, SK-84104
   Bratislava, Slovakia.
   <br>[Pribilova, Anna] Slovak Univ Technol Bratislava, Fac Elect Engn &amp;
   Informat Technol, Inst Elect &amp; Photon, Ilkovicova 3, SK-81219
   Bratislava, Slovakia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Pribil, J (reprint author), SAS, Inst Measurement Sci, Dubravska Cesta 9, SK-84104 Bratislava, Slovakia.</td>
</tr>

<tr>
<td valign="top">EM </td><td>Jiri.Pribil@savba.sk; Anna.Pribilova@stuba.sk</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>48</td>
</tr>

<tr>
<td valign="top">BP </td><td>137</td>
</tr>

<tr>
<td valign="top">EP </td><td>149</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/978-3-319-28109-4_14</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000417253600014</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Bhaysar, HN
   <br>Patel, TB
   <br>Patil, HA</td>
</tr>

<tr>
<td valign="top">AF </td><td>Bhaysar, Himanshu N.
   <br>Patel, Tanvina B.
   <br>Patil, Hemant A.</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Novel Nonlinear Prediction Based Features for Spoofed Speech Detection</td>
</tr>

<tr>
<td valign="top">SO </td><td>17TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2016), VOLS 1-5: UNDERSTANDING SPEECH
   PROCESSING IN HUMANS AND MACHINES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>17th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2016)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 08-12, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>San Francisco, CA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speaker verification; spoof detection; linear prediction; long-term
   prediction; nonlinear prediction</td>
</tr>

<tr>
<td valign="top">AB </td><td>Several speech synthesis and voice conversion techniques can easily generate or manipulate speech to deceive the speaker verification (SV) systems. Hence, there is a need to develop spoofing countermeasures to detect the human speech from spoofed speech. System-based features have been known to contribute significantly to this task. In this paper, we extend a recent study of Linear Prediction (LP) and Long-Term Prediction (LTP)-based features to LP and Nonlinear Prediction (NLP)-based features. To evaluate the effectiveness of the proposed countermeasure, we use the corpora provided at the ASVspoof 2015 challenge. A Gaussian Mixture Model (GMM)-based classifier is used and the % Equal Error Rate (EER) is used as a performance measure. On the development set, it is found that LP-LTP and LP-NLP features gave an average EER of 4.78 % and 9.18 %, respectively. Score-level fusion of LP-LTP (and LP-NLP) with Mel Frequency Cepstral Coefficients (MFCC) gave an EER of 0.8 % (and 1.37 %), respectively. After score-level fusion of LP-LTP, LP-NLP and MFCC features, the EER is significantly reduced to 0.57 %. The LP-LTP and LP-NLP features have found to work well even for Blizzard Challenge 2012 speech database.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Bhaysar, Himanshu N.; Patel, Tanvina B.; Patil, Hemant A.] DA IICT,
   Gandhinagar 382007, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Bhaysar, HN (reprint author), DA IICT, Gandhinagar 382007, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>himanshu_bhaysar@daiict.ac.in; tanvina_bhupendrabhai_patel@daiict.ac.in;
   hemant_patil@daiict.ac.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>4</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>4</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>155</td>
</tr>

<tr>
<td valign="top">EP </td><td>159</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2016-1002</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science; Engineering; Linguistics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000409394400033</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Xie, FL
   <br>Soong, FK
   <br>Li, HF</td>
</tr>

<tr>
<td valign="top">AF </td><td>Xie, Feng-Long
   <br>Soong, Frank K.
   <br>Li, Haifeng</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>A KL Divergence and DNN-based Approach to Voice Conversion without
   Parallel Training Sentences</td>
</tr>

<tr>
<td valign="top">SO </td><td>17TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2016), VOLS 1-5: UNDERSTANDING SPEECH
   PROCESSING IN HUMANS AND MACHINES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>17th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2016)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 08-12, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>San Francisco, CA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; Kullback-Leibler divergence; deep neural networks</td>
</tr>

<tr>
<td valign="top">ID </td><td>ARTIFICIAL NEURAL-NETWORKS</td>
</tr>

<tr>
<td valign="top">AB </td><td>We extend our recently proposed approach to cross-lingual TTS training to voice conversion, without using parallel training sentences. It employs Speaker Independent, Deep Neural Net (SI-DNN) ASR to equalize the difference between source and target speakers and Kullback-Leibler Divergence (KLD) to convert spectral parameters probabilistically in the phonetic space via ASR senone posterior probabilities of the two speakers. With or without knowing the transcriptions of the target speaker's training speech, the approach can be either supervised or unsupervised. In a supervised mode, where adequate training data of the target speaker with transcriptions is used to train a GMM-HMM TTS of the target speaker, each frame of the source speakers input data is mapped to the closest senone in thus trained TTS. The mapping is done via the posterior probabilities computed by SI-DNN ASR and the minimum KLD matching. In a unsupervised mode, all training data of the target speaker is first grouped into phonetic clusters where KLD is used as the sole distortion measure. Once the phonetic clusters are trained, each frame of the source speakers input is then mapped to the mean of the closest phonetic cluster. The final converted speech is generated with the max probability trajectory generation algorithm. Both objective and subjective evaluations show the proposed approach can achieve higher speaker similarity and better spectral distortions, when comparing with the baseline system based upon our sequential error minimization trained DNN algorithm.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Xie, Feng-Long; Li, Haifeng] Harbin Inst Technol, Harbin, Heilongjiang,
   Peoples R China.
   <br>[Xie, Feng-Long; Soong, Frank K.] Microsoft Res Asia, Beijing, Peoples R
   China.
   <br>[Xie, Feng-Long] Microsoft Res Asia, Speech Grp, Beijing, Peoples R
   China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Xie, FL (reprint author), Harbin Inst Technol, Harbin, Heilongjiang, Peoples R China.; Xie, FL (reprint author), Microsoft Res Asia, Beijing, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>v-fxie@microsoft.com; frankkps@microsoft.com; lihaifeng@hit.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>8</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>8</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>287</td>
</tr>

<tr>
<td valign="top">EP </td><td>291</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2016-116</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science; Engineering; Linguistics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000409394400060</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Aihara, R
   <br>Takiguchi, T
   <br>Ariki, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Aihara, Ryo
   <br>Takiguchi, Tetsuya
   <br>Ariki, Yasuo</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Parallel Dictionary Learning for Voice Conversion Using Discriminative
   Graph-embedded Non-negative Matrix Factorization</td>
</tr>

<tr>
<td valign="top">SO </td><td>17TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2016), VOLS 1-5: UNDERSTANDING SPEECH
   PROCESSING IN HUMANS AND MACHINES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>17th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2016)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 08-12, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>San Francisco, CA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; speech synthesis; NMF; spare representation</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPARSE REPRESENTATION; SPEECH; ALGORITHMS</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper proposes a discriminative learning method for Non negative Matrix Factorization (NMF)-based Voice Conversion (VC). NMF-based VC has been researched because of the natural-sounding voice it produces compared with conventional Gaussian Mixture Model (GMM)-based VC. In conventional NMF-based VC, parallel exemplars are used as the dictionary; therefore, dictionary learning is not adopted. In order to enhance the conversion quality of NMF-based VC, we propose Discriminative Graph-embedded Non-negative Matrix Factorization (DGNMF). Parallel dictionaries of the source and target speakers are discriminatively estimated by using DGNMF based on the phoneme labels of the training data. Experimental results show that our proposed method can not only improve the conversion quality but also reduce the computational times.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Aihara, Ryo; Takiguchi, Tetsuya; Ariki, Yasuo] Kobe Univ, Grad Sch Syst
   Informat, Nada Ku, 1-1 Rokkodai, Kobe, Hyogo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Aihara, R (reprint author), Kobe Univ, Grad Sch Syst Informat, Nada Ku, 1-1 Rokkodai, Kobe, Hyogo, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>aihara@me.cs.scitec.kobe-u.ac.jp; takigu@kobe-u.ac.jp;
   ariki@kobe-u.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>292</td>
</tr>

<tr>
<td valign="top">EP </td><td>296</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2016-227</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science; Engineering; Linguistics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000409394400061</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Yang, Y
   <br>Uchida, H
   <br>Saito, D
   <br>Minematsu, N</td>
</tr>

<tr>
<td valign="top">AF </td><td>Yang, Yi
   <br>Uchida, Hidetsugu
   <br>Saito, Daisuke
   <br>Minematsu, Nobuaki</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice conversion based on matrix variate Gaussian mixture model using
   multiple frame features</td>
</tr>

<tr>
<td valign="top">SO </td><td>17TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2016), VOLS 1-5: UNDERSTANDING SPEECH
   PROCESSING IN HUMANS AND MACHINES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>17th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2016)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 08-12, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>San Francisco, CA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; Gaussian mixture model; matrix variate Gaussian
   mixture model; multiple frame features</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a novel voice conversion method based on matrix variate Gaussian mixture model (MV-GMM) using features of multiple frames. In voice conversion studies, approaches based on Gaussian mixture models (GMM) are still widely utilized because of their flexibility and easiness in handling. They treat the joint probability density function (PDF) of feature vectors from source and target speakers as that of joint vectors of the two vectors. Addition of dynamic features to the feature vectors in GMM-based approaches achieves certain performance improvements because the correlation between multiple frames is taken into account. Recently, a voice conversion framework based on MV-GMM, in which the joint PDF is modeled in a matrix variate space, has been proposed and it is able to precisely model both the characteristics of the feature spaces and the relation between the source and target speakers. In this paper, in order to additionally model the correlation between multiple frames in the framework more consistently, MV-GMM is constructed in a matrix variate space containing the features of neighboring frames. Experimental results show that an certain performance improvement in both objective and subjective evaluations is observed.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Yang, Yi; Uchida, Hidetsugu; Saito, Daisuke; Minematsu, Nobuaki] Univ
   Tokyo, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Yang, Y (reprint author), Univ Tokyo, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>yang@gavo.t.u-tokyo.ac.jp; uchida@gavo.t.u-tokyo.ac.jp;
   dsk_saito@gavo.t.utokyoac.jp; mine@gavo.t.utokyo.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>302</td>
</tr>

<tr>
<td valign="top">EP </td><td>306</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2016-705</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science; Engineering; Linguistics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000409394400063</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hosaka, N
   <br>Hashimoto, K
   <br>Oura, K
   <br>Nankaku, Y
   <br>Tokuda, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hosaka, Naoki
   <br>Hashimoto, Kei
   <br>Oura, Keiichiro
   <br>Nankaku, Yoshihiko
   <br>Tokuda, Keiichi</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice conversion based on trajectory model training of neural networks
   considering global variance</td>
</tr>

<tr>
<td valign="top">SO </td><td>17TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2016), VOLS 1-5: UNDERSTANDING SPEECH
   PROCESSING IN HUMANS AND MACHINES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>17th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2016)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 08-12, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>San Francisco, CA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; statistical model; neural network; trajectory model;
   global variance</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH RECOGNITION; HMM</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper proposes a new training method of deep neural networks (DNNs) for statistical voice conversion. DNNs are now being used as conversion models that represent mapping from source features to target features in statistical voice conversion. However, there are two major problems to be solved in conventional DNN-based voice conversion: 1) the inconsistency between the training and synthesis criteria, and 2) the over smoothing of the generated parameter trajectories. In this paper, we introduce a parameter trajectory generation process considering the global variance (GV) into the training of DNNs for voice conversion. A consistent framework using the same criterion for both training and synthesis provides better conversion accuracy in the original static feature domain, and the over-smoothing can be avoided by optimizing the DNN parameters on the basis of the trajectory likelihood considering the GV. Experimental results show that the proposed method outperforms the DNN-based method in term of both speech quality and speaker similarity.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Hosaka, Naoki; Hashimoto, Kei; Oura, Keiichiro; Nankaku, Yoshihiko;
   Tokuda, Keiichi] Nagoya Inst Technol, Dept Sci &amp; Engn Simulat, Nagoya,
   Aichi, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hosaka, N (reprint author), Nagoya Inst Technol, Dept Sci &amp; Engn Simulat, Nagoya, Aichi, Japan.</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>307</td>
</tr>

<tr>
<td valign="top">EP </td><td>311</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2016-1035</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science; Engineering; Linguistics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000409394400064</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Aryal, S
   <br>Gutierrez-Osuna, R</td>
</tr>

<tr>
<td valign="top">AF </td><td>Aryal, Sandesh
   <br>Gutierrez-Osuna, Ricardo</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Comparing articulatory and acoustic strategies for reducing non-native
   accents</td>
</tr>

<tr>
<td valign="top">SO </td><td>17TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2016), VOLS 1-5: UNDERSTANDING SPEECH
   PROCESSING IN HUMANS AND MACHINES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>17th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2016)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 08-12, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>San Francisco, CA</td>
</tr>

<tr>
<td valign="top">DE </td><td>non-native accents; articulatory synthesis; electromagnetic
   articulography; voice conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>NORMALIZATION; MODEL</td>
</tr>

<tr>
<td valign="top">AB </td><td>This article presents an experimental comparison of two types of techniques, articulatory and acoustic, for transforming non-native speech to sound more native-like. Articulatory techniques use articulators from a native speaker to drive an articulatory synthesizer of the non-native speaker. These methods have a good theoretical justification, but articulatory measurements (e.g., via electromagnetic articulography) are difficult to obtain. In contrast, acoustic methods use techniques from the voice conversion literature to build a mapping between the two acoustic spaces, making them more attractive for practical applications (e.g., language learning). We compare two representative implementations of these approaches, both based on statistical parametric speech synthesis. Through a series of perceptual listening tests, we evaluate the two approaches in terms of accent reduction, speech intelligibility and speaker quality. Out results show that the acoustic method is more effective than the articulatory method in reducing perceptual ratings of non-native accents, and also produces synthesis of higher intelligibility while preserving voice quality.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Aryal, Sandesh; Gutierrez-Osuna, Ricardo] Texas A&amp;M Univ, Dept Comp Sci
   &amp; Engn, College Stn, TX 77843 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Aryal, S (reprint author), Texas A&amp;M Univ, Dept Comp Sci &amp; Engn, College Stn, TX 77843 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>sandesh@cse.tamu.edu; rgutier@cse.tamu.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>312</td>
</tr>

<tr>
<td valign="top">EP </td><td>316</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2016-1131</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science; Engineering; Linguistics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000409394400065</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ijima, Y
   <br>Asami, T
   <br>Mizuno, H</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ijima, Yusuke
   <br>Asami, Taichi
   <br>Mizuno, Hideyuki</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Objective Evaluation Using Association Between Dimensions Within
   Spectral Features for Statistical Parametric Speech Synthesis</td>
</tr>

<tr>
<td valign="top">SO </td><td>17TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2016), VOLS 1-5: UNDERSTANDING SPEECH
   PROCESSING IN HUMANS AND MACHINES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>17th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2016)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 08-12, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>San Francisco, CA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Statistical parametric speech synthesis; objective evaluation; spectral
   features; maximal information coefficient</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION; NEURAL-NETWORKS; GENERATION ALGORITHM; VARIANCE</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a novel objective evaluation technique for statistical parametric speech synthesis. One of its novel features is that it focuses on the association between dimensions within the spectral features. We first use a maximal information coefficient to analyze the relationship between subjective scores and associations of spectral features obtained from natural and various types of synthesized speech. The analysis results indicate that the scores improve as the association becomes weaker. We then describe the proposed objective evaluation technique, which uses a voice conversion method to detect the associations within spectral features. We perform subjective and objective experiments to investigate the relationship between subjective scores and objective scores. The proposed objective scores are compared to the mel-cepstral distortion. The results indicate that our objective scores achieve dramatically higher correlation to subjective scores than the mel-cepstral distortion.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ijima, Yusuke; Asami, Taichi] NTT Corp, NTT Media Intelligence Labs,
   Tokyo, Japan.
   <br>[Mizuno, Hideyuki] Tokyo Univ Sci, Suwa, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ijima, Y (reprint author), NTT Corp, NTT Media Intelligence Labs, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>337</td>
</tr>

<tr>
<td valign="top">EP </td><td>341</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2016-584</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science; Engineering; Linguistics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000409394400070</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nakashika, T
   <br>Minami, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nakashika, Toru
   <br>Minami, Yasuhiro</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Generative Acoustic-Phonemic-Speaker Model Based on Three-Way Restricted
   Boltzmann Machine</td>
</tr>

<tr>
<td valign="top">SO </td><td>17TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2016), VOLS 1-5: UNDERSTANDING SPEECH
   PROCESSING IN HUMANS AND MACHINES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>17th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2016)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 08-12, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>San Francisco, CA</td>
</tr>

<tr>
<td valign="top">DE </td><td>speech modeling; three-way restricted Boltzmann machine;
   speaker-adaptive training; voice conversion; speech recognition; speaker
   recognition</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we argue the way of modeling speech signals based on three-way restricted Boltzmann machine (3WRBM) for separating phonetic-related information and speaker-related information from an observed signal automatically. The proposed model is an energy-based probabilistic model that includes three-way potentials of three variables: acoustic features, latent phonetic features, and speaker-identity features. We train the model so that it automatically captures the undirected relationships among the three variables.. Once the model is trained, it can be applied to many tasks in speech signal processing. For example, given a speech signal, estimating speaker-identity features is equivalent to speaker recognition; on the other hand, estimated latent phonetic features may be helpful for speech recognition because they contain more phonetic-related information than the acoustic features. Since the model is generative, we can also apply it to voice conversion; i.e., we just estimate acoustic features from the phonetic features that were estimated given the source speakers acoustic features along with the desired speaker-identity features. In our experiments, we discuss the effectiveness of the speech modeling through a speaker recognition, a speech (continuous phone) recognition, and a voice conversion tasks.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nakashika, Toru; Minami, Yasuhiro] Univ Electrocommun, Grad Sch
   Informat Syst, Chofu, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nakashika, T (reprint author), Univ Electrocommun, Grad Sch Informat Syst, Chofu, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>nakashika@uec.ac.jp; minami.yasuhiro@is.uec.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>1487</td>
</tr>

<tr>
<td valign="top">EP </td><td>1491</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2016-1105</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science; Engineering; Linguistics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000409394400310</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Toda, T
   <br>Chen, LH
   <br>Saito, D
   <br>Villavicencio, F
   <br>Wester, M
   <br>Wu, ZZ
   <br>Yamagishi, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Toda, Tomoki
   <br>Chen, Ling-Hui
   <br>Saito, Daisuke
   <br>Villavicencio, Fernando
   <br>Wester, Mirjam
   <br>Wu, Zhizheng
   <br>Yamagishi, Junichi</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>The Voice Conversion Challenge 2016</td>
</tr>

<tr>
<td valign="top">SO </td><td>17TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2016), VOLS 1-5: UNDERSTANDING SPEECH
   PROCESSING IN HUMANS AND MACHINES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>17th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2016)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 08-12, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>San Francisco, CA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; speech synthesis; evaluation challenge</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPARSE REPRESENTATION; NEURAL-NETWORKS; SPEECH</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper describes the Voice Conversion Challenge 2016 devised by the authors to better understand different voice conversion (VC) techniques by comparing their performance on a common dataset. The task of the challenge was speaker conversion, i.e., to transform the voice identity of a source speaker into that of a target speaker while preserving the linguistic content. Using a common dataset consisting of 162 utterances for training and 54 utterances for evaluation from each of 5 source and 5 target speakers, 17 groups working in VC around the world developed their own VC systems for every combination of the source and target speakers, i.e., 25 systems in total, and generated voice samples converted by the developed systems. These samples were evaluated in terms of target speaker similarity and naturalness by 200 listeners in a controlled environment. This paper summarizes the design of the challenge, its result, and a future plan to share views about unsolved problems and challenges faced by the current VC techniques.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Toda, Tomoki] Nagoya Univ, Informat Technol Ctr, Nagoya, Aichi, Japan.
   <br>[Chen, Ling-Hui] Univ Sci &amp; Technol China, Hefei, Anhui, Peoples R China.
   <br>[Saito, Daisuke] Univ Tokyo, Tokyo, Japan.
   <br>[Villavicencio, Fernando; Yamagishi, Junichi] Natl Inst Informat, Tokyo,
   Japan.
   <br>[Wester, Mirjam; Wu, Zhizheng; Yamagishi, Junichi] Univ Edinburgh, Ctr
   Speech Technol Res, Edinburgh, Midlothian, Scotland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Toda, T (reprint author), Nagoya Univ, Informat Technol Ctr, Nagoya, Aichi, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>vcc2016@vc-challenge.org</td>
</tr>

<tr>
<td valign="top">TC </td><td>20</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>20</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>1632</td>
</tr>

<tr>
<td valign="top">EP </td><td>1636</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2016-1066</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science; Engineering; Linguistics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000409394401024</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wester, M
   <br>Wu, ZZ
   <br>Yamagishi, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wester, Mirjam
   <br>Wu, Zhizheng
   <br>Yamagishi, Junichi</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Analysis of the Voice Conversion Challenge 2016 Evaluation Results</td>
</tr>

<tr>
<td valign="top">SO </td><td>17TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2016), VOLS 1-5: UNDERSTANDING SPEECH
   PROCESSING IN HUMANS AND MACHINES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>17th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2016)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 08-12, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>San Francisco, CA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice Conversion Challenge; evaluation</td>
</tr>

<tr>
<td valign="top">AB </td><td>The Voice Conversion Challenge 2016 is the first Voice Conversion Challenge in which different voice conversion systems and approaches using the same voice data were compared. This paper describes the design of the evaluation, it presents the results and statistical analyses of the results.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Wester, Mirjam; Wu, Zhizheng; Yamagishi, Junichi] Univ Edinburgh, Ctr
   Speech Technol Res, Edinburgh, Midlothian, Scotland.
   <br>[Yamagishi, Junichi] Natl Inst Informat, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Wester, M (reprint author), Univ Edinburgh, Ctr Speech Technol Res, Edinburgh, Midlothian, Scotland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>mwester@inf.ed.ac.uk; zhizheng.wu@ed.ac.uk; jyamagis@inf.ed.ac.uk</td>
</tr>

<tr>
<td valign="top">TC </td><td>8</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>8</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>1637</td>
</tr>

<tr>
<td valign="top">EP </td><td>1641</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2016-1331</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science; Engineering; Linguistics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000409394401025</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Chen, LH
   <br>Liu, LJ
   <br>Ling, ZH
   <br>Jiang, Y
   <br>Dai, LR</td>
</tr>

<tr>
<td valign="top">AF </td><td>Chen, Ling-Hui
   <br>Liu, Li-Juan
   <br>Ling, Zhen-Hua
   <br>Jiang, Yuan
   <br>Dai, Li-Rong</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>The USTC System for Voice Conversion Challenge 2016: Neural Network
   Based Approaches for Spectrum, Aperiodicity and F-0 Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>17TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2016), VOLS 1-5: UNDERSTANDING SPEECH
   PROCESSING IN HUMANS AND MACHINES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>17th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2016)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 08-12, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>San Francisco, CA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; frequency warping; DNN; RNN; LSTM</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH SYNTHESIS; SPACE</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper introduces the methods we adopt to build our system for the evaluation event of Voice Conversion Challenge (VCC) 2016. We propose to use neural network-based approaches to convert both spectral and excitation features. First, the generatively trained deep neural network (GTDNN) is adopted for spectral envelope conversion after the spectral envelopes have been pre-processed by frequency warping. Second, we propose to use a recurrent neural network (RNN) with long short-term memory (LSTM) cells for F0 trajectory conversion. In addition, we adopt a DNN for band aperiodicity conversion. Both internal tests and formal VCC evaluation results demonstrate the effectiveness of the proposed methods.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Chen, Ling-Hui; Ling, Zhen-Hua; Dai, Li-Rong] Univ Sci &amp; Technol China,
   Natl Engn Lab Speech &amp; Language Informat Proc, Hefei, Anhui, Peoples R
   China.
   <br>[Chen, Ling-Hui; Liu, Li-Juan; Jiang, Yuan] IFLYTEK Res, Hefei, Anhui,
   Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Chen, LH (reprint author), Univ Sci &amp; Technol China, Natl Engn Lab Speech &amp; Language Informat Proc, Hefei, Anhui, Peoples R China.; Chen, LH (reprint author), IFLYTEK Res, Hefei, Anhui, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>chenlh@ustc.edu.cn; ljliu@iflytek.com; zhling@ustc.edu.cn;
   yuanjiang@iflytek.com; lrdai@ustc.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>1642</td>
</tr>

<tr>
<td valign="top">EP </td><td>1646</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2016-456</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science; Engineering; Linguistics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000409394401026</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Mohammadi, SH
   <br>Kain, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Mohammadi, Seyed Hamidreza
   <br>Kain, Alexander</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Voice Conversion Mapping Function based on a Stacked Joint-Autoencoder</td>
</tr>

<tr>
<td valign="top">SO </td><td>17TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2016), VOLS 1-5: UNDERSTANDING SPEECH
   PROCESSING IN HUMANS AND MACHINES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>17th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2016)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 08-12, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>San Francisco, CA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; deep neural network; autoencoder; joint-autoencoder</td>
</tr>

<tr>
<td valign="top">ID </td><td>MAXIMUM-LIKELIHOOD-ESTIMATION; NEURAL-NETWORKS</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this study, we propose a novel method for training a regression function and apply it to a voice conversion task. The regression function is constructed using a Stacked Joint-Autoencoder (SJAE). Previously, we have used a more primitive version of this architecture for pre-training a Deep Neural Network (DNN). Using objective evaluation criteria, we show that the lower levels of the SJAE perform best with a low degree of jointness, and higher levels with a higher degree of jointness. We demonstrate that our proposed approach generates features that do not suffer from the averaging effect inherent in back propagation training. We also carried out subjective listening experiments to evaluate speech quality and speaker similarity. Our results show that the SJAE approach has both higher quality and similarity than a SJAE+DNN approach, where the SJAE is used for pre-training a DNN, and the fine-tuned DNN is then used for mapping. We also present the system description and results of our submission to Voice Conversion Challenge 2016.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Mohammadi, Seyed Hamidreza; Kain, Alexander] Oregon Hlth &amp; Sci Univ,
   Ctr Spoken Language Understanding, Portland, OR 97201 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Mohammadi, SH (reprint author), Oregon Hlth &amp; Sci Univ, Ctr Spoken Language Understanding, Portland, OR 97201 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>mohammah@ohsu.edu; kaina@ohsu.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>1647</td>
</tr>

<tr>
<td valign="top">EP </td><td>1651</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2016-1437</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science; Engineering; Linguistics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000409394401027</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wu, YC
   <br>Hwang, HT
   <br>Hsu, CC
   <br>Tsao, Y
   <br>Wang, HM</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wu, Yi-Chiao
   <br>Hwang, Hsin-Te
   <br>Hsu, Chin-Cheng
   <br>Tsao, Yu
   <br>Wang, Hsin-Min</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Locally Linear Embedding for Exemplar-Based Spectral Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>17TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2016), VOLS 1-5: UNDERSTANDING SPEECH
   PROCESSING IN HUMANS AND MACHINES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>17th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2016)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 08-12, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>San Francisco, CA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; exemplar; locally linear embedding; voice conversion
   challenge</td>
</tr>

<tr>
<td valign="top">ID </td><td>NONLINEAR DIMENSIONALITY REDUCTION; VOICE CONVERSION; NEURAL-NETWORKS;
   REPRESENTATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper describes a novel exemplar-based spectral conversion (SC) system developed by the AST (Academia Sinica, Taipei) team for the 2016 voice conversion challenge (vcc2016). The key feature of our system is that it integrates the locally linear embedding (LLE) algorithm, a manifold learning algorithm that has been successfully applied for the super-resolution task in image processing, with the conventional exemplar-based SC method. To further improve the quality of the converted speech, our system also incorporates (1) the maximum likelihood parameter generation (MLPG) algorithm, (2) the postfiltering-based global variance (GV) compensation method, and (3) a high-resolution feature extraction process. The results of subjective evaluation conducted by the vcc2016 organizer show that our LLE-exemplarbased SC system notably outperforms the baseline GMM-based system (implemented by the vcc2016 organizer). Moreover, our own internal evaluation results confirm the effectiveness of the major LLE-exemplar-based SC method and the three additional approaches with improved speech quality.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Wu, Yi-Chiao; Hwang, Hsin-Te; Hsu, Chin-Cheng; Wang, Hsin-Min] Acad
   Sinica, Inst Informat Sci, Taipei, Taiwan.
   <br>[Tsao, Yu] Acad Sinica, Res Ctr Informat Technol Innovat, Taipei, Taiwan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hwang, HT (reprint author), Acad Sinica, Inst Informat Sci, Taipei, Taiwan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>hwanght@iis.sinica.edu.tw; yu.tsao@citi.sinica.edu.tw;
   whm@iis.sinica.edu.tw</td>
</tr>

<tr>
<td valign="top">TC </td><td>9</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>9</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>1652</td>
</tr>

<tr>
<td valign="top">EP </td><td>1656</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2016-567</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science; Engineering; Linguistics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000409394401028</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Villavicencio, F
   <br>Yamagishi, J
   <br>Bonada, J
   <br>Espic, F</td>
</tr>

<tr>
<td valign="top">AF </td><td>Villavicencio, Fernando
   <br>Yamagishi, Junichi
   <br>Bonada, Jordi
   <br>Espic, Felipe</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Applying Spectral Normalisation and Efficient Envelope Estimation and
   Statistical Transformation for the Voice Conversion Challenge 2016</td>
</tr>

<tr>
<td valign="top">SO </td><td>17TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2016), VOLS 1-5: UNDERSTANDING SPEECH
   PROCESSING IN HUMANS AND MACHINES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>17th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2016)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 08-12, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>San Francisco, CA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; speech synthesis; statistical spectral transformation;
   spectral envelope modeling</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this work we present our entry for the Voice Conversion Challenge 2016, denoting new features to previous work on GMM-based voice conversion. We incorporate frequency warping and pitch transposition strategies to perform a normalisation of the spectral conditions, with benefits confirmed by objective and perceptual means. Moreover, the results of the challenge showed our entry among the highest performing systems in terms of perceived naturalness while maintaining the target similarity performance of GMM-based conversion.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Villavicencio, Fernando; Yamagishi, Junichi] Natl Inst Informat NII,
   Tokyo, Japan.
   <br>[Bonada, Jordi] Univ Pompeu Fabra UPF, Barcelona, Spain.
   <br>[Espic, Felipe] Ctr Speech Technol Res CSTR, Edinburgh, Midlothian,
   Scotland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Villavicencio, F (reprint author), Natl Inst Informat NII, Tokyo, Japan.</td>
</tr>

<tr xmlns:date="http://exslt.org/dates-and-times">
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Bonada, Jordi</display_name>&nbsp;</font></td><td><font size="3">0000-0002-8671-0729&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>1657</td>
</tr>

<tr>
<td valign="top">EP </td><td>1661</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2016-305</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science; Engineering; Linguistics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000409394401029</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Erro, D
   <br>Alonso, A
   <br>Serrano, L
   <br>Tavarez, D
   <br>Odriozola, I
   <br>Sarasola, X
   <br>Del Blanco, E
   <br>Sanchez, J
   <br>Saratxaga, I
   <br>Navas, E
   <br>Hernaez, I</td>
</tr>

<tr>
<td valign="top">AF </td><td>Erro, D.
   <br>Alonso, A.
   <br>Serrano, L.
   <br>Tavarez, D.
   <br>Odriozola, I.
   <br>Sarasola, X.
   <br>Del Blanco, E.
   <br>Sanchez, J.
   <br>Saratxaga, I.
   <br>Navas, E.
   <br>Hernaez, I.</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>ML Parameter Generation with a Reformulated MGE Training
   Criterion-Participation in the Voice Conversion Challenge 2016</td>
</tr>

<tr>
<td valign="top">SO </td><td>17TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2016), VOLS 1-5: UNDERSTANDING SPEECH
   PROCESSING IN HUMANS AND MACHINES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>17th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2016)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 08-12, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>San Francisco, CA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; maximum likelihood parameter generation; minimum
   generation error; linear regression; cepstral postfilter</td>
</tr>

<tr>
<td valign="top">ID </td><td>FREQUENCY; TRANSFORMATION; MODEL</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper describes our entry to the Voice Conversion Challenge 2016. Based on the maximum likelihood parameter generation algorithm, the method is a reformulation of the minimum generation error training criterion. It uses a GMM for soft classification, a Mel-cepstral vocoder for acoustic analysis and an improved dynamic time warping procedure for source-target alignment. To compensate the oversmoothing effect, the generated parameters are filtered through a speaker-independent post filter implemented as a linear transform in cepstral domain. The process is completed with mean and variance adaptation of the log-fundamental frequency and duration modification by a constant factor. The results of the evaluation show that the proposed system achieves a high conversion accuracy in comparison with other systems, while its naturalness scores are intermediate.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Erro, D.; Alonso, A.; Serrano, L.; Tavarez, D.; Odriozola, I.;
   Sarasola, X.; Del Blanco, E.; Sanchez, J.; Saratxaga, I.; Navas, E.;
   Hernaez, I.] Univ Basque Country, Aholab, Bilbao, Spain.
   <br>[Erro, D.] Basque Fdn Sci, Ikerbasque, Bilbao, Spain.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Erro, D (reprint author), Univ Basque Country, Aholab, Bilbao, Spain.; Erro, D (reprint author), Basque Fdn Sci, Ikerbasque, Bilbao, Spain.</td>
</tr>

<tr>
<td valign="top">EM </td><td>derro@aholab.ehu.es</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Hernaez Rioja, Inmaculada</display_name>&nbsp;</font></td><td><font size="3">K-8303-2012&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Sanchez, Jon</display_name>&nbsp;</font></td><td><font size="3">H-6882-2015&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Navas, Eva</display_name>&nbsp;</font></td><td><font size="3">H-4317-2013&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Saratxaga, Ibon</display_name>&nbsp;</font></td><td><font size="3">H-6423-2015&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Hernaez Rioja, Inmaculada</display_name>&nbsp;</font></td><td><font size="3">0000-0003-4447-7575&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Sanchez, Jon</display_name>&nbsp;</font></td><td><font size="3">0000-0002-7236-4474&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Navas, Eva</display_name>&nbsp;</font></td><td><font size="3">0000-0003-3804-4984&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Saratxaga, Ibon</display_name>&nbsp;</font></td><td><font size="3">0000-0001-7282-2765&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>1662</td>
</tr>

<tr>
<td valign="top">EP </td><td>1666</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2016-219</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science; Engineering; Linguistics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000409394401030</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kobayashi, K
   <br>Takamichi, S
   <br>Nakamura, S
   <br>Toda, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kobayashi, Kazuhiro
   <br>Takamichi, Shinnosuke
   <br>Nakamura, Satoshi
   <br>Toda, Tomoki</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>The NU-NAIST voice conversion system for the Voice Conversion Challenge
   2016</td>
</tr>

<tr>
<td valign="top">SO </td><td>17TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2016), VOLS 1-5: UNDERSTANDING SPEECH
   PROCESSING IN HUMANS AND MACHINES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>17th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2016)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 08-12, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>San Francisco, CA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion challenge 2016; speaker identity; segmental feature;
   Gaussian mixture model; STRAIGHT analysis</td>
</tr>

<tr>
<td valign="top">ID </td><td>PARAMETRIC SPEECH SYNTHESIS; PLUS NOISE MODEL; SPARSE REPRESENTATION;
   NEURAL-NETWORKS</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents the NU-NAIST voice conversion (VC) system for the Voice Conversion Challenge 2016 (VCC 2016) developed by a joint team of Nagoya University and Nara Institute of Science and Technology. Statistical VC based on a Gaussian mixture model makes it possible to convert speaker identity of a source speaker' voice into that of a target speaker by converting several speech parameters. However, various factors such as parameterization errors and over-smoothing effects usually cause speech quality degradation of the converted voice. To address this issue, we have proposed a direct waveform modification technique based on spectral differential filtering and have successfully applied it to singing voice conversion where excitation features are not necessary converted. In this paper, we propose a method to apply this technique to a standard voice conversion task where excitation feature conversion is needed. The result of VCC 2016 demonstrates that the NU-NAIST VC system developed by the proposed method yields the best conversion accuracy for speaker identity (more than 70% of the correct rate) and quite high naturalness score (more than 3 of the mean opinion score). This paper presents detail descriptions of the NU-NAIST VC system and additional results of its performance evaluation.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kobayashi, Kazuhiro; Takamichi, Shinnosuke; Nakamura, Satoshi] Nara
   Inst Sci &amp; Technol NAIST, Ikoma, Japan.
   <br>[Toda, Tomoki] Nagoya Univ, Informat Technol Ctr, Nagoya, Aichi, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kobayashi, K (reprint author), Nara Inst Sci &amp; Technol NAIST, Ikoma, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>kazuhiro-k@is.naist.jp; shinnosuke-t@is.naist.jp;
   s-nakamura@is.naist.jp; tomoki@icts.nagoya-u.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>4</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>4</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>1667</td>
</tr>

<tr>
<td valign="top">EP </td><td>1671</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2016-970</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science; Engineering; Linguistics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000409394401031</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sriskandaraja, K
   <br>Sethu, V
   <br>Le, PN
   <br>Ambikairajah, E</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sriskandaraja, Kaavya
   <br>Sethu, Vidhyasaharan
   <br>Phu Ngoc Le
   <br>Ambikairajah, Eliathamby</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Investigation of Sub-Band Discriminative Information between Spoofed and
   Genuine Speech</td>
</tr>

<tr>
<td valign="top">SO </td><td>17TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2016), VOLS 1-5: UNDERSTANDING SPEECH
   PROCESSING IN HUMANS AND MACHINES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>17th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2016)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 08-12, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>San Francisco, CA</td>
</tr>

<tr>
<td valign="top">DE </td><td>speech recognition; automatic speaker verification; spoofing and
   anti-spoofing; voice conversion; SAS; speech synthetic computational
   paralinguistic</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEAKER VERIFICATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>A speaker verification system should include effective precautions against malicious spoofing attacks, and although some initial countermeasures have been recently proposed, this remains a challenging research problem. This paper investigates discrimination between spoofed and genuine speech, as a function of frequency bands, across the speech bandwidth. Findings from our investigation inform some proposed filter bank design approaches for discrimination of spoofed speech. Experiments are conducted on the Spoofing and Anti-Spoofing (SAS) corpus using the proposed frequency-selective approach demonstrates an 11% relative improvement in terms of equal error rate compared with a conventional mel filter bank.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Sriskandaraja, Kaavya; Sethu, Vidhyasaharan; Phu Ngoc Le; Ambikairajah,
   Eliathamby] UNSW, Sch Elect Engn &amp; Telecommun, Sydney, NSW, Australia.
   <br>[Sriskandaraja, Kaavya; Phu Ngoc Le; Ambikairajah, Eliathamby] Natl ICT
   Australia NICTA, ATP Res Lab, Sydney, NSW, Australia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sriskandaraja, K (reprint author), UNSW, Sch Elect Engn &amp; Telecommun, Sydney, NSW, Australia.; Sriskandaraja, K (reprint author), Natl ICT Australia NICTA, ATP Res Lab, Sydney, NSW, Australia.</td>
</tr>

<tr>
<td valign="top">EM </td><td>k.sriskandaraja@unsw.edu.au; v.sethu@unsw.edu.au; ngoc.le@unsw.edu.au;
   e.ambikairajah@unsw.edu.au</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Sethu, Vidhyasaharan</display_name>&nbsp;</font></td><td><font size="3">0000-0001-8492-1787&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>8</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>8</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>1710</td>
</tr>

<tr>
<td valign="top">EP </td><td>1714</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2016-844</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science; Engineering; Linguistics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000409394401040</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Jiao, Y
   <br>Tu, M
   <br>Berisha, V
   <br>Liss, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Jiao, Yishan
   <br>Tu, Ming
   <br>Berisha, Visar
   <br>Liss, Julie</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Accent Identification by Combining Deep Neural Networks and Recurrent
   Neural Networks Trained on Long and Short Term Features</td>
</tr>

<tr>
<td valign="top">SO </td><td>17TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2016), VOLS 1-5: UNDERSTANDING SPEECH
   PROCESSING IN HUMANS AND MACHINES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>17th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2016)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 08-12, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>San Francisco, CA</td>
</tr>

<tr>
<td valign="top">DE </td><td>accent identification; deep neural networks; prosody; articulation</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH RECOGNITION; FORMANT</td>
</tr>

<tr>
<td valign="top">AB </td><td>Automatic identification of foreign accents is valuable for many speech systems, such as speech recognition, speaker identification, voice conversion, etc. The INTERSPEECH 2016 Native Language Sub-Challenge is to identify the native languages of non-native English speakers from eleven countries. Since differences in accent are due to both prosodic and articulation characteristics, a combination of long-term and short-term training is proposed in this paper. Each speech sample is processed into multiple speech segments with equal length. For each segment, deep neural networks (DNNs) are used to train on long-term statistical features, while recurrent neural networks (RNNs) are used to train on short-term acoustic features. The result for each speech sample is calculated by linearly fusing the results from the two sets of networks on all segments. The performance of the proposed system greatly surpasses the provided baseline system. Moreover, by fusing the results with the baseline system, the performance can be further improved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Jiao, Yishan; Tu, Ming; Berisha, Visar; Liss, Julie] Arizona State
   Univ, Dept Speech &amp; Hearing Sci, Tempe, AZ 85287 USA.
   <br>[Berisha, Visar] Arizona State Univ, Sch Elect Comp &amp; Energy Engn,
   Tempe, AZ 85287 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Jiao, Y (reprint author), Arizona State Univ, Dept Speech &amp; Hearing Sci, Tempe, AZ 85287 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>yjiao16@asu.edu; mingtu@asu.edu; visar@asu.edu; julie.liss@asu.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>2388</td>
</tr>

<tr>
<td valign="top">EP </td><td>2392</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2016-1148</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science; Engineering; Linguistics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000409394401182</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ming, HP
   <br>Huang, DY
   <br>Xie, L
   <br>Wu, J
   <br>Dong, MH
   <br>Li, HZ</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ming, Huaiping
   <br>Huang, Dongyan
   <br>Xie, Lei
   <br>Wu, Jie
   <br>Dong, Minghui
   <br>Li, Haizhou</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Deep Bidirectional LSTM Modeling of Timbre and Prosody for Emotional
   Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>17TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2016), VOLS 1-5: UNDERSTANDING SPEECH
   PROCESSING IN HUMANS AND MACHINES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>17th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2016)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 08-12, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>San Francisco, CA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; prosody; long short-term memory; recurrent neural
   networks</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH; F0</td>
</tr>

<tr>
<td valign="top">AB </td><td>Emotional voice conversion aims at converting speech from one emotion state to another. This paper proposes to model timbre and prosody features using a deep bidirectional long shortterm memory (DBLSTM) for emotional voice conversion. A continuous wavelet transform (CWT) representation of fundamental frequency (FO) and energy contour are used for prosody modeling. Specifically, we use CWT to decompose FO into a five-scale representation, and decompose energy contour into a ten-scale representation, where each feature scale corresponds to a temporal scale. Both spectrum and prosody (FO and energy contour) features are simultaneously converted by a sequence to sequence conversion method with DBLSTM model, which captures both frame-wise and long-range relationship between source and target voice. The converted speech signals are evaluated both objectively and subjectively, which confirms the effectiveness of the proposed method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ming, Huaiping; Huang, Dongyan; Dong, Minghui; Li, Haizhou] ASTAR, Inst
   Infocomm Res, Singapore, Singapore.
   <br>[Xie, Lei; Wu, Jie] Northwestern Polytech Univ, Sch Comp Sci, Xian,
   Shaanxi, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ming, HP (reprint author), ASTAR, Inst Infocomm Res, Singapore, Singapore.</td>
</tr>

<tr>
<td valign="top">EM </td><td>minghp@i2r.a-star.edu.sg; huang@i2r.a-star.edu.sg; lxie@nwpu.edu.cn;
   jiewu@nwpu.edu.cn; mhdong@i2r.a-star.edu.sg; hli@i2r.a-star.edu.sg</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Li, Haizhou</display_name>&nbsp;</font></td><td><font size="3">Q-6438-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>4</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>4</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>2453</td>
</tr>

<tr>
<td valign="top">EP </td><td>2457</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2016-1053</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science; Engineering; Linguistics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000409394401195</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Zheng, HD
   <br>Cai, WC
   <br>Zhou, TY
   <br>Zhang, SL
   <br>Li, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Zheng, Huadi
   <br>Cai, Weicheng
   <br>Zhou, Tianyan
   <br>Zhang, Shilei
   <br>Li, Ming</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Text-Independent Voice Conversion Using Deep Neural Network Based
   Phonetic Level Features</td>
</tr>

<tr>
<td valign="top">SO </td><td>2016 23RD INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION (ICPR)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Pattern Recognition</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>23rd International Conference on Pattern Recognition (ICPR)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 04-08, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Mexican Assoc Comp Vis Robot &amp; Neural Comp, Cancun, MEXICO</td>
</tr>

<tr>
<td valign="top">HO </td><td>Mexican Assoc Comp Vis Robot &amp; Neural Comp</td>
</tr>

<tr>
<td valign="top">DE </td><td>Gaussian mixture model; phoneme posterior probability; voice conversion;
   deep neural network</td>
</tr>

<tr>
<td valign="top">ID </td><td>MAXIMUM-LIKELIHOOD-ESTIMATION; SPEECH; REPRESENTATION; EXTRACTION;
   PARAMETER; SYSTEMS</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a phonetically-aware joint density Gaussian mixture model (JD-GMM) framework for voice conversion that no longer requires parallel data from source speaker at the training stage. Considering that the phonetic level features contain text information which should be preserved in the conversion task, we propose a method that only concatenates phonetic discriminant features and spectral features extracted from the same target speakers speech to train a JD-GMM. After the mapping relationship of these two features is trained, we can use phonetic discriminant features from source speaker to estimate target speaker's spectral features at conversion stage. The phonetic discriminant features are extracted using PCA from the output layer of a deep neural network (DNN) in an automatic speaker recognition (ASR) system. It can be seen as a low dimensional representation of the senone posteriors. We compare the proposed phonetically-aware method with conventional JD-GMM method on the Voice Conversion Challenge 2016 training database. The experimental results show that our proposed phonetically-aware feature method can obtain similar performance compared to the conventional JD-GMM in the case of using only target speech as training data.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Cai, Weicheng; Zhou, Tianyan; Li, Ming] Sun Yat Sen Univ, SYSU CMU
   Joint Inst Engn, Guangzhou, Guangdong, Peoples R China.
   <br>[Li, Ming] SYSU CMU Shunde Int Joint Res Inst, Shunde, Guangdong,
   Peoples R China.
   <br>[Zheng, Huadi] Hong Kong Polytech Univ, Dept EIE, Hong Kong, Hong Kong,
   Peoples R China.
   <br>[Zhang, Shilei] IBM China Res, Speech Technol &amp; Solut Grp, Beijing,
   Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Li, M (reprint author), Sun Yat Sen Univ, SYSU CMU Joint Inst Engn, Guangzhou, Guangdong, Peoples R China.; Li, M (reprint author), SYSU CMU Shunde Int Joint Res Inst, Shunde, Guangdong, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>liming46@mail.sysu.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>2872</td>
</tr>

<tr>
<td valign="top">EP </td><td>2877</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000406771302142</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ferreira, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ferreira, Anibal</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>IMPLANTATION OF VOICING ON WHISPERED SPEECH USING FREQUENCY-DOMAIN
   PARAMETRIC MODELLING OF SOURCE AND FILTER INFORMATION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2016 INTERNATIONAL SYMPOSIUM ON SIGNAL, IMAGE, VIDEO AND COMMUNICATIONS
   (ISIVC)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Symposium on Signal, Image, Video and Communications
   (ISIVC)</td>
</tr>

<tr>
<td valign="top">CY </td><td>NOV 21-23, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Tunis, TUNISIA</td>
</tr>

<tr>
<td valign="top">ID </td><td>RECONSTRUCTION</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper we address the transformation of whispered speech into natural voiced speech. Representative state-of-the-art solutions are first reviewed as well as a baseline algorithm. For the most part, these solutions fall in the realm of voice conversion strategies since the output signal is obtained as a projection of an input signal. In this paper, we propose a different approach that addresses flexible parametric synthesis of the voiced signal component, as well as its implantation on the whispered signal, in a linguistically consistent way and while trying to convey idiosyncratic information. The most critical functions of phonetic segmentation, spectral envelope estimation, arbitrary periodic wave shape synthesis, and F0 modulation, are described and their operation illustrated with examples.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ferreira, Anibal] Univ Porto, Fac Engn, Dept Elect &amp; Comp Engn, Oporto,
   Portugal.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ferreira, A (reprint author), Univ Porto, Fac Engn, Dept Elect &amp; Comp Engn, Oporto, Portugal.</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Ferreira, Anibal</display_name>&nbsp;</font></td><td><font size="3">B-6599-2016&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Ferreira, Anibal</display_name>&nbsp;</font></td><td><font size="3">0000-0002-7278-6749&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>159</td>
</tr>

<tr>
<td valign="top">EP </td><td>166</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000406467300029</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Lai, JH
   <br>Chen, B
   <br>Tan, T
   <br>Tong, SB
   <br>Yu, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Lai, Jiahao
   <br>Chen, Bo
   <br>Tan, Tian
   <br>Tong, Sibo
   <br>Yu, Kai</td>
</tr>

<tr>
<td valign="top">BE </td><td>Baozong, Y
   <br>Qiuqi, R
   <br>Yao, Z
   <br>Gaoyun AN</td>
</tr>

<tr>
<td valign="top">TI </td><td>Phone-Aware LSTM-RNN for Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>PROCEEDINGS OF 2016 IEEE 13TH INTERNATIONAL CONFERENCE ON SIGNAL
   PROCESSING (ICSP 2016)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Signal Processing</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>13th IEEE International Conference on Signal Processing (ICSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>NOV 06-10, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Chengdu, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; voice conversion challenge; long short-term memory;
   recurrent neural networks; phone</td>
</tr>

<tr>
<td valign="top">ID </td><td>RECURRENT NEURAL-NETWORKS; LAYER</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper investigates a new voice conversion technique using phone-aware Long Short-Term Memory Recurrent Neural Networks (LSTM-RNNs). Most existing voice conversion methods, including Joint Density Gaussian Mixture Models (JDGMMs), Deep Neural Networks (DNNs) and Bidirectional Long Short-Term Memory Recurrent Neural Networks (BLSTM-RNNs), only take acoustic information of speech as features to train models. We propose to incorporate linguistic information to build voice conversion system by using monophones generated by a speech recognizer as linguistic features. The monophones and spectral features are combined together to train LSTM-RNN based voice conversion models, reinforcing the context-dependency modelling of LSTM-RNNs. The results of the 1st voice conversion challenge shows our system achieves significantly higher performance than baseline (GMM method) and was found among the most competitive scores in similarity test. Meanwhile, the experimental results show phone-aware LSTM-RNN method obtains lower Melcepstral distortion and higher MOS scores than the baseline LSTM-RNNs.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Lai, Jiahao; Chen, Bo; Tan, Tian; Tong, Sibo; Yu, Kai] Shanghai Jiao
   Tong Univ, Key Lab Shanghai Educ Commiss Intelligent Interac, Brain Sci
   &amp; Technol Res Ctr, SpeechLab,Dept Comp Sci &amp; Engn, Shanghai, Peoples R
   China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Lai, JH (reprint author), Shanghai Jiao Tong Univ, Key Lab Shanghai Educ Commiss Intelligent Interac, Brain Sci &amp; Technol Res Ctr, SpeechLab,Dept Comp Sci &amp; Engn, Shanghai, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ljhao1993@sjtu.edu.cn; bobmilk@sjtu.edu.cn; tantian@sjtu.edu.cn;
   supertongsibo@sjtu.edu.cn; kai.yu@sjtu.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>177</td>
</tr>

<tr>
<td valign="top">EP </td><td>182</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000406056300035</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hsu, CC
   <br>Hwang, HT
   <br>Wu, YC
   <br>Tsao, Y
   <br>Wang, HM</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hsu, Chin-Cheng
   <br>Hwang, Hsin-Te
   <br>Wu, Yi-Chiao
   <br>Tsao, Yu
   <br>Wang, Hsin-Min</td>
</tr>

<tr>
<td valign="top">BE </td><td>Lee, T
   <br>Xie, L
   <br>Dang, J
   <br>Wang, HM
   <br>Wei, J
   <br>Feng, H
   <br>Hou, Q
   <br>Wei, Y</td>
</tr>

<tr>
<td valign="top">TI </td><td>Dictionary Update for NMF-based Voice Conversion Using an
   Encoder-Decoder Network</td>
</tr>

<tr>
<td valign="top">SO </td><td>2016 10TH INTERNATIONAL SYMPOSIUM ON CHINESE SPOKEN LANGUAGE PROCESSING
   (ISCSLP)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>10th International Symposium on Chinese Spoken Language Processing
   (ISCSLP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 17-20, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Tianjin, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; autoencoder; NMF; dictionary update</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we propose a dictionary update method for Nonnegative Matrix Factorization (NMF) with high dimensional data in a spectral conversion (SC) task. Voice conversion has been widely studied due to its potential applications such as personalized speech synthesis and speech enhancement. Exemplarbased NMF (ENMF) emerges as an effective and probably the simplest choice among all techniques for SC, as long as a source-target parallel speech corpus is given. ENMF-based SC systems usually need a large amount of bases (exemplars) to ensure the quality of the converted speech. However, a small and effective dictionary is desirable but hard to obtain via dictionary update, in particular when high-dimensional features such as STRAIGHT spectra are used. Therefore, we propose a dictionary update framework for NMF by means of an encoderdecoder reformulation. Regarding NMF as an encoder-decoder network makes it possible to exploit the whole parallel corpus more effectively and efficiently when applied to SC. Our experiments demonstrate significant gains of the proposed system with small dictionaries over conventional ENMF-based systems with dictionaries of same or much larger size.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Hsu, Chin-Cheng; Hwang, Hsin-Te; Wu, Yi-Chiao; Wang, Hsin-Min] Acad
   Sinica, Inst Informat Sci, Taipei, Taiwan.
   <br>[Tsao, Yu] Acad Sinica, Res Ctr Informat Technol Innovat, Taipei, Taiwan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hsu, CC (reprint author), Acad Sinica, Inst Informat Sci, Taipei, Taiwan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jeremvcchsu@iis.sinica.edu.tw; hwanght@iis.sinica.edu.tw;
   tedwu@iis.sinica.edu.tw; yu.tsao@citi.sinica.edu.tw;
   whm@iis.sinica.edu.tw</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000405610900020</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Li, RN
   <br>Wu, ZY
   <br>Meng, H
   <br>Cai, LH</td>
</tr>

<tr>
<td valign="top">AF </td><td>Li, Runnan
   <br>Wu, Zhiyong
   <br>Meng, Helen
   <br>Cai, Lianhong</td>
</tr>

<tr>
<td valign="top">BE </td><td>Lee, T
   <br>Xie, L
   <br>Dang, J
   <br>Wang, HM
   <br>Wei, J
   <br>Feng, H
   <br>Hou, Q
   <br>Wei, Y</td>
</tr>

<tr>
<td valign="top">TI </td><td>DBLSTM-based Multi-task Learning for Pitch Transformation in Voice
   Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>2016 10TH INTERNATIONAL SYMPOSIUM ON CHINESE SPOKEN LANGUAGE PROCESSING
   (ISCSLP)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>10th International Symposium on Chinese Spoken Language Processing
   (ISCSLP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 17-20, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Tianjin, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; pitch transformation; deep bidirectional long
   short-term memory (DBLSTM); multi-task learning (MTL)</td>
</tr>

<tr>
<td valign="top">AB </td><td>While both spectral and prosody transformation are important for voice conversion (VC), traditional methods have focused on the conversion of spectral features with less emphasis on prosody transformation. This paper presents a novel pitch transformation method for VC. As the correlation of spectral features and fundamental frequency in pitch perceptions has been proved, well-converted spectrum should benefit to pitch transformation. Motivated by this, a multi-task learning (MTL) framework based on deep bidirectional long short-term memory (DBLSTM) recurrent neural network (RNN) has been proposed for pitch transformation in VC. DBLSTM is used to model the long short-term dependencies across speech frames for spectral conversion; the converted spectrum and the source pitch contour are further simultaneously modeled to generate the converted target pitch contour and voiced/unvoiced flag; the above tasks are incorporated with the MTL framework to enhance the performances of each other. Experimental results indicate the proposed method outperforms the conventional approaches in pitch transformation.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Li, Runnan; Wu, Zhiyong; Meng, Helen; Cai, Lianhong] Tsinghua Univ,
   Grad Sch Shenzhen, Tsinghua CUHK Joint Res Ctr Media Sci Technol &amp; S,
   Beijing, Peoples R China.
   <br>[Wu, Zhiyong; Meng, Helen] Chinese Univ Hong Kong, Dept Syst Engn &amp; Engn
   Management, Hong Kong, Hong Kong, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Li, RN (reprint author), Tsinghua Univ, Grad Sch Shenzhen, Tsinghua CUHK Joint Res Ctr Media Sci Technol &amp; S, Beijing, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>lirn15@mails.tsinghua.edu.cn; zywu@se.cuhk.edu.hk;
   hmmeng@se.cuhk.edu.hk; clh-dcs@tsinghua.edu.cn</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Meng, Helen</display_name>&nbsp;</font></td><td><font size="3">F-6043-2011&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Meng, Helen</display_name>&nbsp;</font></td><td><font size="3">0000-0002-4427-3532&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000405610900104</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Dhananjaya, MS
   <br>Krupa, BN
   <br>Sushma, R</td>
</tr>

<tr>
<td valign="top">AF </td><td>Dhananjaya, M. S.
   <br>Krupa, Niranjana B.
   <br>Sushma, R.</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Kannada Text to Speech Conversion: A Novel Approach</td>
</tr>

<tr>
<td valign="top">SO </td><td>2016 INTERNATIONAL CONFERENCE ON ELECTRICAL, ELECTRONICS, COMMUNICATION,
   COMPUTER AND OPTIMIZATION TECHNIQUES (ICEECCOT)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Electrical, Electronics, Communication,
   Computer and Optimization Techniques (ICEECCOT)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 09-10, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Mysuru, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speech Synthesis; Kannada Language; Speech Coeffecients; Phonemes;
   Audacity</td>
</tr>

<tr>
<td valign="top">AB </td><td>Conversion of linguistic scripts to speech will help in improving the lives of people who are unable to read. According to literature survey conversion of English text to speech has been attempted by several researchers, however, there is less research in speech synthesis of local languages. In this paper, the authors propose an algorithm to translate Kannada language text to speech. Here, direct concatenation of speech coefficients extracted from prerecorded voice is used for conversion. The proposed algorithm is also compared with another speech synthesizer which is widely used, to evaluate its performance.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Dhananjaya, M. S.; Sushma, R.] PESIT, Dept TCE, Bangalore, Karnataka,
   India.
   <br>[Krupa, Niranjana B.] PES Univ, Dept ECE, Bangalore, Karnataka, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Dhananjaya, MS (reprint author), PESIT, Dept TCE, Bangalore, Karnataka, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>dhanujss@gmail.com; bnkrupa@pes.edu; sushmarawal@pes.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>168</td>
</tr>

<tr>
<td valign="top">EP </td><td>172</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000404444000034</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Djuana, E
   <br>Agoes, S
   <br>Mardian, RD
   <br>Nurmalasari, RN</td>
</tr>

<tr>
<td valign="top">AF </td><td>Djuana, Endang
   <br>Agoes, Suhartati
   <br>Mardian, R. Deiny
   <br>Nurmalasari, Revi Noviananda</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Implementation of Finite Impulse Response Digital Filter in Digital
   Signal Processor Kit for Voice Signal Application</td>
</tr>

<tr>
<td valign="top">SO </td><td>2016 10TH INTERNATIONAL CONFERENCE ON TELECOMMUNICATION SYSTEMS SERVICES
   AND APPLICATIONS (TSSA)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Telecommunication Systems Services and
   Applications</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>10th International Conference on Telecommunication Systems Services and
   Applications (TSSA)</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 06-07, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Denpasar, INDONESIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice signal; FIR digital filter; DSP kit</td>
</tr>

<tr>
<td valign="top">AB </td><td>Transmission and receiving of voice signal must be clear and understandable. To maintain the voice quality, filtering is needed. Filtering is a part of signal processing to improve the output signal quality. Digital filter is being used for filtering. Finite Impulse Response (FIR) filter has many advantages such as linear phase, stability, and minimum error caused by quantization. In this research, a filter implementation using digital signal processor (DSP) kit device with voice signal as input has been done. Voice signal will experience Analog to Digital Conversion (ADC), filtering and Digital to Analog (DAC) processes. Filtering outputs are heard on the loudspeaker and the graphs are shown on Matlab scope.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Djuana, Endang; Agoes, Suhartati; Mardian, R. Deiny; Nurmalasari, Revi
   Noviananda] Trisakti Univ, Fac Ind Technol, Dept Elect Engn, Jakarta,
   Indonesia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Djuana, E (reprint author), Trisakti Univ, Fac Ind Technol, Dept Elect Engn, Jakarta, Indonesia.</td>
</tr>

<tr>
<td valign="top">EM </td><td>edjuana@trisakti.ac.id; sagoes@trisakti.ac.id; deiny_wp@trisakti.ac.id;
   revi.noviananda@gmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">SC </td><td>Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000404344900024</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Shinde, SS
   <br>Autee, RM
   <br>Bhosale, VK</td>
</tr>

<tr>
<td valign="top">AF </td><td>Shinde, Shweta S.
   <br>Autee, Rajesh M.
   <br>Bhosale, Vitthal K.</td>
</tr>

<tr>
<td valign="top">BE </td><td>Krishnan, N
   <br>Karthikeyan, M</td>
</tr>

<tr>
<td valign="top">TI </td><td>Real Time Two Way Communication Approach for Hearing Impaired and Dumb
   Person Based on Image Processing</td>
</tr>

<tr>
<td valign="top">SO </td><td>2016 IEEE INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND
   COMPUTING RESEARCH</td>
</tr>

<tr>
<td valign="top">SE </td><td>IEEE International Conference on Computational Intelligence and
   Computing Research</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>7th IEEE International Conference on Computational Intelligence and
   Computing Research (ICCIC)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 15-17, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Agni Coll Technol, Chennai, INDIA</td>
</tr>

<tr>
<td valign="top">HO </td><td>Agni Coll Technol</td>
</tr>

<tr>
<td valign="top">DE </td><td>Deaf and Dump; Hand Gesture Recognition; Voice Conversion; Gesture to
   Speech; Speech to Gesture Conversion</td>
</tr>

<tr>
<td valign="top">AB </td><td>In the recent years, there has been rapid increase in the number of deaf and dumb victims due to birth defects, accidents and oral diseases. Since deaf and dumb people cannot communicate with normal person so they have to depend on some sort of visual communication. Gesture shows an expressive movement of body parts such as physical movements of head, face, arms, hand or body which convey some message. Gesture recognition is the mathematical interpretation of a human motion by a computing device. Sign language provide best communication platform for the hearing impaired and dumb person to communicate with normal person. The objective of this research is to develop a real time system for hand gesture recognition which recognize hand gestures, features of hands such as peak calculation and angle calculation and then convert gesture images into voice and vice versa. To implement this system we use a simple night vision web-cam with 20 megapixel intensity. The ideas consisted of designing and implement a system using artificial intelligence, image processing and data mining concepts to take input as hand gestures and generate recognizable outputs in the form of text and voice with 91% accuracy.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Shinde, Shweta S.; Autee, Rajesh M.; Bhosale, Vitthal K.] Deogiri Inst
   Engn &amp; Management Studies, Elect &amp; Telecommun Dept, Aurangabad,
   Maharashtra, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Shinde, SS (reprint author), Deogiri Inst Engn &amp; Management Studies, Elect &amp; Telecommun Dept, Aurangabad, Maharashtra, India.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>248</td>
</tr>

<tr>
<td valign="top">EP </td><td>252</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000403590100050</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Bhaskaran, KA
   <br>Nair, AG
   <br>Ram, KD
   <br>Ananthanarayanan, K
   <br>Vardhan, HRN</td>
</tr>

<tr>
<td valign="top">AF </td><td>Bhaskaran, Abhijith K.
   <br>Nair, Anoop G.
   <br>Ram, Deepak K.
   <br>Ananthanarayanan, Krishnan
   <br>Vardhan, H. R. Nandi</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Smart Gloves for Hand Gesture Recognition Sign Language to Speech
   Conversion System</td>
</tr>

<tr>
<td valign="top">SO </td><td>2016 INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION FOR
   HUMANITARIAN APPLICATIONS (RAHA)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Robotics and Automation for Humanitarian
   Applications (RAHA)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 18-20, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Amrita Univ, INDIA</td>
</tr>

<tr>
<td valign="top">HO </td><td>Amrita Univ</td>
</tr>

<tr>
<td valign="top">DE </td><td>Glove; Indian Sign language; Flex Sensors; Inertial Measurement Unit;
   State Estimation Method; Three Dimensional Space; gaming; robotics;
   medical field</td>
</tr>

<tr>
<td valign="top">AB </td><td>People with speech impairment find it difficult to communicate in a society where most of the people do not understand sign language. The idea proposed in this paper is a smart glove which can convert sign language to speech output. The glove is embedded with flex sensors and an Inertial Measurement Unit (IMU) to recognize the gesture. A novel method of State Estimation has been developed to track the motion of hand in three dimensional spaces. The prototype was tested for its feasibility in converting Indian Sign Language to voice output. Though the glove is intended for sign language to speech conversion, it is a multipurpose glove and finds its applications in gaming, robotics and medical field.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Bhaskaran, Abhijith K.] Amrita Sch Engn, Dept Elect &amp; Commun Engn,
   Bengaluru, India.
   <br>Amrita Vishwa Vidyapeetham Amrita Univ, Coimbatore, Tamil Nadu, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Bhaskaran, KA (reprint author), Amrita Sch Engn, Dept Elect &amp; Commun Engn, Bengaluru, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>abhijithbhaskarank@gmail.com; anup21195@gmail.com; deepak7946@gmail.com;
   akrishnan394@gmail.com; hr_nandivardhan@blr.amrita.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>226</td>
</tr>

<tr>
<td valign="top">EP </td><td>231</td>
</tr>

<tr>
<td valign="top">SC </td><td>Automation &amp; Control Systems; Robotics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000403770900036</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Keddie, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Keddie, Amanda</td>
</tr>

<tr>
<td valign="top">TI </td><td>Academisation, school collaboration and the primary school sector in
   England: a story of six school leaders</td>
</tr>

<tr>
<td valign="top">SO </td><td>SCHOOL LEADERSHIP &amp; MANAGEMENT</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>School autonomy; academies; collaboration; primary schools</td>
</tr>

<tr>
<td valign="top">ID </td><td>ISSUES</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents data from a study of five English primary schools. It examines some of the challenges associated with school autonomy and collaboration for state primary schools amid the uncertainty and complexity of governance in the present English education context. The paper features the voices of six leaders gathered from interviews that explored their thoughts about the academies movement. It highlights their fears that academisation, and particularly the imperative to join a large academy chain, will undermine their autonomy as individual schools. Accepting of the inevitability of academisation and the forms of network governance this reform offers, it highlights the head teachers' moves to ensure their autonomy in terms of determining the timing and type of conversion. In relation to these moves, the paper reiterates the significance within effective collaboratives of member schools experiencing a sense of ownership, a common purpose, shared responsibility for students and their learning and relations of trust. The paper considers some of the tensions arising in this space in relation to competition, collaboration and school vulnerability.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Keddie, Amanda] Univ Queensland, Sch Educ, Brisbane, Qld, Australia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Keddie, A (reprint author), Univ Queensland, Sch Educ, Brisbane, Qld, Australia.</td>
</tr>

<tr>
<td valign="top">EM </td><td>amanda.keddie@deakin.edu.au</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>36</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>169</td>
</tr>

<tr>
<td valign="top">EP </td><td>183</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1080/13632434.2016.1196174</td>
</tr>

<tr>
<td valign="top">SC </td><td>Education &amp; Educational Research</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000402202400003</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kobayashi, K
   <br>Toda, T
   <br>Nakamura, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kobayashi, Kazuhiro
   <br>Toda, Tomoki
   <br>Nakamura, Satoshi</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>F0 TRANSFORMATION TECHNIQUES FOR STATISTICAL VOICE CONVERSION WITH
   DIRECTWAVEFORM MODIFICATION WITH SPECTRAL DIFFERENTIAL</td>
</tr>

<tr>
<td valign="top">SO </td><td>2016 IEEE WORKSHOP ON SPOKEN LANGUAGE TECHNOLOGY (SLT 2016)</td>
</tr>

<tr>
<td valign="top">SE </td><td>IEEE Workshop on Spoken Language Technology</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE Workshop on Spoken Language Technology (SLT)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 13-16, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>San Diego, CA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; speaker identity; F-0 transformation; Gaussian mixture
   model; direct waveform modification</td>
</tr>

<tr>
<td valign="top">ID </td><td>PLUS NOISE MODEL</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents several F-0 transformation techniques for statistical voice conversion (VC) with direct waveform modification with spectral differential (DIFFVC). Statistical VC is a technique to convert speaker identity of a source speaker's voice into that of a target speaker by converting several acoustic features, such as spectral and excitation features. This technique usually uses vocoder to generate converted speech waveforms from the converted acoustic features. However, the use of vocoder often causes speech quality degradation of the converted voice owing to insufficient parameterization accuracy. To avoid this issue, we have proposed a direct waveform modification technique based on spectral differential filtering and have successfully applied it to intra-gender singing VC (DIFFSVC) where excitation features are not necessary converted. Moreover, we have also applied it to cross-gender singing VC by implementing F-0 transformation with a constant rate such as one octave increase or decrease. On the other hand, it is not straightforward to apply the DIFFSVC framework to normal speech conversion because the F-0 transformation ratio widely varies depending on a combination of the source and target speakers. In this paper, we propose several F-0 transformation techniques for DIFFVC and compare their performance in terms of speech quality of the converted voice and conversion accuracy of speaker individuality. The experimental results demonstrate that the F-0 transformation technique based on waveform modification achieves the best performance among the proposed techniques.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kobayashi, Kazuhiro; Nakamura, Satoshi] Nara Inst Sci &amp; Technol NAIST,
   Ikoma, Nara, Japan.
   <br>[Toda, Tomoki] Nagoya Univ, Informat Technol Ctr, Nagoya, Aichi, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kobayashi, K (reprint author), Nara Inst Sci &amp; Technol NAIST, Ikoma, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>kazuhiro-k@is.naist.jp; tomoki@icts.nagoya-u.ac.jp;
   s-nakamura@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>693</td>
</tr>

<tr>
<td valign="top">EP </td><td>700</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000399128000101</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Erro, D
   <br>Hernaez, I
   <br>Serrano, L
   <br>Saratxaga, I
   <br>Navas, E</td>
</tr>

<tr>
<td valign="top">AF </td><td>Erro, Daniel
   <br>Hernaez, Inma
   <br>Serrano, Luis
   <br>Saratxaga, Ibon
   <br>Navas, Eva</td>
</tr>

<tr>
<td valign="top">BE </td><td>Abad, A
   <br>Ortega, A
   <br>Teixeira, A
   <br>Mateo, CG
   <br>Hinarejos, CDM
   <br>Perdigao, F
   <br>Batista, F
   <br>Mamede, N</td>
</tr>

<tr>
<td valign="top">TI </td><td>Objective Comparison of Four GMM-Based Methods for PMA-to-Speech
   Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>ADVANCES IN SPEECH AND LANGUAGE TECHNOLOGIES FOR IBERIAN LANGUAGES,
   IBERSPEECH 2016</td>
</tr>

<tr>
<td valign="top">SE </td><td>Lecture Notes in Computer Science</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>5th Iberian SLTech Workshop / 3rd International Conference on Advances
   in Speech and Language Technologies for Iberian Languages (IberSPEECH)</td>
</tr>

<tr>
<td valign="top">CY </td><td>NOV 23-25, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Lisbon, PORTUGAL</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION; ENHANCEMENT; ESOPHAGEAL; MODEL</td>
</tr>

<tr>
<td valign="top">AB </td><td>In silent speech interfaces a mapping is established between biosignals captured by sensors and acoustic characteristics of speech. Recent works have shown the feasibility of a silent interface based on permanent magnet-articulography (PMA). This paper studies the performance of four different mapping methods based on Gaussian mixture models (GMMs), typical from the voice conversion field, when applied to PMA-to-spectrum conversion. The results show the superiority of methods based on maximum likelihood parameter generation (MLPG), especially when the parameters of the mapping function are trained by minimizing the generation error. Informal listening tests reveal that the resulting speech is moderately intelligible for the database under study.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Erro, Daniel; Hernaez, Inma; Serrano, Luis; Saratxaga, Ibon; Navas,
   Eva] Univ Basque Country, UPV EHU, Aholab, Bilbao, Spain.
   <br>[Erro, Daniel] Basque Fdn Sci, Ikerbasque, Bilbao, Spain.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Erro, D (reprint author), Univ Basque Country, UPV EHU, Aholab, Bilbao, Spain.; Erro, D (reprint author), Basque Fdn Sci, Ikerbasque, Bilbao, Spain.</td>
</tr>

<tr>
<td valign="top">EM </td><td>derro@aholab.ehu.es</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Navas, Eva</display_name>&nbsp;</font></td><td><font size="3">H-4317-2013&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Saratxaga, Ibon</display_name>&nbsp;</font></td><td><font size="3">H-6423-2015&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Hernaez Rioja, Inmaculada</display_name>&nbsp;</font></td><td><font size="3">K-8303-2012&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Navas, Eva</display_name>&nbsp;</font></td><td><font size="3">0000-0003-3804-4984&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Saratxaga, Ibon</display_name>&nbsp;</font></td><td><font size="3">0000-0001-7282-2765&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Hernaez Rioja, Inmaculada</display_name>&nbsp;</font></td><td><font size="3">0000-0003-4447-7575&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>10077</td>
</tr>

<tr>
<td valign="top">BP </td><td>24</td>
</tr>

<tr>
<td valign="top">EP </td><td>32</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/978-3-319-49169-1_3</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000389797600003</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Freixes, M
   <br>Socoro, JC
   <br>Alias, F</td>
</tr>

<tr>
<td valign="top">AF </td><td>Freixes, Marc
   <br>Claudi Socoro, Joan
   <br>Alias, Francesc</td>
</tr>

<tr>
<td valign="top">BE </td><td>Abad, A
   <br>Ortega, A
   <br>Teixeira, A
   <br>Mateo, CG
   <br>Hinarejos, CDM
   <br>Perdigao, F
   <br>Batista, F
   <br>Mamede, N</td>
</tr>

<tr>
<td valign="top">TI </td><td>Adding Singing Capabilities to Unit Selection TTS Through HNM-Based
   Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>ADVANCES IN SPEECH AND LANGUAGE TECHNOLOGIES FOR IBERIAN LANGUAGES,
   IBERSPEECH 2016</td>
</tr>

<tr>
<td valign="top">SE </td><td>Lecture Notes in Computer Science</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>5th Iberian SLTech Workshop / 3rd International Conference on Advances
   in Speech and Language Technologies for Iberian Languages (IberSPEECH)</td>
</tr>

<tr>
<td valign="top">CY </td><td>NOV 23-25, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Lisbon, PORTUGAL</td>
</tr>

<tr>
<td valign="top">DE </td><td>Unit-selection TTS; Speech-to-singing; Text-to-singing; Harmonic plus
   Noise Model; Prosody modification</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE</td>
</tr>

<tr>
<td valign="top">AB </td><td>Adding singing capabilities to a corpus-based concatenative text-to-speech (TTS) system can be addressed by explicitly collecting singing samples from the previously recorded speaker. However, this approach is only feasible if the considered speaker is also a singing talent. As an alternative, we consider appending a Harmonic plus Noise Model (HNM) speech-to-singing conversion module to a Unit Selection TTS (US-TTS) system. Two possible text-to-speech-to-singing synthesis approaches are studied: applying the speech-to-singing conversion to the US-TTS synthetic output, or implementing a hybrid US+HNM synthesis framework. The perceptual tests show that the speech-to-singing conversion yields similar singing resemblance than the natural version, but with lower naturalness. Moreover, no statistically significant differences are found between both strategies in terms of naturalness nor singing resemblance. Finally, the hybrid approach allows reducing more than twice the overall computational cost.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Freixes, Marc; Claudi Socoro, Joan; Alias, Francesc] La Salle Univ
   Ramon Llull, GTM Grp Recerca Tecnol Media, Quatre Camins 30, Barcelona
   08022, Spain.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Freixes, M (reprint author), La Salle Univ Ramon Llull, GTM Grp Recerca Tecnol Media, Quatre Camins 30, Barcelona 08022, Spain.</td>
</tr>

<tr>
<td valign="top">EM </td><td>mfreixes@salleur1.edu; jclaudi@salleur1.edu; falias@salleur1.edu</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Freixes, Marc</display_name>&nbsp;</font></td><td><font size="3">0000-0001-9363-9513&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>10077</td>
</tr>

<tr>
<td valign="top">BP </td><td>33</td>
</tr>

<tr>
<td valign="top">EP </td><td>43</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/978-3-319-49169-1_4</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000389797600004</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hashimoto, T
   <br>Saito, D
   <br>Minematsu, N</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hashimoto, Tetsuya
   <br>Saito, Daisuke
   <br>Minematsu, Nobuaki</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Arbitrary speaker conversion based on speaker space bases constructed by
   deep neural networks</td>
</tr>

<tr>
<td valign="top">SO </td><td>2016 ASIA-PACIFIC SIGNAL AND INFORMATION PROCESSING ASSOCIATION ANNUAL
   SUMMIT AND CONFERENCE (APSIPA)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Asia-Pacific Signal and Information Processing Association Annual Summit
   and Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Asia-Pacific Signal and Information Processing Association Annual Summit
   and Conference (APSIPA)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 13-16, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Jeju, SOUTH KOREA</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH RECOGNITION; VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper proposes a novel approach to construct a Deep Neural Network (DNN) based voice conversion (VC) system, where DNNs are integrated with speaker eigenspace. The proposed network consists of multiple DNNs and each of them converts input features to features corresponding to a base of eigenspace. Training of these DNNs is achieved with the assistance of Eigenvoice GMM (EVGMM). Experimental evaluations using one-to-many VC tasks show that the proposed method achieved better performance compared with that of EVGMM.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Hashimoto, Tetsuya; Minematsu, Nobuaki] Univ Tokyo, Grad Sch Engn,
   Tokyo 1138654, Japan.
   <br>[Saito, Daisuke] Univ Tokyo, Grad Sch Informat Sci &amp; Technol, Tokyo
   1138654, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hashimoto, T (reprint author), Univ Tokyo, Grad Sch Engn, Tokyo 1138654, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>hashib@gavo.t.u-tokyo.ac.jp; dsk_saito@gavo.t.u-tokyo.ac.jp;
   mine@gavo.t.u-tokyo.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000393591800159</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Tanaka, K
   <br>Hara, S
   <br>Abe, M
   <br>Minagi, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Tanaka, Kei
   <br>Hara, Sunao
   <br>Abe, Masanobu
   <br>Minagi, Shogo</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Enhancing a Glossectomy Patient's Speech via GMM-based Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>2016 ASIA-PACIFIC SIGNAL AND INFORMATION PROCESSING ASSOCIATION ANNUAL
   SUMMIT AND CONFERENCE (APSIPA)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Asia-Pacific Signal and Information Processing Association Annual Summit
   and Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Asia-Pacific Signal and Information Processing Association Annual Summit
   and Conference (APSIPA)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 13-16, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Jeju, SOUTH KOREA</td>
</tr>

<tr>
<td valign="top">ID </td><td>PROSTHESES</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we describe the use of a voice conversion algorithm for improving the intelligibility of speech by patients with articulation disorders caused by a wide glossectomy and/or segmental mandibulectomy. As a first trial, to demonstrate the difficulty of the task at hand, we implemented a conventional Gaussian mixture model (GMM)-based algorithm using a frame-by-frame approach. We compared voice conversion performance among normal speakers and one with an articulation disorder by measuring the number of training sentences, the number of GMM mixtures, and the variety of speaking styles of training speech. According to our experiment results, the mel-cepstrum (MC) distance was decreased by 40% in all pairs of speakers as compared with that of pre-conversion measures; however, at post-conversion, the MC distance between a pair of a glossectomy speaker and a normal speaker was 28% larger than that between pairs of normal speakers. The analysis of resulting spectrograms showed that the voice conversion algorithm successfully reconstructed high-frequency spectra in phonemes /h/, /t/, /k/, /ts/, and /ch/; we also confirmed improvements of speech intelligibility via informal listening tests.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Tanaka, Kei; Hara, Sunao; Abe, Masanobu] Okayama Univ, Grad Sch Nat Sci
   &amp; Technol, Okayama 7008530, Japan.
   <br>[Minagi, Shogo] Okayama Univ, Grad Sch Med Dent &amp; Pharmaceut Sci,
   Okayama 7008530, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Tanaka, K (reprint author), Okayama Univ, Grad Sch Nat Sci &amp; Technol, Okayama 7008530, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>pjot7wfu@s.okayama-u.ac.jp; hara@cs.okayama-u.ac.jp;
   abe@cs.okayama-u.ac.jp; minagi@md.okayama-u.ac.jp</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>ABE, Masanobu</display_name>&nbsp;</font></td><td><font size="3">B-2626-2011&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>ABE, Masanobu</display_name>&nbsp;</font></td><td><font size="3">0000-0002-3731-6354&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000393591800237</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wen, ZQ
   <br>Li, KH
   <br>Tao, JH
   <br>Lee, CH</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wen, Zhengqi
   <br>Li, Kehuang
   <br>Tao, Jianhua
   <br>Lee, Chin-Hui</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Deep Neural Network based Voice Conversion with A Large Synthesized
   Parallel Corpus</td>
</tr>

<tr>
<td valign="top">SO </td><td>2016 ASIA-PACIFIC SIGNAL AND INFORMATION PROCESSING ASSOCIATION ANNUAL
   SUMMIT AND CONFERENCE (APSIPA)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Asia-Pacific Signal and Information Processing Association Annual Summit
   and Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Asia-Pacific Signal and Information Processing Association Annual Summit
   and Conference (APSIPA)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 13-16, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Jeju, SOUTH KOREA</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH; TRANSFORMATION; ALGORITHMS</td>
</tr>

<tr>
<td valign="top">AB </td><td>we propose a voice conversion framework to map the speech features of a source speaker to a target speaker based on deep neural networks (DNNs). Due to a limited availability of the parallel data needed for a pair of source and target speakers, speech synthesis and dynamic time warping are utilized to construct a large parallel corpus for DNN training. With a small corpus to train DNNs, a lower log spectral distortion can still be seen over the conventional Gaussian mixture model (GMM) approach, trained with the same data. With the synthesized parallel corpus, a speech naturalness preference score of about 54.5% vs. 32.8% and a speech similarity preference score of about 52.5% vs. 23.6% are observed for the DNN-converted speech from the large parallel corpus when compared with the DNN-converted speech from the small parallel corpus.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Wen, Zhengqi; Tao, Jianhua] Chinese Acad Sci, Inst Automat, Natl Lab
   Pattern Recognit, Beijing, Peoples R China.
   <br>[Li, Kehuang; Lee, Chin-Hui] Georgia Inst Technol, Sch ECE, Atlanta, GA
   30332 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Wen, ZQ (reprint author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>zqwen@nlpr.ia.ac.cn; kehle@gatech.edu; jhtao@nlpr.ia.ac.cn;
   chl@ece.gatech.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000393591800044</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wu, JE
   <br>Wu, ZZ
   <br>Xie, L</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wu, Jie
   <br>Wu, Zhizheng
   <br>Xie, Lei</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>On the Use of I-vectors and Average Voice Model for Voice Conversion
   without Parallel Data</td>
</tr>

<tr>
<td valign="top">SO </td><td>2016 ASIA-PACIFIC SIGNAL AND INFORMATION PROCESSING ASSOCIATION ANNUAL
   SUMMIT AND CONFERENCE (APSIPA)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Asia-Pacific Signal and Information Processing Association Annual Summit
   and Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Asia-Pacific Signal and Information Processing Association Annual Summit
   and Conference (APSIPA)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 13-16, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Jeju, SOUTH KOREA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; nonparallel training; average voice model; i-vector;
   long short-term memory</td>
</tr>

<tr>
<td valign="top">AB </td><td>Recently, deep and/or recurrent neural networks (DNNs/RNNs) have been employed for voice conversion, and have significantly improved the performance of converted speech. However, DNNs/RNNs generally require a large amount of parallel training data (e.g., hundreds of utterances) from source and target speakers. It is expensive to collect such a large amount of data, and impossible in some applications, such as cross-lingual conversion. To solve this problem, we propose to use average voice model and i-vectors for long short-term memory (LSTM) based voice conversion, which does not require parallel data from source and target speakers. The average voice model is trained using other speakers' data, and the i-vectors, a compact vector representing the identities of source and target speakers, are extracted independently. Subjective evaluation has confirmed the effectiveness of the proposed approach.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Wu, Jie; Xie, Lei] Northwestern Polytech Univ, Sch Comp Sci, Shaanxi
   Prov Key Lab Speech &amp; Image Informat Proc, Xian, Peoples R China.
   <br>[Wu, Zhizheng] Univ Edinburgh, Ctr Speech Technol Res, Edinburgh EH8
   9YL, Midlothian, Scotland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Wu, J (reprint author), Northwestern Polytech Univ, Sch Comp Sci, Shaanxi Prov Key Lab Speech &amp; Image Informat Proc, Xian, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jiewu@nwpu-aslp.org; wuzhizheng@gmail.com; lxie@nwpu-aslp.org</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000393591800229</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ayodeji, AO
   <br>Oyetunji, SA</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ayodeji, Agbolade Olaide
   <br>Oyetunji, S. A.</td>
</tr>

<tr>
<td valign="top">BE </td><td>AlShibaany, ZYA
   <br>Hameed, AF</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Conversion Using Coefficient Mapping and Neural Network</td>
</tr>

<tr>
<td valign="top">SO </td><td>2016 INTERNATIONAL CONFERENCE FOR STUDENTS ON APPLIED ENGINEERING
   (ICSAE)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference for Students on Applied Engineering (ICSAE)</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 20-21, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Newcastle upon Tyne, ENGLAND</td>
</tr>

<tr>
<td valign="top">DE </td><td>LSF; LPC; stability; neural network; voice conversion; mapping; poles;
   excitation</td>
</tr>

<tr>
<td valign="top">AB </td><td>The research presents a voice conversion model using coefficient mapping and neural network. Most previous works on parametric speech synthesis did not account for losses in spectral details causing over smoothing and invariably, an appreciable deviation of the converted speech from the targeted speaker. An improved model that uses both linear predictive coding (LPC) and line spectral frequency (LSF) coefficients to parametrize the source speech signal was developed in this work to reveal the effect of over-smoothing. Non-linear mapping ability of neural network was employed in mapping the source speech vectors into the acoustic vector space of the target. Training LPC coefficients with neural network yielded a poor result due to the instability of the LPC filter poles. The LPC coefficients were converted to line spectral frequency coefficients before been trained with a 3-layer neural network. The algorithm was tested with noisy data with the result evaluated using Mel-Cepstral Distance measurement. Cepstral distance evaluation shows a 35.7 percent reduction in the spectral distance between the target and the converted speech.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ayodeji, Agbolade Olaide; Oyetunji, S. A.] Fed Univ Technol Akure, Dept
   Elect &amp; Elect Engn, Akure, Nigeria.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ayodeji, AO (reprint author), Fed Univ Technol Akure, Dept Elect &amp; Elect Engn, Akure, Nigeria.</td>
</tr>

<tr>
<td valign="top">EM </td><td>olaideagbolade@gmail.com; Samlove98ng@yahoo.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>479</td>
</tr>

<tr>
<td valign="top">EP </td><td>483</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000392935500089</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sanjoyo, DD
   <br>Munadi, R
   <br>Adjie, LF
   <br>Adiprabowo, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sanjoyo, Danu Dwi
   <br>Munadi, Rendy
   <br>Adjie, Fidar L.
   <br>Adiprabowo, Tjahjo</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Inter-regional Voice Bandwidth Calculation on IMS Network</td>
</tr>

<tr>
<td valign="top">SO </td><td>2016 INTERNATIONAL CONFERENCE ON CONTROL, ELECTRONICS, RENEWABLE ENERGY
   AND COMMUNICATIONS (ICCEREC)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Control, Electronics, Renewable Energy and
   Communications (ICCEREC)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 13-15, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Bandung, INDONESIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>bandwidth; VoIP; inter-regional; IMS; traffic</td>
</tr>

<tr>
<td valign="top">AB </td><td>The inter-regional telecommunication network design in Indonesia is strongly influenced by the bandwidth of Voice over Internet Protocol (VoIP), where 40% of the national bandwidth is used to pass voice communication. Indonesia region is divided into seven regional areas; each of these is supported by two IMS Cores, which serves as the active core and the stand-by core. Regionals are interconnected with each other through an IP backbone network. It serves to get through a number of bandwidth when there is communication between the regionals. The bandwidth requirement can be obtained through the calculation of traffic in regional, inter-regional traffic, and along with demographic data as well as the number of customers who have registered on the IMS network. Since voice traffic is passed in IP networks, the voice bandwidth is calculated in bps (bits per second) units. Standard voice codecs used for the conversion are G.711 and G.729. The national bandwidth load for voice traffic on the IMS backbone network can be obtained using mathematical calculations. The results of the calculation are presented in bandwidth matrices of the seven regions traffic for each standard (G.711 and G.729).</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Sanjoyo, Danu Dwi; Munadi, Rendy; Adiprabowo, Tjahjo] Telkom Univ, Sch
   Elect Engn, Bandung, Indonesia.
   <br>[Adjie, Fidar L.] IDeC Telkom, Bandung, Indonesia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sanjoyo, DD (reprint author), Telkom Univ, Sch Elect Engn, Bandung, Indonesia.</td>
</tr>

<tr>
<td valign="top">EM </td><td>danudwj@telkomuniversity.ac.id; rendymunadi@telkomuniversity.ac.id;
   720416@telkom.co.id; tjahjoadiprabowo@telkomuniversity.ac.id</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>35</td>
</tr>

<tr>
<td valign="top">EP </td><td>40</td>
</tr>

<tr>
<td valign="top">SC </td><td>Automation &amp; Control Systems; Science &amp; Technology - Other Topics;
   Energy &amp; Fuels; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000393296200007</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Dong, SL
   <br>Wan, JH
   <br>Jia, ZY
   <br>Yu, Y
   <br>Tao, WG
   <br>Song, W</td>
</tr>

<tr>
<td valign="top">AF </td><td>Dong, Shiliang
   <br>Wan, Jinhua
   <br>Jia, Ziyan
   <br>Yu, Yang
   <br>Tao, Weige
   <br>Song, Wei</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Voice Transmission System Based on Visible Light Communication</td>
</tr>

<tr>
<td valign="top">SO </td><td>2016 IEEE 5TH GLOBAL CONFERENCE ON CONSUMER ELECTRONICS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>5th IEEE Global Conference on Consumer Electronics</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 11-14, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Kyoto, JAPAN</td>
</tr>

<tr>
<td valign="top">DE </td><td>VLC; voice transmission sytem; V-F converter; F-V converter</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we present an analog voice transmission system based on Visible Light Communication (VLC). At the transmitter of the system, by the Voltage Frequency (V-F) conversion circuit, the analog voice signal is transformed into the frequency signal, which is linear with the amplitude of the signals. At the receiver, the voice signal can be demodulated by the Frequency-Voltage (F-V) conversion circuit. Finally the test of the voice transmission system based on visible light communication is carried out, which shows that the input and output waveforms are consistent, the transmitted voice can be received without distortion. The test result shows that the design is correct and feasible.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Dong, Shiliang; Wan, Jinhua; Jia, Ziyan; Yu, Yang; Tao, Weige; Song,
   Wei] Jiangsu Univ Technol, Coll Elect &amp; Informat Engn, Changzhou 213001,
   Jiangsu, Peoples R China.
   <br>[Jia, Ziyan] Southeast Univ, Nation Mobile Commun Res Lab, Nanjing
   210096, Jiangsu, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Dong, SL (reprint author), Jiangsu Univ Technol, Coll Elect &amp; Informat Engn, Changzhou 213001, Jiangsu, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>dongshiliang616@163.com; 729499782@qq.com; jiaziyan@jsut.edu.cn;
   dxyy@jsut.edu.cn; taowg@jsut.edu.cn; dxxsw@jsut.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000392288200230</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Komulainen, J
   <br>Anina, I
   <br>Holappa, J
   <br>Boutellaa, E
   <br>Hadid, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Komulainen, Jukka
   <br>Anina, Iryna
   <br>Holappa, Jukka
   <br>Boutellaa, Elhocine
   <br>Hadid, Abdenour</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>On the robustness of audiovisual liveness detection to visual speech
   animation</td>
</tr>

<tr>
<td valign="top">SO </td><td>2016 IEEE 8TH INTERNATIONAL CONFERENCE ON BIOMETRICS THEORY,
   APPLICATIONS AND SYSTEMS (BTAS)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Biometrics Theory Applications and Systems</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>8th IEEE International Conference on Biometrics - Theory, Applications
   and Systems (BTAS)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 06-09, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Niagara Falls, NY</td>
</tr>

<tr>
<td valign="top">ID </td><td>CO-INERTIA ANALYSIS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Audiovisual speech synchrony detection is an important liveness check for talking face verification systems to make sure that the (pre-defined) content and timing of the given audible and visual speech samples match. Nowadays, there exists virtually no technical limitations for combining transferable facial animation and voice conversion (or synthesis) to create an ultimate audiovisual artifact that is able to spoof even advanced random challenge-response based liveness detection. In this study, we investigate the performance of the state-of-the-art text-independent lip-sync detection techniques under presentation attacks consisting of audio recordings of the targeted person and corresponding animated visual speech. Our experimental analysis with three different photo-realistic visual speech animation techniques reveals that generic synchrony models can be fooled even with underarticulated but synchronized lip movements. Thus, measuring audio-video synchrony or content alone is not enough for securing audiovisual biometric systems. Our preliminary findings suggest though that adaptation of person-specific audiovisual speech dynamics is one possible approach to tackle these kinds of high-effort attacks.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Komulainen, Jukka; Anina, Iryna; Holappa, Jukka; Hadid, Abdenour] Univ
   Oulu, Ctr Machine Vis &amp; Signal Anal, SF-90100 Oulu, Finland.
   <br>[Boutellaa, Elhocine] Ctr Dev Technol Avancees, Telecom Div, Algiers,
   Algeria.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Komulainen, J (reprint author), Univ Oulu, Ctr Machine Vis &amp; Signal Anal, SF-90100 Oulu, Finland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jukmaatt@ee.oulu.fi; isavelie@ee.oulu.fi; jukkaho@ee.oulu.fi;
   eboutell@ee.oulu.fi; hadid@ee.oulu.fi</td>
</tr>

<tr xmlns:date="http://exslt.org/dates-and-times">
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Komulainen, Jukka</display_name>&nbsp;</font></td><td><font size="3">O-6240-2017&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Komulainen, Jukka</display_name>&nbsp;</font></td><td><font size="3">M-3183-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Komulainen, Jukka</display_name>&nbsp;</font></td><td><font size="3">0000-0002-0102-7868&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Komulainen, Jukka</display_name>&nbsp;</font></td><td><font size="3">0000-0002-0102-7868&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">SC </td><td>Mathematical &amp; Computational Biology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000392217100008</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nakashika, T
   <br>Minami, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nakashika, Toru
   <br>Minami, Yasuhiro</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>3WRBM-Based Speech Factor Modeling for Arbitrary-Source and Non-Parallel
   Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>2016 24TH EUROPEAN SIGNAL PROCESSING CONFERENCE (EUSIPCO)</td>
</tr>

<tr>
<td valign="top">SE </td><td>European Signal Processing Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>24th European Signal Processing Conference (EUSIPCO)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 28-SEP 02, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Budapest, HUNGARY</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; three-way restricted Boltzmann machine; unsupervised
   learning; speaker adaptation; non-parallel training</td>
</tr>

<tr>
<td valign="top">AB </td><td>In recent years, voice conversion (VC) becomes a popular technique since it can be applied to various speech tasks. Most existing approaches on VC must use aligned speech pairs (parallel data) of the source speaker and the target speaker in training, which makes hard to handle it. Furthermore, VC methods proposed so far require to specify the source speaker in conversion stage, even though we just want to obtain the speech of the target speaker from the other speakers in many cases of VC. In this paper, we propose a VC method where it is not necessary to use any parallel data in the training, nor to specify the source speaker in the conversion. Our approach models a joint probability of acoustic, phonetic, and speaker features using a three-way restricted Boltzmann machine (3WRBM). Speaker-independent (SI) and speaker-dependent (SD) parameters in our model are simultaneously estimated under the maximum likelihood (ML) criteria using a speech set of multiple speakers. In conversion stage, phonetic features are at first estimated in a probabilistic manner given a speech of an arbitrary speaker, then a voice-converted speech is produced using the SD parameters of the target speaker. Our experimental results showed not only that our approach outperformed other non-parallel VC methods, but that the performance of the arbitrary-source VC was close to those of the traditional source-specified VC in our approach.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nakashika, Toru; Minami, Yasuhiro] Univ Electrocommun, Grad Sch
   Informat &amp; Engn, Chofu, Tokyo 1828585, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nakashika, T (reprint author), Univ Electrocommun, Grad Sch Informat &amp; Engn, Chofu, Tokyo 1828585, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>nakashika@uec.ac.jp; minami.yasuhiro@is.uec.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>607</td>
</tr>

<tr>
<td valign="top">EP </td><td>611</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000391891900118</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Xu, N
   <br>Yao, X
   <br>Jiang, AM
   <br>Liu, XF
   <br>Bao, JY</td>
</tr>

<tr>
<td valign="top">AF </td><td>Xu, Ning
   <br>Yao, Xiao
   <br>Jiang, Aimin
   <br>Liu, Xiaofeng
   <br>Bao, Jingyi</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>High Quality Voice Conversion by Post-Filtering the Outputs of Gaussian
   Processes</td>
</tr>

<tr>
<td valign="top">SO </td><td>2016 24TH EUROPEAN SIGNAL PROCESSING CONFERENCE (EUSIPCO)</td>
</tr>

<tr>
<td valign="top">SE </td><td>European Signal Processing Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>24th European Signal Processing Conference (EUSIPCO)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 28-SEP 02, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Budapest, HUNGARY</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; over-smoothing; post-filtering; Gaussian processes</td>
</tr>

<tr>
<td valign="top">ID </td><td>LIMITED TRAINING DATA</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion is a technique that aims to transform the individuality of source speech so as to mimic that of target speech while keeping the message unaltered, where the Gaussian mixture model based methods are most commonly used. However, these methods suffer from over-smoothing and over-fitting problems. In our previous work, we proposed to use Gaussian processes to alleviate over-fitting. Despite its effectiveness, this method will inevitably lead to over-smoothing due to choosing the mean of predictive distribution of Gaussian processes as optimal estimation. Thus, in this paper we focus on addressing the over-smoothing problem by post-filtering the outputs of the standard Gaussian processes, resulting in more dynamics in the converted feature parameters. Experiments have confirmed the validity of the proposed method both objectively and subjectively.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Xu, Ning; Yao, Xiao; Jiang, Aimin; Liu, Xiaofeng] Hohai Univ, Coll IoT
   Engn, Dept Commun Engn, Changzhou, Peoples R China.
   <br>[Xu, Ning; Yao, Xiao; Jiang, Aimin; Liu, Xiaofeng] Hohai Univ, Coll IoT
   Engn, Changzhou Key Lab Robot &amp; Intelligent Technol, Changzhou, Peoples
   R China.
   <br>[Bao, Jingyi] Changzhou Inst Technol, Sch Elect Informat &amp; Elect Engn,
   Changzhou, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Xu, N (reprint author), Hohai Univ, Coll IoT Engn, Dept Commun Engn, Changzhou, Peoples R China.; Xu, N (reprint author), Hohai Univ, Coll IoT Engn, Changzhou Key Lab Robot &amp; Intelligent Technol, Changzhou, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>xuningdlts@gmail.com; baojy@czu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>863</td>
</tr>

<tr>
<td valign="top">EP </td><td>867</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000391891900166</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Prablanc, P
   <br>Ozerov, A
   <br>Duong, NQK
   <br>Perez, P</td>
</tr>

<tr>
<td valign="top">AF </td><td>Prablanc, Pierre
   <br>Ozerov, Alexey
   <br>Duong, Ngoc Q. K.
   <br>Perez, Patrick</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>TEXT-INFORMED SPEECH INPAINTING VIA VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2016 24TH EUROPEAN SIGNAL PROCESSING CONFERENCE (EUSIPCO)</td>
</tr>

<tr>
<td valign="top">SE </td><td>European Signal Processing Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>24th European Signal Processing Conference (EUSIPCO)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 28-SEP 02, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Budapest, HUNGARY</td>
</tr>

<tr>
<td valign="top">DE </td><td>Audio inpainting; speech inpainting; voice conversion; Gaussian mixture
   model; speech synthesis</td>
</tr>

<tr>
<td valign="top">ID </td><td>MAXIMUM-LIKELIHOOD; AUDIO; TRANSFORMATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>The problem of speech inpainting consists in recovering some parts in a speech signal that are missing for some reasons. To our best knowledge none of the existing methods allows satisfactory inpainting of missing parts of large size such as one second and longer. In this work we address this challenging scenario. Since in the case of such long missing parts entire words can be lost, we assume that the full text uttered in the speech signal is known. This leads to a new concept of text-informed speech inpainting. To solve this problem we propose a method that is based on synthesizing the missing speech by a speech synthesizer, on modifying its vocal characteristics via a voice conversion method, and on filling in the missing part with the resulting converted speech sample. We carried subjective listening tests to compare the proposed approach with two baseline methods.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Prablanc, Pierre; Ozerov, Alexey; Duong, Ngoc Q. K.; Perez, Patrick]
   Technicolor, 975 Ave Champs Blanes,CS 17616, F-35576 Cesson Sevigne,
   France.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Prablanc, P (reprint author), Technicolor, 975 Ave Champs Blanes,CS 17616, F-35576 Cesson Sevigne, France.</td>
</tr>

<tr>
<td valign="top">EM </td><td>pierre.prablanc@technicolor.com; alexey.ozerov@technicolor.com;
   quang-khanh-ngoc.duong@technicolor.com; patrick.perez@technicolor.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>878</td>
</tr>

<tr>
<td valign="top">EP </td><td>882</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000391891900169</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Magarinos, C
   <br>Lopez-Otero, P
   <br>Docio-Fernandez, L
   <br>Banga, ER
   <br>Garcia-Mateo, C
   <br>Erro, D</td>
</tr>

<tr>
<td valign="top">AF </td><td>Magarinos, Carmen
   <br>Lopez-Otero, Paula
   <br>Docio-Fernandez, Laura
   <br>Banga, Eduardo R.
   <br>Garcia-Mateo, Carmen
   <br>Erro, Daniel</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Piecewise Linear Definition of Transformation Functions for Speaker
   De-Identification</td>
</tr>

<tr>
<td valign="top">SO </td><td>2016 FIRST INTERNATIONAL WORKSHOP ON SENSING, PROCESSING AND LEARNING
   FOR INTELLIGENT MACHINES (SPLINE)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>1st International Workshop on Sensing, Processing and Learning for
   Intelligent Machines (SPLINE)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUL 06-08, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Aalborg Univ, Aalborg, DENMARK</td>
</tr>

<tr>
<td valign="top">HO </td><td>Aalborg Univ</td>
</tr>

<tr>
<td valign="top">AB </td><td>The main drawback of speaker de-identification approaches using voice conversion techniques is the need for parallel corpora to train transformation functions between the source and target speakers. In this paper, a voice conversion approach that does not require training any parameters is proposed: it consists in manually defining frequency warping (FW) based transformations by using piecewise linear approximations. An analysis of the de-identification capabilities of the proposed approach using FW only or combined with F0 modification and spectral amplitude scaling (AS) was performed. Experimental results show that, using the manually defined transformations using only FW, it is not possible to obtain de-identified natural sounding speech. Nevertheless, when modifying the F0, both de-identification accuracy and naturalness increase to a great extent. A slight improvement in de-identification was also obtained when applying spectral amplitude scaling.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Magarinos, Carmen; Lopez-Otero, Paula; Docio-Fernandez, Laura; Banga,
   Eduardo R.; Garcia-Mateo, Carmen] AtlantTIC Res Ctr, Multimedia Technol
   Grp, EE Telecomunicac, Campus Univ S-N, Vigo 36310, Spain.
   <br>[Erro, Daniel] Univ Basque Country, Ikerbasque, Aholab, Ingn Goi Eskola
   Teknikoa, Bilbao 48013, Spain.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Magarinos, C (reprint author), AtlantTIC Res Ctr, Multimedia Technol Grp, EE Telecomunicac, Campus Univ S-N, Vigo 36310, Spain.</td>
</tr>

<tr>
<td valign="top">EM </td><td>cmagui@gts.uvigo.es; plopez@gts.uvigo.es; ldocio@gts.uvigo.es;
   erbanga@gts.uvigo.es; carmen@gts.uvigo.es; derro@aholab.ehu.es</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Rodriguez Banga, Eduardo</display_name>&nbsp;</font></td><td><font size="3">C-4296-2011&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Docio-Fernandez, Laura</display_name>&nbsp;</font></td><td><font size="3">D-3189-2018&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Garcia-Mateo, Carmen</display_name>&nbsp;</font></td><td><font size="3">I-4144-2015&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Rodriguez Banga, Eduardo</display_name>&nbsp;</font></td><td><font size="3">0000-0001-7989-4526&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Docio-Fernandez, Laura</display_name>&nbsp;</font></td><td><font size="3">0000-0003-3838-2406&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Garcia-Mateo, Carmen</display_name>&nbsp;</font></td><td><font size="3">0000-0001-6856-939X&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000390709400018</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Pribil, J
   <br>Pribilova, A
   <br>Matousek, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Pribil, Jiri
   <br>Pribilova, Anna
   <br>Matousek, Jindrich</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>GMM-Based Speaker Gender and Age Classification After Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>2016 FIRST INTERNATIONAL WORKSHOP ON SENSING, PROCESSING AND LEARNING
   FOR INTELLIGENT MACHINES (SPLINE)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>1st International Workshop on Sensing, Processing and Learning for
   Intelligent Machines (SPLINE)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUL 06-08, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Aalborg Univ, Aalborg, DENMARK</td>
</tr>

<tr>
<td valign="top">HO </td><td>Aalborg Univ</td>
</tr>

<tr>
<td valign="top">DE </td><td>GMM classifier; speech features; speaker gender and age classification;
   text-to-speech system; voice tranformation</td>
</tr>

<tr>
<td valign="top">ID </td><td>TO-SPEECH SYSTEM; IDENTIFICATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper describes an experiment using the Gaussian mixture models (GMM) for classification of the speaker gender/age and for evaluation of the achieved success in the voice conversion process. The main motivation of the work was to test whether this type of the classifier can be utilized as an alternative approach instead of the conventional listening test in the area of speech evaluation. The proposed two-level GMM classifier was first verified for detection of four age categories (child, young, adult, senior) as well as discrimination of gender for all but children's voices in Czech and Slovak languages. Then the classifier was applied for gender/age determination of the basic adult male/female original speech together with its conversion. The obtained resulting classification accuracy confirms usability of the proposed evaluation method and effectiveness of the performed voice conversions.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Pribil, Jiri] SAS, Inst Measurement Sci, Bratislava, Slovakia.
   <br>[Pribilova, Anna] FEE&amp;IT SUT, Inst Elect &amp; Photon, Bratislava, Slovakia.
   <br>[Matousek, Jindrich] UWB, Fac Sci Appl, Dept Cybernet, Plzen, Czech
   Republic.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Pribil, J (reprint author), SAS, Inst Measurement Sci, Bratislava, Slovakia.</td>
</tr>

<tr>
<td valign="top">EM </td><td>Jiri.Pribil@savba.sk; Anna.Pribilova@stuba.sk; jmatouse@kky.zcu.cz</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Matousek, Jindrich</display_name>&nbsp;</font></td><td><font size="3">C-2146-2011&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Matousek, Jindrich</display_name>&nbsp;</font></td><td><font size="3">0000-0002-7408-7730&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000390709400001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Krishnaveni, M
   <br>Subashini, P
   <br>Dhivyaprabha, TT
   <br>Priya, AS</td>
</tr>

<tr>
<td valign="top">AF </td><td>Krishnaveni, M.
   <br>Subashini, P.
   <br>Dhivyaprabha, T. T.
   <br>Priya, A. Sathiya</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Optimized Classification Based on Particle Swarm Optimization Algorithm
   for Tamil Sign Language Digital Images</td>
</tr>

<tr>
<td valign="top">SO </td><td>2016 INTERNATIONAL CONFERENCE ON COMPUTATION SYSTEM AND INFORMATION
   TECHNOLOGY FOR SUSTAINABLE SOLUTIONS (CSITSS)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>1st IEEE International Conference on Computational Systems and
   Information Technology for Sustainable Solutions (CSITSS)</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 06-08, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>R V Coll Engn, Bengaluru, INDIA</td>
</tr>

<tr>
<td valign="top">HO </td><td>R V Coll Engn</td>
</tr>

<tr>
<td valign="top">DE </td><td>Tamil Sign Language; Hand Gesture; Feature Extraction; Classification;
   Machine Learning Algorithm; Particle Swarm Optimization</td>
</tr>

<tr>
<td valign="top">AB </td><td>Sign is the way of communication when there is no appropriate verbal communication for deaf and dump persons. Sign Language Recognition System (SLRS) acts as a contrivance to perform the conversion of sign language into text or voice. Many classification techniques and algorithms with a variety of methods already exist in SLRS. But still only a few considerable contributions are there in Tamil Sign Language Recognition. The objective of this paper is to extract the features from the pre-processed image and propose an optimization based classifier to improve the classification accuracy in Tamil Sign Language images. The experimental result of the proposed work demonstrated that BPN-PSO outperforms existing classification techniques.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Krishnaveni, M.; Subashini, P.; Dhivyaprabha, T. T.; Priya, A. Sathiya]
   Avinashilingam Inst Home Sci &amp; Higher Educ Women, Dept Comp Sci,
   Coimbatore, Tamil Nadu, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Krishnaveni, M (reprint author), Avinashilingam Inst Home Sci &amp; Higher Educ Women, Dept Comp Sci, Coimbatore, Tamil Nadu, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>krishnaveni.rd@gmail.com; mail.p.subashini@gmail.com;
   ttdhivyaprabha@gmail.com; sathiyapriya1327@gmail.com</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Dhivyaprabha, TT</display_name>&nbsp;</font></td><td><font size="3">0000-0002-8603-6826&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>M, Krishnaveni</display_name>&nbsp;</font></td><td><font size="3">0000-0003-4018-4791&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>53</td>
</tr>

<tr>
<td valign="top">EP </td><td>57</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000390719100010</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Aihara, R
   <br>Masaka, K
   <br>Takiguchi, T
   <br>Ariki, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Aihara, Ryo
   <br>Masaka, Kenta
   <br>Takiguchi, Tetsuya
   <br>Ariki, Yasuo</td>
</tr>

<tr>
<td valign="top">BE </td><td>Lee, R</td>
</tr>

<tr>
<td valign="top">TI </td><td>Parallel Dictionary Learning for Multimodal Voice Conversion Using
   Matrix Factorization</td>
</tr>

<tr>
<td valign="top">SO </td><td>COMPUTER AND INFORMATION SCIENCE</td>
</tr>

<tr>
<td valign="top">SE </td><td>Studies in Computational Intelligence</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>15th IEEE/ACIS International Conference on Computer and Information
   Science (ICIS)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUN 26-29, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Okayama, JAPAN</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH</td>
</tr>

<tr>
<td valign="top">AB </td><td>Parallel dictionary learning for multimodal voice conversion is proposed in this paper. Because of noise robustness of visual features, multimodal feature has been attracted in the field of speech processing, and we have proposed multimodal VC using Non-negative Matrix Factorization (NMF). Experimental results showed that our conventional multimodal VC can effectively converted in a noisy environment, however, the difference of conversion quality between audio input VC and multimodal VC is not so large in a clean environment. We assume this is because our exemplar dictionary is over-complete. Moreover, because of non-negativity constraint for visual features, our conventional multimodal NMF-based VC cannot factorize visual features effectively. In order to enhance the conversion quality of our NMF-based multimodal VC, we propose parallel dictionary learning. Non-negative constraint for visual features is removed so that we can handle visual features which include negative values. Experimental results showed that our proposed method effectively converted multimodal features in a clean environment.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Aihara, Ryo; Masaka, Kenta; Takiguchi, Tetsuya; Ariki, Yasuo] Kobe
   Univ, Grad Sch Syst Informat, 1-1 Rokkodai, Kobe, Hyogo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Aihara, R (reprint author), Kobe Univ, Grad Sch Syst Informat, 1-1 Rokkodai, Kobe, Hyogo, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>aihara@me.cs.scitec.kobe-u.ac.jp; makka@me.cs.scitec.kobe-u.ac.jp;
   takigu@kobe-u.ac.jp; ariki@kobe-u.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>656</td>
</tr>

<tr>
<td valign="top">BP </td><td>27</td>
</tr>

<tr>
<td valign="top">EP </td><td>40</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/978-3-319-40171-3_3</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000390679100003</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Luo, ZJ
   <br>Takiguchi, T
   <br>Ariki, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Luo, Zhaojie
   <br>Takiguchi, Tetsuya
   <br>Ariki, Yasuo</td>
</tr>

<tr>
<td valign="top">BE </td><td>Uehara, K
   <br>Nakamura, M</td>
</tr>

<tr>
<td valign="top">TI </td><td>Emotional Voice Conversion Using Deep Neural Networks with MCC and F0
   Features</td>
</tr>

<tr>
<td valign="top">SO </td><td>2016 IEEE/ACIS 15TH INTERNATIONAL CONFERENCE ON COMPUTER AND INFORMATION
   SCIENCE (ICIS)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>15th IEEE/ACIS International Conference on Computer and Information
   Science (ICIS)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUN 26-29, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Okayama, JAPAN</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH</td>
</tr>

<tr>
<td valign="top">AB </td><td>An artificial neural network is one of the most important models for training features in a voice conversion task. Typically, Neural Networks (NNs) are not effective in processing low-dimensional F0 features, thus this causes that the performance of those methods based on neural networks for training Mel Cepstral Coefficients (MCC) are not outstanding. However, F0 can robustly represent various prosody signals (e.g., emotional prosody). In this study, we propose an effective method based on the NNs to train the normalized-segment-F0 features (NSF0) for emotional prosody conversion. Meanwhile, the proposed method adopts deep belief networks (DBNs) to train spectrum features for voice conversion. By using these approaches, the proposed method can change the spectrum and the prosody for the emotional voice at the same time. Moreover, the experimental results show that the proposed method outperforms other state-of-the-art methods for voice emotional conversion.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Luo, Zhaojie; Takiguchi, Tetsuya; Ariki, Yasuo] Kobe Univ, Grad Sch
   Syst Informat, Kobe, Hyogo 6578501, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Luo, ZJ (reprint author), Kobe Univ, Grad Sch Syst Informat, Kobe, Hyogo 6578501, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>luozhaojie@me.cs.scitec.kobe-u.ac.jp; takigu@kobe-u.ac.jp;
   ariki@kobe-u.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>977</td>
</tr>

<tr>
<td valign="top">EP </td><td>981</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000389539100164</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Patel, TB
   <br>Patil, HA</td>
</tr>

<tr>
<td valign="top">AF </td><td>Patel, Tanvina B.
   <br>Patil, Hemant A.</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>EFFECTIVENESS OF FUNDAMENTAL FREQUENCY (F-0) AND STRENGTH OF EXCITATION
   (SOE) FOR SPOOFED SPEECH DETECTION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2016 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 20-25, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Shanghai, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>F-0; SoE; MFCC; CFCCIF; anti-spoofing</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEAKER VERIFICATION; SYNTHETIC SPEECH; SIGNALS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Current countermeasures used in spoof detectors (for speech synthesis (SS) and voice conversion (VC)) are generally phase-based (as vocoders in SS and VC systems lack phase-information). These approaches may possibly fail for non-vocoder or unit-selection-based spoofs. In this work, we explore excitation source-based features, i.e., fundamental frequency (F-0) contour and strength of excitation (SoE) at the glottis as discriminative features using GMM-based classification system. We use F-0 and SoE1 estimated from speech signal through zero frequency (ZF) filtering method. Further, SoE2 is estimated from negative peaks of derivative of glottal flow waveform (dGFW) at glottal closure instants (GCIs). On the evaluation set of ASVspoof 2015 challenge database, the F-0 and SoEs features along with its dynamic variations achieve an Equal Error Rate (EER) of 12.41%. The source features are fused at score-level with MFCC and recently proposed cochlear filter cepstral coefficients and instantaneous frequency (CFCCIF) features. On fusion with MFCC (CFCCIF), the EER decreases from 4.08% to 3.26% (2.07% to 1.72%). The decrease in EER was evident on both known and unknown vocoder-based attacks. When MFCC, CFCCIF and source features are combined, the EER further decreased to 1.61%. Thus, source features captures complementary information than MFCC and CFCCIF used alone.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Patel, Tanvina B.; Patil, Hemant A.] Dhirubhai Ambani Inst Informat &amp;
   Commun Technol, Gandhinagar 382007, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Patel, TB (reprint author), Dhirubhai Ambani Inst Informat &amp; Commun Technol, Gandhinagar 382007, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>tanvina_bhupendrabhai_patel@daiict.ac.in; hemant_patil@daiict.ac.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>7</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>7</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>5105</td>
</tr>

<tr>
<td valign="top">EP </td><td>5109</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000388373405051</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Aihara, R
   <br>Takiguchi, T
   <br>Ariki, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Aihara, Ryo
   <br>Takiguchi, Tetsuya
   <br>Ariki, Yasuo</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>SEMI-NON-NEGATIVE MATRIX FACTORIZATION USING ALTERNATING DIRECTION
   METHOD OF MULTIPLIERS FOR VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2016 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 20-25, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Shanghai, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>NMF; ADMM; Voice Conversion; Speech Synthesis; Sparse Representation</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPARSE REPRESENTATION; SPEECH</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion (VC) is being widely researched in the field of speech processing because of increased interest in using such processing in applications such as personalized Text-To-Speech systems. A VC method using Non-negative Matrix Factorization (NMF) has been researched because of its natural sounding voice, however, huge memory usage and high computational times have been reported as problems. We present in this paper a new VC method using Semi-Non-negative Matrix Factorization (Semi-NMF) using the Alternating Direction Method of Multipliers (ADMM) in order to tackle the problems associated with NMF-based VC. Dictionary learning using Semi-NMF can create a compact dictionary, and ADMM enables faster convergence than conventional Semi-NMF. Experimental results show that our proposed method is 76 times faster than conventional NMF, and its conversion quality is almost the same as that of the conventional method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Aihara, Ryo; Takiguchi, Tetsuya; Ariki, Yasuo] Kobe Univ, Grad Sch Syst
   Informat, Kobe, Hyogo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Aihara, R (reprint author), Kobe Univ, Grad Sch Syst Informat, Kobe, Hyogo, Japan.</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>5170</td>
</tr>

<tr>
<td valign="top">EP </td><td>5174</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000388373405064</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ming, HP
   <br>Huang, DY
   <br>Xie, L
   <br>Zhang, SF
   <br>Dong, MH
   <br>Li, HZ</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ming, Huaiping
   <br>Huang, Dongyan
   <br>Xie, Lei
   <br>Zhang, Shaofei
   <br>Dong, Minghui
   <br>Li, Haizhou</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>EXEMPLAR-BASED SPARSE REPRESENTATION OF TIMBRE AND PROSODY FOR VOICE
   CONVERSION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2016 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 20-25, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Shanghai, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; exemplar; timbre; prosody; sparse representation</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion (VC) aims to make one speaker (source) to sound like spoken by another speaker (target) without changing the language content. Most of the state-of-the-art voice conversion systems focus only on timbre conversion. However, the speaker identity is characterized by the source-related cues such as fundamental frequency and energy as well. In this work, we propose an exemplar based sparse representation of timbre and prosody for voice conversion that does not necessitate separately timbre conversion and prosody conversions. The experiment results show that, in addition to the conversion of spectral features, the proper conversion of prosody features will improve the quality and speaker identity of the converted speech.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ming, Huaiping; Xie, Lei; Zhang, Shaofei] Northwestern Polytech Univ,
   Sch Comp Sci, Xian, Peoples R China.
   <br>[Ming, Huaiping; Huang, Dongyan; Dong, Minghui; Li, Haizhou] ASTAR, Inst
   Infocomm Res, Singapore, Singapore.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ming, HP (reprint author), Northwestern Polytech Univ, Sch Comp Sci, Xian, Peoples R China.; Ming, HP (reprint author), ASTAR, Inst Infocomm Res, Singapore, Singapore.</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Li, Haizhou</display_name>&nbsp;</font></td><td><font size="3">Q-6438-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Li, Haizhou</display_name>&nbsp;</font></td><td><font size="3">0000-0001-9158-9401&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>8</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>8</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>5175</td>
</tr>

<tr>
<td valign="top">EP </td><td>5179</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000388373405065</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sarria-Paja, M
   <br>Senoussaoui, M
   <br>O'Shaughnessy, D
   <br>Falk, TH</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sarria-Paja, Milton
   <br>Senoussaoui, Mohammed
   <br>O'Shaughnessy, Douglas
   <br>Falk, Tiago H.</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>FEATURE MAPPING, SCORE-, AND FEATURE-LEVEL FUSION FOR IMPROVED NORMAL
   AND WHISPERED SPEECH SPEAKER VERIFICATION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2016 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 20-25, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Shanghai, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Whispered speech; AM-FM model; i-vectors; speaker verification; system
   fusion; feature mapping</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION; NEURAL-NETWORKS; AUDIO STREAMS; IDENTIFICATION;
   RECOGNITION</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, automatic speaker verification using normal and whispered speech is explored. Typically, for speaker verification systems with varying vocal effort inputs, standard solutions such as feature mapping or addition of data during parameter estimation (training) and enrollment stages result in a trade-off between accuracy gains with whispered test data and accuracy losses (up to 70% in equal error rate, EER) with normal test data. To overcome this shortcoming, this paper proposes two innovations. First, we show the complementarity of features derived from AM-FM models over conventional mel-frequency cepstral coefficients, thus signalling the importance of instantaneous phase information for whispered speech speaker verification. Next, two fusion schemes are explored: score-and feature-level fusion. Overall, we show that gains as high as 30% and 84% in EER can be achieved for normal and whispered speech, respectively, using feature-level fusion.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Sarria-Paja, Milton; Senoussaoui, Mohammed; O'Shaughnessy, Douglas;
   Falk, Tiago H.] Univ Quebec, Inst Natl Rech Sci INRS EMT, Montreal, PQ,
   Canada.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sarria-Paja, M (reprint author), Univ Quebec, Inst Natl Rech Sci INRS EMT, Montreal, PQ, Canada.</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Falk, Tiago</display_name>&nbsp;</font></td><td><font size="3">0000-0002-5739-2514&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Sarria-Paja, Milton</display_name>&nbsp;</font></td><td><font size="3">0000-0003-4288-1742&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>6</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>6</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>5480</td>
</tr>

<tr>
<td valign="top">EP </td><td>5484</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000388373405126</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nakashika, T
   <br>Minami, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nakashika, Torsi
   <br>Minami, Yasuhiro</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>SPEAKER ADAPTIVE MODEL BASED ON BOLTZMANN MACHINE FOR NON-PARALLEL
   TRAINING IN VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2016 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 20-25, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Shanghai, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; Boltzmann machine; unsupervised training; speaker
   adaptation; SAT</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we present a voice conversion (VC) method that does not use any parallel data while training the model. VC is a technique where only speaker specific information in source speech is converted while keeping the phonological information unchanged. Most of the existing VC methods rely on parallel data-pairs of speech data from the source and target speakers uttering the same sentences. However, the use of parallel data in training causes several problems; 1) the data used for the training is limited to the pre-defined sentences, 2) the trained model is only applied to the speaker pair used in the training, and 3) mismatch in alignment may happen. Although it is, thus, fairy preferable in VC not to use parallel data, a non-parallel approach is considered difficult to learn. In our approach, we realize the non-parallel training based on speaker-adaptive training (SAT). Speech signals are represented using a probabilistic model based on the Boltzmann machine that defines phonological information and speaker-related information explicitly. Speaker-independent (SI) and speaker-dependent (SD) parameters are simultaneously trained using SAT. In conversion stage, a given speech signal is decomposed into phonological and speaker-related information, the speaker-related information is replaced with that of the desired speaker, and then a voice-converted speech is obtained by mixing the two. Our experimental results showed that our approach unfortunately fell short of the popular conventional GMM-based method that used parallel data, but outperformed the conventional non-parallel approach.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nakashika, Torsi; Minami, Yasuhiro] Univ Electrocommun, Grad Sch
   Informat Syst, Chofu, Tokyo 182, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nakashika, T (reprint author), Univ Electrocommun, Grad Sch Informat Syst, Chofu, Tokyo 182, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>nakashika@uec.ac.jp; minami.yasuhiro@is.uec.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>5530</td>
</tr>

<tr>
<td valign="top">EP </td><td>5534</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000388373405136</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Agiomyrgiannakis, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Agiomyrgiannakis, Yannis</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>THE MATCHING-MINIMIZATION ALGORITHM, THE INCA ALGORITHM AND A
   MATHEMATICAL FRAMEWORK FOR VOICE CONVERSION WITH UNALIGNED CORPORA</td>
</tr>

<tr>
<td valign="top">SO </td><td>2016 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 20-25, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Shanghai, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>INCA; voice-conversion; voice-transformation; matching-minimization;
   nearest-neighbour</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH SYNTHESIS; SPEAKER ADAPTATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a mathematical framework that is suitable for voice conversion and adaptation in speech processing. Voice conversion is formulated as a search for the optimal correspondances between a set of source-speaker spectra and a set of target-speaker spectra under a transform that compensates speaker differences. It is possible to simultaneously recover a bi-directional mapping between two sets of vectors that is a parametric mapping (a transform) in one direction and a non-parametric mapping (correspondences) in the reverse direction. An algorithm referred to as Matching-Minimization (MM) is formally derived with proven convergence and an optimal closed-form solution for each step. The algorithm is closely related to the asymmetric-1 variant of the well-known INCA algorithm [1] for which we also provide a proof within the same framework. The differences between MM and INCA are delineated both theoretically and experimentally. MM outperforms INCA in all scenarios. Like INCA, MM does not require parallel corpora. Unlike INCA, MM is suitable when only a few adaptation data are available.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Agiomyrgiannakis, Yannis] Google, London, England.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Agiomyrgiannakis, Y (reprint author), Google, London, England.</td>
</tr>

<tr>
<td valign="top">EM </td><td>agios@google.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>6</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>6</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>5645</td>
</tr>

<tr>
<td valign="top">EP </td><td>5649</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000388373405159</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Agiomyrgiannakis, Y
   <br>Roupakia, Z</td>
</tr>

<tr>
<td valign="top">AF </td><td>Agiomyrgiannakis, Yannis
   <br>Roupakia, Zoi</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>VOICE MORPHING THAT IMPROVES TTS QUALITY USING AN OPTIMAL DYNAMIC
   FREQUENCY WARPING-AND-WEIGHTING TRANSFORM</td>
</tr>

<tr>
<td valign="top">SO </td><td>2016 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 20-25, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Shanghai, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice-morphing; voice-transformation; DFW; vocaine;
   matching-minimization</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH SYNTHESIS; CONVERSION; ALGORITHM</td>
</tr>

<tr>
<td valign="top">AB </td><td>Dynamic Frequency Warping (DFW) is widely used to align spectra of different speakers. It has long been argued that frequency warping captures inter-speaker differences but DFW practice always involves a tricky preprocessing part to remove spectral tilt. The DFW residual is successfully used in Voice Morphing to improve the quality and the similarity of synthesized speech but the estimation of the DFW residual remains largely heuristic and sub-optimal. This paper presents a dynamic programming algorithm that simultaneously estimates the Optimal Frequency Warping and Weighting transform (ODFWW) and therefore needs no preprocessing step and fine-tuning while source/target-speaker data are matched using the Matching-Minimization algorithm [1]. The transform is used to morph the output of a state-of-the-art Vocaine-based [2] TTS synthesizer in order to generate different voices in runtime with only +8% computational overhead. Some morphed TTS voices exhibit significantly higher quality than the original one as morphing seems to "correct" the voice characteristics of the TTS voice.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Agiomyrgiannakis, Yannis; Roupakia, Zoi] Google, London, England.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Agiomyrgiannakis, Y (reprint author), Google, London, England.</td>
</tr>

<tr>
<td valign="top">EM </td><td>agios@google.com; zr216@cam.ac.uk</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>5650</td>
</tr>

<tr>
<td valign="top">EP </td><td>5654</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000388373405160</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Jin, ZY
   <br>Finkelstein, A
   <br>DiVerdi, S
   <br>Lu, JW
   <br>Mysore, GJ</td>
</tr>

<tr>
<td valign="top">AF </td><td>Jin, Zeyu
   <br>Finkelstein, Adam
   <br>DiVerdi, Stephen
   <br>Lu, Jingwan
   <br>Mysore, Gautham J.</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>CUTE: A CONCATENATIVE METHOD FOR VOICE CONVERSION USING EXEMPLAR-BASED
   UNIT SELECTION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2016 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 20-25, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Shanghai, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; unit selection; concatenative synthesis;
   exemplar-based</td>
</tr>

<tr>
<td valign="top">ID </td><td>TRANSFORMATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>State-of-the art voice conversion methods re-synthesize voice from spectral representations such as MFCCs and STRAIGHT, thereby introducing muffled artifacts. We propose a method that circumvents this concern using concatenative synthesis coupled with exemplarbased unit selection. Given parallel speech from source and target speakers as well as a new query from the source, our method stitches together pieces of the target voice. It optimizes for three goals: matching the query, using long consecutive segments, and smooth transitions between the segments. To achieve these goals, we perform unit selection at the frame level and introduce triphonebased preselection that greatly reduces computation and enforces selection of long, contiguous pieces. Our experiments show that the proposed method has better quality than baseline methods, while preserving high individuality.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Jin, Zeyu; Finkelstein, Adam] Princeton Univ, Princeton, NJ 08540 USA.
   <br>[Jin, Zeyu; DiVerdi, Stephen; Lu, Jingwan; Mysore, Gautham J.] Adobe
   Res, San Francisco, CA 94103 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Jin, ZY (reprint author), Princeton Univ, Princeton, NJ 08540 USA.; Jin, ZY (reprint author), Adobe Res, San Francisco, CA 94103 USA.</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Lu, Jingwan</display_name>&nbsp;</font></td><td><font size="3">0000-0002-3598-9918&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>4</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>4</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>5660</td>
</tr>

<tr>
<td valign="top">EP </td><td>5664</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000388373405162</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Tanaka, K
   <br>Kameoka, H
   <br>Toda, T
   <br>Nakamura, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Tanaka, Kou
   <br>Kameoka, Hirokazu
   <br>Toda, Tomoki
   <br>Nakamura, Satoshi</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>STATISTICAL F-0 PREDICTION FOR ELECTROLARYNGEAL SPEECH ENHANCEMENT
   CONSIDERING GENERATIVE PROCESS OF F-0 CONTOURS WITHIN PRODUCT OF EXPERTS
   FRAMEWORK</td>
</tr>

<tr>
<td valign="top">SO </td><td>2016 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 20-25, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Shanghai, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Electrolaryngeal speech enhancement; F-0 prediction; Generative model;
   Product of Experts</td>
</tr>

<tr>
<td valign="top">ID </td><td>FUNDAMENTAL-FREQUENCY CONTOURS; VOICE CONVERSION; EXTRACTION</td>
</tr>

<tr>
<td valign="top">AB </td><td>We have previously proposed a statistical fundamental frequency (F-0) prediction method that makes it possible to predict the underlying F-0 contour of electrolaryngeal (EL) speech from its spectral feature sequence. Although this method was shown to contribute to improving the naturalness of EL speech as a whole, the predicted F-0 contour was still unnatural compared with that in normal speech. One possible solution to improve the naturalness of the predicted F-0 contours would be to take account of the physical mechanism of vocal phonation. Recently a statistical model of voice F-0 contours was formulated by constructing a stochastic counterpart of the Fujisaki model, a well-founded mathematical model representing the control mechanism of vocal fold vibration. This paper proposes a Product-of -Experts model to incorporate this generative model of voice F-0 contours into the statistical F-0 prediction model. Based on the constructed model, we derive algorithms for parameter training and F-0 prediction. Experimental results revealed that the proposed method successfully outperformed our previously proposed method in terms of the naturalness of the predicted F-0 contours.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Tanaka, Kou; Nakamura, Satoshi] Nara Inst Sci &amp; Technol, Grad Sch
   Informat Sci, Ikoma, Nara, Japan.
   <br>[Kameoka, Hirokazu] NTT Corp, NTT Commun Sci Labs, Tokyo, Tokyo, Japan.
   <br>[Toda, Tomoki] Nagoya Univ, Informat Technol Ctr, Nagoya, Aichi 4648601,
   Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Tanaka, K (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Ikoma, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ko-t@is.naist.jp; kameoka.hirokazu@lab.ntt.co.jp;
   tomoki@icts.nagoya-u.ac.jp; s-nakamura@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>5665</td>
</tr>

<tr>
<td valign="top">EP </td><td>5669</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000388373405163</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kobayashi, K
   <br>Toda, T
   <br>Nakamura, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kobayashi, Kazuhiro
   <br>Toda, Tomoki
   <br>Nakamura, Satoshi</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>IMPLEMENTATION OF F0 TRANSFORMATION FOR STATISTICAL SINGING VOICE
   CONVERSION BASED ON DIRECTWAVEFORM MODIFICATION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2016 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 20-25, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Shanghai, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>statistical singing voice conversion; cross-gender conversion; direct
   waveform modification; spectral differential; F-0 transformation</td>
</tr>

<tr>
<td valign="top">ID </td><td>PLUS NOISE MODEL; SPEECH SYNTHESIS</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a technique for transforming F-0 in a framework of statistical singing voice conversion with direct waveform modification based on spectrum differential (DIFFSVC). The DIFFSVC method converts voice timbre of singing voices of a source singer into that of a target singer without using vocoder-based waveform generation. Although this method achieves high sound quality of the converted singing voices, its use is limited to only intra-gender conversion without the need of F-0 transformation. To make it possible to also use the DIFFSVC method for cross-gender conversion, we propose a method to transform F-0 of an input singing voice for the DIFFSVC. The proposed method is also based on direct waveform modification using overlap-add process and filtering process. Results of subjective evaluations demonstrate that the proposed DIFFSVC method with F-0 transformation significantly improves sound quality of the converted singing voices while preserving the conversion accuracy of singer identity in the cross-gender conversion compared to the conventional SVC with vocoder.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kobayashi, Kazuhiro; Nakamura, Satoshi] Nara Inst Sci &amp; Technol, Grad
   Sch Informat Sci, Ikoma, Nara, Japan.
   <br>[Toda, Tomoki] Nagoya Univ, Ctr Informat Technol, Nagoya, Aichi 4648601,
   Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kobayashi, K (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Ikoma, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">TC </td><td>4</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>4</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>5670</td>
</tr>

<tr>
<td valign="top">EP </td><td>5674</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000388373405164</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Tajiri, Y
   <br>Toda, T
   <br>Nakamura, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Tajiri, Yusuke
   <br>Toda, Tomoki
   <br>Nakamura, Satoshi</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>NOISE SUPPRESSION METHOD FOR BODY-CONDUCTED SOFT SPEECH ENHANCEMENT
   BASED ON EXTERNAL NOISE MONITORING</td>
</tr>

<tr>
<td valign="top">SO </td><td>2016 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 20-25, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Shanghai, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>silent speech communication; nonaudible murmur microphone; noise
   suppression; external noise monitoring; semi-blind source separation</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a novel approach to suppressing adverse effects of external noise on body-conducted soft speech for silent speech communication in noisy environments. Nonaudible murmur (NAM) microphone as one of the body-conductive microphones is capable of detecting very soft speech. However, body-conducted soft speech easily suffers from external noise owing to its faint volume. To address this issue, the proposed method additionally uses an air-conductive microphone to detect only an external noise signal and uses the detected external noise signal to suppress its effect on the body-conducted soft speech. A semi-blind source separation technique is applied to the proposed method for estimating a linear filter to suppress the noise components without voice activity detection. Experimental results demonstrate that the proposed method yields 10 dB SNR improvements in 80 dBA noisy conditions and also yields significant improvements in sound quality of body-conducted soft speech.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Tajiri, Yusuke; Nakamura, Satoshi] Nara Inst Sci &amp; Technol, Grad Sch
   Informat Sci, Ikoma, Nara, Japan.
   <br>[Toda, Tomoki] Nagoya Univ, Ctr Informat Technol, Nagoya, Aichi 4648601,
   Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Tajiri, Y (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Ikoma, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>tailrl.yusuke.tk0@is.naist.jp; tomoki@icts.nagoya-u.ac.ip;
   s-nakamura@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>5935</td>
</tr>

<tr>
<td valign="top">EP </td><td>5939</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000388373406018</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sun, LF
   <br>Li, K
   <br>Wang, H
   <br>Kang, SY
   <br>Meng, HL</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sun, Lifa
   <br>Li, Kun
   <br>Wang, Hao
   <br>Kang, Shiyin
   <br>Meng, Helen</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>PHONETIC POSTERIORGRAMS FOR MANY-TO-ONE VOICE CONVERSION WITHOUT
   PARALLEL DATA TRAINING</td>
</tr>

<tr>
<td valign="top">SO </td><td>2016 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA &amp; EXPO (ICME)</td>
</tr>

<tr>
<td valign="top">SE </td><td>IEEE International Conference on Multimedia and Expo</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Multimedia &amp; Expo (ICME)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUL 11-15, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Seattle, WA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; phonetic posteriorgrams; non-parallel; many-to-one;
   SI-ASR; DBLSTM</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper proposes a novel approach to voice conversion with non-parallel training data. The idea is to bridge between speakers by means of Phonetic PosteriorGrams (PPGs) obtained from a speaker-independent automatic speech recognition (SI-ASR) system. It is assumed that these PPGs can represent articulation of speech sounds in a speaker-normalized space and correspond to spoken content speaker-independently. The proposed approach first obtains PPGs of target speech. Then, a Deep Bidirectional Long Short-Term Memory based Recurrent Neural Network (DBLSTM) structure is used to model the relationships between the PPGs and acoustic features of the target speech. To convert arbitrary source speech, we obtain its PPGs from the same SI-ASR and feed them into the trained DBLSTM for generating converted speech. Our approach has two main advantages: 1) no parallel training data is required; 2) a trained model can be applied to any other source speaker for a fixed target speaker (i.e., many-to-one conversion). Experiments show that our approach performs equally well or better than state-of-the-art systems in both speech quality and speaker similarity.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Sun, Lifa; Li, Kun; Wang, Hao; Kang, Shiyin; Meng, Helen] Chinese Univ
   Hong Kong, Dept Syst Engn &amp; Engn Management, Hong Kong, Hong Kong,
   Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sun, LF (reprint author), Chinese Univ Hong Kong, Dept Syst Engn &amp; Engn Management, Hong Kong, Hong Kong, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>lfsunkli@se.cuhk.edu.hk; kli@se.cuhk.edu.hk; hwang@se.cuhk.edu.hk;
   sykang@se.cuhk.edu.hk; hmmeng@se.cuhk.edu.hk</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Meng, Helen</display_name>&nbsp;</font></td><td><font size="3">F-6043-2011&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Meng, Helen</display_name>&nbsp;</font></td><td><font size="3">0000-0002-4427-3532&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000389574300060</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kobayashi, D
   <br>Nakamura, R</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kobayashi, Daiji
   <br>Nakamura, Ryogo</td>
</tr>

<tr>
<td valign="top">BE </td><td>Yamamoto, S</td>
</tr>

<tr>
<td valign="top">TI </td><td>Designing Effective Vibration Patterns for Tactile Interfaces</td>
</tr>

<tr>
<td valign="top">SO </td><td>HUMAN INTERFACE AND THE MANAGEMENT OF INFORMATION: INFORMATION, DESIGN
   AND INTERACTION, PT I</td>
</tr>

<tr>
<td valign="top">SE </td><td>Lecture Notes in Computer Science</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>18th International Conference on Human-Computer Interaction (HCI
   International)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUL 17-22, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Toronto, CANADA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Tactile interface; Vibration pattern; Notification message</td>
</tr>

<tr>
<td valign="top">AB </td><td>Tactile interfaces presenting vibration stimuli are able to communicate confidentially and silently. Recent advances have enabled the users of mobile devices to create their own vibration patterns using software installed on their devices. Our research investigated vibration patterns with the aim of realizing a method that would allow elderly Japanese users to design memorable vibration patterns. Our previous study led to the conclusion that the vibration patterns characterized by the pronunciation of the message could be effectively recognized by the user. Thus, we now present an experimental study based on the proposed message-to-vibration pattern conversion method for designing comprehensible vibration patterns. First, the effectiveness of vibration patterns characterized by Japanese pronunciation factors such as the beat or the alteration in accent was evaluated. Further, we experimented with vibration patterns created by using the volume of the user's voice during pronunciation. The results confirmed the effectiveness of the method of voice volume-to-vibration patterns.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kobayashi, Daiji; Nakamura, Ryogo] Chitose Inst Sci &amp; Technol, Chitose,
   Hokkaido, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kobayashi, D (reprint author), Chitose Inst Sci &amp; Technol, Chitose, Hokkaido, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>d-kobaya@photon.chitose.ac.jp; b2121580@photon.chitose.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>9734</td>
</tr>

<tr>
<td valign="top">BP </td><td>511</td>
</tr>

<tr>
<td valign="top">EP </td><td>522</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/978-3-319-40349-6_49</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000389464900049</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sathiarekha, K
   <br>Kumaresan, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sathiarekha, K.
   <br>Kumaresan, S.</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Survey on the Evolution of Various Voice Conversion Techniques</td>
</tr>

<tr>
<td valign="top">SO </td><td>2016 3RD INTERNATIONAL CONFERENCE ON ADVANCED COMPUTING AND
   COMMUNICATION SYSTEMS (ICACCS)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Advanced Computing and Communication Systems</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>3rd International Conference on Advanced Computing and Communication
   Systems (ICACCS)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JAN 22-23, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Sri Eswar Coll Engn, Chennai, INDIA</td>
</tr>

<tr>
<td valign="top">HO </td><td>Sri Eswar Coll Engn</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice Conversion; static approaches; codebook mapping; Hidden Markovian
   Models (HMM); Gaussian Mixture Models (GMM); machine learning</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion (VC) is an area of speech processing that deals with the process of hiding a speaker's identity. This area has been gaining a lot of importance in the recent years. In voice conversion, the voice of the source speaker is taken and modified so that it is made to sound as though it was rendered by the target speaker. Several techniques to voice conversion have been presented and this paper is a survey on those various methods to achieve voice conversion. The techniques trace out from the codebook mapping techniques to Hidden Markovian Model to Gaussian Mixture Model and finally have evolved up to the neural networks and machine learning techniques. These methods focus on various tasks such as speech enhancement, emotion conversion and speaking assistance.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Sathiarekha, K.; Kumaresan, S.] Govt Coll Technol, Dept CSE,
   Coimbatore, Tamil Nadu, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sathiarekha, K (reprint author), Govt Coll Technol, Dept CSE, Coimbatore, Tamil Nadu, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>sathiarekha@gmail.com; sukumaresan@gct.ac.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000386953800062</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nathan, B
   <br>Chong, YW
   <br>Hansi, SM</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nathan, Balasubramanian
   <br>Chong, Yung-Wey
   <br>Hansi, Sabri M.</td>
</tr>

<tr>
<td valign="top">BE </td><td>Thampi, SM
   <br>Bandyopadhyay, S
   <br>Krishnan, S
   <br>Li, KC
   <br>Mosin, S
   <br>Ma, M</td>
</tr>

<tr>
<td valign="top">TI </td><td>Acoustic Echo Cancellation Technique for VoIP</td>
</tr>

<tr>
<td valign="top">SO </td><td>ADVANCES IN SIGNAL PROCESSING AND INTELLIGENT RECOGNITION SYSTEMS
   (SIRS-2015)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Advances in Intelligent Systems and Computing</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>2nd International Symposium on Signal Processing and Intelligent
   Recognition Systems (SIRS)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 16-19, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Trivandrum, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Acoustic Echo Canceller; Voice over Internet Protocol; Sampling rate;
   Signal processing; Noise canceller</td>
</tr>

<tr>
<td valign="top">AB </td><td>Acoustic Echo Canceller (AEC) is a crucial element to eliminate acoustic echo in a Voice over Internet Protocol (VoIP) communication. In the past, AEC have been implemented on Digital Signal Processing (DSP) platforms. This work uses software to perform the very demanding real-time signal processing that stack up against traditional implementation of AEC. Several issues are taken into consideration in the design of the AEC software. First, the AEC software is able to handle a mismatch in sampling rate between A/D and D/A conversion. Second, the playback sampling rate of playing CD-quality music with voice chat is considered. Third, the AEC has to overcome the major challenge of separating the reference signal in the presence of noise. Finally, the computational complexity and convergence problem has to be minimal. This work presents an AEC method that enable optimal audio quality in VoIP.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nathan, Balasubramanian; Chong, Yung-Wey; Hansi, Sabri M.] Univ Sains
   Malaysia, Natl Adv IPv6 Ctr, Malaysia, Penang, Malaysia.
   <br>[Hansi, Sabri M.] Seiyun Community Coll, Hadhramount, Yemen.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nathan, B; Hansi, SM (reprint author), Univ Sains Malaysia, Natl Adv IPv6 Ctr, Malaysia, Penang, Malaysia.; Hansi, SM (reprint author), Seiyun Community Coll, Hadhramount, Yemen.</td>
</tr>

<tr>
<td valign="top">EM </td><td>balasubramanian@nav6.usm.my; chong@usm.my; sabri@nav6.usm.my</td>
</tr>

<tr xmlns:date="http://exslt.org/dates-and-times">
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Chong, Yung-Wey</display_name>&nbsp;</font></td><td><font size="3">N-4638-2017&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Chong, Yung-Wey</display_name>&nbsp;</font></td><td><font size="3">0000-0003-1750-7441&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>425</td>
</tr>

<tr>
<td valign="top">BP </td><td>335</td>
</tr>

<tr>
<td valign="top">EP </td><td>344</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/978-3-319-28658-7_29</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000384639900029</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Rehman, MZU
   <br>Shah, SIA
   <br>Javaid, M
   <br>Gilani, SO
   <br>Ansari, U</td>
</tr>

<tr>
<td valign="top">AF </td><td>Rehman, Muhammad Zia Ur
   <br>Shah, Syed Irtiza Ali
   <br>Javaid, Muzzamil
   <br>Gilani, Syed Omer
   <br>Ansari, Umar</td>
</tr>

<tr>
<td valign="top">TI </td><td>Audio signal's test in designing a cost-effective hearing aid device
   using a microcontroller</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERNATIONAL JOURNAL OF BIOMEDICAL ENGINEERING AND TECHNOLOGY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>audio signals; hearing device; frequency; digital analogue conversion;
   impairment; analogue to digital converter; microcontroller; human voice;
   anti-aliasing filter; pre-amplifier</td>
</tr>

<tr>
<td valign="top">AB </td><td>A large number of hearing impaired people in rural sector of Pakistan cannot afford the high prices of digital hearing aids. Moreover, since the analogue devices amplify the speech and noise signals equally, they are not much flexible and have functional limitations. These problems can be solved with digital devices which are generally costly. In this research work, a microcontroller based cost-effective and portable hearing aid is designed and tested. The main components include sound detection circuit, microcontroller and a digital to analogue (DAC) convertor. Device is aimed to process human voice frequencies with an adjustable gain of up to 80 dB. It is tested with an input audio signal and it showed satisfactory results for low frequency signals up to 3.5 kHz which caters for the frequency range of most human voices. The system is now being developed for more effective noise suppression and broader frequency range.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Rehman, Muhammad Zia Ur; Shah, Syed Irtiza Ali; Gilani, Syed Omer] Natl
   Univ Sci &amp; Technol, Dept Robot &amp; Artificial Intelligence, Islamabad,
   Pakistan.
   <br>[Javaid, Muzzamil; Ansari, Umar] Natl Univ Sci &amp; Technol, Dept Biomed
   Engn &amp; Sci, Islamabad, Pakistan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Rehman, MZU (reprint author), Natl Univ Sci &amp; Technol, Dept Robot &amp; Artificial Intelligence, Islamabad, Pakistan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ziaurrehman@smme.edu.pk; irtiza@smme.nust.edu.pk;
   muzamil.javaid@gmail.com; omer@smme.nust.edu.pk; ansari@smme.nust.edu.pk</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Rehman, Muhammad Zia ur</display_name>&nbsp;</font></td><td><font size="3">M-3450-2016&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Rehman, Muhammad Zia ur</display_name>&nbsp;</font></td><td><font size="3">0000-0002-6141-3648&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Gilani, Dr. Syed Omer</display_name>&nbsp;</font></td><td><font size="3">0000-0001-5654-7863&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>20</td>
</tr>

<tr>
<td valign="top">IS </td><td>3</td>
</tr>

<tr>
<td valign="top">BP </td><td>195</td>
</tr>

<tr>
<td valign="top">EP </td><td>207</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1504/IJBET.2016.075421</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000384710300001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Chen, XT
   <br>Zhang, LH</td>
</tr>

<tr>
<td valign="top">AF </td><td>Chen, Xian-tong
   <br>Zhang, Ling-hua</td>
</tr>

<tr>
<td valign="top">BE </td><td>Qi, E</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Conversion Based on Radial Basic Function Network and Joint
   Spectral Parameters</td>
</tr>

<tr>
<td valign="top">SO </td><td>PROCEEDINGS OF THE 6TH INTERNATIONAL ASIA CONFERENCE ON INDUSTRIAL
   ENGINEERING AND MANAGEMENT INNOVATION: CORE THEORY AND APPLICATIONS OF
   INDUSTRIAL ENGINEERING, VOL 1</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>6th International Asia Conference on Industrial Engineering and
   Management Innovation (IEMI)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUL 25-26, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Tianjin Univ, Tianjin, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">HO </td><td>Tianjin Univ</td>
</tr>

<tr>
<td valign="top">DE </td><td>Joint spectral parameters; Radial basic function (RBF); Spectrum
   transformation; Voice conversion</td>
</tr>

<tr>
<td valign="top">AB </td><td>In voice conversion system, vocal tract spectral characteristics and prosodic feature are both important for characteristic analysis. However, the accurate conversion of the former draws more concerns while the latter is often ignored. This paper mainly studies an RBF training method for spectrum transformation based on joint spectral parameters, which include spectral parameters and pitch frequencies. Firstly, source and target joint spectral parameters are extracted by STRAIGHT model. Then those parameters are trained in an RBF network to find rules for transforming the source test speech parameters to the target ones. Experiment results show that the proposed RBF method based on joint spectral parameters is better at spectrum transformation than conventional method as well as RBF method based on only spectral parameters.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Chen, Xian-tong; Zhang, Ling-hua] Nanjing Univ Posts &amp; Telecommun, Coll
   Telecommun &amp; Informat Engn, Nanjing 210003, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Chen, XT (reprint author), Nanjing Univ Posts &amp; Telecommun, Coll Telecommun &amp; Informat Engn, Nanjing 210003, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>chenxt0524@126.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>1043</td>
</tr>

<tr>
<td valign="top">EP </td><td>1053</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.2991/978-94-6239-148-2_103</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Operations Research &amp; Management Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000383912700103</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Zhang, QM
   <br>Tao, L
   <br>Zhou, J
   <br>Wang, HB</td>
</tr>

<tr>
<td valign="top">AF </td><td>Zhang, Qianmin
   <br>Tao, Liang
   <br>Zhou, Jian
   <br>Wang, Huabin</td>
</tr>

<tr>
<td valign="top">BE </td><td>Qin, Y
   <br>Jia, L
   <br>Feng, J
   <br>An, M
   <br>Diao, L</td>
</tr>

<tr>
<td valign="top">TI </td><td>The Voice Conversion Method Based on Sparse Convolutive Non-negative
   Matrix Factorization</td>
</tr>

<tr>
<td valign="top">SO </td><td>PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON ELECTRICAL AND
   INFORMATION TECHNOLOGIES FOR RAIL TRANSPORTATION: TRANSPORTATION</td>
</tr>

<tr>
<td valign="top">SE </td><td>Lecture Notes in Electrical Engineering</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Electrical and Information Technologies for
   Rail Transportation - Electrical Traction</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 28-30, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>China Electrotechn Soc, Elect Equipment Technol Comm Rail Transportat,
   Zhuzhou, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">HO </td><td>China Electrotechn Soc, Elect Equipment Technol Comm Rail Transportat</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speech intelligibility; Voice conversion; Itakura-Saito distance;
   Time-frequency basis</td>
</tr>

<tr>
<td valign="top">ID </td><td>ALGORITHMS</td>
</tr>

<tr>
<td valign="top">AB </td><td>We propose a voice conversion method based on sparse convolutive non-negative matrix factorization. The method utilizes the Itakura-Saito distance as the objective cost function, making the smaller matrix element with a smaller reconstruction error due to the property of scale invariant of the cost function. The time-frequency basis of the source and target were extracted during the training phase, and the speech is converted through time-frequency basis substitution. The effect of whisper-to-normal speech conversion experiment is also conducted. Experimental results show that the proposed voice conversion method outperforms the method based on the conventional convolutive non-negative matrix factorization and the method based on the Kullback-Leibler (K-L) cost function in the aspects of speech intelligibility.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Zhang, Qianmin; Tao, Liang; Zhou, Jian; Wang, Huabin] Anhui Univ, Sch
   Comp Sci &amp; Technol, 111 Jiulong Rd, Hefei 230039, Anhui, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Zhang, QM (reprint author), Anhui Univ, Sch Comp Sci &amp; Technol, 111 Jiulong Rd, Hefei 230039, Anhui, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>qmzhang002@sina.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>378</td>
</tr>

<tr>
<td valign="top">BP </td><td>259</td>
</tr>

<tr>
<td valign="top">EP </td><td>267</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/978-3-662-49370-0_27</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Transportation</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000375984300027</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Bourdine, AV
   <br>Bukashkin, SA
   <br>Buzov, AV
   <br>Kubanov, VP
   <br>Praporshchikov, DE
   <br>Tyazhev, AI</td>
</tr>

<tr>
<td valign="top">AF </td><td>Bourdine, Anton V.
   <br>Bukashkin, Sergey A.
   <br>Buzov, Alexander V.
   <br>Kubanov, Victor P.
   <br>Praporshchikov, Denis E.
   <br>Tyazhev, Anatoly I.</td>
</tr>

<tr>
<td valign="top">BE </td><td>Andreev, VA
   <br>Bourdine, AV
   <br>Burdin, VA
   <br>Morozov, OG
   <br>Sultanov, AH</td>
</tr>

<tr>
<td valign="top">TI </td><td>Concept of distributed corporative wireless vehicle voice networks based
   on Radio-Over-Fiber technique</td>
</tr>

<tr>
<td valign="top">SO </td><td>OPTICAL TECHNOLOGIES FOR TELECOMMUNICATIONS 2015</td>
</tr>

<tr>
<td valign="top">SE </td><td>Proceedings of SPIE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>14th International Conference on Optical Technologies for
   Telecommunications</td>
</tr>

<tr>
<td valign="top">CY </td><td>NOV 16-18, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Ufa State Aviat Tech Univ, Ufa, RUSSIA</td>
</tr>

<tr>
<td valign="top">HO </td><td>Ufa State Aviat Tech Univ</td>
</tr>

<tr>
<td valign="top">DE </td><td>Radio-over-Fiber; wireless networks; integration of wireless and fiber
   optic networks; central station; base station; remote antenna unit</td>
</tr>

<tr>
<td valign="top">ID </td><td>OPTICAL-FIBER; CONVERSION; MULTIMODE; TRANSPORT</td>
</tr>

<tr>
<td valign="top">AB </td><td>This work is concerned on description of the concept of corporative wireless vehicle voice networks based on Radio-over-Fiber (RoF) technology, which is integration of wireless and fiber optic networks. The concept of RoF means to transport data over optical fibers by modulating lightwave with radio frequency signal or at the intermediate frequency/baseband that provides to take advantage of the low loss and large bandwidth of an optical fiber together with immunity to electromagnetic influence, flexibility and transparence. A brief overview of key RoF techniques as well as comparative analysis and ability of its application for wireless vehicle voice network realization is presented.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Bourdine, Anton V.; Kubanov, Victor P.; Praporshchikov, Denis E.;
   Tyazhev, Anatoly I.] Povolzhskiy State Univ Telecommun &amp; Informat, Dept
   Commun Lines, 77 Moscow Av, Samara 443090, Russia.
   <br>[Bukashkin, Sergey A.; Buzov, Alexander V.] Stock Co Concern Automat, 25
   Botanicheskaja St, Moscow 127106, Russia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Praporshchikov, DE (reprint author), Povolzhskiy State Univ Telecommun &amp; Informat, Dept Commun Lines, 77 Moscow Av, Samara 443090, Russia.</td>
</tr>

<tr>
<td valign="top">EM </td><td>dep82@mail.ru</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Tyazhev, Anatoly</display_name>&nbsp;</font></td><td><font size="3">Q-6402-2017&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Bourdine, Anton</display_name>&nbsp;</font></td><td><font size="3">Q-1533-2015&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Praporshchikov, Denis</display_name>&nbsp;</font></td><td><font size="3">N-3298-2017&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Kubanov, Victor</display_name>&nbsp;</font></td><td><font size="3">V-4685-2017&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Tyazhev, Anatoly</display_name>&nbsp;</font></td><td><font size="3">0000-0002-4402-1597&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Bourdine, Anton</display_name>&nbsp;</font></td><td><font size="3">0000-0001-8737-5486&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>9807</td>
</tr>

<tr>
<td valign="top">AR </td><td>98070A</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1117/12.2234577</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Optics; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000374452600010</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sathe-Pathak, B
   <br>Patil, S
   <br>Panat, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sathe-Pathak, Bageshree
   <br>Patil, Shalaka
   <br>Panat, Ashish</td>
</tr>

<tr>
<td valign="top">BE </td><td>Satapathy, SC
   <br>Mandal, JK
   <br>Udgata, SK
   <br>Bhateja, V</td>
</tr>

<tr>
<td valign="top">TI </td><td>Application of Three Different Artificial Neural Network Architectures
   for Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>INFORMATION SYSTEMS DESIGN AND INTELLIGENT APPLICATIONS, VOL 2, INDIA
   2016</td>
</tr>

<tr>
<td valign="top">SE </td><td>Advances in Intelligent Systems and Computing</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>3rd International Conference on Information System Design and
   Intelligent Applications (INDIA)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JAN 08-09, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>ANITS Campus, Visakhapatnam, INDIA</td>
</tr>

<tr>
<td valign="top">HO </td><td>ANITS Campus</td>
</tr>

<tr>
<td valign="top">DE </td><td>Artificial neural network; Discrete wavelet transform; Packet
   decomposition; Spectral transformation; Speech transformation</td>
</tr>

<tr>
<td valign="top">ID </td><td>ALGORITHM</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper designs a Multi-scale Spectral transformation technique for Voice Conversion. The proposed algorithm uses Spectral transformation technique designed using multi-resolution wavelet feature set and a Neural Network to generate a mapping function between source and target speech. Dynamic Frequency Warping technique is used for aligning source and target speech and Overlap-Add method is used for minimizing the distortions that occur in the reconstruction process. With the use of Neural Network, mapping of spectral parameters between source and target speech has been achieved more efficiently. In this paper, the mapping function is generated in three different ways, using three types of Neural Networks namely, Feed Forward Neural Network, Generalized Regression Neural Network and Radial Basis Neural Network. Results of all three Neural Networks are compared using execution time requirements and Subjective analysis. The main advantage of this approach is that it is speech as well as speaker independent algorithm.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Sathe-Pathak, Bageshree; Panat, Ashish] Priyadarshani Coll Engn,
   Nagpur, Maharashtra, India.
   <br>[Patil, Shalaka] Cummins Coll Engn, Pune, Maharashtra, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sathe-Pathak, B (reprint author), Priyadarshani Coll Engn, Nagpur, Maharashtra, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>bvpathak100@yahoo.com; shalakapatils@gmail.com; ashishpanat@gmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>434</td>
</tr>

<tr>
<td valign="top">BP </td><td>237</td>
</tr>

<tr>
<td valign="top">EP </td><td>246</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/978-81-322-2752-6_23</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000373009100023</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kanrar, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kanrar, Soumen</td>
</tr>

<tr>
<td valign="top">BE </td><td>Das, S
   <br>Pal, T
   <br>Kar, S
   <br>Satapathy, SC
   <br>Mandal, JK</td>
</tr>

<tr>
<td valign="top">TI </td><td>Impact of Threshold to Identify Vocal Tract</td>
</tr>

<tr>
<td valign="top">SO </td><td>PROCEEDINGS OF THE 4TH INTERNATIONAL CONFERENCE ON FRONTIERS IN
   INTELLIGENT COMPUTING: THEORY AND APPLICATIONS (FICTA) 2015</td>
</tr>

<tr>
<td valign="top">SE </td><td>Advances in Intelligent Systems and Computing</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>4th International Conference on Frontiers in Intelligent Computing -
   Theory and Applications (FICTA)</td>
</tr>

<tr>
<td valign="top">CY </td><td>NOV 16-18, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Durgapur, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speaker identification; Gaussian mixture mode; Acoustic feature vectors;
   Decision threshold; False accept; False reject</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEAKER VERIFICATION; IDENTIFICATION; SYSTEMS; MODELS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Speaker identification process is to identify a particular vocal cord from a set of existing speakers. In the speaker identification processes, the unknown speaker voice sample targets each of the existing speakers in the system and gives a predication. The predication is more than one existing known speaker voice and is very close to the unknown speaker voice. It is a one to many mapping. The mapping function gives a set of predicated values associated with the order pair of speakers. In the order pair, the first coordinate is the unknown speaker, and the second coordinates is the existing known speaker from the speaker recognition system. The set of predicated values helps to identify the unknown speaker. The identification process makes a comparison of the unknown speaker model with the models of the existing voice in the system. In this paper, the model is a Gaussian mixture model built by the extraction of the acoustic feature vectors. This paper presents the impact of the decision threshold based on false accepts and false reject for an unknown number of speaker conversion in the speaker identification result. In the simulation, the considered known speaker voices are collected through different channels. In the testing, the GMM voice models of the known speakers are distributed among the numbers of clusters in the test dataset.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kanrar, Soumen] Vehere Interact Pvt Ltd, Kolkata 53, W Bengal, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kanrar, S (reprint author), Vehere Interact Pvt Ltd, Kolkata 53, W Bengal, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>Soumen.kanrar@veheretech.com</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Kanrar, Soumen</display_name>&nbsp;</font></td><td><font size="3">N-3113-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Kanrar, Soumen</display_name>&nbsp;</font></td><td><font size="3">0000-0002-0331-4932&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>404</td>
</tr>

<tr>
<td valign="top">BP </td><td>97</td>
</tr>

<tr>
<td valign="top">EP </td><td>105</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000371332400009</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sepehripour, AH
   <br>Chaudhry, UA
   <br>Suliman, A
   <br>Kidher, E
   <br>Sayani, N
   <br>Ashrafian, H
   <br>Harling, L
   <br>Athanasiou, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sepehripour, Amir H.
   <br>Chaudhry, Umar A.
   <br>Suliman, Amna
   <br>Kidher, Emaddin
   <br>Sayani, Nusrat
   <br>Ashrafian, Hutan
   <br>Harling, Leanne
   <br>Athanasiou, Thanos</td>
</tr>

<tr>
<td valign="top">TI </td><td>How revascularization on the beating heart with cardiopulmonary bypass
   compares to off-pump? A meta-analysis of observational studies</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERACTIVE CARDIOVASCULAR AND THORACIC SURGERY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Coronary artery bypass graft surgery; Off-pump; On-pump; Beating heart;
   Outcomes</td>
</tr>

<tr>
<td valign="top">ID </td><td>CORONARY-ARTERY-BYPASS; ON-PUMP; SURGERY; OUTCOMES; MORTALITY</td>
</tr>

<tr>
<td valign="top">AB </td><td>Off-pump coronary artery bypass surgery has been a controversial area of debate and the outcome profile of the technique has been thoroughly investigated. Scepticism regarding the reported outcomes and the conduct of the randomized trials comparing this technique with conventional on-pump coronary artery bypass surgery has been widely voiced, and the technique of off-pump surgery remains as an infrequently adopted approach to myocardial revascularization worldwide. Criticisms of the technique are related to lower rates of complete revascularization and its unknown long-term consequences, the significant detrimental effects on mortality and major adverse events when emergency conversion is required, and the significant lack of long-term survival and morbidity data. The hybrid technique of myocardial revascularization on the beating heart with the use of cardiopulmonary bypass may theoretically provide the beneficial effects of off-pump surgery in terms of myocardial protection and organ protection, while providing the safety and stability of on-pump surgery to allow complete revascularization. Large randomized comparison to support evidence-based choices is currently lacking. In this article, we have meta-analysed the outcomes of on-pump beating heart surgery in comparison with off-pump surgery focusing on major adverse cardiovascular and cerebrovascular adverse events (MACCE) including mortality, stroke and myocardial infarction and the degree of revascularization and number of bypass grafts performed. It was demonstrated that the beating heart on-pump technique allows a significantly higher number of bypass grafts to be performed, resulting in significantly higher degree of revascularization. We have also demonstrated a slightly higher rate of 30-day mortality and MACCE with the technique although not at a statistically significant level. These results should be considered alongside the population risk profile, where a significantly higher risk cohort had undergone the beating heart on-pump technique. Long-term survival and morbidity figures are required to assess the impact of these findings in the coronary surgery patient population.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Sepehripour, Amir H.; Chaudhry, Umar A.; Suliman, Amna; Kidher,
   Emaddin; Sayani, Nusrat; Ashrafian, Hutan; Harling, Leanne; Athanasiou,
   Thanos] Univ London Imperial Coll Sci Technol &amp; Med, Dept Surg &amp; Canc,
   London, England.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sepehripour, AH (reprint author), St Marys Hosp, Dept Surg &amp; Canc, 10th Floor QEQM Bldg, London W2 1NY, England.</td>
</tr>

<tr>
<td valign="top">EM </td><td>amir.sepehripour@gmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>4</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>5</td>
</tr>

<tr>
<td valign="top">PD </td><td>JAN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>22</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>63</td>
</tr>

<tr>
<td valign="top">EP </td><td>71</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1093/icvts/ivv291</td>
</tr>

<tr>
<td valign="top">SC </td><td>Cardiovascular System &amp; Cardiology; Respiratory System; Surgery</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000368372300011</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Yadav, J
   <br>Rao, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Yadav, Jainath
   <br>Rao, K. Sreenivasa</td>
</tr>

<tr>
<td valign="top">TI </td><td>Prosodic Mapping Using Neural Networks for Emotion Conversion in Hindi
   Language</td>
</tr>

<tr>
<td valign="top">SO </td><td>CIRCUITS SYSTEMS AND SIGNAL PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Feedforward neural network (FFNN); Emotion conversion; Prosody
   parameters; Instants of significant excitation (epochs); Vowel onset
   point; Objective measures</td>
</tr>

<tr>
<td valign="top">ID </td><td>TIME-SCALE MODIFICATION; SIGNIFICANT EXCITATION; VOICE CONVERSION;
   SPEECH SIGNALS; IMPLEMENTATION; INSTANTS; SYSTEM</td>
</tr>

<tr>
<td valign="top">AB </td><td>An emotion is made of several components such as physiological changes in the body, subjective feelings and expressive behaviors. These changes in speech signal are mainly observed in prosody parameters such as pitch, duration and energy. Hindi language is mostly syllabic in nature. Syllables are the most suitable basic units for the analysis and synthesis of speech. Therefore, vowel onset point detection method is used to segment the speech utterance into syllable like units. In this work, prosody parameters are modified using instants of significant excitation (epochs) and these instants are detected using zero frequency filtering-based method. Epoch locations in the voiced speech correspond to instants of glottal closure, and in the unvoiced region, they correspond to some random instants of significant excitation. Anger, happiness and sadness emotions are considered as target emotions in the proposed emotion conversion framework. Feedforward neural network models are explored for mapping the prosodic parameters between neutral and target emotions. Predicted prosodic parameters of the target emotion are incorporated into neutral speech at syllable level to produce the desired emotional speech. After incorporating the emotion-specific prosody, perceptual quality of the transformed speech is evaluated by subjective tests.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Yadav, Jainath; Rao, K. Sreenivasa] Indian Inst Technol, Sch Informat
   Technol, Kharagpur 721302, W Bengal, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Yadav, J (reprint author), Indian Inst Technol, Sch Informat Technol, Kharagpur 721302, W Bengal, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>Jaibhu38@gmail.com; ksrao@iitkgp.ac.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>7</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>7</td>
</tr>

<tr>
<td valign="top">PD </td><td>JAN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>35</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>139</td>
</tr>

<tr>
<td valign="top">EP </td><td>162</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/s00034-015-0051-3</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000368688200008</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hsu, CC
   <br>Hwang, HT
   <br>Wu, YC
   <br>Tsao, Y
   <br>Wang, HM</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hsu, Chin-Cheng
   <br>Hwang, Hsin-Te
   <br>Wu, Yi-Chiao
   <br>Tsao, Yu
   <br>Wang, Hsin-Min</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Conversion from Non-parallel Corpora Using Variational
   Auto-encoder</td>
</tr>

<tr>
<td valign="top">SO </td><td>2016 ASIA-PACIFIC SIGNAL AND INFORMATION PROCESSING ASSOCIATION ANNUAL
   SUMMIT AND CONFERENCE (APSIPA)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Asia-Pacific Signal and Information Processing Association Annual Summit
   and Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Asia-Pacific Signal and Information Processing Association Annual Summit
   and Conference (APSIPA)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 13-16, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Jeju, SOUTH KOREA</td>
</tr>

<tr>
<td valign="top">AB </td><td>We propose a flexible framework for spectral conversion (SC) that facilitates training with unaligned corpora. Many SC frameworks require parallel corpora, phonetic alignments, or explicit frame-wise correspondence for learning conversion functions or for synthesizing a target spectrum with the aid of alignments. However, these requirements gravely limit the scope of practical applications of SC due to scarcity or even unavailability of parallel corpora. We propose an SC framework based on variational auto-encoder which enables us to exploit non-parallel corpora. The framework comprises an encoder that learns speaker-independent phonetic representations and a decoder that learns to reconstruct the designated speaker. It removes the requirement of parallel corpora or phonetic alignments to train a spectral conversion system. We report objective and subjective evaluations to validate our proposed method and compare it to SC methods that have access to aligned corpora.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Hsu, Chin-Cheng; Hwang, Hsin-Te; Wu, Yi-Chiao; Wang, Hsin-Min] Acad
   Sinica, Inst Informat Sci, Taipei, Taiwan.
   <br>[Tsao, Yu] Acad Sinica, Res Ctr Informat Technol Innovat, Taipei, Taiwan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hsu, CC (reprint author), Acad Sinica, Inst Informat Sci, Taipei, Taiwan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jeremycchsu@iis.sinica.edu.tw; hwanght@iis.sinica.edu.tw;
   tedwu@iis.sinica.edu.tw; yu.tsao@citi.sinica.edu.tw;
   whm@iis.sinica.edu.tw</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000393591800114</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hasan, M
   <br>Sajib, TH
   <br>Dey, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hasan, Muttaki
   <br>Sajib, Tanvir Hossain
   <br>Dey, Mrinmoy</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Machine Learning Based Approach for the Detection and Recognition of
   Bangla Sign Language</td>
</tr>

<tr>
<td valign="top">SO </td><td>2016 INTERNATIONAL CONFERENCE ON MEDICAL ENGINEERING, HEALTH INFORMATICS
   AND TECHNOLOGY (MEDITEC)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>1st International Conference on Medical Engineering, Health Informatics
   and Technology (MediTec)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 17-18, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Dhaka, BANGLADESH</td>
</tr>

<tr>
<td valign="top">DE </td><td>SVM; Classification; BdSL; TTS Engine; Feature; HOG; Contouring;
   Prediction; Recognition rate</td>
</tr>

<tr>
<td valign="top">AB </td><td>Speech impaired people are detached from the mainstream society due to the lacking of proper communication aid. Sign language is the primary means of communication for them which normal people do not understand. In order to facilitate the conversation conversion of sign language to audio is very necessary. This paper aims at conversion of sign language to speech so that disabled people have their own voice to communicate with the general people. In this paper, Hand Gesture recognition is performed using HOG (Histogram of Oriented Gradients) for extraction of features from the gesture image and SVM (Support Vector Machine) as classifier. Finally, predict the gesture image with output text. This output text is converted into audible sound using TTS (Text to Speech) converter.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Hasan, Muttaki; Sajib, Tanvir Hossain; Dey, Mrinmoy] Chittagong Univ
   Engn &amp; Technol, Dept Elect &amp; Elect Engn, Chittagong 4349, Bangladesh.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Dey, M (reprint author), Chittagong Univ Engn &amp; Technol, Dept Elect &amp; Elect Engn, Chittagong 4349, Bangladesh.</td>
</tr>

<tr>
<td valign="top">EM </td><td>tusar4577@gmail.com; tanvirsajibm277@gmail.com; mrinmoycuet@gmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Medical Informatics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000401659400031</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kayte, SN
   <br>Mundada, M
   <br>Gaikwad, S
   <br>Gawali, B</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kayte, Sangramsing N.
   <br>Mundada, Monica
   <br>Gaikwad, Santosh
   <br>Gawali, Bharti</td>
</tr>

<tr>
<td valign="top">BE </td><td>Satapathy, SC
   <br>Bhatt, YC
   <br>Joshi, A
   <br>Mishra, DK</td>
</tr>

<tr>
<td valign="top">TI </td><td>Performance Evaluation of Speech Synthesis Techniques for English
   Language</td>
</tr>

<tr>
<td valign="top">SO </td><td>PROCEEDINGS OF THE INTERNATIONAL CONGRESS ON INFORMATION AND
   COMMUNICATION TECHNOLOGY, ICICT 2015, VOL 2</td>
</tr>

<tr>
<td valign="top">SE </td><td>Advances in Intelligent Systems and Computing</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Congress on Information and Communication Technology
   (ICICT)</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 09-10, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Udaipur, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>TTS; MOS; HMM; Unit selection; Mean; Variance; MSE; PSNR</td>
</tr>

<tr>
<td valign="top">AB </td><td>The conversion of text to synthetic production of speech is known as text-to-speech synthesis (TTS). This can be achieved by the method of concate-native speech synthesis (CSS) and hidden Markov model techniques. Quality is the important paradigm for the artificial speech produced. The study involves the comparative analysis for quality of speech synthesis using hidden Markov model and unit selection approach. The quality of synthesized speech is evaluated with the two methods, i.e., subjective measurement using mean opinion score and objective measurement based on mean square score and peak signal-to-noise ratio (PSNR). Mel-frequency cepstral coefficient features are also extracted for synthesized speech. The experimental analysis shows that unit selection method results in better synthesized voice than hidden Markov model.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kayte, Sangramsing N.; Mundada, Monica; Gaikwad, Santosh; Gawali,
   Bharti] Dr Babasaheb Ambedkar Marathwada Univ, Dept Comp Sci &amp; Informat
   Technol, Aurangabad, Maharashtra, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kayte, SN (reprint author), Dr Babasaheb Ambedkar Marathwada Univ, Dept Comp Sci &amp; Informat Technol, Aurangabad, Maharashtra, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>bsangramsing@gmail.com; monicamundada5@gmail.com;
   santosh.gaikwadcsit@gmail.com; bharti_rokade@yahoo.co.in</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>gaikwad, santosh</display_name>&nbsp;</font></td><td><font size="3">0000-0003-0730-6363&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>439</td>
</tr>

<tr>
<td valign="top">BP </td><td>253</td>
</tr>

<tr>
<td valign="top">EP </td><td>262</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/978-981-10-0755-2_27</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000387430200027</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nose, T
   <br>Igarashi, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nose, Takashi
   <br>Igarashi, Yuki</td>
</tr>

<tr>
<td valign="top">TI </td><td>Real-Time Talking Avatar on the Internet Using Kinect and Voice
   Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Talking avatar; Voice conversion; Kinect; Internet; Real-time
   communication</td>
</tr>

<tr>
<td valign="top">AB </td><td>We have more chances to communicate via the internet. We often use text/video chat, but there are some problems, such as a lack of communication and anonymity. In this paper, we propose and implement a real-time talking avatar, where we can communicate with each other by synchronizing character's voice and motion from ours while keeping anonymity by using a voice conversion technique. For the voice conversion, we improve accuracy of the voice conversion by specializing to the target character's voice. Finally, we conduct subjective experiments and show the possibility of a new style of communication on the internet.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nose, Takashi; Igarashi, Yuki] Tohoku Univ, Grad Sch Engn, Sendai,
   Miyagi 980, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nose, T (reprint author), Tohoku Univ, Grad Sch Engn, Sendai, Miyagi 980, Japan.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>DEC</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">VL </td><td>6</td>
</tr>

<tr>
<td valign="top">IS </td><td>12</td>
</tr>

<tr>
<td valign="top">BP </td><td>301</td>
</tr>

<tr>
<td valign="top">EP </td><td>307</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000369852000041</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hueber, T
   <br>Girin, L
   <br>Alameda-Pineda, X
   <br>Bailly, G</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hueber, Thomas
   <br>Girin, Laurent
   <br>Alameda-Pineda, Xavier
   <br>Bailly, Gerard</td>
</tr>

<tr>
<td valign="top">TI </td><td>Speaker-Adaptive Acoustic-Articulatory Inversion Using Cascaded Gaussian
   Mixture Regression</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Acoustic-articulatory inversion; EM algorithm; Gaussian mixture
   regression; pronunciation training; speaker adaptation; speech
   production; talking head</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH PRODUCTION-MODEL; MAXIMUM-LIKELIHOOD; VOICE CONVERSION;
   ADAPTATION; GENERATION; MOVEMENTS; FEATURES</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper addresses the adaptation of an acoustic-articulatory model of a reference speaker to the voice of another speaker, using a limited amount of audio-only data. In the context of pronunciation training, a virtual talking head displaying the internal speech articulators (e.g., the tongue) could be automatically animated by means of such a model using only the speaker's voice. In this study, the articulatory-acoustic relationship of the reference speaker is modeled by a gaussian mixture model (GMM). To address the speaker adaptation problem, we propose a new framework called cascaded Gaussian mixture regression (C-GMR), and derive two implementations. The first one, referred to as Split-C-GMR, is a straightforward chaining of two distinct GMRs: one mapping the acoustic features of the source speaker into the acoustic space of the reference speaker, and the other estimating the articulatory trajectories with the reference model. In the second implementation, referred to as Integrated-C-GMR, the two mapping steps are tied together in a single probabilistic model. For this latter model, we present the full derivation of the exact EM training algorithm, that explicitly exploits the missing data methodology of machine learning. Other adaptation schemes based on maximum-a posteriori (MAP), maximum likelihood linear regression (MLLR) and direct cross-speaker acoustic-to-articulatory GMR are also investigated. Experiments conducted on two speakers for different amount of adaptation data show the interest of the proposed C-GMR techniques.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Hueber, Thomas; Bailly, Gerard] CNRS, GIPSA Lab, F-38400 St Martin
   Dheres, France.
   <br>[Hueber, Thomas; Bailly, Gerard] Univ Grenoble Alpes, GIPSA Lab, F-38400
   St Martin Dheres, France.
   <br>[Girin, Laurent] Univ Grenoble Alpes, GIPSA Lab, F-38330 Montbonnot St
   Martin, France.
   <br>[Girin, Laurent; Alameda-Pineda, Xavier] INRIA Grenoble Rhone Alpes,
   F-38330 Montbonnot St Martin, France.
   <br>[Alameda-Pineda, Xavier] Univ Trent, I-38122 Trento, Italy.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hueber, T (reprint author), CNRS, GIPSA Lab, F-38400 St Martin Dheres, France.</td>
</tr>

<tr>
<td valign="top">EM </td><td>thomas.hueber@gipsa-lab.grenoble-inp.fr;
   lau-rent.girin@gipsa-lab.grenoble-inp.fr; xavier.alamedapineda@unitn.it;
   gerard.bailly@gipsa-lab.grenoble-inp.fr</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>bailly, gerard</display_name>&nbsp;</font></td><td><font size="3">0000-0002-6053-0818&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Alameda-Pineda, Xavier</display_name>&nbsp;</font></td><td><font size="3">0000-0002-5354-1084&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>10</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>10</td>
</tr>

<tr>
<td valign="top">PD </td><td>DEC</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">VL </td><td>23</td>
</tr>

<tr>
<td valign="top">IS </td><td>12</td>
</tr>

<tr>
<td valign="top">BP </td><td>2246</td>
</tr>

<tr>
<td valign="top">EP </td><td>2259</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASLP.2015.2464702</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000361752600013</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Percybrooks, WS
   <br>Moore, E</td>
</tr>

<tr>
<td valign="top">AF </td><td>Percybrooks, Winston S.
   <br>Moore, Elliot</td>
</tr>

<tr>
<td valign="top">TI </td><td>A New HMM-Based Voice Conversion Methodology Evaluated on Monolingual
   and Cross-Lingual Conversion Tasks</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; cross-lingual conversion; excitation estimation;
   hidden Markov models (HMMs); subjective testing</td>
</tr>

<tr>
<td valign="top">ID </td><td>ARTIFICIAL NEURAL-NETWORKS; SPEAKER IDENTIFICATION; SPEECH SYNTHESIS;
   REPRESENTATION; TRANSFORMATION; INDIVIDUALITY; ALGORITHM</td>
</tr>

<tr>
<td valign="top">AB </td><td>The work presented here proposes a new voice conversion (VC) approach based on hidden Markov models (HMMs) for spectral conversion and excitation estimation. This paper is divided in two main parts: First, an initial HMM-based VC system is presented and compared to a state-of-the-art ML-GMM VC system in a monolingual conversion scenario with parallel training data; The second part shows the necessary modifications to use the HMM VC system in a cross-lingual conversion scenario and compares it with a cross-lingual VC system based on artificial neural networks (ANNs). The results of the tests show improved performance of the proposed HMM VC system compared with both the ML-GMM and ANN-based VC alternatives, while at the same time keeping most of the flexibility afforded by the ANN approach with respect to training data requirements.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Percybrooks, Winston S.] Univ del Norte, Dept Elect &amp; Elect Engn,
   Barranquilla, Colombia.
   <br>[Moore, Elliot] Georgia Inst Technol, Dept Elect &amp; Comp Engn, Atlanta,
   GA 30332 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Percybrooks, WS (reprint author), Univ del Norte, Dept Elect &amp; Elect Engn, Barranquilla, Colombia.</td>
</tr>

<tr>
<td valign="top">EM </td><td>wpercyb@uni-norte.edu.co; em80@mail.gatech.edu</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Percybrooks, Winston</display_name>&nbsp;</font></td><td><font size="3">X-2913-2018&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Percybrooks, Winston</display_name>&nbsp;</font></td><td><font size="3">0000-0002-0169-7562&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>DEC</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">VL </td><td>23</td>
</tr>

<tr>
<td valign="top">IS </td><td>12</td>
</tr>

<tr>
<td valign="top">BP </td><td>2298</td>
</tr>

<tr>
<td valign="top">EP </td><td>2310</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASLP.2015.2479040</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000361752600017</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Aihara, R
   <br>Fujii, T
   <br>Nakashika, T
   <br>Takiguchi, T
   <br>Ariki, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Aihara, Ryo
   <br>Fujii, Takao
   <br>Nakashika, Toru
   <br>Takiguchi, Tetsuya
   <br>Ariki, Yasuo</td>
</tr>

<tr>
<td valign="top">TI </td><td>Small-parallel exemplar-based voice conversion in noisy environments
   using affine non-negative matrix factorization</td>
</tr>

<tr>
<td valign="top">SO </td><td>EURASIP JOURNAL ON AUDIO SPEECH AND MUSIC PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; Speech synthesis; Speaker adaptation; Noise
   robustness; Small parallel corpus</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPARSE REPRESENTATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>The need to have a large amount of parallel data is a large hurdle for the practical use of voice conversion (VC). This paper presents a novel framework of exemplar-based VC that only requires a small number of parallel exemplars. In our previous work, a VC technique using non-negative matrix factorization (NMF) for noisy environments was proposed. This method requires parallel exemplars (which consist of the source exemplars and target exemplars that have the same texts uttered by the source and target speakers) for dictionary construction. In the framework of conventional Gaussian mixture model (GMM)-based VC, some approaches that do not need parallel exemplars have been proposed. However, in the framework of exemplar-based VC for noisy environments, such a method has never been proposed. In this paper, an adaptation matrix in an NMF framework is introduced to adapt the source dictionary to the target dictionary. This adaptation matrix is estimated using only a small parallel speech corpus. We refer to this method as affine NMF, and the effectiveness of this method has been confirmed by comparing its effectiveness with that of a conventional NMF-based method and a GMM-based method in noisy environments.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Aihara, Ryo; Fujii, Takao; Takiguchi, Tetsuya; Ariki, Yasuo] Kobe Univ,
   Grad Sch Syst Informat, Nada Ku, Kobe, Hyogo 657, Japan.
   <br>[Nakashika, Toru] Univ Electrocommun, Grad Sch Informat Syst, Chofu,
   Tokyo 182, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Aihara, R (reprint author), Kobe Univ, Grad Sch Syst Informat, Nada Ku, 1-1 Rokkodai, Kobe, Hyogo 657, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>aihara@me.cs.scitec.kobe-u.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PD </td><td>NOV 25</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">AR </td><td>32</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1186/s13636-015-0075-4</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000365756400001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wu, ZZ
   <br>Chng, ES
   <br>Li, HZ</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wu, Zhizheng
   <br>Chng, Eng Siong
   <br>Li, Haizhou</td>
</tr>

<tr>
<td valign="top">TI </td><td>Exemplar-based voice conversion using joint nonnegative matrix
   factorization</td>
</tr>

<tr>
<td valign="top">SO </td><td>MULTIMEDIA TOOLS AND APPLICATIONS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speech synthesis; Voice conversion; Exemplar; Sparse representation;
   Nonnegative matrix factorization; Joint nonnegative matrix factorization</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH</td>
</tr>

<tr>
<td valign="top">AB </td><td>Exemplar-based sparse representation is a nonparametric framework for voice conversion. In this framework, a target spectrum is generated as a weighted linear combination of a set of basis spectra, namely exemplars, extracted from the training data. This framework adopts coupled source-target dictionaries consisting of acoustically aligned source-target exemplars, and assumes they can share the same activation matrix. At runtime, a source spectrogram is factorized as a product of the source dictionary and the common activation matrix, which is applied to the target dictionary to generate the target spectrogram. In practice, either low-resolution mel-scale filter bank energies or high-resolution spectra are adopted in the source dictionary. Low-resolution features are flexible in capturing the temporal information without increasing the computational cost and the memory occupation significantly, while high-resolution spectra contain significant spectral details. In this paper, we propose a joint nonnegative matrix factorization technique to find the common activation matrix using low- and high-resolution features at the same time. In this way, the common activation matrix is able to benefit from low- and high-resolution features directly. We conducted experiments on the VOICES database to evaluate the performance of the proposed method. Both objective and subjective evaluations confirmed the effectiveness of the proposed methods.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Wu, Zhizheng; Chng, Eng Siong] Nanyang Technol Univ, Sch Comp Engn,
   Singapore 639798, Singapore.
   <br>[Wu, Zhizheng] Univ Edinburgh, Ctr Speech Technol Res, Edinburgh EH8
   9YL, Midlothian, Scotland.
   <br>[Li, Haizhou] Nanyang Technol Univ, Human Language Technol Dept, Inst
   Infocomm Res, Sch Comp Engn, Singapore 639798, Singapore.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Wu, ZZ (reprint author), Univ Edinburgh, Ctr Speech Technol Res, Edinburgh EH8 9YL, Midlothian, Scotland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>zhizheng.wu@ed.ac.uk; aseschng@ntu.edu.sg; hli@i2r.a-star.edu.sg</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Li, Haizhou</display_name>&nbsp;</font></td><td><font size="3">Q-6438-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Li, Haizhou</display_name>&nbsp;</font></td><td><font size="3">0000-0001-9158-9401&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>11</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>14</td>
</tr>

<tr>
<td valign="top">PD </td><td>NOV</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">VL </td><td>74</td>
</tr>

<tr>
<td valign="top">IS </td><td>22</td>
</tr>

<tr>
<td valign="top">BP </td><td>9943</td>
</tr>

<tr>
<td valign="top">EP </td><td>9958</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/s11042-014-2180-2</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000364019400007</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Chen, LH
   <br>Raitio, T
   <br>Valentini-Botinhao, C
   <br>Ling, ZH
   <br>Yamagishi, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Chen, Ling-Hui
   <br>Raitio, Tuomo
   <br>Valentini-Botinhao, Cassia
   <br>Ling, Zhen-Hua
   <br>Yamagishi, Junichi</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Deep Generative Architecture for Postfiltering in Statistical
   Parametric Speech Synthesis</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Deep generative architecture; hidden Markov model (HMM); modulation
   spectrum; postfilter; segmental quality; speech synthesis</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION; NEURAL-NETWORKS</td>
</tr>

<tr>
<td valign="top">AB </td><td>The generated speech of hidden Markov model (HMM)-based statistical parametric speech synthesis still sounds "muffled." One cause of this degradation in speech quality may be the loss of fine spectral structures. In this paper, we propose to use a deep generative architecture, a deep neural network (DNN) generatively trained, as a postfilter. The network models the conditional probability of the spectrum of natural speech given that of synthetic speech to compensate for such gap between synthetic and natural speech. The proposed probabilistic postfilter is generatively trained by cascading two restricted Boltzmann machines (RBMs) or deep belief networks (DBNs) with one bidirectional associative memory (BAM). We devised two types of DNN postfilters: one operating in the mel-cepstral domain and the other in the higher dimensional spectral domain. We compare these two new data-driven postfilters with other types of postfilters that are currently used in speech synthesis: a fixed mel-cepstral based postfilter, the global variance based parameter generation, and the modulation spectrum-based enhancement. Subjective evaluations using the synthetic voices of a male and female speaker confirmed that the proposed DNN-based postfilter in the spectral domain significantly improved the segmental quality of synthetic speech compared to that with conventional methods.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Chen, Ling-Hui; Ling, Zhen-Hua] Univ Sci &amp; Technol China, NELSLIP,
   Hefei 230027, Peoples R China.
   <br>[Chen, Ling-Hui] iFLYTEK Co Ltd, Hefei 230088, Peoples R China.
   <br>[Raitio, Tuomo] Aalto Univ, Dept Signal Proc &amp; Acoust, FI-02015 Espoo,
   Finland.
   <br>[Valentini-Botinhao, Cassia; Yamagishi, Junichi] Univ Edinburgh, CSTR,
   Edinburgh EH8 9AB, Midlothian, Scotland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Chen, LH (reprint author), Univ Sci &amp; Technol China, NELSLIP, Hefei 230027, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>chenlh@mail.ustc.edu.cn; tuomo.raitio@aalto.fi; cvbotinh@inf.ed.ac.uk;
   zhling@ustc.edu.cn; jyamagis@inf.ed.ac.uk</td>
</tr>

<tr>
<td valign="top">TC </td><td>19</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>19</td>
</tr>

<tr>
<td valign="top">PD </td><td>NOV</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">VL </td><td>23</td>
</tr>

<tr>
<td valign="top">IS </td><td>11</td>
</tr>

<tr>
<td valign="top">BP </td><td>2003</td>
</tr>

<tr>
<td valign="top">EP </td><td>2014</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASLP.2015.2461448</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000360835000025</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Lachhab, O
   <br>Di Martino, J
   <br>Ibn Elhaj, E
   <br>Hammouch, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Lachhab, Othman
   <br>Di Martino, Joseph
   <br>Ibn Elhaj, Elhassane
   <br>Hammouch, Ahmed</td>
</tr>

<tr>
<td valign="top">TI </td><td>A preliminary study on improving the recognition of esophageal speech
   using a hybrid system based on statistical voice conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>SPRINGERPLUS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speech enhancement; Esophageal speech assessment; Voice conversion;
   Pathological voices; Automatic speech recognition (ASR)</td>
</tr>

<tr>
<td valign="top">ID </td><td>ENHANCEMENT</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we propose a hybrid system based on a modified statistical GMM voice conversion algorithm for improving the recognition of esophageal speech. This hybrid system aims to compensate for the distorted information present in the esophageal acoustic features by using a voice conversion method. The esophageal speech is converted into a "target" laryngeal speech using an iterative statistical estimation of a transformation function. We did not apply a speech synthesizer for reconstructing the converted speech signal, given that the converted Mel cepstral vectors are used directly as input of our speech recognition system. Furthermore the feature vectors are linearly transformed by the HLDA (heteroscedastic linear discriminant analysis) method to reduce their size in a smaller space having good discriminative properties. The experimental results demonstrate that our proposed system provides an improvement of the phone recognition accuracy with an absolute increase of 3.40 % when compared with the phone recognition accuracy obtained with neither HLDA nor voice conversion.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Lachhab, Othman; Hammouch, Ahmed] Mohammed 5 Univ, LRGE Lab, ENSET,
   Madinat Al Irfane, Rabat, Morocco.
   <br>[Di Martino, Joseph] LORIA, F-54506 Vandoeuvre Les Nancy, France.
   <br>[Ibn Elhaj, Elhassane] INPT, Madinat Al Irfane, Rabat, Morocco.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Lachhab, O (reprint author), Mohammed 5 Univ, LRGE Lab, ENSET, Madinat Al Irfane, Rabat, Morocco.</td>
</tr>

<tr>
<td valign="top">EM </td><td>othmanlachhab@yahoo.fr</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PD </td><td>OCT 26</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">VL </td><td>4</td>
</tr>

<tr>
<td valign="top">AR </td><td>644</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1186/s40064-015-1428-2</td>
</tr>

<tr>
<td valign="top">SC </td><td>Science &amp; Technology - Other Topics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000364953500003</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Song, P
   <br>Zheng, WM
   <br>Zhang, XR
   <br>Jin, Y
   <br>Zha, C
   <br>Xin, MH</td>
</tr>

<tr>
<td valign="top">AF </td><td>Song, Peng
   <br>Zheng, Wenming
   <br>Zhang, Xinran
   <br>Jin, Yun
   <br>Zha, Cheng
   <br>Xin, Minghai</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Novel Iterative Speaker Model Alignment Method from Non-Parallel
   Speech for Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEICE TRANSACTIONS ON FUNDAMENTALS OF ELECTRONICS COMMUNICATIONS AND
   COMPUTER SCIENCES</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>non-parallel speech; voice conversion; iterative speaker model
   alignment; Gaussian mixture model</td>
</tr>

<tr>
<td valign="top">ID </td><td>NEURAL-NETWORKS; REGRESSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Most of the current voice conversion methods are conducted based on parallel speech, which is not easily obtained in practice. In this letter, a novel iterative speaker model alignment (ISMA) method is proposed to address this problem. First, the source and target speaker models are each trained from the background model by adopting maximum a posteriori (MAP) algorithm. Then, a novel ISMA method is presented for alignment and transformation of spectral features. Finally, the proposed ISMA approach is further combined with a Gaussian mixture model (GMM) to improve the conversion performance. A series of objective and subjective experiments are carried out on CMU ARCTIC dataset, and the results demonstrate that the proposed method significantly outperforms the state-of-the-art approach.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Song, Peng] Yantai Univ, Sch Comp &amp; Control Engn, Yantai 264005,
   Peoples R China.
   <br>[Zheng, Wenming; Jin, Yun; Xin, Minghai] Southeast Univ, Minist Educ,
   Key Lab Child Dev &amp; Learning Sci, Nanjing 210096, Jiangsu, Peoples R
   China.
   <br>[Zhang, Xinran; Zha, Cheng] Southeast Univ, Sch Informat Sci &amp; Engn,
   Nanjing 210096, Jiangsu, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Song, P (reprint author), Yantai Univ, Sch Comp &amp; Control Engn, Yantai 264005, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>pengsongseu@gmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PD </td><td>OCT</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">VL </td><td>E98A</td>
</tr>

<tr>
<td valign="top">IS </td><td>10</td>
</tr>

<tr>
<td valign="top">BP </td><td>2178</td>
</tr>

<tr>
<td valign="top">EP </td><td>2181</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1587/transfun.E98.A.2178</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000364414100022</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Murphy, EKM</td>
</tr>

<tr>
<td valign="top">AF </td><td>Murphy, Emilie K. M.</td>
</tr>

<tr>
<td valign="top">TI </td><td>Music and Catholic culture in post-Reformation Lancashire: piety,
   protest, and conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>BRITISH CATHOLIC HISTORY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>music; conversion; performance; ballads; politics</td>
</tr>

<tr>
<td valign="top">AB </td><td>This essay adds to our existing understanding of what it meant to be a member of the English Catholic community during the late Elizabeth and early Stuart period by exploring Catholic musical culture in Lancashire. This was a uniquely Catholic village, which, like the majority of villages, towns and cities in early modern England, was filled with the singing of ballads. Ballads have almost exclusively been treated in scholarship as a 'Protestant' phenomenon and the 'godly ballad' associated with the very fabric of a distinctively Protestant Elizabethan and Stuart entertainment culture. By investigating the songs and ballads in two manuscript collections from the Catholic network surrounding the Blundell family this essay will show how Catholics both composed and 'converted' existing ballads to voice social, devotional, and political concerns. The ballads performed in Little Crosby highlight a vibrant Catholic community, where musical expression was fundamental. Performance widened the parochial religious divide, whilst enhancing Catholic integration. This essay uncovers the way Catholics used music to voice religious and exhort protest as much as prayer. Finally, by investigating the tunes and melodies preserved in the manuscripts, I demonstrate how priests serving this network used ballads as part of their missionary strategy.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Murphy, Emilie K. M.] Natl Univ Ireland, Univ Rd, Galway, Ireland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Murphy, EKM (reprint author), Natl Univ Ireland, Univ Rd, Galway, Ireland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>emilie.murphy@nuigalway.ie</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Murphy, Emilie</display_name>&nbsp;</font></td><td><font size="3">0000-0001-7725-2423&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PD </td><td>OCT</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">VL </td><td>32</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">BP </td><td>492</td>
</tr>

<tr>
<td valign="top">EP </td><td>525</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1017/bch.2015.18</td>
</tr>

<tr>
<td valign="top">SC </td><td>History; Religion</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000217598300004</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Shevah, D
   <br>Kallus, R</td>
</tr>

<tr>
<td valign="top">AF </td><td>Shevah, Dana
   <br>Kallus, Rachel</td>
</tr>

<tr>
<td valign="top">TI </td><td>Community participation on the fringe of practice</td>
</tr>

<tr>
<td valign="top">SO </td><td>PROCEEDINGS OF THE INSTITUTION OF CIVIL ENGINEERS-URBAN DESIGN AND
   PLANNING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>disputes &amp; arbitration; municipal &amp; public service engineering; town and
   city planning</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper discusses the gap between the theoretical discourse of the community and community participation in planning, and how community participation actually functions in a contested urban arena. It focuses on a conflict between the municipality and residents of a neighbourhood in the town of Karmiel in Israel, over an elementary-school building conversion into a Kolel (rabbinical study centre). Archival material, planning and legal documents, and media and personal accounts present the conflicting perspectives of the concept of 'community' and its actual use; that is, the institutional against the local viewpoints and how they shape and define planning practices. This case study exemplifies the importance of many voices in the planning arena, particularly those of the local community, and suggests that interdisciplinary collaboration among professionals can expand planning knowledge and input on community participation in contested arenas.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Shevah, Dana; Kallus, Rachel] Technion, Fac Architecture &amp; Town
   Planning, Haifa, Israel.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Shevah, D (reprint author), Technion, Fac Architecture &amp; Town Planning, Haifa, Israel.</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PD </td><td>OCT</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">VL </td><td>168</td>
</tr>

<tr>
<td valign="top">IS </td><td>5</td>
</tr>

<tr>
<td valign="top">BP </td><td>223</td>
</tr>

<tr>
<td valign="top">EP </td><td>230</td>
</tr>

<tr>
<td valign="top">AR </td><td>1400039</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1680/udap.14.00039</td>
</tr>

<tr>
<td valign="top">SC </td><td>Urban Studies</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000215444800002</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Danelli, L
   <br>Marelli, M
   <br>Berlingeri, M
   <br>Tettamanti, M
   <br>Sberna, M
   <br>Paulesu, E
   <br>Luzzatti, C</td>
</tr>

<tr>
<td valign="top">AF </td><td>Danelli, Laura
   <br>Marelli, Marco
   <br>Berlingeri, Manuela
   <br>Tettamanti, Marco
   <br>Sberna, Maurizio
   <br>Paulesu, Erado
   <br>Luzzatti, Claudio</td>
</tr>

<tr>
<td valign="top">TI </td><td>Framing effects reveal discrete lexical-semantic and sublexical
   procedures in reading: an fMRI study</td>
</tr>

<tr>
<td valign="top">SO </td><td>FRONTIERS IN PSYCHOLOGY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>reading; fMRI; list-manipulation paradigm; dual-route model;
   lexical-semantic procedure; sublexical procedure; multi-voxel pattern
   analysis (MVPA)</td>
</tr>

<tr>
<td valign="top">ID </td><td>VISUAL WORD RECOGNITION; PHONOLOGICAL DYSLEXIA; REGULAR ORTHOGRAPHY;
   OBJECT RECOGNITION; FUNCTIONAL-ANATOMY; STRATEGIC CONTROL; NEURAL BASIS;
   FORM AREA; BRAIN; ALOUD</td>
</tr>

<tr>
<td valign="top">AB </td><td>According to the dual-route model, a printed string of letters can be processed by either a grapheme-to-phoneme conversion (GPO) route or a lexical-semantic route. Although meta analyses of the imaging literature support the existence of distinct but interacting reading procedures, individual neuroimaging studies that explored neural correlates of reading yielded inconclusive results. We used a list-manipulation paradigm to provide a fresh empirical look at this issue and to isolate specific areas that underlie the two reading procedures. In a lexical condition, we embedded disyllabic Italian words (target stimuli) in lists of either loanwords or trisyllabic Italian words with unpredictable stress position. In a GPO condition, similar target stimuli were included within lists of pseudowords. The procedure was designed to induce participants to emphasize either the lexical semantic or the GPO reading procedure, while controlling for possible linguistic confounds and keeping the reading task requirements stable across the two conditions. Thirty-three adults participated in the behavioral study, and 20 further adult participants were included in the fMRI study. At the behavioral level, we found sizeable effects of the framing manipulations that included slower voice onset times for stimuli in the pseudoword frames. At the functional anatomical level, the occipital and temporal regions, and the intraparietal sulcus were specifically activated when subjects were reading target words in a lexical frame. The inferior parietal and anterior fusiform cortex were specifically activated in the GPO condition. These patterns of activation represented a valid classifying model of fMRI images associated with target reading in both frames in the multi-voxel pattern analyses. Further activations were shared by the two procedures in the occipital and inferior parietal areas, in the premotor cortex, in the frontal regions and the left supplementary motor area. These regions are most likely involved in either early input or late output processes.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Danelli, Laura; Marelli, Marco; Berlingeri, Manuela; Paulesu, Erado;
   Luzzatti, Claudio] Univ Milano Bicocca, Dept Psychol, Milan, Italy.
   <br>[Danelli, Laura; Berlingeri, Manuela; Paulesu, Erado; Luzzatti, Claudio]
   NeuroMI Milan Ctr Neurosci, Milan, Italy.
   <br>[Marelli, Marco] Univ Trento, Ctr Mind Brain Sci, Rovereto, Italy.
   <br>[Tettamanti, Marco] Ist Sci San Raffaele, Div Neurosci, I-20132 Milan,
   Italy.
   <br>[Tettamanti, Marco] Ist Sci San Raffaele, Dept Nucl Med, I-20132 Milan,
   Italy.
   <br>[Sberna, Maurizio] Osped Niguarda Ca Granda, Dept Neuroradiol, Milan,
   Italy.
   <br>[Paulesu, Erado] IRCCS Galeazzi, fMRI Unit, Milan, Italy.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Danelli, L (reprint author), Univ Milano Bicocca, Dipartimento Psicol, Pizza Ateneo Nuovo 1, I-20126 Milan, Italy.</td>
</tr>

<tr>
<td valign="top">EM </td><td>laura.danelli@unimib.it</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>marelli, marco</display_name>&nbsp;</font></td><td><font size="3">0000-0001-5831-5441&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Berlingeri, Manuela</display_name>&nbsp;</font></td><td><font size="3">0000-0002-3159-2809&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>6</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>6</td>
</tr>

<tr>
<td valign="top">PD </td><td>SEP 23</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">VL </td><td>6</td>
</tr>

<tr>
<td valign="top">AR </td><td>1328</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.3389/fpsyg.2015.01328</td>
</tr>

<tr>
<td valign="top">SC </td><td>Psychology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000361811500001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Masaka, K
   <br>Aihara, R
   <br>Takiguchi, T
   <br>Ariki, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Masaka, Kenta
   <br>Aihara, Ryo
   <br>Takiguchi, Tetsuya
   <br>Ariki, Yasuo</td>
</tr>

<tr>
<td valign="top">TI </td><td>Multimodal voice conversion based on non-negative matrix factorization</td>
</tr>

<tr>
<td valign="top">SO </td><td>EURASIP JOURNAL ON AUDIO SPEECH AND MUSIC PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; Multimodal; Image features; Non-negative matrix
   factorization; Noise robustness</td>
</tr>

<tr>
<td valign="top">AB </td><td>A multimodal voice conversion (VC) method for noisy environments is proposed. In our previous non-negative matrix factorization (NMF)-based VC method, source and target exemplars are extracted from parallel training data, in which the same texts are uttered by the source and target speakers. The input source signal is then decomposed into source exemplars, noise exemplars, and their weights. Then, the converted speech is constructed from the target exemplars and the weights related to the source exemplars. In this study, we propose multimodal VC that improves the noise robustness of our NMF-based VC method. Furthermore, we introduce the combination weight between audio and visual features and formulate a new cost function to estimate audio-visual exemplars. Using the joint audio-visual features as source features, VC performance is improved compared with that of a previous audio-input exemplar-based VC method. The effectiveness of the proposed method is confirmed by comparing its effectiveness with that of a conventional audio-input NMF-based method and a Gaussian mixture model-based method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Masaka, Kenta; Aihara, Ryo] Kobe Univ, Grad Sch Syst Informat, Nada Ku,
   Kobe, Hyogo 6578501, Japan.
   <br>[Takiguchi, Tetsuya; Ariki, Yasuo] Kobe Univ, Org Adv Sci &amp; Technol,
   Nada Ku, Kobe, Hyogo 6578501, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Masaka, K (reprint author), Kobe Univ, Grad Sch Syst Informat, Nada Ku, 1-1 Rokkodai, Kobe, Hyogo 6578501, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>makka@me.cs.scitec.kobe-u.ac.jp; aihara@me.cs.scitec.kobe-u.ac.jp;
   takigu@kobe-u.ac.jp; ariki@kobe-u.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PD </td><td>SEP 4</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">AR </td><td>24</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1186/s13636-015-0067-4</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000360703400001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Lorenz, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Lorenz, Jan</td>
</tr>

<tr>
<td valign="top">TI </td><td>Counting as one Moral encounters and criteria of affinity in a Polish
   Jewish congregation</td>
</tr>

<tr>
<td valign="top">SO </td><td>HAU-JOURNAL OF ETHNOGRAPHIC THEORY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>affinity; ethics; morality; Jews; Judaism; Poland</td>
</tr>

<tr>
<td valign="top">ID </td><td>FREEDOM; ETHICS</td>
</tr>

<tr>
<td valign="top">AB </td><td>The resurgence and transformation of Poland's Jewish communal institutions and religious life in the last twenty years has inspired debate concerning the criteria for being and becoming Jewish. The voices in that discussion come not only from different generations, but also from different geographies of Jewish life. Drawing on fieldwork in a contemporary Jewish congregation in Poland, this article discusses ethics in the context of different rationalities of affinity. Poland's "Jewish revival" confronted values and affects grounded in intergenerational experiences of the post-Holocaust era with categories of belonging and religious conversion enabled by new laws, transnational programs of education and socialization, and the impact of religious leaders from abroad. The apparent incommensurability of these standpoints, and the taxing attempts at their reconciliation, invite us to reconceptualize the notion of moral tradition.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Lorenz, Jan] Univ Manchester, Dept Social Anthropol, Manchester M13
   9PL, Lancs, England.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Lorenz, J (reprint author), Univ Manchester, Sch Social Sci, Social Anthropol, Arthur Lewis Bldg,Oxford Rd, Manchester M13 9PL, Lancs, England.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jan.lorenz@manchester.ac.uk</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PD </td><td>FAL</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">VL </td><td>5</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>301</td>
</tr>

<tr>
<td valign="top">EP </td><td>323</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.14318/hau5.2.017</td>
</tr>

<tr>
<td valign="top">SC </td><td>Anthropology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000375127000017</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Pribil, J
   <br>Pribilova, A
   <br>Durackova, D</td>
</tr>

<tr>
<td valign="top">AF </td><td>Pribil, Jiri
   <br>Pribilova, Anna
   <br>Durackova, Daniela</td>
</tr>

<tr>
<td valign="top">TI </td><td>STORYTELLING VOICE CONVERSION: EVALUATION EXPERIMENT USING GAUSSIAN
   MIXTURE MODELS</td>
</tr>

<tr>
<td valign="top">SO </td><td>JOURNAL OF ELECTRICAL ENGINEERING-ELEKTROTECHNICKY CASOPIS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>storytelling voice conversion; spectral and prosodic features of speech;
   evaluation of speech quality; GMM classifier</td>
</tr>

<tr>
<td valign="top">ID </td><td>EMOTIONAL SPEECH SYNTHESIS; FEATURES; CLASSIFIER; SYSTEM; TEXT</td>
</tr>

<tr>
<td valign="top">AB </td><td>In the development of the voice conversion and personification of the text-to-speech (TTS) systems, it is very necessary to have feedback information about the users' opinion on the resulting synthetic speech quality. Therefore, the main aim of the experiments described in this paper was to find out whether the classifier based on Gaussian mixture models (GMM) could be applied for evaluation of different storytelling voices created by transformation of the sentences generated by the Czech and Slovak TTS system. We suppose that it is possible to combine this GMM-based statistical evaluation with the classical one in the form of listening tests or it can replace them. The results obtained in this way were in good correlation with the results of the conventional listening test, so they confirm practical usability of the developed GMM classifier With the help of the performed analysis, the optimal setting of the initial parameters and the structure of the input feature set for recognition of the storytelling voices was finally determined.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Pribil, Jiri] Slovak Acad Sci, Dept Imaging Methods, Inst Measurement
   Sci, Bratislava, Slovakia.
   <br>[Pribilova, Anna; Durackova, Daniela] Fac Elect Engn &amp; Informat Technol
   STU, Inst Elect &amp; Photon, SK-81219 Bratislava, Slovakia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Pribil, J (reprint author), Slovak Acad Sci, Dept Imaging Methods, Inst Measurement Sci, Bratislava, Slovakia.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jiri.pribil@savba.sk</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUL-AUG</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">VL </td><td>66</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">BP </td><td>194</td>
</tr>

<tr>
<td valign="top">EP </td><td>202</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.2478/jee-2015-0032</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000362388800002</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Rao, KS
   <br>Koolagudi, SG</td>
</tr>

<tr>
<td valign="top">AF </td><td>Rao, K. Sreenivasa
   <br>Koolagudi, Shashidhar G.</td>
</tr>

<tr>
<td valign="top">TI </td><td>Recognition of emotions from video using acoustic and facial features</td>
</tr>

<tr>
<td valign="top">SO </td><td>SIGNAL IMAGE AND VIDEO PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Emotion recognition; Autoassociative neural network (AANN); Spectral and
   prosodic features; Facial features; Acoustic features</td>
</tr>

<tr>
<td valign="top">ID </td><td>EXPRESSION ANALYSIS; VOICE CONVERSION; SPEECH; CLASSIFICATION; FACE;
   NETWORK</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, acoustic and facial features extracted from video are explored for recognizing emotions. The temporal variation of gray values of the pixels within eye and mouth regions is used as a feature to capture the emotion-specific knowledge from the facial expressions. Acoustic features representing spectral and prosodic information are explored for recognizing emotions from the speech signal. Autoassociative neural network models are used to capture the emotion-specific information from acoustic and facial features. The basic objective of this work is to examine the capability of the proposed acoustic and facial features in view of capturing the emotion-specific information. Further, the correlations among the feature sets are analyzed by combining the evidences at different levels. The performance of the emotion recognition system developed using acoustic and facial features is observed to be 85.71 and 88.14 %, respectively. It has been observed that combining the evidences of models developed using acoustic and facial features improved the recognition performance to 93.62 %. The performance of the emotion recognition systems developed using neural network models is compared with hidden Markov models, Gaussian mixture models and support vector machine models. The proposed features and models are evaluated on real-life emotional database, Interactive Emotional Dyadic Motion Capture database, which was recently collected at University of Southern California.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Rao, K. Sreenivasa] Indian Inst Technol, Sch Informat Technol,
   Kharagpur 721302, W Bengal, India.
   <br>[Koolagudi, Shashidhar G.] Natl Inst Technol Karnataka, Dept Comp Sci &amp;
   Engn, Surathkal 575025, Karnataka, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Rao, KS (reprint author), Indian Inst Technol, Sch Informat Technol, Kharagpur 721302, W Bengal, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ksrao@iitkgp.ac.in; koolagudi@nitk.ac.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>4</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>4</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUL</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">VL </td><td>9</td>
</tr>

<tr>
<td valign="top">IS </td><td>5</td>
</tr>

<tr>
<td valign="top">BP </td><td>1029</td>
</tr>

<tr>
<td valign="top">EP </td><td>1045</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/s11760-013-0522-6</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Imaging Science &amp; Photographic Technology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000355979700004</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Rus-Calafell, M
   <br>Garety, P
   <br>Ward, T
   <br>Williams, G
   <br>Huckvale, M
   <br>Leff, J
   <br>Craig, TKJ</td>
</tr>

<tr>
<td valign="top">AF </td><td>Rus-Calafell, Mar
   <br>Garety, Philippa
   <br>Ward, Tom
   <br>Williams, Geoff
   <br>Huckvale, Mark
   <br>Leff, Julian
   <br>Craig, Thomas K. J.</td>
</tr>

<tr>
<td valign="top">TI </td><td>Confronting Auditory Hallucinations Using Virtual Reality: The Avatar
   Therapy</td>
</tr>

<tr>
<td valign="top">SO </td><td>ANNUAL REVIEW OF CYBERTHERAPY AND TELEMEDICINE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Auditory hallucinations; voices; psychosis; virtual reality; avatar</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICES; SCHIZOPHRENIA; BEHAVIOR; OMNIPOTENCE; MECHANISMS; DOMINANT;
   PEOPLE; HEAR</td>
</tr>

<tr>
<td valign="top">AB </td><td>The AVATAR therapy is a computer-based intervention which aims to reduce the frequency and severity of voices. The approach is based on computer technology which enables each patient to create an avatar of the entity (human or non-human) that they believe is talking to them. The therapist promotes a dialogue between the patient and the avatar in which the avatar progressively comes under the patient's control. Using real-time voice conversion delivery software, the therapist can modify the relationship between the patient and his/her voice. The innovation of this new intervention is discussed in the present paper as well as the advantages of using a computer based system. The subjective view of the technology from a participant's point of view is also presented.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Rus-Calafell, Mar; Garety, Philippa; Ward, Tom; Craig, Thomas K. J.]
   Kings Coll London, Inst Psychiat Psychol &amp; Neurosci, London WC2R 2LS,
   England.
   <br>[Williams, Geoff; Huckvale, Mark; Leff, Julian] UCL, London, England.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Rus-Calafell, M (reprint author), Kings Coll London, Inst Psychiat Psychol &amp; Neurosci, London WC2R 2LS, England.</td>
</tr>

<tr>
<td valign="top">EM </td><td>maria.rus-calafell@kcl.ac.uk</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PD </td><td>SUM</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">VL </td><td>13</td>
</tr>

<tr>
<td valign="top">BP </td><td>190</td>
</tr>

<tr>
<td valign="top">EP </td><td>194</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000375005900035</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kameoka, H
   <br>Yoshizato, K
   <br>Ishihara, T
   <br>Kadowaki, K
   <br>Ohishi, Y
   <br>Kashino, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kameoka, Hirokazu
   <br>Yoshizato, Kota
   <br>Ishihara, Tatsuma
   <br>Kadowaki, Kento
   <br>Ohishi, Yasunori
   <br>Kashino, Kunio</td>
</tr>

<tr>
<td valign="top">TI </td><td>Generative Modeling of Voice Fundamental Frequency Contours</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Expectation-maximization algorithm; Fujisaki model; prosody; voice
   fundamental frequency contour</td>
</tr>

<tr>
<td valign="top">ID </td><td>AUTOMATIC EXTRACTION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper introduces a generative model of voice fundamental frequency (F-0) contours that allows us to extract prosodic features from raw speech data. The present F-0 contour model is formulated by translating the Fujisaki model, a well-founded mathematical model representing the control mechanism of vocal fold vibration, into a probabilistic model described as a discrete-time stochastic process. There are two motivations behind this formulation. One is to derive a general parameter estimation framework for the Fujisaki model that allows the introduction of powerful statistical methods. The other is to construct an automatically trainable version of the Fujisaki model that we can incorporate into statistical-model-based text-to-speech synthesizers in such a way that the Fujisaki-model parameters can be learned from a speech corpus in a unified manner. It could also be useful for other speech applications such as emotion recognition, speaker identification, speech conversion and dialogue systems, in which prosodic information plays a significant role. We quantitatively evaluated the performance of the proposed Fujisaki model parameter extractor using real speech data. Experimental results revealed that our method was superior to a state-of-the-art Fujisaki model parameter extractor.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kameoka, Hirokazu; Yoshizato, Kota; Ishihara, Tatsuma; Kadowaki, Kento]
   Univ Tokyo, Grad Sch Informat Sci &amp; Technol, Tokyo 1138656, Japan.
   <br>[Kameoka, Hirokazu; Ohishi, Yasunori; Kashino, Kunio] NTT Corp, NTT
   Commun Sci Labs, Tokyo 2430198, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kameoka, H (reprint author), Univ Tokyo, Grad Sch Informat Sci &amp; Technol, Tokyo 1138656, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>kameoka@hil.t.u-tokyo.ac.jp; yoshizato@hil.t.u-tokyo.ac.jp;
   ishihara@hil.t.u-tokyo.ac.jp; kadowaki@hil.t.u-tokyo.ac.jp;
   ohishi.yasunori@lab.ntt.co.jp; kashino.kunio@lab.ntt.co.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>10</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>10</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">VL </td><td>23</td>
</tr>

<tr>
<td valign="top">IS </td><td>6</td>
</tr>

<tr>
<td valign="top">BP </td><td>1042</td>
</tr>

<tr>
<td valign="top">EP </td><td>1053</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASLP.2015.2418576</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000354536200008</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Pal, M
   <br>Saha, G</td>
</tr>

<tr>
<td valign="top">AF </td><td>Pal, Monisankha
   <br>Saha, Goutam</td>
</tr>

<tr>
<td valign="top">TI </td><td>On robustness of speech based biometric systems against voice conversion
   attack</td>
</tr>

<tr>
<td valign="top">SO </td><td>APPLIED SOFT COMPUTING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speaker identification; Speaker verification; Voice conversion; GMM;
   WFW; SVM</td>
</tr>

<tr>
<td valign="top">ID </td><td>AUTOMATIC SPEAKER RECOGNITION; SUPPORT VECTOR MACHINES;
   MAXIMUM-LIKELIHOOD; IDENTIFICATION; VERIFICATION; TRANSFORMATION;
   ALGORITHM; MODELS; DESIGN; GMM</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion (VC) approach, which morphs the voice of a source speaker to be perceived as spoken by a specified target speaker, can be intentionally used to deceive the speaker identification (SID) and speaker verification (SV) systems that use speech biometric. Voice conversion spoofing attacks to imitate a particular speaker pose potential threat to these kinds of systems. In this paper, we first present an experimental study to evaluate the robustness of such systems against voice conversion disguise. We use Gaussian mixture model (GMM) based SID systems, GMM with universal background model (GMM-UBM) based SV systems and GMM supervector with support vector machine (GMM-SVM) based SV systems for this. Voice conversion is conducted by using three different techniques: GMM based VC technique, weighted frequency warping (WFW) based conversion method and its variation, where energy correction is disabled (WFW-). Evaluation is done by using intra-gender and cross-gender voice conversions between fifty male and fifty female speakers taken from TIMIT database. The result is indicated by degradation in the percentage of correct identification (POC) score in SID systems and degradation in equal error rate (EER) in all SV systems. Experimental results show that the GMM-SVM SV systems are more resilient against voice conversion spoofing attacks than GMM-UBM SV systems and all SID and SV systems are most vulnerable towards GMM based conversion than WFW and WFW- based conversion. From the results, it can also be said that, in general terms, all SID and SV systems are slightly more robust to voices converted through cross-gender conversion than intra-gender conversion. This work extended the study to find out the relationship between VC objective score and SV system performance in CMU ARCTIC database, which is a parallel corpus. The results of this experiment show an approach on quantifying objective score of voice conversion that can be related to the ability to spoof an SV system. (C) 2015 Elsevier B.V. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Pal, Monisankha; Saha, Goutam] Indian Inst Technol, Dept Elect &amp; Elect
   Commun Engn, Kharagpur 721302, W Bengal, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Pal, M (reprint author), Indian Inst Technol, Dept Elect &amp; Elect Commun Engn, Kharagpur 721302, W Bengal, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>monisankha.pal@gmail.com; gsaha@ece.iitkgp.ernet.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>12</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>12</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAY</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">VL </td><td>30</td>
</tr>

<tr>
<td valign="top">BP </td><td>214</td>
</tr>

<tr>
<td valign="top">EP </td><td>228</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.asoc.2015.01.036</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000351296200019</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sanchez, J
   <br>Saratxaga, I
   <br>Hernaez, I
   <br>Navas, E
   <br>Erro, D
   <br>Raitio, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sanchez, Jon
   <br>Saratxaga, Ibon
   <br>Hernaez, Inma
   <br>Navas, Eva
   <br>Erro, Daniel
   <br>Raitio, Tuomo</td>
</tr>

<tr>
<td valign="top">TI </td><td>Toward a Universal Synthetic Speech Spoofing Detection Using Phase
   Information</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>BIO-MODA-VOI; voice biometrics; anti-spoofing; phase information;
   synthetic speech detection</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION; SPEAKER; MODELS</td>
</tr>

<tr>
<td valign="top">AB </td><td>In the field of speaker verification (SV) it is nowadays feasible and relatively easy to create a synthetic voice to deceive a speech driven biometric access system. This paper presents a synthetic speech detector that can be connected at the front-end or at the back-end of a standard SV system, and that will protect it from spoofing attacks coming from state-of-the-art statistical Text to Speech (TTS) systems. The system described is a Gaussian Mixture Model (GMM) based binary classifier that uses natural and copy-synthesized signals obtained from the Wall Street Journal database to train the system models. Three different state-of-the-art vocoders are chosen and modeled using two sets of acoustic parameters: 1) relative phase shift and 2) canonical Mel Frequency Cepstral Coefficients (MFCC) parameters, as baseline. The vocoder dependency of the system and multivocoder modeling features are thoroughly studied. Additional phase-aware vocoders are also tested. Several experiments are carried out, showing that the phase-based parameters perform better and are able to cope with new unknown attacks. The final evaluations, testing synthetic TTS signals obtained from the Blizzard challenge, validate our proposal.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Sanchez, Jon; Saratxaga, Ibon; Hernaez, Inma; Navas, Eva; Erro, Daniel]
   Univ Basque Country, Aholab Signal Proc Lab, Bilbao 48013, Spain.
   <br>[Erro, Daniel] Ikerbasque, Bilbao 48013, Spain.
   <br>[Raitio, Tuomo] Aalto Univ, Dept Signal Proc &amp; Acoust, Helsinki 00076,
   Finland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sanchez, J (reprint author), Univ Basque Country, Aholab Signal Proc Lab, Bilbao 48013, Spain.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ion@aholab.ehu.es; ibon@aholab.ehu.es; inma@aholab.ehu.es;
   eva@aholab.ehu.es; derro@aholab.ehu.es; tuomo.raitio@aalto.fi</td>
</tr>

<tr xmlns:date="http://exslt.org/dates-and-times">
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Hernaez Rioja, Inmaculada</display_name>&nbsp;</font></td><td><font size="3">K-8303-2012&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Saratxaga, Ibon</display_name>&nbsp;</font></td><td><font size="3">H-6423-2015&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Erro, Daniel</display_name>&nbsp;</font></td><td><font size="3">H-7043-2015&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Sanchez, Jon</display_name>&nbsp;</font></td><td><font size="3">H-6882-2015&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Navas, Eva</display_name>&nbsp;</font></td><td><font size="3">H-4317-2013&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Hernaez Rioja, Inmaculada</display_name>&nbsp;</font></td><td><font size="3">0000-0003-4447-7575&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Saratxaga, Ibon</display_name>&nbsp;</font></td><td><font size="3">0000-0001-7282-2765&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Erro, Daniel</display_name>&nbsp;</font></td><td><font size="3">0000-0003-0954-6942&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Sanchez, Jon</display_name>&nbsp;</font></td><td><font size="3">0000-0002-7236-4474&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Navas, Eva</display_name>&nbsp;</font></td><td><font size="3">0000-0003-3804-4984&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>28</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>28</td>
</tr>

<tr>
<td valign="top">PD </td><td>APR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">VL </td><td>10</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">BP </td><td>810</td>
</tr>

<tr>
<td valign="top">EP </td><td>820</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TIFS.2015.2398812</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000351753400004</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sizov, A
   <br>Khoury, E
   <br>Kinnunen, T
   <br>Wu, ZZ
   <br>Marcel, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sizov, Aleksandr
   <br>Khoury, Elie
   <br>Kinnunen, Tomi
   <br>Wu, Zhizheng
   <br>Marcel, Sebastien</td>
</tr>

<tr>
<td valign="top">TI </td><td>Joint Speaker Verification and Antispoofing in the i-Vector Space</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speaker recognition; spoofing; voice conversion attack; i-vector; joint
   verification and anti-spoofing</td>
</tr>

<tr>
<td valign="top">ID </td><td>SYNTHETIC SPEECH; SECURITY; RECOGNITION; BIOMETRICS; FUSION; MODELS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Any biometric recognizer is vulnerable to spoofing attacks and hence voice biometric, also called automatic speaker verification (ASV), is no exception; replay, synthesis, and conversion attacks all provoke false acceptances unless countermeasures are used. We focus on voice conversion (VC) attacks considered as one of the most challenging for modern recognition systems. To detect spoofing, most existing countermeasures assume explicit or implicit knowledge of a particular VC system and focus on designing discriminative features. In this paper, we explore back-end generative models for more generalized countermeasures. In particular, we model synthesis-channel subspace to perform speaker verification and antispoofing jointly in the i-vector space, which is a well-established technique for speaker modeling. It enables us to integrate speaker verification and antispoofing tasks into one system without any fusion techniques. To validate the proposed approach, we study vocoder-matched and vocoder-mismatched ASV and VC spoofing detection on the NIST 2006 speaker recognition evaluation data set. Promising results are obtained for standalone countermeasures as well as their combination with ASV systems using score fusion and joint approach.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Sizov, Aleksandr; Kinnunen, Tomi] Univ Eastern Finland, Sch Comp,
   Joensuu 80130, Finland.
   <br>[Khoury, Elie; Marcel, Sebastien] Idiap Res Inst, CH-1920 Martigny,
   Switzerland.
   <br>[Wu, Zhizheng] Univ Edinburgh, Ctr Speech Technol Res, Edinburgh EH8
   9YL, Midlothian, Scotland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sizov, A (reprint author), Univ Eastern Finland, Sch Comp, Joensuu 80130, Finland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>sizov.ay@yandex.ru; elie.khoury@idiap.ch; tomi.kinnunen@uef.fi;
   zhizheng.wu@ed.ac.uk; marcel@idiap.ch</td>
</tr>

<tr>
<td valign="top">TC </td><td>26</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>27</td>
</tr>

<tr>
<td valign="top">PD </td><td>APR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">VL </td><td>10</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">BP </td><td>821</td>
</tr>

<tr>
<td valign="top">EP </td><td>832</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TIFS.2015.2407362</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000351753400005</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Gu, HY
   <br>Tsai, SF</td>
</tr>

<tr>
<td valign="top">AF </td><td>Gu, Hung-Yan
   <br>Tsai, Sung-Feng</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Conversion Method Combining Segmental GMM Mapping with Target
   Frame Selection</td>
</tr>

<tr>
<td valign="top">SO </td><td>JOURNAL OF INFORMATION SCIENCE AND ENGINEERING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; Gaussian mixture model; frame selection; discrete
   cepstrum coefficients; dynamic programming</td>
</tr>

<tr>
<td valign="top">ID </td><td>MAXIMUM-LIKELIHOOD; SPEECH SYNTHESIS; ALGORITHM</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, a voice conversion approach that combines two distinct ideas is proposed to improve the converted-voice quality. The first idea is to map spectral features, e.g. discrete cepstrum coefficients (DCC), with segmental Gaussian mixture models (GMMs). That is, a single GMM of a large number of mixture components is replaced here with several voice-content specific GMMs each consisting of much fewer mixture components. In addition, the second idea is to find a frame, of spectral features near to the mapped feature vector, from the target-speaker frame pool corresponding to the segment class as the input frame belongs to. Both ideas are intended to alleviate the problem encountered by a traditional GMM based conversion method, i.e. converted spectral envelopes are usually over smoothed. To apply the first idea to implement an on-line voice conversion system, we have proposed an automatic GMM selection algorithm based on dynamic programming (DP). Furthermore, as pointed out by previous researchers, mapping with a single selected Gaussian probability density function (PDF) instead of a combination of several Gaussian PDFs is helpful to obtain better converted-voice quality. Therefore, we have also proposed a Gaussian PDF selection algorithm and integrated it into our system. As to the implementation of the second idea, an algorithm based on DP is adopted which will consider both frame matching and connecting distances. For evaluating the performance of the two ideas studied here, three voice conversion systems are constructed, and used to conduct listening tests. The results of the tests show that the system with the two ideas combined can indeed obtain much improved voice quality besides improvement in timbre similarity.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Gu, Hung-Yan; Tsai, Sung-Feng] Natl Taiwan Univ Sci &amp; Technol, Dept
   Comp Sci &amp; Informat Engn, Taipei 106, Taiwan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Gu, HY (reprint author), Natl Taiwan Univ Sci &amp; Technol, Dept Comp Sci &amp; Informat Engn, Taipei 106, Taiwan.</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">VL </td><td>31</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>609</td>
</tr>

<tr>
<td valign="top">EP </td><td>626</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000351401300014</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nakashika, T
   <br>Takiguchi, T
   <br>Ariki, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nakashika, Toru
   <br>Takiguchi, Tetsuya
   <br>Ariki, Yasuo</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Conversion Using RNN Pre-Trained by Recurrent Temporal Restricted
   Boltzmann Machines</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Deep Learning; recurrent neural network; recurrent temporal restricted
   Boltzmann machine (RTRBM); speaker specific features; voice conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH RECOGNITION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a voice conversion (VC) method that utilizes the recently proposed probabilistic models called recurrent temporal restricted Boltzmann machines (RTRBMs). One RTRBM is used for each speaker, with the goal of capturing high-order temporal dependencies in an acoustic sequence. Our algorithm starts from the separate training of one RTRBM for a source speaker and another for a target speaker using speaker-dependent training data. Because each RTRBM attempts to discover abstractions to maximally express the training data at each time step, as well as the temporal dependencies in the training data, we expect that the models represent the linguistic-related latent features in high-order spaces. In our approach, we convert (match) features of emphasis for the source speaker to those of the target speaker using a neural network (NN), so that the entire network (consisting of the two RTRBMs and the NN) acts as a deep recurrent NN and can be fine-tuned. Using VC experiments, we confirm the high performance of our method, especially in terms of objective criteria, relative to conventional VC methods such as approaches based on Gaussian mixture models and on NNs.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nakashika, Toru] Kobe Univ, Grad Sch Syst Informat, Kobe, Hyogo
   6578501, Japan.
   <br>[Takiguchi, Tetsuya; Ariki, Yasuo] Kobe Univ, Org Adv Sci &amp; Technol,
   Kobe, Hyogo 6578501, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nakashika, T (reprint author), Kobe Univ, Grad Sch Syst Informat, Kobe, Hyogo 6578501, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>nakashika@me.cs.scitec.kobe-u.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>22</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>23</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">VL </td><td>23</td>
</tr>

<tr>
<td valign="top">IS </td><td>3</td>
</tr>

<tr>
<td valign="top">BP </td><td>580</td>
</tr>

<tr>
<td valign="top">EP </td><td>587</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASLP.2014.2379589</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000350876100015</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ghorbandoost, M
   <br>Sayadiyan, A
   <br>Ahangar, M
   <br>Sheikhzadeh, H
   <br>Shahrebabaki, AS
   <br>Amini, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ghorbandoost, Mostafa
   <br>Sayadiyan, Abolghasem
   <br>Ahangar, Mohsen
   <br>Sheikhzadeh, Hamid
   <br>Shahrebabaki, Abdoreza Sabzi
   <br>Amini, Jamal</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice conversion based on feature combination with limited training data</td>
</tr>

<tr>
<td valign="top">SO </td><td>SPEECH COMMUNICATION</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; Feature combination; Gaussian mixture models (GMM);
   Dynamic kernel partial least square regression (DKPLS)</td>
</tr>

<tr>
<td valign="top">ID </td><td>MAXIMUM-LIKELIHOOD-ESTIMATION; LEAST-SQUARES REGRESSION; SPEECH
   SYNTHESIS; PARAMETER; HMMS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Typically, voice conversion systems just use one type of spectral feature to convert acoustical characteristics of one speaker to another speaker. In this paper, we first study four different spectral features. Then, we compare these features and choose two features that perform better than others. Our experiments showed that cepstral features are more suitable than all-pole features for clustering and all-pole features are better for the analysis/synthesis stages. Hence, we propose a new voice conversion algorithm that uses both cepstral and all-pole features in order to utilize their desired properties simultaneously. We have two ideas to utilize this feature combination strategy. Our first idea is to apply feature combination to classical Gaussian mixture models (GMM)-based voice conversion method. The second idea is to apply feature combination to dynamic kernel partial least square regression (DKPLS) method. Results of our evaluations show that our proposed methods outperform the modern voice conversion methods in terms of speech quality and speaker individuality. Our methods are also robust to limited training data. (C) 2014 Elsevier B.V. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ghorbandoost, Mostafa; Sayadiyan, Abolghasem; Ahangar, Mohsen;
   Sheikhzadeh, Hamid; Shahrebabaki, Abdoreza Sabzi; Amini, Jamal]
   Amirkabir Univ Technol, Dept Elect Engn, Tehran, Iran.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ghorbandoost, M (reprint author), Amirkabir Univ Technol, Dept Elect Engn, POB 15875-4413,424 Hafez Ave, Tehran, Iran.</td>
</tr>

<tr>
<td valign="top">EM </td><td>m.ghorbandoost@aut.ac.ir; eeas35@aut.ac.ir; Ahangar@aut.ac.ir;
   hsheikh@aut.ac.ir; rezasabzi@aut.ac.ir; Jamal.amini@aut.ac.ir</td>
</tr>

<tr>
<td valign="top">TC </td><td>9</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>12</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">VL </td><td>67</td>
</tr>

<tr>
<td valign="top">BP </td><td>113</td>
</tr>

<tr>
<td valign="top">EP </td><td>128</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.specom.2014.12.004</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000350090900010</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Belanger, SD</td>
</tr>

<tr>
<td valign="top">AF </td><td>Belanger, Sarah-Daniele</td>
</tr>

<tr>
<td valign="top">TI </td><td>Writing his voice toward the sacred: Kerouac facing the border</td>
</tr>

<tr>
<td valign="top">SO </td><td>ANALYSES-REVUE DE CRITIQUE ET DE THEORIE LITTERAIRE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper intends to reflect on Kerouac's spiritual writing practice, which appears as an attempt at overcoming the mediation that normally belongs to writing (may it be of a literary nature or not). It argues that, through his writing discipline making the mind and the writing hand the slaves of a projected inner voice, his writing impulse and ethics essentially aim for conversion.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Belanger, Sarah-Daniele] Univ Toronto, Toronto, ON M5S 1A1, Canada.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Belanger, SD (reprint author), Univ Toronto, Toronto, ON M5S 1A1, Canada.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>SPR-SUM</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">VL </td><td>10</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>276</td>
</tr>

<tr>
<td valign="top">EP </td><td>288</td>
</tr>

<tr>
<td valign="top">SC </td><td>Literature</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000384715500013</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Erro, D
   <br>Alonso, A
   <br>Serrano, L
   <br>Navas, E
   <br>Hernaez, I</td>
</tr>

<tr>
<td valign="top">AF </td><td>Erro, Daniel
   <br>Alonso, Agustin
   <br>Serrano, Luis
   <br>Navas, Eva
   <br>Hernaez, Inma</td>
</tr>

<tr>
<td valign="top">TI </td><td>Interpretable parametric voice conversion functions based on Gaussian
   mixture models and constrained transformations</td>
</tr>

<tr>
<td valign="top">SO </td><td>COMPUTER SPEECH AND LANGUAGE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; Gaussian mixture models; Frequency warping; Amplitude
   scaling; Spectral tilt</td>
</tr>

<tr>
<td valign="top">ID </td><td>ARTIFICIAL NEURAL-NETWORKS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion functions based on Gaussian mixture models and parametric speech signal representations are opaque in the sense that it is not straightforward to interpret the physical meaning of the conversion parameters. Following the line of recent works based on the frequency warping plus amplitude scaling paradigm, in this article we show that voice conversion functions can be designed according to physically meaningful constraints in such manner that they become highly informative. The resulting voice conversion method can be used to visualize the differences between source and target voices or styles in terms of formant location in frequency, spectral tilt and amplitude in a number of spectral bands. (C) 2014 Elsevier Ltd. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Erro, Daniel; Alonso, Agustin; Serrano, Luis; Navas, Eva; Hernaez,
   Inma] Univ Basque Country, Aholab, Bilbao, Spain.
   <br>[Erro, Daniel] Ikerbasque, Basque Fdn Sci, Bilbao, Spain.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Erro, D (reprint author), Univ Basque Country, Aholab, Bilbao, Spain.</td>
</tr>

<tr>
<td valign="top">EM </td><td>derro@aholab.ehu.es</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Erro, Daniel</display_name>&nbsp;</font></td><td><font size="3">H-7043-2015&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Hernaez Rioja, Inmaculada</display_name>&nbsp;</font></td><td><font size="3">K-8303-2012&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Serrano-Macin, Luis Felipe</display_name>&nbsp;</font></td><td><font size="3">M-1157-2015&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Navas, Eva</display_name>&nbsp;</font></td><td><font size="3">H-4317-2013&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Erro, Daniel</display_name>&nbsp;</font></td><td><font size="3">0000-0003-0954-6942&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Hernaez Rioja, Inmaculada</display_name>&nbsp;</font></td><td><font size="3">0000-0003-4447-7575&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Serrano-Macin, Luis Felipe</display_name>&nbsp;</font></td><td><font size="3">0000-0003-0648-1012&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Navas, Eva</display_name>&nbsp;</font></td><td><font size="3">0000-0003-3804-4984&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>8</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>9</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">VL </td><td>30</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>3</td>
</tr>

<tr>
<td valign="top">EP </td><td>15</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.csl.2014.03.001</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000345556600002</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nakashika, T
   <br>Takiguchi, T
   <br>Ariki, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nakashika, Toru
   <br>Takiguchi, Tetsuya
   <br>Ariki, Yasuo</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice conversion using speaker-dependent conditional restricted
   Boltzmann machine</td>
</tr>

<tr>
<td valign="top">SO </td><td>EURASIP JOURNAL ON AUDIO SPEECH AND MUSIC PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; Conditional restricted Boltzmann machine; Deep
   learning; Recurrent neural network; Speaker-specific features</td>
</tr>

<tr>
<td valign="top">ID </td><td>ARTIFICIAL NEURAL-NETWORKS; SPEECH RECOGNITION; LEARNING ALGORITHM;
   TRANSFORMATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a voice conversion (VC) method that utilizes conditional restricted Boltzmann machines (CRBMs) for each speaker to obtain high-order speaker-independent spaces where voice features are converted more easily than those in an original acoustic feature space. The CRBM is expected to automatically discover common features lurking in time-series data. When we train two CRBMs for a source and target speaker independently using only speaker-dependent training data, it can be considered that each CRBM tries to construct subspaces where there are fewer phonemes and relatively more speaker individuality than the original acoustic space because the training data include various phonemes while keeping the speaker individuality unchanged. Each obtained high-order feature is then concatenated using a neural network (NN) from the source to the target. The entire network (the two CRBMs and the NN) can be also fine-tuned as a recurrent neural network (RNN) using the acoustic parallel data since both the CRBMs and the concatenating NN have network-based representation with time dependencies. Through voice-conversion experiments, we confirmed the high performance of our method especially in terms of objective evaluation, comparing it with conventional GMM, NN, RNN, and our previous work, speaker-dependent DBN approaches.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nakashika, Toru] Kobe Univ, Grad Sch Syst Informat, Nada Ku, Kobe,
   Hyogo 6578501, Japan.
   <br>[Takiguchi, Tetsuya; Ariki, Yasuo] Kobe Univ, Org Adv Sci &amp; Technol,
   Nada Ku, Kobe, Hyogo 6578501, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nakashika, T (reprint author), Kobe Univ, Grad Sch Syst Informat, Nada Ku, 1-1 Rokkodai, Kobe, Hyogo 6578501, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>nakashika@me.cs.scitec.kobe-u.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>9</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>9</td>
</tr>

<tr>
<td valign="top">PD </td><td>FEB 25</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">AR </td><td>8</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1186/s13636-014-0044-3</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000350676900001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wu, ZZ
   <br>Evans, N
   <br>Kinnunen, T
   <br>Yamagishi, J
   <br>Alegre, F
   <br>Li, HZ</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wu, Zhizheng
   <br>Evans, Nicholas
   <br>Kinnunen, Tomi
   <br>Yamagishi, Junichi
   <br>Alegre, Federico
   <br>Li, Haizhou</td>
</tr>

<tr>
<td valign="top">TI </td><td>Spoofing and countermeasures for speaker verification: A survey</td>
</tr>

<tr>
<td valign="top">SO </td><td>SPEECH COMMUNICATION</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Automatic speaker verification; Spoofing attack; Anti-Spoofing;
   Countermeasure; Security</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH SYNTHESIS; NEURAL-NETWORKS; RECOGNITION; SECURITY; FUSION;
   VARIABILITY; ADAPTATION; ROBUSTNESS; BIOMETRICS</td>
</tr>

<tr>
<td valign="top">AB </td><td>While biometric authentication has advanced significantly in recent years, evidence shows the technology can be susceptible to malicious spoofing attacks. The research community has responded with dedicated countermeasures which aim to detect and deflect such attacks. Even if the literature shows that they can be effective, the problem is far from being solved; biometric systems remain vulnerable to spoofing. Despite a growing momentum to develop spoofing countermeasures for automatic speaker verification, now that the technology has matured sufficiently to support mass deployment in an array of diverse applications, greater effort will be needed in the future to ensure adequate protection against spoofing. This article provides a survey of past work and identifies priority research directions for the future. We summarise previous studies involving impersonation, replay, speech synthesis and voice conversion spoofing attacks and more recent efforts to develop dedicated countermeasures. The survey shows that future research should address the lack of standard datasets and the over-fitting of existing countermeasures to specific, known spoofing attacks. (C) 2014 Elsevier B.V. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Wu, Zhizheng; Li, Haizhou] Nanyang Technol Univ, Singapore 639798,
   Singapore.
   <br>[Evans, Nicholas; Alegre, Federico] EURECOM, Biot, France.
   <br>[Kinnunen, Tomi] Univ Eastern Finland, Espoo, Finland.
   <br>[Yamagishi, Junichi] Univ Edinburgh, Edinburgh EH8 9YL, Midlothian,
   Scotland.
   <br>[Li, Haizhou] Inst Infocomm Res, Singapore, Singapore.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Wu, ZZ (reprint author), Univ Edinburgh, Edinburgh EH8 9YL, Midlothian, Scotland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>zhizheng.wu@ed.ac.uk; evans@eurecom.fr; tkinnu@cs.uef.fi;
   jyamagis@inf.ed.ac.uk; alegre@eurecom.fr; hli@i2r.a-star.edu.sg</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Li, Haizhou</display_name>&nbsp;</font></td><td><font size="3">Q-6438-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Li, Haizhou</display_name>&nbsp;</font></td><td><font size="3">0000-0001-9158-9401&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>151</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>155</td>
</tr>

<tr>
<td valign="top">PD </td><td>FEB</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">VL </td><td>66</td>
</tr>

<tr>
<td valign="top">BP </td><td>130</td>
</tr>

<tr>
<td valign="top">EP </td><td>153</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.specom.2014.10.005</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000348261700010</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Deleforge, A
   <br>Forbes, F
   <br>Horaud, R</td>
</tr>

<tr>
<td valign="top">AF </td><td>Deleforge, Antoine
   <br>Forbes, Florence
   <br>Horaud, Radu</td>
</tr>

<tr>
<td valign="top">TI </td><td>Acoustic Space Learning for Sound-Source Separation and Localization on
   Binaural Manifolds</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERNATIONAL JOURNAL OF NEURAL SYSTEMS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Binaural hearing; sound localization; sound-source separation; manifold
   learning; mixture of regressors; EM inference</td>
</tr>

<tr>
<td valign="top">ID </td><td>AUDIO SOURCE SEPARATION; SLICED INVERSE REGRESSION; LINEAR REGRESSIONS;
   VOICE CONVERSION; MIXTURES; MODEL</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we address the problems of modeling the acoustic space generated by a full-spectrum sound source and using the learned model for the localization and separation of multiple sources that simultaneously emit sparse-spectrum sounds. We lay theoretical and methodological grounds in order to introduce the binaural manifold paradigm. We perform an in-depth study of the latent low-dimensional structure of the high-dimensional interaural spectral data, based on a corpus recorded with a human-like audiomotor robot head. A nonlinear dimensionality reduction technique is used to show that these data lie on a two-dimensional (2D) smooth manifold parameterized by the motor states of the listener, or equivalently, the sound-source directions. We propose a probabilistic piecewise affine mapping model (PPAM) specifically designed to deal with high-dimensional data exhibiting an intrinsic piecewise linear structure. We derive a closed-form expectation-maximization (EM) procedure for estimating the model parameters, followed by Bayes inversion for obtaining the full posterior density function of a sound-source direction. We extend this solution to deal with missing data and redundancy in real-world spectrograms, and hence for 2D localization of natural sound sources such as speech. We further generalize the model to the challenging case of multiple sound sources and we propose a variational EM framework. The associated algorithm, referred to as variational EM for source separation and localization (VESSL) yields a Bayesian estimation of the 2D locations and time-frequency masks of all the sources. Comparisons of the proposed approach with several existing methods reveal that the combination of acoustic-space learning with Bayesian inference enables our method to outperform state-of-the-art methods.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Deleforge, Antoine; Forbes, Florence; Horaud, Radu] INRIA Grenoble
   Rhone Alpes, F-38334 Saint Ismier, France.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Horaud, R (reprint author), INRIA Grenoble Rhone Alpes, 655 Ave Europe, F-38334 Saint Ismier, France.</td>
</tr>

<tr>
<td valign="top">EM </td><td>antoine.deleforge@inria.fr; florence.forbes@inria.fr;
   radu.horaud@inria.fr</td>
</tr>

<tr>
<td valign="top">TC </td><td>30</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>30</td>
</tr>

<tr>
<td valign="top">PD </td><td>FEB</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">VL </td><td>25</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">AR </td><td>1440003</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1142/S0129065714400036</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000347965500001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Uddin, MA
   <br>Sakib, N
   <br>Rupu, EF
   <br>Hossain, MA
   <br>Huda, MN</td>
</tr>

<tr>
<td valign="top">AF </td><td>Uddin, Mir Ashraf
   <br>Sakib, Nazmus
   <br>Rupu, Esrat Farjana
   <br>Hossain, Md. Afzal
   <br>Huda, Md. Nurul</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Phoneme based Bangla Text to Speech Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>2015 18th International Conference on Computer and Information
   Technology (ICCIT)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>18th International Conference on Computer and Information Technology
   (ICCIT)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 21-23, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Dhaka, BANGLADESH</td>
</tr>

<tr>
<td valign="top">DE </td><td>Text normalization; Speech synthesis; Phoneme</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a phoneme based Bangla Text to Speech (TTS) Synthesis framework which uses a new approach for recording Bangla phoneme to improve the speech quality. Main objective of our method is to produce more natural speech sound during Bangla text to speech conversion. In this approach, the size of the dictionary remains small, but produces more smooth and natural sounds than any other phoneme, syllable or diphone based approaches. In the proposed framework, the voice sound of each Bangla alphabet is recorded. Afterwards, the recorded voice sound is separated into their constituent phonemes by a voice cutter. This dataset can be used for future conversion of input text into its corresponding natural sounding speech from a sequence of phonemes. Before generating the phonemes sequence for each word, we normalized the text by considering numbers, abbreviations, acronyms, currency, dates and URLs. Our implemented system supports UNICODE input for Bangla text.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Uddin, Mir Ashraf; Sakib, Nazmus; Rupu, Esrat Farjana; Hossain, Md.
   Afzal; Huda, Md. Nurul] Mil Inst Sci &amp; Technol MIST, Dhaka 1216,
   Bangladesh.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Uddin, MA (reprint author), Mil Inst Sci &amp; Technol MIST, Dhaka 1216, Bangladesh.</td>
</tr>

<tr>
<td valign="top">EM </td><td>mirashrafuddion@yahoo.com; nazmus20000@gmail.com; esrat0167@gmail.com;
   head@cse.mist.ac.bd; mnh@cse.uiu.ac.bd</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>531</td>
</tr>

<tr>
<td valign="top">EP </td><td>533</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000389308000097</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Dong, JM</td>
</tr>

<tr>
<td valign="top">AF </td><td>Dong, Jianming</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Design and Achieve the Chinese Voice Learning System based on
   Intelligent Computer</td>
</tr>

<tr>
<td valign="top">SO </td><td>2015 INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND
   COMMUNICATION NETWORKS (CICN)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Confernce on Computational Intelligence and Communication
   Networks</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>7th International Conference on Computational Intelligence and
   Communication Networks (CICN)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 12-14, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Jabalpur, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>computer; speech synthesis system; text-to-speech conversion; chinese
   voice</td>
</tr>

<tr>
<td valign="top">AB </td><td>with the development of the information society and computer science, voice information service is extensively used, which needs the support of speech synthesis system. Multilingual speech synthesis system has become a hot research topic. The development state and the future prospect for computer speech synthesis system are introduced and discussed. Application samples of speech synthesis are enumerated and analyzed.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Dong, Jianming] Northeast Dianli Univ, Sch Foreign Languages, Jilin,
   Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Dong, JM (reprint author), Northeast Dianli Univ, Sch Foreign Languages, Jilin, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>Jingmd01@126.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>617</td>
</tr>

<tr>
<td valign="top">EP </td><td>621</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/CICN.2015.126</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000387128200127</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Shiota, S
   <br>Villavicencio, F
   <br>Yamagishi, J
   <br>Ono, N
   <br>Echizen, I
   <br>Matsui, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Shiota, Sayaka
   <br>Villavicencio, Fernando
   <br>Yamagishi, Junichi
   <br>Ono, Nobutaka
   <br>Echizen, Isao
   <br>Matsui, Tomoko</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INT SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice liveness detection algorithms based on pop noise caused by human
   breath for automatic speaker verification</td>
</tr>

<tr>
<td valign="top">SO </td><td>16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2015), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>16th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2015)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 06-10, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Dresden, GERMANY</td>
</tr>

<tr>
<td valign="top">DE </td><td>automatic speaker verification; voice liveness detection; anti-spoofing;
   countermeasure; pop noise</td>
</tr>

<tr>
<td valign="top">ID </td><td>SECURITY; MODELS; FACE</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper proposes a novel countermeasure framework to detect spoofing attacks to reduce the vulnerability of automatic speaker verification (ASV) systems. Recently, ASV systems have reached equivalent performances equivalent to those of other biometric modalities. However, spoofing techniques against these systems have also progressed drastically. Experimentation using advanced speech synthesis and voice conversion techniques has showed unacceptable false acceptance rates and several new countermeasure algorithms have been explored to detect spoofing materials accurately. However, the countermeasures proposed so far are based on the acoustic differences between natural speech signals and artificial speech signals, expected to become gradually smaller in the near future. In this paper, we focus on voice liveness detection, which aims to validate whether the presented speech signals originated from a live human. We use the phenomenon of pop noise, which is a distortion that happens when human breath reaches a microphone, as liveness evidence. This paper proposes pop noise detection algorithms and shows through an experimental study that they can be used to discriminate live voice signals from artificial ones generated by means of speech synthesis techniques.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Shiota, Sayaka] Tokyo Metropolitan Univ, Tokyo 1910065, Japan.
   <br>[Villavicencio, Fernando; Yamagishi, Junichi; Ono, Nobutaka; Echizen,
   Isao] Natl Inst Informat, Chiyoda Ku, Tokyo 1018430, Japan.
   <br>[Matsui, Tomoko] Inst Stat &amp; Math, Tachikawa, Tokyo 1908562, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Shiota, S (reprint author), Tokyo Metropolitan Univ, Tokyo 1910065, Japan.</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Shiota, Sayaka</display_name>&nbsp;</font></td><td><font size="3">P-9668-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Ono, Nobutaka</display_name>&nbsp;</font></td><td><font size="3">0000-0003-4242-2773&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>12</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>12</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>239</td>
</tr>

<tr>
<td valign="top">EP </td><td>243</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380581600049</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Mohammadi, SH
   <br>Kain, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Mohammadi, Seyed Hamidreza
   <br>Kain, Alexander</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INT SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Semi-supervised Training of a Voice Conversion Mapping Function using a
   Joint-Autoencoder</td>
</tr>

<tr>
<td valign="top">SO </td><td>16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2015), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>16th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2015)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 06-10, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Dresden, GERMANY</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; deep neural network; semi-supervised learning;
   pre-training</td>
</tr>

<tr>
<td valign="top">ID </td><td>NEURAL-NETWORKS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Recently, researchers have begun to investigate Deep Neural Network (DNN) architectures as mapping functions in voice conversion systems. In this study, we propose a novel Stacked-Joint-Autoencoder (SJAE) architecture, which aims to find a common encoding of parallel source and target features. The SJAE is initialized from a Stacked-Autoencoder (SAE) that has been trained on a large general-purpose speech database. We also propose to train the SJAE using unrelated speakers that are similar to the source and target speaker, instead of using only the source and target speakers. The final DNN is constructed from the source-encoding part and the target-decoding part of the SJAE, and then fine-tuned using back-propagation. The use of this semi-supervised training approach allows us to use multiple frames during mapping, since we have previously learned the general structure of the acoustic space and also the general structure of similar source-target speaker mappings. We train two speaker conversions and compare several system configurations objectively and subjectively while varying the number of available training sentences. The results show that each of the individual contributions of SAE, SJAE, and using unrelated speakers to initialize the mapping function increases conversion performance.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Mohammadi, Seyed Hamidreza; Kain, Alexander] Oregon Hlth &amp; Sci Univ,
   Ctr Spoken Language Understanding, Portland, OR 97201 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Mohammadi, SH (reprint author), Oregon Hlth &amp; Sci Univ, Ctr Spoken Language Understanding, Portland, OR 97201 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>mohammah@ohsu.edu; kaina@ohsu.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>284</td>
</tr>

<tr>
<td valign="top">EP </td><td>288</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380581600058</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Huber, S
   <br>Roebel, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Huber, Stefan
   <br>Roebel, Axel</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INT SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>On glottal source shape parameter transformation using a novel
   deterministic and stochastic speech analysis and synthesis system</td>
</tr>

<tr>
<td valign="top">SO </td><td>16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2015), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>16th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2015)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 06-10, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Dresden, GERMANY</td>
</tr>

<tr>
<td valign="top">DE </td><td>Parametric speech analysis / synthesis; Glottal source; Voice quality;
   LF model; R-d shape parameter</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE SOURCE; DECOMPOSITION; MODEL</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper we present a flexible deterministic plus stochastic model (DSM) approach for parametric speech analysis and synthesis with high quality. The novelty of the proposed speech processing system lies in its extended means to estimate the unvoiced stochastic component and to robustly handle the transformation of the glottal excitation source. It is therefore well suited as speech system within the context of Voice Transformation and Voice Conversion. The system is evaluated in the context of a voice quality transformation on natural human speech. The voice quality of a speech phrase is altered by means of re synthesizing the deterministic component with different pulse shapes of the glottal excitation source. A subjective listening test suggests that the speech processing system is able to successfully synthesize and arise to a listener the perceptual sensation of different voice quality characteristics. Additionally, improvements of the speech synthesis quality compared to a baseline method are demonstrated.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Huber, Stefan; Roebel, Axel] IRCAM CNRS UPMC STMS, Sound Anal Synth
   Team, F-75004 Paris, France.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Huber, S (reprint author), IRCAM CNRS UPMC STMS, Sound Anal Synth Team, F-75004 Paris, France.</td>
</tr>

<tr>
<td valign="top">EM </td><td>stefan.huber@ircam.fr; axel.roebel@ircam.fr</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>289</td>
</tr>

<tr>
<td valign="top">EP </td><td>293</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380581600059</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Toman, M
   <br>Pucher, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Toman, Markus
   <br>Pucher, Michael</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INT SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Evaluation of state mapping based foreign accent conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2015), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>16th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2015)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 06-10, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Dresden, GERMANY</td>
</tr>

<tr>
<td valign="top">DE </td><td>speech synthesis; accent perception; foreign accent conversion</td>
</tr>

<tr>
<td valign="top">AB </td><td>We present an evaluation of the perception of foreign-accented natural and synthetic speech in comparison to accent-reduced synthetic speech. Our method for foreign accent conversion is based on mapping of Hidden Semi-Markov Model states between accented and non-accented voice models and does not need an average voice model of accented speech. We employ the method on recorded data of speakers with first language (LI) from different European countries and second language (L2) being Austrian German. Results from a subjective evaluation show that the proposed method is able to significantly reduce the perceived accent. It also retains speaker similarity when an average voice model of the same gender is used. Accentedness of synthetic speech was rated significantly lower than natural speech by the participants and listeners were unable to identify accents correctly for 81% of the natural and 85% of the synthesized samples. Our evaluation shows the feasibility of accent conversion with a limited amount of speech resources.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Toman, Markus; Pucher, Michael] FTW Telecommun Res Ctr Vienna, Vienna,
   Austria.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Toman, M (reprint author), FTW Telecommun Res Ctr Vienna, Vienna, Austria.</td>
</tr>

<tr>
<td valign="top">EM </td><td>toman@ftw.at; pucher@ftw.at</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>304</td>
</tr>

<tr>
<td valign="top">EP </td><td>308</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380581600062</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Uchida, H
   <br>Saito, D
   <br>Minematsu, N
   <br>Hirose, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Uchida, Hidetsugu
   <br>Saito, Daisuke
   <br>Minematsu, Nobuaki
   <br>Hirose, Keikichi</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INT SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Statistical Acoustic-to-Articulatory Mapping Unified with Speaker
   Normalization Based on Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2015), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>16th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2015)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 06-10, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Dresden, GERMANY</td>
</tr>

<tr>
<td valign="top">DE </td><td>acoustic-to-articulatory mapping; Gaussian mixture model; voice
   conversion; speaker normalization</td>
</tr>

<tr>
<td valign="top">ID </td><td>MOVEMENTS; MODEL</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper proposes a model of speaker-normalized acoustic-to articulatory mapping using statistical voice conversion. A mapping function from acoustic parameters to articulatory parameters is usually developed with a single speaker's parallel data. Hence the constructed mapping model can work appropriately only for this specific speaker, and applying this model to other speakers degrades the performance of acoustic-to-articulatory mapping. In this paper, two models of speaker conversion and acoustic-to-articulatory mapping are implemented using Gaussian Mixture Models (GMM), and by integrating these two models, we propose two methods of speaker-normalized acoustic-to-articulatory mapping. One is concatenating these models sequentially, and the other integrates the two models into a unified model, where acoustic parameters of a speaker can be converted directly to articulatory parameters of another speaker. Experiments show that both methods can improve the mapping accuracy and that the latter method works better than the former method. Especially in the case of velar stop consonants, the mapping accuracy is higher by 0.6 mm.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Uchida, Hidetsugu; Saito, Daisuke; Minematsu, Nobuaki; Hirose,
   Keikichi] Univ Tokyo, Bunkyo Ku, 7-3-1 Hongo, Tokyo 1138656, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Uchida, H (reprint author), Univ Tokyo, Bunkyo Ku, 7-3-1 Hongo, Tokyo 1138656, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>uchida@gavo.t.u-tokyo.ac.jp; dsk_saito@gavo.t.u-tokyo.ac.jp;
   mine@gavo.t.u-tokyo.ac.jp; hirose@gavo.t.u-tokyo.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>588</td>
</tr>

<tr>
<td valign="top">EP </td><td>592</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380581600119</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Liberatore, C
   <br>Aryal, S
   <br>Wang, ZL
   <br>Polsley, S
   <br>Gutierrez-Osuna, R</td>
</tr>

<tr>
<td valign="top">AF </td><td>Liberatore, Christopher
   <br>Aryal, Sandesh
   <br>Wang, Zelun
   <br>Polsley, Seth
   <br>Gutierrez-Osuna, Ricardo</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INT SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>SABR: Sparse, Anchor-Based Representation of the Speech Signal</td>
</tr>

<tr>
<td valign="top">SO </td><td>16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2015), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>16th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2015)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 06-10, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Dresden, GERMANY</td>
</tr>

<tr>
<td valign="top">DE </td><td>speech analysis; voice conversion; speaker independent representation;
   auditory phonetics; sparse coding</td>
</tr>

<tr>
<td valign="top">ID </td><td>LINEAR-REGRESSION; LEAST-SQUARES</td>
</tr>

<tr>
<td valign="top">AB </td><td>We present SABR (Sparse, Anchor-Based Representation), an analysis technique to decompose the speech signal into speaker-dependent and speaker-independent components. Given a collection of utterances for a particular speaker, SABR uses the centroid for each phoneme as an acoustic "anchor," then applies Lasso regularization to represent each speech frame as a sparse non-negative combination of the anchors. We illustrate the performance of the method on a speaker-independent phoneme recognition task and a voice conversion task. Using a linear classifier, SABR weights achieve significantly higher phoneme recognition rates than Mel frequency Cepstral coefficients. SABR weights can also be used directly to perform accent conversion without the need to train a speaker-to-speaker regression model.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Liberatore, Christopher; Aryal, Sandesh; Wang, Zelun; Polsley, Seth;
   Gutierrez-Osuna, Ricardo] Texas A&amp;M Univ, Dept Comp Sci &amp; Engn, College
   Stn, TX 77843 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Liberatore, C (reprint author), Texas A&amp;M Univ, Dept Comp Sci &amp; Engn, College Stn, TX 77843 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>cliberatore@tamu.edu; saryal@tamu.edu; wang14359@tamu.edu;
   spolsley@tamu.edu; rgutier@tamu.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>608</td>
</tr>

<tr>
<td valign="top">EP </td><td>612</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380581600123</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wester, M
   <br>Wu, ZZ
   <br>Yamagishi, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wester, Mirjam
   <br>Wu, Zhizheng
   <br>Yamagishi, Junichi</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INT SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Human vs Machine Spoofing Detection on Wideband and Narrowband Data</td>
</tr>

<tr>
<td valign="top">SO </td><td>16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2015), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>16th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2015)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 06-10, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Dresden, GERMANY</td>
</tr>

<tr>
<td valign="top">DE </td><td>spoofing; human performance; automatic spoofing detection</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEAKER VERIFICATION; NONNATIVE LISTENERS; RECOGNITION</td>
</tr>

<tr>
<td valign="top">AB </td><td>How well do humans detect spoofing attacks directed at automatic speaker verification systems? This paper investigates the performance of humans at detecting spoofing attacks from speech synthesis and voice conversion systems. Two speaker verification tasks, in which the speakers were either humans or machines, were also conducted. The three tasks were carried out with two types of data: wideband (16kHz) and narrowband (8kHz) telephone line simulated data. Spoofing detection by humans was compared to automatic spoofing detection (ASD) algorithms. Listening tests were carefully constructed to ensure the human and automatic tasks were as similar as possible taking into consideration listener's constraints (e.g., fatigue and memory limitations). Results for human trials show the error rates on narrowband data double compared to on wide band data. The second verification task, which included only artificial speech, showed equal overall acceptance rates for both 8kHz and 16kHz. In the spoofing detection task, there was a drop in performance on most of the artificial trials as well as on human trials. At 8kHz, 20% of human trials were incorrectly classified as artificial, compared to 12% at 16kHz. The ASD algorithms also showed a drop in performance on 8kHz data, but outperformed human listeners across the board.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Wester, Mirjam; Wu, Zhizheng; Yamagishi, Junichi] Univ Edinburgh, Ctr
   Speech Technol Res, Edinburgh EH8 9YL, Midlothian, Scotland.
   <br>[Yamagishi, Junichi] Natl Inst Informat, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Wester, M (reprint author), Univ Edinburgh, Ctr Speech Technol Res, Edinburgh EH8 9YL, Midlothian, Scotland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>m.wester@inf.ed.ac.uk; zhizheng.wu@ed.ac.uk; jyamagis@inf.ed.ac.uk</td>
</tr>

<tr>
<td valign="top">TC </td><td>6</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>6</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>2047</td>
</tr>

<tr>
<td valign="top">EP </td><td>2051</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380581600428</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Xiao, X
   <br>Tian, XH
   <br>Du, S
   <br>Xu, HH
   <br>Chng, ES
   <br>Li, HZ</td>
</tr>

<tr>
<td valign="top">AF </td><td>Xiao, Xiong
   <br>Tian, Xiaohai
   <br>Du, Steven
   <br>Xu, Haihua
   <br>Chng, Eng Siong
   <br>Li, Haizhou</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INT SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Spoofing Speech Detection Using High Dimensional Magnitude and Phase
   Features: the NTU Approach for ASVspoof 2015 Challenge</td>
</tr>

<tr>
<td valign="top">SO </td><td>16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2015), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>16th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2015)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 06-10, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Dresden, GERMANY</td>
</tr>

<tr>
<td valign="top">DE </td><td>Spoofing attack; voice conversion; automatic speaker verification; phase
   feature; ASVspoof 2015</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Recent improvement in text-to-speech (TTS) and voice conversion (VC) techniques presents a threat to automatic speaker verification (ASV) systems. An attacker can use the TTS or VC systems to impersonate a target speaker's voice. To overcome such a challenge, we study the detection of such synthetic speech (called spoofing speech) in this paper. We propose to use high dimensional magnitude and phase based features and long term temporal information for the task. In total, 2 types of magnitude based features and 5 types of phase based features are used. For each feature type, we build a component system using a multilayer perceptron to predict the posterior probabilities of the input features extracted from spoofing speech. The probabilities of all component systems are averaged to produce the score for final decision. When tested on the ASVspoof 2015 benchmarking task, an equal error rate (EER) of 0.29% is obtained for known spoofing types, which demonstrates the highly effectiveness of the 7 features used. For unknown spoofing types, the EER is much higher at 5.23%, suggesting that future research should be focused on improving the generalization of the techniques.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Xiao, Xiong; Du, Steven; Xu, Haihua; Chng, Eng Siong; Li, Haizhou]
   Nanyang Technol Univ, Temasek Labs, Singapore, Singapore.
   <br>[Tian, Xiaohai; Du, Steven; Chng, Eng Siong] Nanyang Technol Univ, Sch
   Comp Engn, Singapore, Singapore.
   <br>[Tian, Xiaohai] Nanyang Technol Univ, Joint NTU UBC Res Ctr Excellence
   Act Living Elder, Singapore, Singapore.
   <br>[Li, Haizhou] Inst Infocomm Res, Human Language Technol Dept, Singapore,
   Singapore.
   <br>[Li, Haizhou] Univ New South Wales, Sch EE &amp; Telecom, Kensington, NSW,
   Australia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Xiao, X (reprint author), Nanyang Technol Univ, Temasek Labs, Singapore, Singapore.</td>
</tr>

<tr>
<td valign="top">EM </td><td>xiaoxiong@ntu.edu.sg; xhtian@ntu.edu.sg; sjdu@ntu.edu.sg;
   haihuaxu@ntu.edu.sg; aseschng@ntu.edu.sg; hli@i2r.a-star.edu.sg</td>
</tr>

<tr xmlns:date="http://exslt.org/dates-and-times">
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Li, Haizhou</display_name>&nbsp;</font></td><td><font size="3">Q-6438-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Li, Haizhou</display_name>&nbsp;</font></td><td><font size="3">0000-0001-9158-9401&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>20</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>20</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>2052</td>
</tr>

<tr>
<td valign="top">EP </td><td>2056</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380581600429</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hanilci, C
   <br>Kinnunen, T
   <br>Sahidullah, M
   <br>Sizov, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hanilci, Cemal
   <br>Kinnunen, Tomi
   <br>Sahidullah, Md
   <br>Sizov, Aleksandr</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INT SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Classifiers for Synthetic Speech Detection: A Comparison</td>
</tr>

<tr>
<td valign="top">SO </td><td>16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2015), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>16th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2015)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 06-10, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Dresden, GERMANY</td>
</tr>

<tr>
<td valign="top">DE </td><td>spoof detection; countermeasures; speaker recognition</td>
</tr>

<tr>
<td valign="top">ID </td><td>SUPPORT VECTOR MACHINES; SPEAKER VERIFICATION; MODELS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Automatic speaker verification (ASV) systems are highly vulnerable against spoofing attacks, also known as imposture. With recent developments in speech synthesis and voice conversion technology, it has become important to detect synthesized or voice-converted speech for the security of ASV systems. In this paper, we compare five different classifiers used in speaker recognition to detect synthetic speech. Experimental results conducted on the ASVspoof 2015 dataset show that support vector machines with generalized linear discriminant kernel (GLDS-SVM) yield the best performance on the development set with the EER of 0.12 % whereas Gaussian mixture model (GMM) trained using maximum likelihood (ML) criterion with the EER of 3.01 % is superior for the evaluation set.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Hanilci, Cemal; Kinnunen, Tomi; Sahidullah, Md; Sizov, Aleksandr] Univ
   Eastern Finland, Sch Comp, Joensuu, Finland.
   <br>[Hanilci, Cemal] Bursa Tech Univ, Dept Elect &amp; Elect Engn, Bursa, Turkey.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hanilci, C (reprint author), Univ Eastern Finland, Sch Comp, Joensuu, Finland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>chanil@cs.uef.fi; tkinnu@cs.joensuu.fi; sahid@cs.uef.fi; sizov@cs.uef.fi</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Hanilci, Cemal</display_name>&nbsp;</font></td><td><font size="3">S-4967-2016&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Sahidullah, Md</display_name>&nbsp;</font></td><td><font size="3">0000-0002-0624-2903&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>17</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>17</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>2057</td>
</tr>

<tr>
<td valign="top">EP </td><td>2061</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380581600430</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Patel, TB
   <br>Patil, HA</td>
</tr>

<tr>
<td valign="top">AF </td><td>Patel, Tanvina B.
   <br>Patil, Hemant A.</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INT SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Combining Evidences from Mel Cepstral, Cochlear Filter Cepstral and
   Instantaneous Frequency Features for Detection of Natural vs. Spoofed
   Speech</td>
</tr>

<tr>
<td valign="top">SO </td><td>16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2015), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>16th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2015)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 06-10, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Dresden, GERMANY</td>
</tr>

<tr>
<td valign="top">DE </td><td>CFCC; instantaneous frequency; spoofed speech; GMM; EER</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEAKER VERIFICATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Speech synthesis and voice conversion techniques can pose threats to current speaker verification (SV) systems. For this purpose, it is essential to develop front end systems that are able to distinguish human speech vs. spoofed speech (synthesized or voice converted). In this paper, for the ASVspoof 2015 challenge, we propose a detector based on combination of cochlear filter cepstral coefficients (CFCC) and change in instantaneous frequency (IF), (i.e., CFCCIF) to detect natural vs. spoofed speech. The CFCCIF features were extracted at frame-level and Gaussian mixture model (GMM)-based classification system was used. On the development set, the proposed features (i.e., CFCCIF) after fusion with Mel frequency cepstral coefficients (MFCC) features achieved an EER of 1.52 %, which is a significant reduction from MFCC (3.26 %) and CFCCIF (2.29 %) alone using 12-D static features. The EER further decreases to 0.89 % and 0.83 % for delta and delta-delta features, respectively. Experimental results on evaluation set show that fusion of MFCC and CFCCIF works relatively well with an EER of 0.41 % for known attacks and 2.013 % EER for unknown attacks. On an average, fusion of MFCC and CFCCIF features provided relatively best EER of 1.211 % for the challenge.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Patel, Tanvina B.; Patil, Hemant A.] DA IICT, Gandhinagar 382007,
   Gujarat, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Patel, TB (reprint author), DA IICT, Gandhinagar 382007, Gujarat, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>tanvina_bhupendrabhai_patel@daiict.ac.in; hemant_patil@daiict.ac.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>34</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>34</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>2062</td>
</tr>

<tr>
<td valign="top">EP </td><td>2066</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380581600431</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Janicki, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Janicki, Artur</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INT SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Spoofing Countermeasure Based on Analysis of Linear Prediction Error</td>
</tr>

<tr>
<td valign="top">SO </td><td>16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2015), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>16th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2015)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 06-10, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Dresden, GERMANY</td>
</tr>

<tr>
<td valign="top">DE </td><td>speaker verification; spoofing; linear prediction; local binary
   patterns; binary classification</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEAKER VERIFICATION; VULNERABILITY</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper a novel speaker verification spoofing countermeasure based on analysis of linear prediction error is presented. The method analyses the energy of the prediction error, prediction gain and temporal parameters related to the prediction error signal. The idea of the proposed algorithm and its implementation is described in detail. Various binary classifiers were researched to separate human and spoof classes. When tested on the corpora provided for the ASVspoof 2015 Challenge, the proposed countermeasure yielded much better results than the baseline spoofing detector based on local binary patterns (LBP). It is hoped that the proposed method can help in developing a generalised countermeasure able to detect spoofing attacks based on different variants of speech synthesis, voice conversion, and, potentially, also other spoofing algorithms.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Janicki, Artur] Warsaw Univ Technol, Inst Telecommun, Warsaw, Poland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Janicki, A (reprint author), Warsaw Univ Technol, Inst Telecommun, Warsaw, Poland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>A.Janicki@tele.pw.edu.pl</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Janicki, Artur</display_name>&nbsp;</font></td><td><font size="3">0000-0002-9937-4402&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>21</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>21</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>2077</td>
</tr>

<tr>
<td valign="top">EP </td><td>2081</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380581600434</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sahidullah, M
   <br>Kinnunen, T
   <br>Hanilci, C</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sahidullah, Md
   <br>Kinnunen, Tomi
   <br>Hanilci, Cenral</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INT SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Comparison of Features for Synthetic Speech Detection</td>
</tr>

<tr>
<td valign="top">SO </td><td>16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2015), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>16th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2015)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 06-10, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Dresden, GERMANY</td>
</tr>

<tr>
<td valign="top">DE </td><td>anti-spoofing; ASVspoof 2015; feature extraction; countermeasures</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEAKER VERIFICATION; VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>The performance of biometric systems based on automatic speaker recognition technology is severely degraded due to spoofing attacks with synthetic speech generated using different voice conversion (VC) and speech synthesis (SS) techniques. Various countermeasures are proposed to detect this type of attack, and in this context, choosing an appropriate feature extraction technique for capturing relevant information from speech is an important issue. This paper presents a concise experimental review of different features for synthetic speech detection task. A wide variety of features considered in this study include previously investigated features as well as some other potentially useful features for characterizing real and synthetic speech. The experiments are conducted on recently released ASVspoof 2015 corpus containing speech data from a large number of VC and SS technique. Comparative results using two different classifiers indicate that features representing spectral information in high-frequency region, dynamic information of speech, and detailed information related to subband characteristics are considerably more useful in detecting synthetic speech.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Sahidullah, Md; Kinnunen, Tomi; Hanilci, Cenral] Univ Eastern Finland,
   Sch Comp, Joensuu, Finland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sahidullah, M (reprint author), Univ Eastern Finland, Sch Comp, Joensuu, Finland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>sahid@cs.uef.fi; tkinnu@cs.joensuu.fi; chanil@cs.uef.fi</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Hanilci, Cemal</display_name>&nbsp;</font></td><td><font size="3">S-4967-2016&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Sahidullah, Md</display_name>&nbsp;</font></td><td><font size="3">0000-0002-0624-2903&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>36</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>36</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>2087</td>
</tr>

<tr>
<td valign="top">EP </td><td>2091</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380581600436</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Aihara, R
   <br>Takiguchi, T
   <br>Ariki, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Aihara, Ryo
   <br>Takiguchi, Testuya
   <br>Ariki, Yasuo</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INT SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Many-to-many Voice Conversion Based on Multiple Non-negative Matrix
   Factorization</td>
</tr>

<tr>
<td valign="top">SO </td><td>16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2015), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>16th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2015)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 06-10, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Dresden, GERMANY</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; speech synthesis; many-to-many; exemplar-based; NMF</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH</td>
</tr>

<tr>
<td valign="top">AB </td><td>We present in this paper an exemplar-based Voice Conversion (VC) method using Non-negative Matrix Factorization (NMF), which is different from conventional statistical VC. NMF-based VC has advantages of noise robustness and naturalness of converted voice compared to Gaussian Mixture Model (GMM)-based VC. However, because NMF-based VC is based on parallel training data of source and target speakers, we cannot convert the voice of arbitrary speakers in this framework. In this paper, we propose a many-to-many VC method that makes use of Multiple Non-negative Matrix Factorization (Multi-NMF). By using Multi-NMF, an arbitrary speaker's voice is converted to another arbitrary speaker's voice without the need for any input or output speaker training data. We assume that this method is flexible because we can adopt it to voice quality control or noise robust VC.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Aihara, Ryo; Takiguchi, Testuya; Ariki, Yasuo] Kobe Univ, Grad Sch Syst
   Informat, Nada Ku, 1-1 Rokkodai, Kobe, Hyogo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Aihara, R (reprint author), Kobe Univ, Grad Sch Syst Informat, Nada Ku, 1-1 Rokkodai, Kobe, Hyogo, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>aihara@me.cs.scitec.kobe-u.ac.jp; takigu@kobe-u.ac.jp;
   ariki@kobe-u.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>2749</td>
</tr>

<tr>
<td valign="top">EP </td><td>2753</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380581601114</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kobayashi, K
   <br>Toda, T
   <br>Neubig, G
   <br>Sakti, S
   <br>Nakamura, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kobayashi, Kazuhiro
   <br>Toda, Tomoki
   <br>Neubig, Graham
   <br>Sakti, Sakriani
   <br>Nakamura, Satoshi</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INT SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Statistical Singing Voice Conversion based on Direct Waveform
   Modification with Global Variance</td>
</tr>

<tr>
<td valign="top">SO </td><td>16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2015), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>16th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2015)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 06-10, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Dresden, GERMANY</td>
</tr>

<tr>
<td valign="top">DE </td><td>statistical singing voice conversion; direct wave-form modification;
   spectral differential; global variance; Gaussian mixture model</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents techniques to improve the quality of voices generated through statistical singing voice conversion with direct waveform modification based on spectrum differential (DIFFSVC). The DIFFSVC method makes it possible to convert singing voice characteristics of a source singer into those of a target singer without using vocoder-based waveform generation. However, quality of the converted singing voice still degrades compared to that of a natural singing voice due to various factors, such as the over-smoothing of the converted spectral parameter trajectory. To alleviate this over-smoothing, we propose a technique to restore the global variance of the converted spectral parameter trajectory within the framework of the DIFFSVC method. We also propose another technique to specifically avoid over-smoothing at unvoiced frames. Results of subjective and objective evaluations demonstrate that the proposed techniques significantly improve speech quality of the converted singing voice while preserving the conversion accuracy of singer identity compared to the conventional DIFFSVC.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kobayashi, Kazuhiro; Toda, Tomoki; Neubig, Graham; Sakti, Sakriani;
   Nakamura, Satoshi] Nara Inst Sci &amp; Technol NAIST, Grad Sch Informat Sci,
   Nara, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kobayashi, K (reprint author), Nara Inst Sci &amp; Technol NAIST, Grad Sch Informat Sci, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>kazuhiro-k@is.naist.jp; tomoki@is.naist.jp; neubig@is.naist.jp;
   ssakti@is.naist.jp; s-nakamura@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>7</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>7</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>2754</td>
</tr>

<tr>
<td valign="top">EP </td><td>2758</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380581601115</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Tian, XH
   <br>Wu, ZZ
   <br>Lee, SW
   <br>Hy, NQ
   <br>Dong, MH
   <br>Chng, ES</td>
</tr>

<tr>
<td valign="top">AF </td><td>Tian, Xiaohai
   <br>Wu, Zhizheng
   <br>Lee, Siu Wa
   <br>Nguyen Quy Hy
   <br>Dong, Minghui
   <br>Chng, Eng Siong</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INT SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>System Fusion for High-Performance Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2015), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>16th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2015)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 06-10, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Dresden, GERMANY</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; system fusion; high-performance; frequency warping;
   GMM</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH SYNTHESIS; NEURAL-NETWORKS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Recently, a number of voice conversion methods have been developed. These methods attempt to improve conversion performance by using diverse mapping techniques in various acoustic domains, e.g. high-resolution spectra and low-resolution Mel-cepstral coefficients. Each individual method has its own pros and cons. In this paper, we introduce a system fusion framework, which leverages and synergizes the merits of these state-of-the-art and even potential future conversion methods. For instance, methods delivering high speech quality are fused with methods capturing speaker characteristics, bringing another level of performance gain. To examine the feasibility of the proposed framework, we select two state-of-the-art methods, Gaussian mixture model and frequency warping based systems, as a case study. Experimental results reveal that the fusion system outperforms each individual method in both objective and subjective evaluation, and demonstrate the effectiveness of the proposed fusion framework.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Tian, Xiaohai; Nguyen Quy Hy; Chng, Eng Siong] Nanyang Technol Univ,
   Sch Comp Engn, Singapore, Singapore.
   <br>[Tian, Xiaohai; Nguyen Quy Hy; Chng, Eng Siong] Nanyang Technol Univ,
   Joint NTU UBC Res Ctr Excellence Act Living Elder, Singapore, Singapore.
   <br>[Wu, Zhizheng] Univ Edinburgh, Ctr Speech Technol Res, Edinburgh EH8
   9YL, Midlothian, Scotland.
   <br>[Lee, Siu Wa; Dong, Minghui] Inst Infocomm Res, Human Language Technol
   Dept, Singapore, Singapore.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Tian, XH (reprint author), Nanyang Technol Univ, Sch Comp Engn, Singapore, Singapore.</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>2759</td>
</tr>

<tr>
<td valign="top">EP </td><td>2763</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380581601116</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Tajiri, Y
   <br>Tanaka, K
   <br>Toda, T
   <br>Neubig, G
   <br>Sakti, S
   <br>Nakamura, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Tajiri, Yusuke
   <br>Tanaka, Kou
   <br>Toda, Tomoki
   <br>Neubig, Graham
   <br>Sakti, Sakriani
   <br>Nakamura, Satoshi</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INT SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Non-Audible Murmur Enhancement based on Statistical Conversion using
   Air- and Body-Conductive Microphones in Noisy Environments</td>
</tr>

<tr>
<td valign="top">SO </td><td>16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2015), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>16th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2015)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 06-10, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Dresden, GERMANY</td>
</tr>

<tr>
<td valign="top">DE </td><td>silent speech communication; Non-Audible Murmur; statistical voice
   conversion; air- and body-conducted speech signals; noisy conditions</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION; SPEECH</td>
</tr>

<tr>
<td valign="top">AB </td><td>Non-Audible Murmur (NAM) is an extremely soft whispered voice detected by a special body-conductive microphone called a NAM microphone. Although NAM is a promising medium for silent speech communication, its quality is significantly degraded by its faint volume and spectral changes caused by body-conductive recording. To improve the quality of NAM, several enhancement methods based on statistical voice conversion (VC) techniques have been proposed, and their effectiveness has been confirmed in quiet environments. However, it can be expected that NAM will be used not only in quiet, but also in noisy environments, and it is thus necessary to develop enhancement methods that will also work in these cases. In this paper, we propose a framework for NAM enhancement using not only the NAM microphone but also an air-conductive microphone. Air- and body-conducted NAM signals are used as the input of VC to estimate a more naturally sounding speech signal. To clarify adverse effects of external noises on the performance of the proposed framework and investigate a possibility to alleviate them by revising VC models, we also implement noise-dependent VC models within the proposed framework. Experimental results demonstrate that the proposed framework yields significant improvements in the spectral conversion accuracy and listenability of enhanced speech under both quiet and noisy environments.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Tajiri, Yusuke; Tanaka, Kou; Toda, Tomoki; Neubig, Graham; Sakti,
   Sakriani; Nakamura, Satoshi] Nara Inst Sci &amp; Technol, Grad Sch Informat
   Sci, Ikoma, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Tajiri, Y (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Ikoma, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>tajiri.yusuke.tk0@is.naist.jp; ko-t@is.naist.jp; tomoki@is.naist.jp;
   neubig@is.naist.jp; ssakti@is.naist.jp; s-nakamura@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>4</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>4</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>2769</td>
</tr>

<tr>
<td valign="top">EP </td><td>2773</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380581601118</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Aryal, S
   <br>Gutierrez-Osuna, R</td>
</tr>

<tr>
<td valign="top">AF </td><td>Aryal, Sandesh
   <br>Gutierrez-Osuna, Ricardo</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INT SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Articulatory-based conversion of foreign accents with deep neural
   networks</td>
</tr>

<tr>
<td valign="top">SO </td><td>16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2015), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>16th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2015)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 06-10, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Dresden, GERMANY</td>
</tr>

<tr>
<td valign="top">DE </td><td>articulatory synthesis; deep neural networks; electromagnetic
   articulography; voice conversion</td>
</tr>

<tr>
<td valign="top">AB </td><td>We present an articulatory -based method for real-time accent conversion using deep neural networks (DNN). The approach consists of two steps. First, we train a DNN articulatory synthesizer for the non-native speaker that estimates acoustics from contextualized articulatory gestures. Then we drive the DNN with articulatory gestures from a reference native speaker -mapped to the nonnative articulatory space via a Procrustes transform. We evaluate the accent-conversion performance of the DNN through a series of listening tests of intelligibility, voice identity and nonnative accentedness. Compared to a baseline method based on Gaussian mixture models, the DNN accent conversions were found to be 31% more intelligible, and were perceived more native-like in 68% of the cases. The DNN also succeeded in preserving the voice identity of the nonnative speaker.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Aryal, Sandesh; Gutierrez-Osuna, Ricardo] Texas A&amp;M Univ, Dept Comp Sci
   &amp; Engn, College Stn, TX 77843 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Aryal, S (reprint author), Texas A&amp;M Univ, Dept Comp Sci &amp; Engn, College Stn, TX 77843 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>sandesh@cse.tamu.edu; rgutier@cse.tamu.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>3385</td>
</tr>

<tr>
<td valign="top">EP </td><td>3389</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380581601243</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Weng, ST
   <br>Chen, SS
   <br>Yu, L
   <br>Wu, XW
   <br>Cai, WC
   <br>Liu, Z
   <br>Zhou, YM
   <br>Li, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Weng, Shitao
   <br>Chen, Shushan
   <br>Yu, Lei
   <br>Wu, Xuewei
   <br>Cai, Weicheng
   <br>Liu, Zhi
   <br>Zhou, Yiming
   <br>Li, Ming</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>The SYSU System for the Interspeech 2015 Automatic Speaker Verification
   Spoofing and Countermeasures Challenge</td>
</tr>

<tr>
<td valign="top">SO </td><td>2015 ASIA-PACIFIC SIGNAL AND INFORMATION PROCESSING ASSOCIATION ANNUAL
   SUMMIT AND CONFERENCE (APSIPA)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Asia-Pacific Signal and Information Processing Association Annual Summit
   and Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Asia-Pacific-Signal-and-Information-Processing-Association Annual Summit
   and Conference (APSIPA ASC)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 16-19, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hong Kong, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>speaker verification; spoofing and countermeasures; i-vector; modified
   group delay cepstral coefficients; phoneme posterior probability</td>
</tr>

<tr>
<td valign="top">ID </td><td>RECOGNITION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Many existing speaker verification systems are reported to be vulnerable against different spoofing attacks, for example speech synthesis, voice conversion, play back, etc. In order to detect these spoofed speech signals as a countermeasure, we propose a score level fusion approach with several different i-vector subsystems. We show that the acoustic level Mel-frequency cepstral coefficients (MFCC) features, the phase level modified group delay cepstral coefficients (MGDCC) and the phonetic level phoneme posterior probability (PPP) tandem features are effective for the countermeasure. Furthermore, feature level fusion of these features before i-vector modeling also enhance the performance. A polynomial kernel support vector machine is adopted as the supervised classifier. In order to enhance the generalizability of the countermeasure, we also adopted the cosine similarity and PLDA scoring as one-class classifications methods. By combining the proposed i-vector subsystems with the OpenSMILE baseline which covers the acoustic and prosodic information further improves the final performance. The proposed fusion system achieves 0.29% and 3.26% EER on the development and test set of the database provided by the INTERSPEECH 2015 automatic speaker verification spoofing and countermeasures challenge.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Cai, Weicheng; Li, Ming] Sun Yat Sen Univ, SYSU CMU Joint Inst Engn,
   Guangzhou, Guangdong, Peoples R China.
   <br>[Weng, Shitao; Chen, Shushan; Yu, Lei; Wu, Xuewei; Liu, Zhi] SYSU CMU
   Shunde Int Joint Res Inst, Guangzhou, Guangdong, Peoples R China.
   <br>[Zhou, Yiming] Zhejiang Univ, Dept Phys, Hangzhou, Zhejiang, Peoples R
   China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Weng, ST (reprint author), SYSU CMU Shunde Int Joint Res Inst, Guangzhou, Guangdong, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>wengsht@mail2.sysu.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>152</td>
</tr>

<tr>
<td valign="top">EP </td><td>155</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Imaging Science &amp; Photographic Technology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000382954100032</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wang, SS
   <br>Hwang, HT
   <br>Lai, YH
   <br>Tsao, Y
   <br>Lu, XG
   <br>Wang, HM
   <br>Su, B</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wang, Syu-Siang
   <br>Hwang, Hsin-Te
   <br>Lai, Ying-Hui
   <br>Tsao, Yu
   <br>Lu, Xugang
   <br>Wang, Hsin-Min
   <br>Su, Borching</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Improving Denoising Auto-encoder Based Speech Enhancement With the
   Speech Parameter Generation Algorithm</td>
</tr>

<tr>
<td valign="top">SO </td><td>2015 ASIA-PACIFIC SIGNAL AND INFORMATION PROCESSING ASSOCIATION ANNUAL
   SUMMIT AND CONFERENCE (APSIPA)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Asia-Pacific Signal and Information Processing Association Annual Summit
   and Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Asia-Pacific-Signal-and-Information-Processing-Association Annual Summit
   and Conference (APSIPA ASC)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 16-19, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hong Kong, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">ID </td><td>NOISE; NETWORKS</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper investigates the use of the speech parameter generation (SPG) algorithm, which has been successfully adopted in deep neural network (DNN)-based voice conversion (VC) and speech synthesis (SS), for incorporating temporal information to improve the deep denoising auto-encoder (DDAE)-based speech enhancement. In our previous studies, we have confirmed that DDAE could effectively suppress noise components from noise corrupted speech. However, because DDAE converts speech in a frame by frame manner, the enhanced speech shows some level of discontinuity even though context features are used as input to the DDAE. To handle this issue, this study proposes using the SPG algorithm as a post-processor to transform the DDAE processed feature sequence to one with a smoothed trajectory. Two types of temporal information with SPG are investigated in this study: static-dynamic and context features. Experimental results show that the SPG with context features outperforms the SPG with static-dynamic features and the baseline system, which considers context features without SPG, in terms of standardized objective tests in different noise types and SNRs.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Wang, Syu-Siang; Su, Borching] Natl Taiwan Univ, Grad Inst Commun Engn,
   Taipei, Taiwan.
   <br>[Hwang, Hsin-Te; Wang, Hsin-Min] Acad Sinica, Inst Informat Sci, Taipei,
   Taiwan.
   <br>[Wang, Syu-Siang; Lai, Ying-Hui; Tsao, Yu] Acad Sinica, Res Ctr Informat
   Technol Innovat, Taipei, Taiwan.
   <br>[Lu, Xugang] Natl Inst Informat &amp; Commun Technol, Koganei, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Wang, SS (reprint author), Natl Taiwan Univ, Grad Inst Commun Engn, Taipei, Taiwan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>d02942007@ntu.edu.tw; hwanght@iis.sinica.edu.tw;
   yu.tsao@citi.sinica.edu.tw</td>
</tr>

<tr>
<td valign="top">TC </td><td>7</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>7</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>365</td>
</tr>

<tr>
<td valign="top">EP </td><td>369</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Imaging Science &amp; Photographic Technology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000382954100074</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Dong, MH
   <br>Yang, CY
   <br>Lu, YF
   <br>Ehnes, JW
   <br>Huang, DY
   <br>Ming, HP
   <br>Tong, R
   <br>Lee, SW
   <br>Li, HZ</td>
</tr>

<tr>
<td valign="top">AF </td><td>Dong, Minghui
   <br>Yang, Chenyu
   <br>Lu, Yanfeng
   <br>Ehnes, Jochen Walter
   <br>Huang, Dongyan
   <br>Ming, Huaiping
   <br>Tong, Rong
   <br>Lee, Siu Wa
   <br>Li, Haizhou</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Mapping Frames with DNN-HMM Recognizer for Non-parallel Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>2015 ASIA-PACIFIC SIGNAL AND INFORMATION PROCESSING ASSOCIATION ANNUAL
   SUMMIT AND CONFERENCE (APSIPA)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Asia-Pacific Signal and Information Processing Association Annual Summit
   and Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Asia-Pacific-Signal-and-Information-Processing-Association Annual Summit
   and Conference (APSIPA ASC)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 16-19, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hong Kong, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">ID </td><td>TRANSFORMATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>To convert one speaker's voice to another's, the mapping of the corresponding speech segments from source speaker to target speaker must be obtained first. In parallel voice conversion, normally dynamic time warping (DTW) method is used to align signals of source and target voices. However, for conversion between non-parallel speech data, the DTW based mapping method does not work. In this paper, we propose to use a DNN-HMM recognizer to recognize each frame for both source and target speech signals. The vector of pseudo likelihood is then used to represent the frame. Similarity between two frames is measured with the distance between the vectors. A clustering method is used to group both source and target frames. Frame mapping from source to target is then established based on the clustering result. The experiments show that the proposed method can generate similar conversion results compared to parallel voice conversion.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Dong, Minghui; Yang, Chenyu; Lu, Yanfeng; Ehnes, Jochen Walter; Huang,
   Dongyan; Ming, Huaiping; Tong, Rong; Lee, Siu Wa; Li, Haizhou] ASTAR,
   Inst Infocomm Res, Human Language Technol Dept, Singapore, Singapore.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Dong, MH (reprint author), ASTAR, Inst Infocomm Res, Human Language Technol Dept, Singapore, Singapore.</td>
</tr>

<tr>
<td valign="top">EM </td><td>mhdong@i2r.a-star.edu.sg; yangc@i2r.a-star.edu.sg;
   luyf@i2r.a-star.edu.sg; jwehnes@i2r.a-star.edu.sg;
   huang@i2r.a-star.edu.sg; minghp@i2r.a-star.edu.sg;
   tongrong@i2r.a-star.edu.sg; SWylee@i2r.a-star.edu.sg;
   hli@i2r.a-star.edu.sg</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Li, Haizhou</display_name>&nbsp;</font></td><td><font size="3">Q-6438-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Li, Haizhou</display_name>&nbsp;</font></td><td><font size="3">0000-0001-9158-9401&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>488</td>
</tr>

<tr>
<td valign="top">EP </td><td>494</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Imaging Science &amp; Photographic Technology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000382954100097</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hwang, HT
   <br>Tsao, Y
   <br>Wang, HM
   <br>Wang, YR
   <br>Chen, SH</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hwang, Hsin-Te
   <br>Tsao, Yu
   <br>Wang, Hsin-Min
   <br>Wang, Yih-Ru
   <br>Chen, Sin-Horng</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Probabilistic Interpretation for Artificial Neural Network-based Voice
   Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>2015 ASIA-PACIFIC SIGNAL AND INFORMATION PROCESSING ASSOCIATION ANNUAL
   SUMMIT AND CONFERENCE (APSIPA)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Asia-Pacific Signal and Information Processing Association Annual Summit
   and Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Asia-Pacific-Signal-and-Information-Processing-Association Annual Summit
   and Conference (APSIPA ASC)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 16-19, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hong Kong, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">ID </td><td>ALGORITHM; HMM</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion (VC) using artificial neural networks (ANNs) has shown its capability to produce better sound quality of the converted speech than that using Gaussian mixture model (GMM). Although ANN-based VC works reasonably well, there is still room for further improvement. One of the promising ways is to adopt the successful techniques in statistical model-based parameter generation (SMPG), such as trajectory-based mapping approaches that are originally designed for GMM-based VC and hidden Markov model (HMM)-based speech synthesis. This study presents a probabilistic interpretation for ANN-based VC. In this way, ANN-based VC can easily incorporate the successful techniques in SMPG. Experimental results demonstrate that the performance of ANN-based VC can be effectively improved by two trajectory-based mapping techniques (maximum likelihood parameter generation (MLPG) algorithm and maximum likelihood-based trajectory mapping considering global variance (referred to as MLGV)), compared to the conventional ANN-based VC with frame-based mapping and the GMM-based VC with the MLPG algorithm. Moreover, ANN-based VC with the trajectory-based mapping techniques can achieve comparable performance when compared to the state-of-the-art GMM-based VC with the MLGV algorithm.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Hwang, Hsin-Te; Wang, Yih-Ru; Chen, Sin-Horng] Natl Chiao Tung Univ,
   Dept Elect &amp; Comp Engn, Hsinchu, Taiwan.
   <br>[Tsao, Yu] Acad Sinica, Res Ctr Informat Technol Innovat, Taipei, Taiwan.
   <br>[Hwang, Hsin-Te; Wang, Hsin-Min] Acad Sinica, Inst Informat Sci, Taipei,
   Taiwan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hwang, HT (reprint author), Natl Chiao Tung Univ, Dept Elect &amp; Comp Engn, Hsinchu, Taiwan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>hwanght@iis.sinica.edu.tw; yu.tsao@citi.sinica.edu.tw;
   whm@iis.sinica.edu.tw; yrwang@cc.nctu.edu.tw; schen@mail.nctu.edu.tw</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>552</td>
</tr>

<tr>
<td valign="top">EP </td><td>558</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Imaging Science &amp; Photographic Technology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000382954100106</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Maheshwari, R
   <br>Gupta, A
   <br>Chandra, N</td>
</tr>

<tr>
<td valign="top">AF </td><td>Maheshwari, Radhika
   <br>Gupta, Apoorva
   <br>Chandra, Nidhi</td>
</tr>

<tr>
<td valign="top">BE </td><td>Hoda, MN</td>
</tr>

<tr>
<td valign="top">TI </td><td>Secure Authentication Using Biometric Templates in Kerberos</td>
</tr>

<tr>
<td valign="top">SO </td><td>2015 2ND INTERNATIONAL CONFERENCE ON COMPUTING FOR SUSTAINABLE GLOBAL
   DEVELOPMENT (INDIACOM)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>2nd International Conference on Computing for Sustainable Global
   Development (INDIACom)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 11-13, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>New Delhi, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Biometric templates; distributed system; Data Security; Hadoop;
   Kerberos; master slave architecture</td>
</tr>

<tr>
<td valign="top">AB </td><td>The paper suggests the use of biometric templates for achieving the authentication in distributed systems and networks using Kerberos. The most important advantage in using the biometric templates is implying biologically inspired passwords such as pupil, fingerprints, face, iris, hand geometry, voice, palm print, handwritten signatures and gait. Using biometric templates in Kerberos gives more reliability to client server architectures for analysis in distributed platform while dealing with sensitive and confidential information. Even today the companies face challenge of security of confidential data. Although the main focus of the development of hadoop, CDBMS like technologies was primarily oriented towards the big data analysis, data management and further conversion of huge chunks of raw data into useful information. Hence, implementing biometric templates in Kerberos makes various frameworks on master slave architecture to be more reliable providing an added security advantage.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Maheshwari, Radhika; Gupta, Apoorva; Chandra, Nidhi] Amity Univ, Amity
   Sch Engn &amp; Technol, Noida, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Maheshwari, R (reprint author), Amity Univ, Amity Sch Engn &amp; Technol, Noida, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>radhika240990@gmail.com; apoorva.2901@gmail.com;
   srivastavanidhi8@gmail.com</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Chandra, Nidhi</display_name>&nbsp;</font></td><td><font size="3">0000-0001-9312-5854&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>1247</td>
</tr>

<tr>
<td valign="top">EP </td><td>1250</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000381554300242</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kumar, TR
   <br>Padmapriya, S
   <br>Bai, VT
   <br>Devamalar, PMB
   <br>Suresh, GR</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kumar, Rajesh T.
   <br>Padmapriya, S.
   <br>Bai, V. Thulasi
   <br>Devamalar, P. M. Beulah
   <br>Suresh, G. R.</td>
</tr>

<tr>
<td valign="top">BE </td><td>Karthigaikumar, P
   <br>Arulmurugan, C
   <br>Manojkumar, T</td>
</tr>

<tr>
<td valign="top">TI </td><td>Conversion of Non-Audible Murmur to Normal Speech through Wi-Fi
   Transceiver for Speech Recognition based on GMM Model</td>
</tr>

<tr>
<td valign="top">SO </td><td>2015 2ND INTERNATIONAL CONFERENCE ON ELECTRONICS AND COMMUNICATION
   SYSTEMS (ICECS)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>2nd International Conference on Electronics and Communication Systems
   (ICECS)</td>
</tr>

<tr>
<td valign="top">CY </td><td>FEB 26-27, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Coimbatore, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Non-Audible Murmur Sensor; Body-conducted speech; Wi-Fi Transceivers;
   Speech Recognition; Gaussian mixture model (GMM)</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Capturing of speech information like non-audible murmur or whispered voice from human body using NAM microphone(Non-audible murmur) and transmits via Wi-Fi transmitters and focus on its application in automatic speech recognition systems. The NAM microphone serially connected to Wi-Fi transceiver is attached behind the murmured human ear, and captures the quietly uttered murmur. It is applicable in automatic speech recognition systems, where privacy is important in human-machine communication. Moreover the NAM sensor and microphone receives the speech signal directly from the body and transmits to the recognition system using Wi-Fi transmitters, provides robustness against the environmental noises. The application of speech processing is speech recognition, speech analysis, speech transform and can be used by sound impaired people. By providing a vocabulary dictation system the murmured voices will be recognized and can provide an accuracy of speech with an analysis.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kumar, Rajesh T.; Padmapriya, S.; Bai, V. Thulasi; Devamalar, P. M.
   Beulah] Prathyusha Inst Technol &amp; Management, Madras, Tamil Nadu, India.
   <br>[Suresh, G. R.] Easwari Engn Coll, Dept ECE, Madras, Tamil Nadu, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kumar, TR (reprint author), Prathyusha Inst Technol &amp; Management, Madras, Tamil Nadu, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>t.rajesh61074@gmail.com; sureshgr@rediffmail.com</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>G R, Suresh</display_name>&nbsp;</font></td><td><font size="3">0000-0001-8721-0665&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>802</td>
</tr>

<tr>
<td valign="top">EP </td><td>808</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380619600157</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Jin, G
   <br>Johnson, MT
   <br>Liu, J
   <br>Lin, XK</td>
</tr>

<tr>
<td valign="top">AF </td><td>Jin, Gui
   <br>Johnson, Michael T.
   <br>Liu, Jia
   <br>Lin, Xiaokang</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Conversion Based on Gaussian Mixture Modules with Minimum Distance
   Spectral Mapping</td>
</tr>

<tr>
<td valign="top">SO </td><td>2015 5TH INTERNATIONAL CONFERENCE ON INFORMATION SCIENCE AND TECHNOLOGY
   (ICIST)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Information Science and Technology</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>5th International Conference on Information Science and Technology
   (ICIST)</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 24-26, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Changsha, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice Conversion; Gaussian mixture models; frequency warping;
   point-to-point mapping</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion (VC) is the task of modifying a source speaker's voice to match that of a specific target speaker. Traditional methods use Gaussian mixture models (GMM), but the converted speech quality is often badly degraded due to over-smoothing. More recent approaches such as Dynamic Frequency Warping (DFW) maintain more spectrum details during transformation, but require specific formant frequency estimates, with estimation errors resulting in poor similarity between source and target speakers. This paper proposes a new method for voice conversion called Minimum Distance Spectral Mapping (MDSM), based on a frequency-warped point-to-point mapping that robustly and accurately transforms formant frequencies while also maintaining spectral details. The proposed MDSM method uses a minimum distance alignment between source and target speakers, rather than direct formant estimates, which increases robustness and also preserves other spectral details such as formant bandwidth. Results show that the proposed method offers a good trade-off between voice quality and identity similarity, outperforming traditional GMM and DFW in both subjective and objective evaluations.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Jin, Gui; Johnson, Michael T.; Liu, Jia; Lin, Xiaokang] Tsinghua Univ,
   Dept Elect Engn, Tsinghua Natl Lab Informat Sci &amp; Technol, Beijing
   100084, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Jin, G (reprint author), Tsinghua Univ, Dept Elect Engn, Tsinghua Natl Lab Informat Sci &amp; Technol, Beijing 100084, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jing12@mails.tsinghua.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>356</td>
</tr>

<tr>
<td valign="top">EP </td><td>359</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380935900068</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sudhkar, RS
   <br>Anil, MC</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sudhkar, Rode Snehal
   <br>Anil, Manjare Chandraprabha</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Analysis of Speech Features for Emotion Detection : A review</td>
</tr>

<tr>
<td valign="top">SO </td><td>1ST INTERNATIONAL CONFERENCE ON COMPUTING COMMUNICATION CONTROL AND
   AUTOMATION ICCUBEA 2015</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>First International Conference on Computing Communication Control and
   Automation (ICCUBEA)*</td>
</tr>

<tr>
<td valign="top">CY </td><td>FEB 26-27, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Pune, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Prosody; Classifier; KLD; GMM; HMM; pitch contour</td>
</tr>

<tr>
<td valign="top">AB </td><td>Emotion detection of speech in human machine interaction is very important. Framework for emotion detection is essential, that includes various modules performing actions like speech to text conversion, feature extraction, feature selection and classification of those features to identify the emotions. The features used for emotion detection of speech are prosody features, spectral features and voice quality features. The classifications of features involve the training of various emotional models to perform the classification appropriately. The features selected to be classified must be salient to detect the emotions correctly. And these features should have to convey the measurable level of emotional modulation.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Sudhkar, Rode Snehal] JSPMs Jaywantrao Sawant Coll Engn, Dept Elect &amp;
   Telecomunicat Engn, Pune 411028, Maharashtra, India.
   <br>[Anil, Manjare Chandraprabha] JSPMs Rajashri Shahu Coll Engn, Dept Elect
   &amp; Telecomunicat Engn, Pune 411033, Maharashtra, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sudhkar, RS (reprint author), JSPMs Jaywantrao Sawant Coll Engn, Dept Elect &amp; Telecomunicat Engn, Pune 411028, Maharashtra, India.</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>661</td>
</tr>

<tr>
<td valign="top">EP </td><td>664</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/ICCUBEA.2015.135</td>
</tr>

<tr>
<td valign="top">SC </td><td>Automation &amp; Control Systems; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380620000133</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Tian, XH
   <br>Du, S
   <br>Xiao, X
   <br>Xu, HH
   <br>Chng, ES
   <br>Li, HZ</td>
</tr>

<tr>
<td valign="top">AF </td><td>Tian, Xiaohai
   <br>Du, Steven
   <br>Xiao, Xiong
   <br>Xu, Haihua
   <br>Chng, Eng Siong
   <br>Li, Haizhou</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>DETECTING SYNTHETIC SPEECH USING LONG TERM MAGNITUDE AND PHASE
   INFORMATION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2015 IEEE CHINA SUMMIT &amp; INTERNATIONAL CONFERENCE ON SIGNAL AND
   INFORMATION PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE China Summit &amp; International Conference on Signal and Information
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUL 12-15, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Chengdu, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Spoofing attack; voice conversion; instantaneous frequency</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE TRANSFORMATION; REPRESENTATIONS; IDENTIFICATION; CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Synthetic speech is speech signals generated by text-to-speech (TTS) and voice conversion (VC) techniques. They impose a threat to speaker verification (SV) systems as an attacker may make use of TTS or VC to synthesize a speakers voice to cheat the SV system. To address this challenge, we study the detection of synthetic speech using long term magnitude and phase information of speech. As most of the TTS and VC techniques make use of vocoders for speech analysis and synthesis, we focus on differentiating speech signals generated by vocoders from natural speech. Log magnitude spectrum and two phase-based features, including instantaneous frequency derivation and modified group delay, were studied in this work. We conducted experiments on the CMU-ARCTIC database using various speech features and a neural network classifier. During training, the synthetic speech detection is formulated as a 2-class classification problem and the neural network is trained to differentiate synthetic speech from natural speech. During testing, the posterior scores generated by the neural network is used for the detection of synthetic speech. The synthetic speech used in training and testing are generated by different types of vocoders and VC methods. Experimental results show that long term information up to 0.3s is important for synthetic speech detection. In addition, the high dimensional log magnitude spectrum features significantly outperforms the low dimensional MFCC features, showing that it is important to retain the detailed spectral information for detecting synthetic speech. Furthermore, the two phase-based features are found to perform well and complementary to the log magnitude spectrum features. The fusion of these features produces an equal error rate (EER) of 0.09%.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Tian, Xiaohai; Du, Steven; Chng, Eng Siong; Li, Haizhou] Nanyang
   Technol Univ, Sch Comp Engn, Singapore, Singapore.
   <br>[Tian, Xiaohai; Chng, Eng Siong] Nanyang Technol Univ, Joint NTU UBC Res
   Ctr Excellence Act Living Elder, Singapore, Singapore.
   <br>[Du, Steven; Xiao, Xiong; Xu, Haihua; Chng, Eng Siong] Nanyang Technol
   Univ, Temasek Labs, Singapore, Singapore.
   <br>[Li, Haizhou] Inst Infocomm Res, Human Language Technol Dept, Singapore,
   Singapore.
   <br>[Li, Haizhou] Univ New South Wales, Sch EE &amp; Telecom, Sydney, NSW 2052,
   Australia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Tian, XH (reprint author), Nanyang Technol Univ, Sch Comp Engn, Singapore, Singapore.</td>
</tr>

<tr>
<td valign="top">EM </td><td>xhtian@ntu.edu.sg; sjdu@ntu.edu.sg; xiaoxiong@ntu.edu.sg;
   haihuaxu@ntu.edu.sg; aseschng@ntu.edu.sg; hli@i2r.a-star.edu.sg</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Li, Haizhou</display_name>&nbsp;</font></td><td><font size="3">Q-6438-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Li, Haizhou</display_name>&nbsp;</font></td><td><font size="3">0000-0001-9158-9401&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>611</td>
</tr>

<tr>
<td valign="top">EP </td><td>615</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380436500126</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nakashika, T
   <br>Takiguchi, T
   <br>Ariki, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nakashika, Toru
   <br>Takiguchi, Tetsuya
   <br>Ariki, Yasuo</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>SPARSE NONLINEAR REPRESENTATION FOR VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2015 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA &amp; EXPO (ICME)</td>
</tr>

<tr>
<td valign="top">SE </td><td>IEEE International Conference on Multimedia and Expo</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Multimedia &amp; Expo (ICME)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUN 29-JUL 03, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Turin, ITALY</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice Conversion; Restricted Boltzmann Machine; Joint Density; Sparse
   Representation; Parallel Dictionary Learning</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH RECOGNITION</td>
</tr>

<tr>
<td valign="top">AB </td><td>In voice conversion, sparse-representation-based methods have recently been garnering attention because they are, relatively speaking, not affected by over-fitting or over-smoothing problems. In these approaches, voice conversion is achieved by estimating a sparse vector that determines which dictionaries of the target speaker should be used, calculated from the matching of the input vector and dictionaries of the source speaker. The sparse-representation-based voice conversion methods can be broadly divided into two approaches: 1) an approach that uses raw acoustic features in the training data as parallel dictionaries, and 2) an approach that trains parallel dictionaries from the training data. In our approach, we follow the latter approach and systematically estimate the parallel dictionaries using a joint-density restricted Boltzmann machine with sparse constraints. Through voice-conversion experiments, we confirmed the high-performance of our method, comparing it with the conventional Gaussian mixture model (GMM)-based approach, and a non-negative matrix factorization (NMF)-based approach, which is based on sparse representation.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nakashika, Toru] Univ Electrocommun, Grad Sch Informat Syst, 1-5-1
   Chofugaoka, Chofu, Tokyo, Japan.
   <br>[Takiguchi, Tetsuya; Ariki, Yasuo] Kobe Univ, Grad Sch Syst Informat,
   Kobe, Hyogo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nakashika, T (reprint author), Univ Electrocommun, Grad Sch Informat Syst, 1-5-1 Chofugaoka, Chofu, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>nakashika@is.uec.ac.jp; takigu@kobe-u.ac.jp; ariki@kobe-u.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380486500060</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kachare, P
   <br>Cheeran, A
   <br>Nirmal, J
   <br>Zaveri, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kachare, Pramod
   <br>Cheeran, Alice
   <br>Nirmal, Jagganath
   <br>Zaveri, Mukesh</td>
</tr>

<tr>
<td valign="top">BE </td><td>Mauri, JL
   <br>Thampi, SM
   <br>Wozniak, M
   <br>Marques, O
   <br>Krishnaswamy, D
   <br>Sahni, S
   <br>Callegari, C
   <br>Takagi, H
   <br>Bojkovic, ZS
   <br>Vinod, M
   <br>Prasad, NR
   <br>Calero, JMA
   <br>Rodrigues, J
   <br>Que, XY
   <br>Meghanathan, N
   <br>Sandhu, R
   <br>Au, E</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice conversion: Wavelet based residual selection</td>
</tr>

<tr>
<td valign="top">SO </td><td>2015 INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTING, COMMUNICATIONS
   AND INFORMATICS (ICACCI)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Advances in Computing, Communications and
   Informatics ICACCI</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 10-13, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>SCMS Grp of Inst, Aluva, INDIA</td>
</tr>

<tr>
<td valign="top">HO </td><td>SCMS Grp of Inst</td>
</tr>

<tr>
<td valign="top">ID </td><td>LINEAR PREDICTION; FREQUENCY; SPECTRUM; SPEECH</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion has been studied over past few decades and yet no flawless system has been developed. Primary restriction in developing conversion systems is decayed output speech quality. Work presented here alleviates this problem by mapping higher order excitation features along with state of the art spectral parameters. Well known linear predictive analysis is used to extract shape of the vocal tract and corresponding residual signal. Higher feature dimensionality of the excitation signal is confronted using synchronous segmentation and windowing of the signal. Each of the resulting frames are wavelet analyzed to calculate normalized sub-band energy coefficients forming a codebook. Conversion is obtained by selecting target residual corresponding to minimized energy cost function. Primary advantage of this technique is reduced dimensionality with satisfactory conversion statistics. Proposed method is compared with baseline residual selection approach using various subjective and objective tests. Wavelet features provide better selection criteria with slight improvement in output speech individuality.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kachare, Pramod; Cheeran, Alice] Ramrao Adik Inst Technol, Dept Elect &amp;
   Telecommun Engn, Nerul 400706, Navi Mumbai, India.
   <br>[Nirmal, Jagganath] Sardar Vallabhbhai Natl Inst Technol, Dept Electon
   Engn, Surat 395007, Gujarat, India.
   <br>[Zaveri, Mukesh] Sardar Vallabhbhai Natl Inst Technol, Dept Comp Engn,
   Surat 395007, Gujarat, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kachare, P (reprint author), Ramrao Adik Inst Technol, Dept Elect &amp; Telecommun Engn, Nerul 400706, Navi Mumbai, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>pramod_1991@yahoo.com; ancheeran@vjti.org.in; jhnirmal1975@gmail.com;
   mazaveri@coed.svnit.ac.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>1513</td>
</tr>

<tr>
<td valign="top">EP </td><td>1518</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380475900252</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>D'souza, K
   <br>Talele, KTV</td>
</tr>

<tr>
<td valign="top">AF </td><td>D'souza, Kevin
   <br>Talele, K. T. V.</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Conversion Using Gaussian Mixture Models</td>
</tr>

<tr>
<td valign="top">SO </td><td>2015 International Conference on Communication, Information &amp; Computing
   Technology (ICCICT)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Communication, Information &amp; Computing
   Technology (ICCICT)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JAN 15-17, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Inst Technol Mumbai, Mumbai, INDIA</td>
</tr>

<tr>
<td valign="top">HO </td><td>Inst Technol Mumbai</td>
</tr>

<tr>
<td valign="top">DE </td><td>Line spectrum frequencies; LP Residual; Gaussian Mixture Models (GMMs);
   Regression Model</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion is an emergent problem in voice and speech processing with increasing commercial interest. The main aim of the voice conversion system is to modify the speaker specific characteristics with respect to target specific characteristics. The vocal tract transfer function, shape of the glottal pulse and the prosodic features uniquely characterize a particular speaker. This work present a method of extracting features of vocal tract level i.e. Line Spectrum Frequencies (LSFs) using Regressions Model and the source characteristics level feature i.e. LP Residual is mapped using Gaussian Mixture Models for improving the performance of voice conversion system. The Performance of Gaussian Based Models based voice conversion system is conducted using objective measures and subjective measures.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[D'souza, Kevin] Sardar Patel Inst Technol, Dept Elect &amp; Telecommun
   Engn, Bombay 400058, Maharashtra, India.
   <br>[Talele, K. T. V.] Sardar Patel Inst Technol, Dept Elect Engn, Bombay
   400058, Maharashtra, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>D'souza, K (reprint author), Sardar Patel Inst Technol, Dept Elect &amp; Telecommun Engn, Bombay 400058, Maharashtra, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>kevin.dsouza712@gmail.com; talelesir@yahoo.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380538100085</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kanrar, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kanrar, Soumen</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Environment Based Threshold for Speaker Identification</td>
</tr>

<tr>
<td valign="top">SO </td><td>2015 INTERNATIONAL CONFERENCE ON ELECTRICAL, ELECTRONICS, SIGNALS,
   COMMUNICATION AND OPTIMIZATION (EESCO)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Electrical, Electronics, Signals,
   Communication and Optimization (EESCO)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JAN 24-25, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Vignans Inst of Information Technology, Visakhapatnam, INDIA</td>
</tr>

<tr>
<td valign="top">HO </td><td>Vignans Inst of Information Technology</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speaker Identification; Gaussian mixture mode; acoustic feature vectors;
   decision threshold; false accept; false reject</td>
</tr>

<tr>
<td valign="top">ID </td><td>VERIFICATION; MODELS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Speaker Identification process is to identify a particular vocal cord from a set of existing speakers. In the speaker identification processes, the unknown speaker voice sample targets each of the existing speakers in the system and gives a predication. The predication is more than one existing known speaker voice and is very close to the unknown speaker voice. It is a one to many mapping. The mapping function gives a set of predicated values associated with the order pair of speakers. In the order pair, the first coordinate is the unknown speaker and the second coordinates is the existing known speaker from the speaker recognition system. The set of predicated values helps to identify the unknown speaker. The identification process makes a comparison of the unknown speaker model with the models of the existing voice in the system. In this paper, the model is a Gaussian mixture model built by the extraction of the acoustic feature vectors. This paper presents the impact of the decision threshold based on false accepts and false reject for an unknown number of speaker conversion in the speaker identification result. In the simulation, the considered known speaker voices are collected through different channels. In the testing, the GMM voice models of the known speakers are distributed among the numbers of clusters in the test data set.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kanrar, Soumen] Vehere Interact Pvt Ltd, Kolkata 53, W Bengal, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kanrar, S (reprint author), Vehere Interact Pvt Ltd, Kolkata 53, W Bengal, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>Soumen.kanrar@veheretech.com</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Kanrar, Soumen</display_name>&nbsp;</font></td><td><font size="3">N-3113-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Kanrar, Soumen</display_name>&nbsp;</font></td><td><font size="3">0000-0002-0331-4932&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380439200002</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Khare, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Khare, Shivam</td>
</tr>

<tr>
<td valign="top">BE </td><td>Karwal, V
   <br>Gupta, R</td>
</tr>

<tr>
<td valign="top">TI </td><td>Finger Gesture and Pattern Recognition Based Device Security System</td>
</tr>

<tr>
<td valign="top">SO </td><td>2015 INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING AND COMMUNICATION
   (ICSC)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Signal Processing and Communication (ICSC)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 16-18, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Noida, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>HMI (Human Machine Interface); MATLAB; SIFT (Scale Invariant Feature
   Transform); SURF (Speed Up Robust Features); ORB; SSIM (Structural
   Similarity Index Measure); Computer vision</td>
</tr>

<tr>
<td valign="top">AB </td><td>This research aims at introduction of a hand gesture recognition based system to recognize real time gestures in natural environment and compare patterns with image database for matching of image pairs to trigger unlocking of mobile devices. The efforts made in this direction during past relating to security systems for mobile devices has been a major concern and methods like draw pattern unlock, passcodes, facial and voice recognition technologies have already been employed to a fair level of extent, but these are quiet susceptible to hacks and greater ratio of recognition failure errors ( especially in cases of voice and facial). A next step in HMI would be use of fingertip tracking based unlocking mechanism, which would employ minimalistic hardware like webcam or smartphone front camera. Image acquisition through MATLAB is followed up by conversion to grayscale and application of optimal filter for edge detection utilized in different conditions for optimal results in recognizing fingertips up to a precise level of accuracy. Pattern is traced at 60 fps for tracking and tracing and therefore cross referenced with the training image by deployment of neural networks for improved recognition efficiency. Data is registered in real time and device is unlocked at instance when SSIM takes a value above predefined threshold percentage or number. The aforementioned mechanism is employed in applications via user friendly GUI frontend and computational modelling through MATLAB for backend.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Khare, Shivam] Shri Mata Vaishno Devi Univ Katra, Sch Elect &amp; Commun,
   Katra, Jammu &amp; Kashmir, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Khare, S (reprint author), Shri Mata Vaishno Devi Univ Katra, Sch Elect &amp; Commun, Katra, Jammu &amp; Kashmir, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>shivamkhare2012@gmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>443</td>
</tr>

<tr>
<td valign="top">EP </td><td>447</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380462800080</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ye, W
   <br>Yu, YB</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ye, Wei
   <br>Yu, Yibiao</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Conversion Using Deep Neural Network in Super-Frame Feature Space</td>
</tr>

<tr>
<td valign="top">SO </td><td>2015 SIXTH INTERNATIONAL CONFERENCE ON INTELLIGENT CONTROL AND
   INFORMATION PROCESSING (ICICIP)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE Proceedings Sixth Int Conference Intelligent Control Inform
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>NOV 26-28, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Wuhan, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>DNN; voice conversion; spectral conversion; superframes</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a voice conversion technique using deep neural networks (DNNs) to map the spectral envelopes of a source speaker to that of a target speaker. Short-time spectral envelopes are represented by the linear predication cepstrum coefficients (LPCC) parameters, and neighbor frames are gathered to form super-frames. Then the powerful mapping ability of DNN which has a five-layer architecture consisting of three restricted Boltzmann machines (RBMs) was exploited to derive the spectral conversion function. A comparative study of voice conversion using a DNN model and the conventional Gaussian mixture model (GMM) is conducted. Experimental results show the speaker identification rate of conversion speech achieves 97.5% which is 0.8% higher than the performance of GMM method, and the value of average cepstrum distortion is 0.87 which is 5.4% higher than the performance of GMM method. ABX and MOS evaluations indicate that the conversion performance is better than the traditional GMM method under the parallel corpora condition.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ye, Wei; Yu, Yibiao] Soochow Univ, Sch Elect &amp; Informat Engn, Suzhou,
   Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ye, W (reprint author), Soochow Univ, Sch Elect &amp; Informat Engn, Suzhou, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>kebiqiuyw@foxmail.com; yuyb@suda.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>465</td>
</tr>

<tr>
<td valign="top">EP </td><td>468</td>
</tr>

<tr>
<td valign="top">SC </td><td>Automation &amp; Control Systems; Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380441000083</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Vinchurkar, DP
   <br>Sasikumar, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Vinchurkar, Deepika P.
   <br>Sasikumar, M.</td>
</tr>

<tr>
<td valign="top">BE </td><td>Sampson, DG
   <br>Huang, RH
   <br>Hwang, GJ
   <br>Liu, TC
   <br>Chen, NS
   <br>Kinshuk
   <br>Tsai, CC</td>
</tr>

<tr>
<td valign="top">TI </td><td>INTELLIGENT TUTORING SYSTEM FOR VOICE CONVERSION IN ENGLISH</td>
</tr>

<tr>
<td valign="top">SO </td><td>15TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED LEARNING TECHNOLOGIES
   (ICALT 2015)</td>
</tr>

<tr>
<td valign="top">SE </td><td>IEEE International Conference on Advanced Learning Technologies</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>15th IEEE International Conference on Advanced Learning Technologies
   (ICALT)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUL 06-09, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hualien, TAIWAN</td>
</tr>

<tr>
<td valign="top">DE </td><td>Intelligent tutoring system; ICALL; grammatical voice conversion;
   template structure</td>
</tr>

<tr>
<td valign="top">AB </td><td>Effective systems for teaching languages like English has always been in demand, and is an active area of research. This paper discusses a system for tutoring the grammatical voice conversion aimed at students in the age group of 12 to 16 years. One major concern in such systems is in generating adequate range of sentences for teaching and evaluation. We use a sentence generation mechanism, based on a template structure, for this purpose. Use of template structure helps in generation of a large number of meaningful and grammatically correct sentences. Another concern is to assess the student response correctly, to identify the error and provide proper feedback. Our system addresses this through a carefully designed set of knowledge bases. Currently the system can handle the conversion for simple sentences in almost all common tenses; this is being extended to handle more complex sentences. A pilot study has been conducted to test the utility of the system, and the results are very positive.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Vinchurkar, Deepika P.] SVKMs NMIMS Univ, MPSTME, Dept Comp Engn,
   Bombay, Maharashtra, India.
   <br>[Sasikumar, M.] CDAC, Bombay, Maharashtra, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Vinchurkar, DP (reprint author), SVKMs NMIMS Univ, MPSTME, Dept Comp Engn, Bombay, Maharashtra, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>deepika.vinchurkar@gmail.com; sasi@cdac.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>314</td>
</tr>

<tr>
<td valign="top">EP </td><td>316</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/ICALT.2015.147</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380365400094</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Xu, N
   <br>Tang, YB
   <br>Bao, JY
   <br>Yao, X
   <br>Jiang, AM
   <br>Liu, XF</td>
</tr>

<tr>
<td valign="top">AF </td><td>Xu, Ning
   <br>Tang, Yibin
   <br>Bao, Jingyi
   <br>Yao, Xiao
   <br>Jiang, Aimin
   <br>Liu, Xiaofeng</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Conversion Based on Em pirical Conditional Distribution in
   Resource-limited Scenarios</td>
</tr>

<tr>
<td valign="top">SO </td><td>2015 IEEE INTERNATIONAL CONFERENCE ON CONSUMER ELECTRONICS - TAIWAN
   (ICCE-TW)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Consumer Electronics</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Consumer Electronics - Taiwan (ICCE-TW
   2015)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUN 06-08, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Taipei, TAIWAN</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, a computationally efficient voice conversion system has been designed in order to improve the performance in resource-limited scenarios. First, mixtures of Gaussians (MoGs) at fixed locations of Mel frequencies have been used to represent the spectrum of STRAIGHT compactly. Second, the key conditional distributions for prediction are approximated by building histograms of aligned features empirically. Experiments have confirmed that our proposed method can obtain fairly good results compared to the traditional method without huge computational costs.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Xu, Ning; Tang, Yibin; Yao, Xiao; Jiang, Aimin; Liu, Xiaofeng] Hohai
   Univ, Coll IoT Engn, Changzhou, Peoples R China.
   <br>[Bao, Jingyi] Changzhou Inst Technol, Sch Elect Informat &amp; Elect Engn,
   Changzhou, Peoples R China.
   <br>[Xu, Ning; Yao, Xiao; Jiang, Aimin; Liu, Xiaofeng] Changzhou Key Lab
   Robot &amp; Intelligent Technol, Changzhou, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Xu, N (reprint author), Hohai Univ, Coll IoT Engn, Changzhou, Peoples R China.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>172</td>
</tr>

<tr>
<td valign="top">EP </td><td>173</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380469500086</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Villavicencio, F
   <br>Bonada, J
   <br>Hisaminato, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Villavicencio, Fernando
   <br>Bonada, Jordi
   <br>Hisaminato, Yuji</td>
</tr>

<tr>
<td valign="top">BE </td><td>Erdogmus, D
   <br>Akcakaya, M
   <br>Kozat, S
   <br>Larsen, J</td>
</tr>

<tr>
<td valign="top">TI </td><td>OBSERVATION-MODEL ERROR COMPENSATION FOR ENHANCED SPECTRAL ENVELOPE
   TRANSFORMATION IN VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2015 IEEE INTERNATIONAL WORKSHOP ON MACHINE LEARNING FOR SIGNAL
   PROCESSING</td>
</tr>

<tr>
<td valign="top">SE </td><td>IEEE International Workshop on Machine Learning for Signal Processing</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Workshop on Machine Learning for Signal Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 17-20, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Boston, NY</td>
</tr>

<tr>
<td valign="top">DE </td><td>speech synthesis; voice conversion; voice transformation; linear
   regression</td>
</tr>

<tr>
<td valign="top">AB </td><td>A strategy to enhance the signal quality and naturalness was designed for performing probabilistic spectral envelope transformation in voice conversion. The existing modeling error of the probabilistic mixture to represent the observed envelope features is translated generally as an averaging of the information in the spectral domain, resulting in over-smoothed spectra. Moreover, a transformation based on poorly modeled features might not be considered reliable. Our strategy consists of a novel definition of the spectral transformation to compensate the effect of both over-smoothing and poor modeling. The results of an experimental evaluation show that the perceived naturalness of converted speech was enhanced.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Villavicencio, Fernando] Natl Inst Informat, Sound &amp; Media Grp, Tokyo,
   Japan.
   <br>[Bonada, Jordi] Univ Pompeu Fabra, Mus Technol Grp, Barcelona, Spain.
   <br>[Hisaminato, Yuji] Yamaha Corp, Speech Technol Grp, Hamamatsu, Shizuoka,
   Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Villavicencio, F (reprint author), Natl Inst Informat, Sound &amp; Media Grp, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>villavicencio@nii.ac.jp; jordi.bonada@upf.edu;
   yuji.hisaminato@music.yamaha.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380402700016</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hayashi, M
   <br>Bachelder, S
   <br>Nakajima, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hayashi, Masaki
   <br>Bachelder, Steven
   <br>Nakajima, Masayuki</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Open Framework Facilitating Automatic Generation of CG Animation from
   Web Site</td>
</tr>

<tr>
<td valign="top">SO </td><td>2015 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Cyberworlds CW</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 07-09, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Visby, SWEDEN</td>
</tr>

<tr>
<td valign="top">DE </td><td>animation; text visualization; media conversion; scripting langauge</td>
</tr>

<tr>
<td valign="top">AB </td><td>We have been studying and developing the system which enables to generate Computer Graphics Animation (CGA) automatically by processing HTML data of Web site. In this paper, we propose an open framework to facilitate this. The framework is functioning all at a server side, obtaining the HTML, converting it to a script describing the CGA story and updating the script. And at a client side, a user accesses the script on the server to visualize it by using real-time CG character with synthesized voice, camera work, superimposing, sound file playback etc. We have constructed the framework on the server and deployed the substantial engines to convert Web sites to CGAs. This paper describes the detail of the framework and also shows some example projects providing automatically generated News show, Talk show and personal Blog visualization.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Hayashi, Masaki; Bachelder, Steven; Nakajima, Masayuki] Uppsala Univ,
   Dept Game Design, Campus Gotland, Visby, Sweden.
   <br>[Nakajima, Masayuki] Kanagawa Inst Technol, Atsugi, Kanagawa, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hayashi, M (reprint author), Uppsala Univ, Dept Game Design, Campus Gotland, Visby, Sweden.</td>
</tr>

<tr>
<td valign="top">EM </td><td>masaki.hayashi@speldesign.uu.se; steven.bachelder@speldesign.uu.se;
   masayuki.nakajima@gmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>42</td>
</tr>

<tr>
<td valign="top">EP </td><td>45</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/CW.2015.11</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380483300007</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Maheshwari, T
   <br>Kumar, U
   <br>Nagpal, C
   <br>Ojha, C
   <br>Mittal, VK</td>
</tr>

<tr>
<td valign="top">AF </td><td>Maheshwari, Tushar
   <br>Kumar, Upendra
   <br>Nagpal, Chaitanya
   <br>Ojha, Chandrakant
   <br>Mittal, V. K.</td>
</tr>

<tr>
<td valign="top">BE </td><td>Saini, H
   <br>Ghrera, SP
   <br>Sehgal, VK</td>
</tr>

<tr>
<td valign="top">TI </td><td>Capturing the Spied Image-Video Data Using a Flexi-Controlled Spy-Robot</td>
</tr>

<tr>
<td valign="top">SO </td><td>2015 THIRD INTERNATIONAL CONFERENCE ON IMAGE INFORMATION PROCESSING
   (ICIIP)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>3rd International Conference on Image Information Processing (ICIIP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 21-24, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Waknaghat, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Spy-robot; flexi-controls; multiple modalities; DTMF; WI-Fi; Bluetooth;
   smart phone controlled spy-robot</td>
</tr>

<tr>
<td valign="top">AB </td><td>Wireless operated spy-robots can be immensely useful if they can be controlled remotely over a larger operating ranges. Availability of multiple modalities for their wireless control operation can further enhance their capabilities and the range of applications. In this paper we develop a prototype spy-robot that can be controlled remotely, using multiple modalities. The spy-robot can be controlled using a smart phone based DTMF, remote control application, voice commands and tilt-gesture control application. DTMF uses the alpha-numeric keypad of the mobile phone. The remote control application is developed for the Android platform based smart phone. The voice commands use online conversion from speech to text form. The tilt-gestures controls use accelerometer of the smart phone. The control commands to the robot can be transmitted in three ways, using Bluetooth network, over Wi-Fi or using DTMF based telephone calls. The spy-robot can perform forward/backward and turning left/right movements. An on-board spy camera can also be tilted up/down or left/right, in the steps of 30 in each direction. The images or videos captured by the camera are streamed back live over the Wi-Fi network. Performance evaluation is carried to measure various operational limits, with encouraging results. The proposed flexi-controlled spy-robot can be used for range of applications including surveillance, monitoring, tracking and control.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Maheshwari, Tushar; Kumar, Upendra; Nagpal, Chaitanya; Ojha,
   Chandrakant; Mittal, V. K.] Indian Inst Infonnat Technol Chittoor, Sri
   City, AP, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Maheshwari, T (reprint author), Indian Inst Infonnat Technol Chittoor, Sri City, AP, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>tushar.m14@iiits.in; upendra.k14@iiits.in; chaitanya.n14@iiits.in;
   chandrakant.o14@iiits.in; vkmittal@iiits.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>330</td>
</tr>

<tr>
<td valign="top">EP </td><td>335</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380410300062</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Aihara, R
   <br>Fujii, T
   <br>Nakashika, T
   <br>Takiguchi, T
   <br>Ariki, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Aihara, Ryo
   <br>Fujii, Takao
   <br>Nakashika, Toru
   <br>Takiguchi, Tetsuya
   <br>Ariki, Yasuo</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>NOISE-ROBUST VOICE CONVERSION USING A SMALL PARALLE DATA BASED ON
   NON-NEGATIVE MATRIX FACTORIZATION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2015 23RD EUROPEAN SIGNAL PROCESSING CONFERENCE (EUSIPCO)</td>
</tr>

<tr>
<td valign="top">SE </td><td>European Signal Processing Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>23rd European Signal Processing Conference (EUSIPCO)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 31-SEP 04, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Nice, FRANCE</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; speaker adaptation; noisy environments; small parallel
   corpus</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a novel framework of voice conversion (VC) based on non-negative matrix factorization (NMF) using a small parallel corpus. In our previous work, a VC technique using NMF for noisy environments has been proposed, and it requires parallel exemplars (dictionary), which consist of the source exemplars and target exemplars, having the same texts uttered by the source and target speakers. The large parallel corpus is used to construct a conversion function in NMF-based VC (in the same way as common GMM-based VC). In this paper, an adaptation matrix in an NMF framework is introduced to adapt the source dictionary to the target dictionary. This adaptation matrix is estimated using a small parallel speech corpus only. The effectiveness of this method is confirmed by comparing its effectiveness with that of a conventional NMF-based method and a GMM-based method in a noisy environment.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Aihara, Ryo; Fujii, Takao; Takiguchi, Tetsuya; Ariki, Yasuo] Kobe Univ,
   Grad Sch Syst Informat, 1-1 Rokkodai, Kobe, Hyogo, Japan.
   <br>[Nakashika, Toru] Univ Electrocommun, Grad Sch Informat Syst, 1-5-1
   Chofugaoka, Chofu, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Aihara, R (reprint author), Kobe Univ, Grad Sch Syst Informat, 1-1 Rokkodai, Kobe, Hyogo, Japan.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>315</td>
</tr>

<tr>
<td valign="top">EP </td><td>319</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000377943800064</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ming, HP
   <br>Huang, DY
   <br>Dong, MH
   <br>Li, HZ
   <br>Xie, L
   <br>Zhang, SF</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ming, Huaiping
   <br>Huang, Dongyan
   <br>Dong, Minghui
   <br>Li, Haizhou
   <br>Xie, Lei
   <br>Zhang, Shaofei</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Fundamental Frequency Modeling Using Wavelets for Emotional Voice
   Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>2015 INTERNATIONAL CONFERENCE ON AFFECTIVE COMPUTING AND INTELLIGENT
   INTERACTION (ACII)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Affective Computing and Intelligent
   Interaction</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>6th AAAC Affective Computing and Intelligent Interaction International
   Conference (ACII)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 21-24, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Xian, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; prosody; sparse representation; emotion</td>
</tr>

<tr>
<td valign="top">ID </td><td>PROSODY CONVERSION; SPEECH</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper is to show a representation of fundamental frequency (F0) using continuous wavelet transform (CWT) for prosody modeling in emotion conversion. Emotional conversion aims at converting speech from one emotion state to another. Specifically, we use CWT to decompose F0 into a five-scale representation that corresponds to five temporal scales. A neutral voice is converted to an emotional voice under an exemplar-based voice conversion framework, where both spectrum and F0 are simultaneously converted. The simulation results demonstrate that the dynamics of F0 in different temporal scales can be well captured and converted using the five-scale CWT representation. The converted speech signals are evaluated both objectively and subjectively, that confirm the effectiveness of the proposed method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ming, Huaiping; Huang, Dongyan; Dong, Minghui; Li, Haizhou] ASTAR, Inst
   Infocomm Res, 1 Fusionopolis Way,21-01 Connexis, Singapore 138632,
   Singapore.
   <br>[Xie, Lei; Zhang, Shaofei] Northwestern Polytech Univ, Sch Comp Sci,
   Xian 710072, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ming, HP (reprint author), ASTAR, Inst Infocomm Res, 1 Fusionopolis Way,21-01 Connexis, Singapore 138632, Singapore.</td>
</tr>

<tr>
<td valign="top">EM </td><td>minghp@i2r.a-star.edu.sg; huang@i2r.a-star.edu.sg;
   mhdong@i2r.a-star.edu.sg; hli@i2r.a-star.edu.sg; lxie@nwpu.edu.cn;
   zhangshaofei@nwpu.edu.cn</td>
</tr>

<tr xmlns:date="http://exslt.org/dates-and-times">
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Li, Haizhou</display_name>&nbsp;</font></td><td><font size="3">Q-6438-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Li, Haizhou</display_name>&nbsp;</font></td><td><font size="3">0000-0001-9158-9401&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>5</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>5</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>804</td>
</tr>

<tr>
<td valign="top">EP </td><td>809</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000377887000123</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Aihara, R
   <br>Takiguchi, T
   <br>Ariki, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Aihara, Ryo
   <br>Takiguchi, Tetsuya
   <br>Ariki, Yasuo</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>MANY-TO-ONE VOICE CONVERSION USING EXEMPLAR-BASED SPARSE REPRESENTATION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2015 IEEE WORKSHOP ON APPLICATIONS OF SIGNAL PROCESSING TO AUDIO AND
   ACOUSTICS (WASPAA)</td>
</tr>

<tr>
<td valign="top">SE </td><td>IEEE Workshop on Applications of Signal Processing to Audio and
   Acoustics</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE Workshop on Applications of Signal Processing to Audio and
   Acoustics (WASPAA)</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 18-21, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>New Paltz, NY</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; speech synthesis; many-to-one; exemplar-based; NMF</td>
</tr>

<tr>
<td valign="top">ID </td><td>NONNEGATIVE MATRIX FACTORIZATION; SPEECH</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion (VC) is being widely researched in the field of speech processing because of increased interest in using such processing in applications such as personalized Text-to-Speech systems. We present in this paper a many-to-one VC method using exemplar-based sparse representation, which is different from conventional statistical VC. In our previous exemplar-based VC method, input speech was represented by the source dictionary and its sparse coefficients. The source and the target dictionaries are fully coupled and the converted voice is constructed from the source coefficients and the target dictionary. This method requires parallel exemplars (which consist of the source exemplars and target exemplars that have the same texts uttered by the source and target speakers) for dictionary construction. In this paper, we propose a many-to-one VC method in an exemplar-based framework which does not need training data of the source speaker. Some statistical approaches for many-to-one VC have been proposed; however, in the framework of exemplar-based VC, such a method has never been proposed. The effectiveness of our many-to-one VC has been confirmed by comparing its effectiveness with that of a conventional one-to-one NMF-based method and one-to-one GMM-based method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Aihara, Ryo; Takiguchi, Tetsuya; Ariki, Yasuo] Kobe Univ, Grad Sch Syst
   Informat, Kobe, Hyogo 657, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Aihara, R (reprint author), Kobe Univ, Grad Sch Syst Informat, Kobe, Hyogo 657, Japan.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000377205500061</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Melikishvili, D
   <br>Putkaradze, N
   <br>Chichikoshvili, N</td>
</tr>

<tr>
<td valign="top">AF </td><td>Melikishvili, Damana
   <br>Putkaradze, Natia
   <br>Chichikoshvili, Nino</td>
</tr>

<tr>
<td valign="top">BE </td><td>Uslu, F</td>
</tr>

<tr>
<td valign="top">TI </td><td>THE DIFFICULTIES IN THE PROCESS OF TEACHING THE CATEGORY OF VOICE AND
   THE KARTVELIAN VERB</td>
</tr>

<tr>
<td valign="top">SO </td><td>SOCIOINT15: INTERNATIONAL CONFERENCE ON SOCIAL SCIENCES AND HUMANITIES</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>2nd International Conference on Education, Social Sciences and
   Humanities (SOCIOINT)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUN 08-10, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Istanbul, TURKEY</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice; Diathesis; Kartvelian; Georgian; Megrelian; Verb; Classification;
   Dionysius Thrax</td>
</tr>

<tr>
<td valign="top">AB </td><td>In Georgian grammar textbooks until present-day, we have traditional classification of A. Shanidze, who classified the structural forms as the subcategories of the semantic criteria; i.e. he structured the formal classification under the semantical notion that contradicted the form and the meaning. The reason of this disagreement of the form and the function is the polypersonal nature of Georgian verb. Unlikely to Indo-European languages, in Georgian we cannot always get the passive meaning by the conversing of the so called "active voice" verbs. Conversion of the verbs not always gives us the passive voice verb forms, but, mostly, there are derived the verbs with active subject, that are considered to be the active voice verbs with the indirect object and the autoactive voice verbs.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Melikishvili, Damana; Chichikoshvili, Nino] Ivane Javakhishvili Tbilisi
   State Univ, Tbilisi, Georgia.
   <br>[Putkaradze, Natia] Ivane Javakhishvili Tbilisi State Univ, Giorgi
   Akhvlediani Soc Hist Linguist, Tbilisi, Georgia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Putkaradze, N (reprint author), Ivane Javakhishvili Tbilisi State Univ, Giorgi Akhvlediani Soc Hist Linguist, Tbilisi, Georgia.</td>
</tr>

<tr>
<td valign="top">EM </td><td>damanamel@yahoo.com; natia.putkaradze@tsu.ge;
   ninochichikoshvili@yahoo.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>828</td>
</tr>

<tr>
<td valign="top">EP </td><td>834</td>
</tr>

<tr>
<td valign="top">SC </td><td>Arts &amp; Humanities - Other Topics; Social Sciences - Other Topics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000373273800109</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Klein, LF
   <br>Eisenstein, J
   <br>Sun, I</td>
</tr>

<tr>
<td valign="top">AF </td><td>Klein, Lauren F.
   <br>Eisenstein, Jacob
   <br>Sun, Iris</td>
</tr>

<tr>
<td valign="top">TI </td><td>Exploratory Thematic Analysis for Digitized Archival Collections</td>
</tr>

<tr>
<td valign="top">SO </td><td>DIGITAL SCHOLARSHIP IN THE HUMANITIES</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article; Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>2014 Digital Humanties Conference</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUL 07-12, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Alliance Digit Human Org, Lausanne, SWITZERLAND</td>
</tr>

<tr>
<td valign="top">HO </td><td>Alliance Digit Human Org</td>
</tr>

<tr>
<td valign="top">ID </td><td>TEXT</td>
</tr>

<tr>
<td valign="top">AB </td><td>How do humanities scholars make sense of new or otherwise unfamiliar archives? Is there a role for computational text analysis in the process of sensemaking? We propose that topic modeling, when conceived as a process of thematic exploration, can provide a new entry point into this process. To this end, we present research on a new software tool called TOME: Interactive TOpic Model and MEtadata Visualization, designed to support the exploratory thematic analysis of digitized archival collections. TOME is centered around a set of visualizations intended to facilitate the interpretation of the topic model and its incorporation into extant humanities research practices. In contrast to other topic model browsers, which present the model on its own terms, ours is informed by the process of conducting early-stage humanities research. Our article thus also demonstrates the conceptual conversions-in terms of both design and process-that interdisciplinary collaboration necessarily entails. In making these conversions explicit, and exploring the implications of their successes and failures, we take up the call, as voiced by Johanna Drucker (Humanities approaches to graphical display. Digital Humanities Quarterly, 5(1), 2011), to resist the 'intellectual Trojan horse' of visualization. We seek to model a new mode of interdisciplinary inquiry, one that brings the methodological emphasis of the digital humanities to bear on the practices of humanities research and computer science alike.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Klein, Lauren F.; Sun, Iris] Georgia Inst Technol, Sch Literature Media
   &amp; Commun, Atlanta, GA 30313 USA.
   <br>[Eisenstein, Jacob] Georgia Inst Technol, Sch Interact Comp, Atlanta, GA
   30313 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Klein, LF (reprint author), Georgia Inst Technol, Sch Literature Media &amp; Commun, Skiles Classroom Bldg,686 Cherry St NW, Atlanta, GA 30313 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>lauren.klein@lmc.gatech.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>5</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>5</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">VL </td><td>30</td>
</tr>

<tr>
<td valign="top">SU </td><td>1</td>
</tr>

<tr>
<td valign="top">SI </td><td>SI</td>
</tr>

<tr>
<td valign="top">BP </td><td>130</td>
</tr>

<tr>
<td valign="top">EP </td><td>141</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1093/llc/fqv052</td>
</tr>

<tr>
<td valign="top">SC </td><td>Arts &amp; Humanities - Other Topics; Linguistics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000367449700012</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Yun, YS
   <br>Jung, J
   <br>Eun, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Yun, Young-Sun
   <br>Jung, Jinman
   <br>Eun, Seongbae</td>
</tr>

<tr>
<td valign="top">BE </td><td>Ronzhin, A
   <br>Potapova, R
   <br>Fakotakis, N</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Conversion Between Synthesized Bilingual Voices Using Line
   Spectral Frequencies</td>
</tr>

<tr>
<td valign="top">SO </td><td>SPEECH AND COMPUTER (SPECOM 2015)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Lecture Notes in Artificial Intelligence</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>17th International Conference on Speech and Computer (SPECOM)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 20-24, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Athens, GREECE</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; Weighted frequency warping; Formant space; Piecewise
   linear warping; Line spectral frequencies</td>
</tr>

<tr>
<td valign="top">ID </td><td>ACOUSTIC CHARACTERISTICS; NORMALIZATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion is a technique that transforms the source speaker individuality to that of the target speaker. We propose the simple and intuitive voice conversion algorithm not using training data between different languages and it uses text-to-speech generated speech rather than recorded real voices. The suggested method reconstructed the voice after transforming line spectral frequencies (LSF) by formant space warping functions. The formant space is the space consisted of representative four monophthongs for each language. The warping functions are represented by piecewise linear equations using pairs of four formants at matched monophthongs. In this paper, we applied LSF to voice conversion because LSF are not overly sensitive to quantization noise and can be interpolated. From experimental results, LSF based voice conversion shows good results for ABX and MOS tests than the direct frequency warping approaches.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Yun, Young-Sun; Jung, Jinman; Eun, Seongbae] Hannam Univ, Dept Comp
   Commun &amp; Unmanned Technol, Daejeon 306791, South Korea.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Yun, YS (reprint author), Hannam Univ, Dept Comp Commun &amp; Unmanned Technol, Daejeon 306791, South Korea.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ysyun@hnu.kr; jmjung@hnu.kr; sbeun@hnu.kr</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Jung, Jinman</display_name>&nbsp;</font></td><td><font size="3">P-8059-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">VL </td><td>9319</td>
</tr>

<tr>
<td valign="top">BP </td><td>463</td>
</tr>

<tr>
<td valign="top">EP </td><td>471</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/978-3-319-23132-7_57</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000365866300057</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Shchemelinin, V
   <br>Kozlov, A
   <br>Lavrentyeva, G
   <br>Novoselov, S
   <br>Simonchik, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Shchemelinin, Vadim
   <br>Kozlov, Alexandr
   <br>Lavrentyeva, Galina
   <br>Novoselov, Sergey
   <br>Simonchik, Konstantin</td>
</tr>

<tr>
<td valign="top">BE </td><td>Ronzhin, A
   <br>Potapova, R
   <br>Fakotakis, N</td>
</tr>

<tr>
<td valign="top">TI </td><td>Vulnerability of Voice Verification System with STC Anti-spoofing
   Detector to Different Methods of Spoofing Attacks</td>
</tr>

<tr>
<td valign="top">SO </td><td>SPEECH AND COMPUTER (SPECOM 2015)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Lecture Notes in Artificial Intelligence</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>17th International Conference on Speech and Computer (SPECOM)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 20-24, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Athens, GREECE</td>
</tr>

<tr>
<td valign="top">DE </td><td>Spoofing; Anti-spoofing; Speaker recognition; TV; SVM</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper explores the robustness of a text-independent voice verification system against different methods of spoofing attacks based on speech synthesis and voice conversion techniques. Our experiments show that spoofing attacks based on the speech synthesis are most dangerous, but the use of standard TV-JFA approach based spoofing detection module can reduce the False Acceptance error rate of the whole speaker recognition system from 80% to 1%.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Shchemelinin, Vadim; Novoselov, Sergey; Simonchik, Konstantin] ITMO
   Univ, St Petersburg, Russia.
   <br>[Shchemelinin, Vadim; Kozlov, Alexandr; Lavrentyeva, Galina; Novoselov,
   Sergey; Simonchik, Konstantin] Speech Technol Ctr Ltd, St Petersburg,
   Russia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Shchemelinin, V (reprint author), ITMO Univ, St Petersburg, Russia.</td>
</tr>

<tr>
<td valign="top">EM </td><td>shchemelinin@speechpro.com; kozlov-a@speechpro.com;
   lavrentyeva@speechpro.com; novoselov@speechpro.com;
   simonchik@speechpro.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">VL </td><td>9319</td>
</tr>

<tr>
<td valign="top">BP </td><td>480</td>
</tr>

<tr>
<td valign="top">EP </td><td>486</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/978-3-319-23132-7_59</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000365866300059</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Aryal, S
   <br>Gutierrez-Osuna, R</td>
</tr>

<tr>
<td valign="top">AF </td><td>Aryal, Sandesh
   <br>Gutierrez-Osuna, Ricardo</td>
</tr>

<tr>
<td valign="top">TI </td><td>Reduction of non-native accents through statistical parametric
   articulatory synthesis</td>
</tr>

<tr>
<td valign="top">SO </td><td>JOURNAL OF THE ACOUSTICAL SOCIETY OF AMERICA</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">ID </td><td>AUTOMATIC SPEECH RECOGNITION; VOICE CONVERSION; MODEL; FEATURES;
   NORMALIZATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents an articulatory synthesis method to transform utterances from a second language (L2) learner to appear as if they had been produced by the same speaker but with a native (L1) accent. The approach consists of building a probabilistic articulatory synthesizer (a mapping from articulators to acoustics) for the L2 speaker, then driving the model with articulatory gestures from a reference L1 speaker. To account for differences in the vocal tract of the two speakers, a Procrustes transform is used to bring their articulatory spaces into registration. In a series of listening tests, accent conversions were rated as being more intelligible and less accented than L2 utterances while preserving the voice identity of the L2 speaker. No significant effect was found between the intelligibility of accent-converted utterances and the proportion of phones outside the L2 inventory. Because the latter is a strong predictor of pronunciation variability in L2 speech, these results suggest that articulatory resynthesis can decouple those aspects of an utterance that are due to the speaker's physiology from those that are due to their linguistic gestures. (C) 2015 Acoustical Society of America.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Aryal, Sandesh; Gutierrez-Osuna, Ricardo] Texas A&amp;M Univ, Dept Comp Sci
   &amp; Engn, College Stn, TX 77843 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Gutierrez-Osuna, R (reprint author), Texas A&amp;M Univ, Dept Comp Sci &amp; Engn, College Stn, TX 77843 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>rgutier@cse.tamu.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>7</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>7</td>
</tr>

<tr>
<td valign="top">PD </td><td>JAN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">VL </td><td>137</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>433</td>
</tr>

<tr>
<td valign="top">EP </td><td>446</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1121/1.4904701</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Audiology &amp; Speech-Language Pathology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000348369000060</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ergunay, SK
   <br>Khoury, E
   <br>Lazaridis, A
   <br>Marcel, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ergunay, Serife Kucur
   <br>Khoury, Elie
   <br>Lazaridis, Alexandros
   <br>Marcel, Sebastien</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>On the Vulnerability of Speaker Verification to Realistic Voice Spoofing</td>
</tr>

<tr>
<td valign="top">SO </td><td>2015 IEEE 7TH INTERNATIONAL CONFERENCE ON BIOMETRICS THEORY,
   APPLICATIONS AND SYSTEMS (BTAS 2015)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Biometrics Theory Applications and Systems</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE 7th International Conference on Biometrics Theory, Applications and
   Systems (BTAS)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 08-11, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Arlington, VA</td>
</tr>

<tr>
<td valign="top">DE </td><td>spoofing; countermeasure; replay attack; speech synthesis; voice
   conversion; speaker verification</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH SYNTHESIS; COUNTERMEASURES</td>
</tr>

<tr>
<td valign="top">AB </td><td>Automatic speaker verification (ASV) systems are subject to various kinds of malicious attacks. Replay, voice conversion and speech synthesis attacks drastically degrade the performance of a standard ASV system by increasing its false acceptance rates. This issue raised a high level of interest in the speech research community where the possible voice spoofing attacks and their related countermeasures have been investigated. However, much less effort has been devoted in creating realistic and diverse spoofing attack databases that foster researchers to correctly evaluate their countermeasures against attacks. The existing studies are not complete in terms of types of attacks, and often difficult to reproduce because of unavailability of public databases. In this paper we introduce the voice spoofing data-set of AVspoof, a public audio-visual spoofing database. AVspoof includes ten realistic spoofing threats generated using replay, speech synthesis and voice conversion. In addition, we provide a set of experimental results that show the effect of such attacks on current state-of-the-art ASV systems.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ergunay, Serife Kucur; Khoury, Elie; Lazaridis, Alexandros; Marcel,
   Sebastien] Idiap Res Inst, Ctr Parc, Martigny, Switzerland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ergunay, SK (reprint author), Idiap Res Inst, Ctr Parc, Martigny, Switzerland.</td>
</tr>

<tr>
<td valign="top">TC </td><td>8</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>8</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380427900039</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Agiomyrgiannakis, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Agiomyrgiannakis, Yannis</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>VOCAINE THE VOCODER AND APPLICATIONS IN SPEECH SYNTHESIS</td>
</tr>

<tr>
<td valign="top">SO </td><td>2015 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>40th IEEE International Conference on Acoustics, Speech, and Signal
   Processing (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 19-24, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Brisbane, AUSTRALIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>vocoders; statistical parametric speech synthesis; text-to-speech;
   non-stationary; AM-FM; fast cosine generators; phase models;
   overlap-add; sinusoidal speech models</td>
</tr>

<tr>
<td valign="top">ID </td><td>PLUS NOISE MODEL</td>
</tr>

<tr>
<td valign="top">AB </td><td>Vocoders received renewed attention recently as basic components in speech synthesis applications such as voice transformation, voice conversion and statistical parametric speech synthesis. This paper presents a new vocoder synthesizer, referred to as Vocaine, that features a novel Amplitude Modulated-Frequency Modulated (AM-FM) speech model, a new way to synthesize non-stationary sinusoids using quadratic phase splines and a super fast cosine generator. Extensive evaluations are made against several state-of-the-art methods in Copy-Synthesis and Text-To-Speech synthesis experiments. Vocaine matches or outperforms STRAIGHT in Copy-Synthesis experiments and outperforms our baseline real-time optimized Mixed-Excitation vocoder with the same computational cost. We report that Vocaine considerably improves our statistical TTS synthesizers and that our new statistical parametric synthesizer [1] matched the quality of our mature production Unit-Selection system with uncompressed waveforms.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Agiomyrgiannakis, Yannis] Google, Mountain View, CA 94043 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Agiomyrgiannakis, Y (reprint author), Google, Mountain View, CA 94043 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>agios@google.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>17</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>17</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>4230</td>
</tr>

<tr>
<td valign="top">EP </td><td>4234</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000427402904068</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Tian, XH
   <br>Wu, ZZ
   <br>Lee, SW
   <br>Hy, NQ
   <br>Chng, ES
   <br>Dong, MH</td>
</tr>

<tr>
<td valign="top">AF </td><td>Tian, Xiaohai
   <br>Wu, Zhizheng
   <br>Lee, Siu Wa
   <br>Nguyen Quy Hy
   <br>Chng, Eng Siong
   <br>Dong, Minghui</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>SPARSE REPRESENTATION FOR FREQUENCY WARPING BASED VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2015 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>40th IEEE International Conference on Acoustics, Speech, and Signal
   Processing (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 19-24, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Brisbane, AUSTRALIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; frequency warping; sparse representation; exemplar;
   residual compensation</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a sparse representation framework for weighted frequency warping based voice conversion. In this method, a frame-dependent warping function and the corresponding spectral residual vector are first calculated for each source-target spectrum pair. At runtime conversion, a source spectrum is factorised as a linear combination of a set of source spectra in the training data. The linear combination weight matrix, which is constrained to be sparse, is used to interpolate the frame-dependent warping functions and spectral residual vectors. In this way, the proposed method not only avoids the statistical averaging caused by GMM but also preserves the high-resolution spectral details for high-quality converted speech. Experiments are conducted on the VOICES database. Both objective and subjective results confirmed the effectiveness of the proposed method. In particular, the spectral distortion dropped from 5.55 dB of the conventional frequency warping approach to 5.0 dB of the proposed method. Compare to the state-of-the-art GMM-based conversion with global variance (GV) enhancement, our method achieved 68.5 % in an AB preference test.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Tian, Xiaohai; Nguyen Quy Hy; Chng, Eng Siong] Nanyang Technol Univ,
   Sch Comp Engn, Singapore, Singapore.
   <br>[Tian, Xiaohai; Nguyen Quy Hy; Chng, Eng Siong] Nanyang Technol Univ,
   Joint NTU UBC Res Ctr Excellence Act Living Elder, Singapore, Singapore.
   <br>[Wu, Zhizheng] Univ Edinburgh, Ctr Speech Technol Res, Edinburgh,
   Midlothian, Scotland.
   <br>[Lee, Siu Wa; Dong, Minghui] Inst Infocomm Res, Human Language Technol
   Dept, Singapore, Singapore.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Tian, XH (reprint author), Nanyang Technol Univ, Sch Comp Engn, Singapore, Singapore.</td>
</tr>

<tr>
<td valign="top">TC </td><td>4</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>4</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>4235</td>
</tr>

<tr>
<td valign="top">EP </td><td>4239</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000427402904069</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wu, ZZ
   <br>Khodabakhsh, A
   <br>Demiroglu, C
   <br>Yamagishi, J
   <br>Saito, D
   <br>Toda, T
   <br>King, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wu, Zhizheng
   <br>Khodabakhsh, Ali
   <br>Demiroglu, Cenk
   <br>Yamagishi, Junichi
   <br>Saito, Daisuke
   <br>Toda, Tomoki
   <br>King, Simon</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>SAS : A SPEAKER VERIFICATION SPOOFING DATABASE CONTAINING DIVERSE
   ATTACKS</td>
</tr>

<tr>
<td valign="top">SO </td><td>2015 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>40th IEEE International Conference on Acoustics, Speech, and Signal
   Processing (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 19-24, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Brisbane, AUSTRALIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Database; speaker verification; spoofing attack; security; speech
   synthesis; voice conversion</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents the first version of a speaker verification spoofing and anti-spoofing database, named SAS corpus. The corpus includes nine spoofing techniques, two of which are speech synthesis, and seven are voice conversion. We design two protocols, one for standard speaker verification evaluation, and the other for producing spoofing materials. Hence, they allow the speech synthesis community to produce spoofing materials incrementally without knowledge of speaker verification spoofing and anti-spoofing. To provide a set of preliminary results, we conducted speaker verification experiments using two state-of-the-art systems. Without any anti-spoofing techniques, the two systems are extremely vulnerable to the spoofing attacks implemented in our SAS corpus.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Wu, Zhizheng; Yamagishi, Junichi; King, Simon] Univ Edinburgh,
   Edinburgh, Midlothian, Scotland.
   <br>[Khodabakhsh, Ali; Demiroglu, Cenk] Ozyegin Univ, Istanbul, Turkey.
   <br>[Yamagishi, Junichi] Natl Inst Informat, Tokyo, Japan.
   <br>[Saito, Daisuke] Univ Tokyo, Tokyo, Japan.
   <br>[Toda, Tomoki] Nara Inst Sci &amp; Technol, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Wu, ZZ (reprint author), Univ Edinburgh, Edinburgh, Midlothian, Scotland.</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Khodabakhsh, Ali</display_name>&nbsp;</font></td><td><font size="3">0000-0002-2873-4140&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>23</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>23</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>4440</td>
</tr>

<tr>
<td valign="top">EP </td><td>4444</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000427402904110</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Liu, LJ
   <br>Chen, LH
   <br>Ling, ZH
   <br>Dai, LR</td>
</tr>

<tr>
<td valign="top">AF </td><td>Liu, Li-Juan
   <br>Chen, Ling-Hui
   <br>Ling, Zhen-Hua
   <br>Dai, Li-Rong</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>SPECTRAL CONVERSION USING DEEP NEURAL NETWORKS TRAINED WITH MULTI-SOURCE
   SPEAKERS</td>
</tr>

<tr>
<td valign="top">SO </td><td>2015 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>40th IEEE International Conference on Acoustics, Speech, and Signal
   Processing (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 19-24, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Brisbane, AUSTRALIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; deep neural networks; source-speaker-independent
   mapping</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION; BELIEF NETS</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a method for voice conversion using deep neural networks (DNNs) trained with multiple source speakers. The proposed DNNs can be used in two ways for different scenarios: 1) in the absence of training data for source speaker, the DNNs can be treated as source-speaker-independent models and perform conversions directly from arbitrary source speakers to certain target speaker; 2) the DNNs can also be used as initial models for further fine-tuning of source-speaker-dependent DNNs when parallel training data for both source and target speakers are available. Experimental results show that, as source-speaker-independent models, the proposed DNNs can achieve comparable performance to conventional source-speaker-dependent models. On the other hand, the proposed method outperforms the conventional initialization method with restricted Boltzmann machines (RBMs).</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Liu, Li-Juan; Chen, Ling-Hui; Ling, Zhen-Hua; Dai, Li-Rong] Univ Sci &amp;
   Technol China, Natl Engn Lab Speech &amp; Language Informat Proc, Hefei,
   Anhui, Peoples R China.
   <br>[Chen, Ling-Hui] iFLYTEK Co Ltd, iFLYTEK Res, Hefei, Anhui, Peoples R
   China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Liu, LJ (reprint author), Univ Sci &amp; Technol China, Natl Engn Lab Speech &amp; Language Informat Proc, Hefei, Anhui, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ljliu037@mail.ustc.edu.cn; chenlh@ustc.edu.cn; zhling@ustc.edu.cn;
   lrdai@ustc.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>5</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>5</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>4849</td>
</tr>

<tr>
<td valign="top">EP </td><td>4853</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000427402904192</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Takamichi, S
   <br>Toda, T
   <br>Black, AW
   <br>Nakamura, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Takamichi, Shinnosuke
   <br>Toda, Tomoki
   <br>Black, Alan W.
   <br>Nakamura, Satoshi</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>MODULATION SPECTRUM-CONSTRAINED TRAJECTORY TRAINING ALGORITHM FOR
   GMM-BASED VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2015 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>40th IEEE International Conference on Acoustics, Speech, and Signal
   Processing (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 19-24, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Brisbane, AUSTRALIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>GMM-based voice conversion; over-smoothing; modulation spectrum.
   training algorithm</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a novel training algorithm for Gaussian Mixture Model (GMM) -based Voice Conversion (VC). One of the advantages of GMM-based VC is computationally efficient conversion processing enabling to achieve real-time VC applications. On the other hand, the quality of the converted speech is still significantly worse than that of natural speech. In order to address this problem while preserving the computationally efficient conversion processing, the proposed training method enables 1) to use a consistent optimization criterion between training and conversion and 2) to compensate a Modulation Spectrum (MS) of the converted parameter trajectory as a feature sensitively correlated with over-smoothing effects causing quality degradation of the converted speech. The experimental results demonstrate that the proposed algorithm yields significant improvements in term of both the converted speech quality and the conversion accuracy for speaker individuality compared to the basic training algorithm.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Takamichi, Shinnosuke; Toda, Tomoki; Nakamura, Satoshi] Nara Inst Sci &amp;
   Technol NAIST, Grad Sch Informat Sci, Ikoma, Japan.
   <br>[Takamichi, Shinnosuke; Black, Alan W.] Carnegie Mellon Univ, Language
   Technol Inst, Pittsburgh, PA 15213 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Takamichi, S (reprint author), Nara Inst Sci &amp; Technol NAIST, Grad Sch Informat Sci, Ikoma, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>shinnosuke-t@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>8</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>8</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>4859</td>
</tr>

<tr>
<td valign="top">EP </td><td>4863</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000427402904194</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sun, LF
   <br>Kang, SY
   <br>Li, K
   <br>Meng, HL</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sun, Lifa
   <br>Kang, Shiyin
   <br>Li, Kun
   <br>Meng, Helen</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>VOICE CONVERSION USING DEEP BIDIRECTIONAL LONG SHORT-TERM MEMORY BASED
   RECURRENT NEURAL NETWORKS</td>
</tr>

<tr>
<td valign="top">SO </td><td>2015 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>40th IEEE International Conference on Acoustics, Speech, and Signal
   Processing (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 19-24, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Brisbane, AUSTRALIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; bidirectional long short-term memory; recurrent neural
   networks; dynamic features</td>
</tr>

<tr>
<td valign="top">ID </td><td>LSTM</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper investigates the use of Deep Bidirectional Long Short-Term Memory based Recurrent Neural Networks (DBLSTM-RNNs) for voice conversion. Temporal correlations across speech frames are not directly modeled in frame-based methods using conventional Deep Neural Networks (DNNs), which results in a limited quality of the converted speech. To improve the naturalness and continuity of the speech output in voice conversion, we propose a sequence-based conversion method using DBLSTM-RNNs to model not only the frame-wised relationship between the source and the target voice, but also the long-range context-dependencies in the acoustic trajectory. Experiments show that DBLSTM-RNNs outperform DNNs where Mean Opinion Scores are 3.2 and 2.3 respectively. Also, DBLSTM-RNNs without dynamic features have better performance than DNNs with dynamic features.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Sun, Lifa; Kang, Shiyin; Li, Kun; Meng, Helen] Chinese Univ Hong Kong,
   Dept Syst Engn &amp; Engn Management, Human Comp Commun Lab, Hong Kong, Hong
   Kong, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sun, LF (reprint author), Chinese Univ Hong Kong, Dept Syst Engn &amp; Engn Management, Human Comp Commun Lab, Hong Kong, Hong Kong, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>lfsun@se.cuhk.edu.hk; sykang@se.cuhk.edu.hk; kli@se.cuhk.edu.hk;
   hmmeng@se.cuhk.edu.hk</td>
</tr>

<tr>
<td valign="top">TC </td><td>38</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>39</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>4869</td>
</tr>

<tr>
<td valign="top">EP </td><td>4873</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000427402904196</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Aihara, R
   <br>Takiguchi, T
   <br>Ariki, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Aihara, Ryo
   <br>Takiguchi, Tetsuya
   <br>Ariki, Yasuo</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>ACTIVITY-MAPPING NON-NEGATIVE MATRIX FACTORIZATION FOR EXEMPLAR-BASED
   VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2015 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>40th IEEE International Conference on Acoustics, Speech, and Signal
   Processing (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 19-24, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Brisbane, AUSTRALIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; sparse representation; non-negative matrix
   factorization; NMF</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPARSE REPRESENTATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion (VC) is being widely researched in the field of speech processing because of increased interest in using such processing in applications such as personalized Text-To-Speech systems. We present in this paper an exemplar-based VC method using Non-negative Matrix Factorization (NMF), which is different from conventional statistical VC. In our previous exemplar-based VC method, input speech is represented by the source dictionary and its sparse coefficients. The source and the target dictionaries are fully coupled and the converted voice is constructed from the source coefficients and the target dictionary. In this paper, we propose an Activity-mapping NMF approach and introduce mapping matrices between source and target sparse coefficients. The effectiveness of this method was confirmed by comparing its effectiveness with that of a conventional Gaussian Mixture Model (GMM)-based method and a conventional NMF-based method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Aihara, Ryo; Takiguchi, Tetsuya; Ariki, Yasuo] Kobe Univ, Grad Sch Syst
   Informat, Kobe, Hyogo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Aihara, R (reprint author), Kobe Univ, Grad Sch Syst Informat, Kobe, Hyogo, Japan.</td>
</tr>

<tr>
<td valign="top">TC </td><td>5</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>7</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>4899</td>
</tr>

<tr>
<td valign="top">EP </td><td>4903</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000427402905003</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Eksler, V
   <br>Jelinek, M
   <br>Salami, R</td>
</tr>

<tr>
<td valign="top">AF </td><td>Eksler, Vaclav
   <br>Jelinek, Milan
   <br>Salami, Redwan</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>EFFICIENT HANDLING OF MODE SWITCHING AND SPEECH TRANSITIONS IN THE EVS
   CODEC</td>
</tr>

<tr>
<td valign="top">SO </td><td>2015 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>40th IEEE International Conference on Acoustics, Speech, and Signal
   Processing (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 19-24, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Brisbane, AUSTRALIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speech coding; ACELP; LP filter; transitions; EVS</td>
</tr>

<tr>
<td valign="top">AB </td><td>The recently standardized codec for Enhanced Voice Services (EVS) consists of a number of modes to achieve its high coding flexibility. In this paper we focus on techniques that enable a seamless switching between two linear prediction based modes running at different sampling rates within this codec. The first one deals with an efficient conversion of the linear prediction filter coefficients. The other one is based on a constrained-memory ACELP called transition coding (TC) that significantly limits the inter-frame long-term dependency. We show that the use of TC can be successfully extended to improve quality also in coding other transitions, e.g. strong onsets of voiced speech.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Eksler, Vaclav; Jelinek, Milan] Univ Sherbrooke, Sherbrooke, PQ, Canada.
   <br>[Eksler, Vaclav; Jelinek, Milan; Salami, Redwan] VoiceAge Corp,
   Montreal, PQ, Canada.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Eksler, V (reprint author), Univ Sherbrooke, Sherbrooke, PQ, Canada.; Eksler, V (reprint author), VoiceAge Corp, Montreal, PQ, Canada.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>5918</td>
</tr>

<tr>
<td valign="top">EP </td><td>5921</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000427402906008</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Bhuyan, AK
   <br>Nirmal, JH</td>
</tr>

<tr>
<td valign="top">AF </td><td>Bhuyan, Amit Kumar
   <br>Nirmal, Jagannath H.</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Comparative Study of Voice Conversion Framework with Line Spectral
   Frequency and Mel- Frequency Cepstral Coefficients as Features Using
   Artficial Neural Networks</td>
</tr>

<tr>
<td valign="top">SO </td><td>2015 INTERNATIONAL CONFERENCE ON COMPUTERS, COMMUNICATIONS, AND SYSTEMS
   (ICCCS)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Computers, Communications and Systems
   (ICCCS)</td>
</tr>

<tr>
<td valign="top">CY </td><td>NOV 02-03, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Kanyakumari, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice Conversion; Artificial Neural Network; Line Spectral Frequencies;
   Linear Predictive Coefficient; Mel Frequency Cepstral Coefficients;
   Cepstrum</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper is intended to formulate the mapping function using Feed-forward Neural Networks on Line Spectral Frequency and Mel Frequency Cepstral Coefficient and to compare their outcomes to decipher the better solution to the spectral mapping impediment. The experimentation is confined to the augmentation of spectral and excitation (glottal) domains of speech. LSF and MFCC are used to represent the spectrum and as input predictor variables to the above mentioned neural networks. It contains the use of neural network in the voice conversion framework. The function of artificial neural network is to map the spectral characteristics of a source speaker to the target speaker so as to obtain an authenticated voice conversion model. The temporal alignment of the speech uttered by source and the target is attained using Dynamic Time Warping (Dynamic Programming). The excitation mapping is accomplished using Residual Selection method. The performances of these Voice Conversion systems are assessed using subjective and objective measures which assure the genuineness of the conversion system design.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Bhuyan, Amit Kumar; Nirmal, Jagannath H.] KJ Somaiya Coll Engn, Dept
   Elect Engn, Bombay, Maharashtra, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Bhuyan, AK (reprint author), KJ Somaiya Coll Engn, Dept Elect Engn, Bombay, Maharashtra, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>amit.bhuyan@somaiya.edu; jhnirmal@somaiya.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>230</td>
</tr>

<tr>
<td valign="top">EP </td><td>235</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000455049100045</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ishida, K
   <br>Shabanpour, R
   <br>Meister, T
   <br>Boroujeni, K
   <br>Carta, C
   <br>Petti, L
   <br>Munzenrieder, N
   <br>Salvatore, GA
   <br>Troster, G
   <br>Ellinger, F</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ishida, K.
   <br>Shabanpour, R.
   <br>Meister, T.
   <br>Boroujeni, K.
   <br>Carta, C.
   <br>Petti, L.
   <br>Muenzenrieder, N.
   <br>Salvatore, G. A.
   <br>Troester, G.
   <br>Ellinger, F.</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>15 dB Conversion Gain, 20 MHz Carrier Frequency AM Receiver in Flexible
   a-IGZO TFT Technology with Textile Antennas</td>
</tr>

<tr>
<td valign="top">SO </td><td>2015 SYMPOSIUM ON VLSI CIRCUITS (VLSI CIRCUITS)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Symposium on VLSI Circuits-Digest of Papers</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>29th Symposium on VLSI Circuits (VLSI Circuits)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUN 16-19, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Kyoto, JAPAN</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents an AM receiver implemented in a flexible a-IGZO TFT technology. The circuit consists of a four-stage cascode amplifier at the RF input, a detector based on a source follower, and a common source circuit for the baseband amplification. The measured conversion gain is very flat and exceeds 15 dB from 2 to 20 MHz carrier frequency range, which covers a relevant portion of the shortwave radio band. The 3 dB-bandwidth of the audio signal is 400 Hz to 10 kHz, which is comparable to the so-called voice band, and it is also suitable to low-rate data communication. In addition, an integrated demonstration of the AM receiver and textile antennas is carried out. The flexible a-IGZO receiver successfully detected the baseband signal through the textile antennas, demonstrating for the first time wireless transmission for this class of technologies.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ishida, K.; Shabanpour, R.; Meister, T.; Boroujeni, K.; Carta, C.;
   Ellinger, F.] Tech Univ Dresden, D-01062 Dresden, Germany.
   <br>[Petti, L.; Muenzenrieder, N.; Salvatore, G. A.; Troester, G.] Swiss Fed
   Inst Technol, Zurich, Switzerland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ishida, K (reprint author), Tech Univ Dresden, D-01062 Dresden, Germany.</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Carta, Corrado</display_name>&nbsp;</font></td><td><font size="3">I-9096-2019&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Salvatore, Giovanni Antonio</display_name>&nbsp;</font></td><td><font size="3">G-6966-2016&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Carta, Corrado</display_name>&nbsp;</font></td><td><font size="3">0000-0001-9147-0160&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Salvatore, Giovanni Antonio</display_name>&nbsp;</font></td><td><font size="3">0000-0002-8983-3257&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000370961400073</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Garlapati, BM
   <br>Chalamala, SR
   <br>Kakkirala, KR</td>
</tr>

<tr>
<td valign="top">AF </td><td>Garlapati, Bala Mallikarjunarao
   <br>Chalamala, Srinivasa Rao
   <br>Kakkirala, Krishna Rao</td>
</tr>

<tr>
<td valign="top">BE </td><td>AlDabass, D
   <br>Saad, I
   <br>Bolong, N
   <br>Hijazi, MHA</td>
</tr>

<tr>
<td valign="top">TI </td><td>Tamper Detection in Speech based Access Control Systems using
   Watermarking</td>
</tr>

<tr>
<td valign="top">SO </td><td>2015 THIRD INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE,
   MODELLING AND SIMULATION (AIMS 2015)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Artificial Intelligence Modelling &amp;
   Simulation</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>3rd International Conference on Artificial Intelligence, Modelling and
   Simulation (AIMS)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 02-04, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Kota Kinabalu, MALAYSIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>audio watermarking; DA-AD attack; tamper detection; access control</td>
</tr>

<tr>
<td valign="top">AB </td><td>General voice based access control systems are based on voice biometrics. This process enables an unauthorized access by recording the voice of the authorized person. So there is a requirement to prevent unauthorized access through recording speech. Other than voice biometrics, here we have two challenges. (i) To extract the authentication information. (ii) To find the unauthorized source. The speech goes through DA-AD-DA conversion, while it is recorded and used for access control. The watermarking method which will use for this purpose must be robust to DA-AD conversion attack, which is usually involved in recordings. In this work, we propose a method based on casting Log Co-ordinate Mapping (LCM), in which embedding two watermark segments in two different frequency regions, one for authentication information purpose and other for finding unauthorized source. The LCM method has approving performance against DA-AD conversion attacks [ 1]. The modifications made for this does not impact the perceptible auditory quality and the embedding capacity improved by selecting the appropriate frequency regions in the log scale. Our results show that our method robustly extracts the source identification information while detecting the malicious source if the audio is being recorded and played back by unauthorized source.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Garlapati, Bala Mallikarjunarao; Chalamala, Srinivasa Rao; Kakkirala,
   Krishna Rao] TCS Innovat Labs, Hyderabad, Andhra Pradesh, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Garlapati, BM (reprint author), TCS Innovat Labs, Hyderabad, Andhra Pradesh, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>balamallikarjuna.g@tcs.com; chalamala.srao@tcs.com;
   krishna.kakkirala@tcs.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>325</td>
</tr>

<tr>
<td valign="top">EP </td><td>331</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/AIMS.2015.59</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000397991500055</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Shivakumar, KM
   <br>Aravind, KG
   <br>Anoop, TV
   <br>DeepaGupta</td>
</tr>

<tr>
<td valign="top">AF </td><td>Shivakumar, K. M.
   <br>Aravind, K. G.
   <br>Anoop, T., V
   <br>DeepaGupta</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Kannada Speech to Text Conversion Using CMU Sphinx</td>
</tr>

<tr>
<td valign="top">SO </td><td>2016 INTERNATIONAL CONFERENCE ON INVENTIVE COMPUTATION TECHNOLOGIES
   (ICICT), VOL 3</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Inventive Computation Technologies
   (ICICT)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 26-27, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Coimbatore, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speech to text conversion; Speech Recognition; CMUSphinx-4; Acoustic
   modeling; Phoneme Language model</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper investigates the complex problem of speech to text conversion of Kannada Language. We propose a novel Kannada Automated Speech to Text conversion System (ASTC). We train and test the Speech Processing System using CMUSphinx framework. CMU Sphinx is dynamic in nature with support for other languages along with English. We train the Acoustic model for Kannada speech with 1000 general spoken sentences and tested 150 sentences. We build our system utilizing features available in CMU Sphinx, thus showcasing the conceivable flexibility of this framework for Kannada voice to text conversion. In this paper, Kannada sentences with four to ten word length is researched. The speech conversion system permits ordinary people to speak to the computer in order to retrieve information in textual form. The number of alphabets in Kannada are 52. The system investigates extensibility of recognizing all letters and morphological variants of spoken Kannada words.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Shivakumar, K. M.; Aravind, K. G.; Anoop, T., V] Amrita Univ, Amrita
   Vishwa Vidyapeetham Univ, Dept Comp Sci, Mysore Campus, Mysuru, India.
   <br>[DeepaGupta] Amrita Univ, Amrita Vishwavidyapeetham Univ, Amrita Sch
   Engn, Dept Math, Bengaluru, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Shivakumar, KM (reprint author), Amrita Univ, Amrita Vishwa Vidyapeetham Univ, Dept Comp Sci, Mysore Campus, Mysuru, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>shivadvg19@yahoo.com; g_deepa@blr.amrita.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>199</td>
</tr>

<tr>
<td valign="top">EP </td><td>204</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000414242300043</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Phung, TN
   <br>Do, HK
   <br>Nguyen, VT
   <br>Thai, QV</td>
</tr>

<tr>
<td valign="top">AF </td><td>Trung-Nghia Phung
   <br>Huy-Khoi Do
   <br>Van-Tao Nguyen
   <br>Quang-Vinh Thai</td>
</tr>

<tr>
<td valign="top">TI </td><td>Eigennoise Speech Recovery in Adverse Environments with Joint
   Compensation of Additive and Convolutive Noise</td>
</tr>

<tr>
<td valign="top">SO </td><td>ADVANCES IN ACOUSTICS AND VIBRATION</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">AB </td><td>The learning-based speech recovery approach using statistical spectral conversion has been used for some kind of distorted speech as alaryngeal speech and body-conducted speech (or bone-conducted speech). This approach attempts to recover clean speech (undistorted speech) from noisy speech (distorted speech) by converting the statistical models of noisy speech into that of clean speech without the prior knowledge on characteristics and distributions of noise source. Presently, this approach has still not attractedmany researchers to apply in general noisy speech enhancement because of some major problems: those are the difficulties of noise adaptation and the lack of noise robust synthesizable features in different noisy environments. In this paper, we adopted the methods of state-of-the-art voice conversions and speaker adaptation in speech recognition to the proposed speech recovery approach applied in different kinds of noisy environment, especially in adverse environments with joint compensation of additive and convolutive noises. We proposed to use the decorrelated wavelet packet coefficients as a low-dimensional robust synthesizable feature under noisy environments. We also proposed a noise adaptation for speech recovery with the eigennoise similar to the eigenvoice in voice conversion. The experimental results showed that the proposed approach highly outperformed traditional nonlearning-based approaches.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Trung-Nghia Phung; Huy-Khoi Do; Van-Tao Nguyen; Quang-Vinh Thai] Thai
   Nguyen Univ Informat &amp; Commun Technol, Thai Nguyen 250000, Vietnam.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Phung, TN (reprint author), Thai Nguyen Univ Informat &amp; Commun Technol, Thai Nguyen 250000, Vietnam.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ptnghia@ictu.edu.vn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">AR </td><td>170183</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1155/2015/170183</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000214618500001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Togawa, T
   <br>Otani, T
   <br>Suzuki, K
   <br>Taniguchi, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Togawa, Taro
   <br>Otani, Takeshi
   <br>Suzuki, Kaori
   <br>Taniguchi, Tomohiko</td>
</tr>

<tr>
<td valign="top">TI </td><td>Development of speech technologies to support hearing through mobile
   terminal users</td>
</tr>

<tr>
<td valign="top">SO </td><td>APSIPA TRANSACTIONS ON SIGNAL AND INFORMATION PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice enhancement; Speech rate conversion; Speech quality control;
   Hearing compensation</td>
</tr>

<tr>
<td valign="top">ID </td><td>ENHANCEMENT</td>
</tr>

<tr>
<td valign="top">AB </td><td>Mobile terminals have become the most familiar communication tool we use, and various types of people have come to use mobile terminals in various environments. Accordingly, situations in which we talk over the telephone in noisy environments or with someone who speaks fast have increased. However, it is sometimes difficult to hear a person's voice in these cases. To make the voice received through mobile terminals easy to hear, authors have developed two technologies. One is a voice enhancement technology that emphasizes a caller's voice according to the noise surrounding the recipient, and the other is a speech rate conversion technology that slows speech while maintaining voice quality. In this paper, we explain the trends and the features of these technologies and discuss ways to implement their algorithms on mobile terminals.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Togawa, Taro; Otani, Takeshi] Fujitsu Labs Ltd, Media Proc Labs,
   Kawasaki, Kanagawa, Japan.
   <br>[Suzuki, Kaori] Fujitsu Ltd, Adv Technol Div, Ubiquitous Business
   Strategy Unit, Kawasaki, Kanagawa, Japan.
   <br>[Taniguchi, Tomohiko] Fujitsu Labs Ltd, Network Syst Labs, Kawasaki,
   Kanagawa, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Otani, T (reprint author), Fujitsu Labs Ltd, Media Proc Labs, Kawasaki, Kanagawa, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>otani.takeshi@jp.fujitsu.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">VL </td><td>4</td>
</tr>

<tr>
<td valign="top">AR </td><td>e14</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1017/ATSIP.2015.3</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000215263000014</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>de Obalda, C</td>
</tr>

<tr>
<td valign="top">AF </td><td>de Obalda, Carlos</td>
</tr>

<tr>
<td valign="top">BE </td><td>Svensson, P
   <br>Kristiansen, U</td>
</tr>

<tr>
<td valign="top">TI </td><td>VOWEL CONVERSION BY PHONETIC SEGMENTATION</td>
</tr>

<tr>
<td valign="top">SO </td><td>DAFX-15: PROCEEDINGS OF THE 18TH INTERNATIONAL CONFERENCE ON DIGITAL
   AUDIO EFFECTS</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Digital Audio Effects</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>18th International Conference on Digital Audio Effects (DAFx)</td>
</tr>

<tr>
<td valign="top">CY </td><td>NOV 30-DEC 03, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Trondheim, NORWAY</td>
</tr>

<tr>
<td valign="top">ID </td><td>PLUS NOISE MODEL; SPEECH; VOICE</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper a system for vowel conversion between different speakers using short-time speech segments is presented. The input speech signal is segmented into period-length speech segments whose fundamental frequency and first two formants are used to find the perceivable vowel-quality. These segments are used to represent a voiced phoneme, i.e. a vowel. The approach relies on pitch-synchronous analysis and uses a modified PSOLA technique for concatenation of the vowel segments. Vowel conversion between speakers is achieved by exchanging the phonetic constituents of a source speaker's speech waveform in voiced regions of speech whilst preserving prosodic features of the source speaker, thus introducing a method for phonetic segmentation, mapping, and reconstruction of vowels.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[de Obalda, Carlos] Helmut Schmidt Univ, Hamburg, Germany.</td>
</tr>

<tr>
<td valign="top">RP </td><td>de Obalda, C (reprint author), Helmut Schmidt Univ, Hamburg, Germany.</td>
</tr>

<tr>
<td valign="top">EM </td><td>deobaldia@hsu-hh.de</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>300</td>
</tr>

<tr>
<td valign="top">EP </td><td>306</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000432610400044</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Li, L
   <br>Que, DS</td>
</tr>

<tr>
<td valign="top">AF </td><td>Li Lei
   <br>Que Dashun</td>
</tr>

<tr>
<td valign="top">BE </td><td>Jianping, C
   <br>Juan, W</td>
</tr>

<tr>
<td valign="top">TI </td><td>Design of data-glove and chinese sign language recognition system based
   on ARM9</td>
</tr>

<tr>
<td valign="top">SO </td><td>PROCEEDINGS OF 2015 IEEE 12TH INTERNATIONAL CONFERENCE ON ELECTRONIC
   MEASUREMENT &amp; INSTRUMENTS (ICEMI), VOL. 3</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>12th IEEE International Conference on Electronic Measurement and
   Instruments (ICEMI)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUL 16-18, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Qingdao, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>data glove; ARM; sign language recognition; sensor; portable</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents the design and implementation of data glove and sign language recognition system based on an ARM9 and combined flex sensors with 9-axis IMU sensor. Not only can it measure bending degree of fingers as well as hand gesture, but can recognize simple sign language. The use of serial port or Bluetooth transmitted to the embedded systems make the device more portable. Through Real-time collection of data and time domain analysis, getting corresponding gesture by matching the data in the processor. This system has realized the gestures to voice and text real-time conversion to help the deaf-mute communicate with outside world more easily. This system also has the characteristics of portable, scalable and high efficiency of sign language recognition.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Li Lei; Que Dashun] Wuhan Univ Technol, Sch Informat Engn, Wuhan
   430070, Hubei, Peoples R China.
   <br>[Que Dashun] Wuhan Univ Technol, Key Lab Fiber Opt Sensing Technol &amp;
   Informat Proc, Wuhan 430070, Hubei, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Li, L (reprint author), Wuhan Univ Technol, Sch Informat Engn, Wuhan 430070, Hubei, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>leestone@whut.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>7</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>7</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>1130</td>
</tr>

<tr>
<td valign="top">EP </td><td>1134</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Instruments &amp; Instrumentation</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000414630300005</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Pribil, J
   <br>Pribilova, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Pribil, Jiri
   <br>Pribilova, Anna</td>
</tr>

<tr>
<td valign="top">TI </td><td>GMM-Based Evaluation of Emotional Style Transformation in Czech and
   Slovak</td>
</tr>

<tr>
<td valign="top">SO </td><td>COGNITIVE COMPUTATION</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Emotional speech transformation; Spectral and prosodic features of
   speech; GMM-based emotion classification</td>
</tr>

<tr>
<td valign="top">ID </td><td>GAUSSIAN MIXTURE-MODELS; SUPPORT VECTOR MACHINES; SPEAKER
   IDENTIFICATION; RECOGNITION; SPEECH; CLASSIFICATION; FEATURES</td>
</tr>

<tr>
<td valign="top">AB </td><td>In the development of the voice conversion and the emotional speech style transformation in the text-to-speech systems, it is very important to obtain feedback information about the users' opinion on the resulting synthetic speech quality. For this reason, the evaluations of the quality of the produced synthetic speech must often be performed for comparison. The main aim of the experiments described in this paper was to find out whether the classifier based on Gaussian mixture models (GMMs) could be applied for evaluation of male and female resynthesized speech that had been transformed from neutral to four emotional states (joy, surprise, sadness, and anger) spoken in Czech and Slovak languages. We suppose that it is possible to combine this GMM-based statistical evaluation with the classical one in the form of listening tests or it can replace them. For verification of our working hypothesis, a simple GMM emotional speech classifier with a one-level structure was realized. The next task of the performed experiment was to investigate the influence of different types and values (mean, median, standard deviation, relative maximum, etc.) of the used speech features (spectral and/or supra-segmental) on the GMM classification accuracy. The obtained GMM evaluation scores are compared with the results of the conventional listening tests based on the mean opinion scores. In addition, correctness of the GMM classification is analyzed with respect to the influence of the setting of the parameters during the GMM training-the number of mixture components and the types of speech features. The paper also describes the comparison experiment with the reference speech corpus taken from the Berlin database of emotional speech in German language as the benchmark for the evaluation of the performance of our one-level GMM classifier. The obtained results confirm practical usability of the developed GMM classifier, so we will continue in this research with the aim to increase the classification accuracy and compare it with other approaches like the support vector machines.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Pribil, Jiri] Slovak Acad Sci, Inst Measurement Sci, Bratislava 84104,
   Slovakia.
   <br>[Pribilova, Anna] SUT, Inst Elect &amp; Photon, Fac Elect Engn &amp; Informat
   Technol, Bratislava 81219, Slovakia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Pribil, J (reprint author), Slovak Acad Sci, Inst Measurement Sci, Dubravska Cesta 9, Bratislava 84104, Slovakia.</td>
</tr>

<tr>
<td valign="top">EM </td><td>Jiri.Pribil@savba.sk</td>
</tr>

<tr>
<td valign="top">TC </td><td>6</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>6</td>
</tr>

<tr>
<td valign="top">PD </td><td>DEC</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">VL </td><td>6</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">SI </td><td>SI</td>
</tr>

<tr>
<td valign="top">BP </td><td>928</td>
</tr>

<tr>
<td valign="top">EP </td><td>939</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/s12559-014-9283-y</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Neurosciences &amp; Neurology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000345994900023</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Chen, LH
   <br>Ling, ZH
   <br>Liu, LJ
   <br>Dai, LR</td>
</tr>

<tr>
<td valign="top">AF </td><td>Chen, Ling-Hui
   <br>Ling, Zhen-Hua
   <br>Liu, Li-Juan
   <br>Dai, Li-Rong</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Conversion Using Deep Neural Networks With Layer-Wise Generative
   Training</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Bidirectional associative memory; deep neural network; Gaussian mixture
   model; restricted Boltzmann machine; spectral envelope conversion; voice
   conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH SYNTHESIS; ENHANCEMENT; HMM</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a new spectral envelope conversion method using deep neural networks (DNNs). The conventional joint density Gaussian mixture model (JDGMM) based spectral conversion methods perform stably and effectively. However, the speech generated by these methods suffer severe quality degradation due to the following two factors: 1) inadequacy of JDGMM in modeling the distribution of spectral features as well as the non-linear mapping relationship between the source and target speakers, 2) spectral detail loss caused by the use of high-level spectral features such as mel-cepstra. Previously, we have proposed to use the mixture of restricted Boltzmann machines (MoRBM) and the mixture of Gaussian bidirectional associative memories (MoGBAM) to cope with these problems. In this paper, we propose to use a DNN to construct a global non-linear mapping relationship between the spectral envelopes of two speakers. The proposed DNN is generatively trained by cascading two RBMs, which model the distributions of spectral envelopes of source and target speakers respectively, using a Bernoulli BAM (BBAM). Therefore, the proposed training method takes the advantage of the strong modeling ability of RBMs in modeling the distribution of spectral envelopes and the superiority of BAMs in deriving the conditional distributions for conversion. Careful comparisons and analysis among the proposed method and some conventional methods are presented in this paper. The subjective results show that the proposed method can significantly improve the performance in terms of both similarity and naturalness compared to conventional methods.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Chen, Ling-Hui; Ling, Zhen-Hua; Liu, Li-Juan; Dai, Li-Rong] Univ Sci &amp;
   Technol China, Natl Engn Lab Speech &amp; Language Informat Proc, Hefei
   230027, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ling, ZH (reprint author), Univ Sci &amp; Technol China, Natl Engn Lab Speech &amp; Language Informat Proc, Hefei 230027, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>chenlh@ustc.edu.cn; zhling@ustc.edu.cn; ljliu037@mail.ustc.edu.cn;
   lrdai@ustc.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>81</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>83</td>
</tr>

<tr>
<td valign="top">PD </td><td>DEC</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">VL </td><td>22</td>
</tr>

<tr>
<td valign="top">IS </td><td>12</td>
</tr>

<tr>
<td valign="top">BP </td><td>1859</td>
</tr>

<tr>
<td valign="top">EP </td><td>1872</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASLP.2014.2353991</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000341627500015</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Li, JJ
   <br>McLoughlin, IV
   <br>Dai, LR
   <br>Ling, ZH</td>
</tr>

<tr>
<td valign="top">AF </td><td>Li, Jing-jie
   <br>McLoughlin, Ian V.
   <br>Dai, Li-Rong
   <br>Ling, Zhen-hua</td>
</tr>

<tr>
<td valign="top">TI </td><td>Whisper-to-speech conversion using restricted Boltzmann machine arrays</td>
</tr>

<tr>
<td valign="top">SO </td><td>ELECTRONICS LETTERS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">AB </td><td>Whispers are a natural vocal communication mechanism, in which vocal cords do not vibrate normally. Lack of glottal-induced pitch leads to low energy, and an inherent noise-like spectral distribution reduces intelligibility. Much research has been devoted to processing of whispers, including conversion of whispers to speech. Unfortunately, among several approaches, the best reconstructed speech to date still contains obviously artificial muffles and suffers from an unnatural prosody. To address these issues, the novel use of multiple restricted Boltzmann machines (RBMs) is reported as a statistical conversion model between whisper and speech spectral envelopes. Moreover, the accuracy of estimated pitch is improved using machine learning techniques for pitch estimation within only voiced (V) regions. Both objective and subjective evaluations show that this new method improves the quality of whisper-reconstructed speech compared with the state-of-the-art approaches.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Li, Jing-jie; McLoughlin, Ian V.; Dai, Li-Rong; Ling, Zhen-hua] Univ
   Sci &amp; Technol China, Hefei 230026, Anhui, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Li, JJ (reprint author), Univ Sci &amp; Technol China, Hefei 230026, Anhui, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ivm@ustc.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>7</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>8</td>
</tr>

<tr>
<td valign="top">PD </td><td>NOV 20</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">VL </td><td>50</td>
</tr>

<tr>
<td valign="top">IS </td><td>24</td>
</tr>

<tr>
<td valign="top">BP </td><td>1781</td>
</tr>

<tr>
<td valign="top">EP </td><td>U141</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1049/el.2014.1645</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000345695100004</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Gao, WX
   <br>Cao, QY
   <br>Qian, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Gao, Weixun
   <br>Cao, Qiying
   <br>Qian, Yao</td>
</tr>

<tr>
<td valign="top">TI </td><td>Cross-Dialectal Voice Conversion with Neural Networks</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEICE TRANSACTIONS ON INFORMATION AND SYSTEMS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; neural network; cross-dialectal; frequency warping;
   pre-training; sequence training</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH SYNTHESIS; REPRESENTATIONS; SELECTION</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we use neural networks (NNs) for cross-dialectal (Mandarin-Shanghainese) voice conversion using a bi-dialectal speakers' recordings. This system employs a nonlinear mapping function, which is trained by parallel mandarin features of source and target speakers, to convert source speaker's Shanghainese features to those of target speaker. This study investigates three training aspects: a) Frequency warping, which is supposed to be language independent; b) Pre-training, which drives weights to a better starting point than random initialization or be regarded as unsupervised feature learning; and c) Sequence training, which minimizes sequence-level errors and matches objectives used in training and converting. Experimental results show that the performance of cross-dialectal voice conversion is close to that of intra-dialectal. This benefit is likely from the strong learning capabilities of NNs, e.g., exploiting feature correlations between fundamental frequency (F0) and spectrum. The objective measures: log spectral distortion (LSD) and root mean squared error (RMSE) of F0, both show that pre-training and sequence training outperform the frame-level mean square error (MSE) training. The naturalness of the converted Shanghainese speech and the similarity between converted Shanghainese speech and target Mandarin speech are significantly improved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Gao, Weixun] Donghua Univ, Sch Informat Sci &amp; Technol, Shanghai,
   Peoples R China.
   <br>[Cao, Qiying] Donghua Univ, Coll Comp Sci &amp; Technol, Shanghai, Peoples R
   China.
   <br>[Qian, Yao] Microsoft Res Asia, Speech Grp, Beijing, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Gao, WX (reprint author), Donghua Univ, Sch Informat Sci &amp; Technol, Shanghai, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>gwx@shnu.edu.cn</td>
</tr>

<tr xmlns:date="http://exslt.org/dates-and-times">
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Cao, Qiying</display_name>&nbsp;</font></td><td><font size="3">0000-0001-7976-6364&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>NOV</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">VL </td><td>E97D</td>
</tr>

<tr>
<td valign="top">IS </td><td>11</td>
</tr>

<tr>
<td valign="top">BP </td><td>2872</td>
</tr>

<tr>
<td valign="top">EP </td><td>2880</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1587/transinf.2014EDP7116</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000348141300006</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Martins, RHG
   <br>Tavares, ELM
   <br>Ranalli, PF
   <br>Branco, A
   <br>Pessin, ABB</td>
</tr>

<tr>
<td valign="top">AF </td><td>Garcia Martins, Regina Helena
   <br>Mendes Tavares, Elaine Lara
   <br>Ranalli, Paula Ferreira
   <br>Branco, Anete
   <br>Benito Pessin, Adriana Bueno</td>
</tr>

<tr>
<td valign="top">TI </td><td>Psychogenic dysphonia: diversity of clinical and vocal manifestations in
   a case series</td>
</tr>

<tr>
<td valign="top">SO </td><td>BRAZILIAN JOURNAL OF OTORHINOLARYNGOLOGY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice disorders; Clinical psychology; Psychopathology; Psychotherapy</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE DISORDERS; DIAGNOSIS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Introduction: Psychogenic dysphonia is a functional disorder with variable clinical manifestations.
   <br>Objective: To assess the clinical and vocal characteristics of patients with psychogenic dysphonia in a case series.
   <br>Methods: The study included 28 adult patients with psychogenic dysphonia, evaluated at a University hospital in the last ten years. Assessed variables included gender, age, occupation, vocal symptoms, vocal characteristics, and videolaryngostroboscopic findings.
   <br>Results: 28 patients (26 women and 2 men) were assessed. Their occupations included: housekeeper (n = 17), teacher (n = 4), salesclerk (n = 4), nurse (n = 1), retired (n = 1), and psychologist (n = 1). Sudden symptom onset was reported by 16 patients and progressive symptom onset was reported by 12; intermittent evolution was reported by 15; symptom duration longer than three months was reported by 21 patients. Videolaryngostroboscopy showed only functional disorders; no patient had structural lesions or changes in vocal fold mobility. Conversion aphonia, skeletal muscle tension, and intermittent voicing were the most frequent vocal emission manifestation forms.
   <br>Conclusions: In this case series of patients with psychogenic dysphonia, the most frequent form of clinical presentation was conversion aphonia, followed by musculoskeletal tension and intermittent voicing. The clinical and vocal aspects of 28 patients with psychogenic dysphonia, as well as the particularities of each case, are discussed. (C) 2014 Associacao Brasileira de Otorrinolaringologia e Cirurgia Cervico-Facial. Published by Elsevier Editora Ltda. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Garcia Martins, Regina Helena; Mendes Tavares, Elaine Lara; Ranalli,
   Paula Ferreira; Branco, Anete; Benito Pessin, Adriana Bueno] Univ
   Estadual Paulista, UNESP, Fac Med Botucatu, Botucatu, SP, Brazil.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Martins, RHG (reprint author), Univ Estadual Paulista, UNESP, Fac Med Botucatu, Botucatu, SP, Brazil.</td>
</tr>

<tr>
<td valign="top">EM </td><td>rmartins@fmb.unesp.br</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Martins, Regina Helena Garcia</display_name>&nbsp;</font></td><td><font size="3">M-8119-2013&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Martins, Regina Helena Garcia</display_name>&nbsp;</font></td><td><font size="3">0000-0003-0772-1962&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>4</td>
</tr>

<tr>
<td valign="top">PD </td><td>NOV-DEC</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">VL </td><td>80</td>
</tr>

<tr>
<td valign="top">IS </td><td>6</td>
</tr>

<tr>
<td valign="top">BP </td><td>497</td>
</tr>

<tr>
<td valign="top">EP </td><td>502</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.bjorl.2014.09.002</td>
</tr>

<tr>
<td valign="top">SC </td><td>Otorhinolaryngology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000347129800008</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nirmal, J
   <br>Zaveri, M
   <br>Patnaik, S
   <br>Kachare, P</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nirmal, Jagannath
   <br>Zaveri, Mukesh
   <br>Patnaik, Suprava
   <br>Kachare, Pramod</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice conversion using General Regression Neural Network</td>
</tr>

<tr>
<td valign="top">SO </td><td>APPLIED SOFT COMPUTING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Gaussian Mixture Model; General Regression Neural Network; Pitch
   contour; Radial Basis Function; Voice conversion; Wavelet transform</td>
</tr>

<tr>
<td valign="top">ID </td><td>TRANSFORMATION; SIGNALS; FRAMEWORK; FEATURES; SYSTEM; BRAIN</td>
</tr>

<tr>
<td valign="top">AB </td><td>The objective of voice conversion system is to formulate the mapping function which can transform the source speaker characteristics to that of the target speaker. In this paper, we propose the General Regression Neural Network (GRNN) based model for voice conversion. It is a single pass learning network that makes the training procedure fast and comparatively less time consuming. The proposed system uses the shape of the vocal tract, the shape of the glottal pulse (excitation signal) and long term prosodic features to carry out the voice conversion task. In this paper, the shape of the vocal tract and the shape of source excitation of a particular speaker are represented using Line Spectral Frequencies (LSFs) and Linear Prediction (LP) residual respectively. GRNN is used to obtain the mapping function between the source and target speakers. The direct transformation of the time domain residual using Artificial Neural Network (ANN) causes phase change and generates artifacts in consecutive frames. In order to alleviate it, wavelet packet decomposed coefficients are used to characterize the excitation of the speech signal. The long term prosodic parameters namely, pitch contour (intonation) and the energy profile of the test signal are also modified in relation to that of the target (desired) speaker using the baseline method. The relative performances of the proposed model are compared to voice conversion system based on the state of the art RBF and GMM models using objective and subjective evaluation measures. The evaluation measures show that the proposed GRNN based voice conversion system performs slightly better than the state of the art models. (C) 2014 Elsevier B.V. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nirmal, Jagannath] KJSCOE, Dept Elect Engn, Mumbai, Maharashtra, India.
   <br>[Zaveri, Mukesh] SVNIT, Dept Comp Engn, Surat, India.
   <br>[Patnaik, Suprava; Kachare, Pramod] SVNIT, Dept Elect Engn, Surat, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nirmal, J (reprint author), KJSCOE, Dept Elect Engn, Mumbai, Maharashtra, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jhnirmal@somaiya.edu</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Patnaik, Suprava</display_name>&nbsp;</font></td><td><font size="3">0000-0002-7068-5960&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>15</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>17</td>
</tr>

<tr>
<td valign="top">PD </td><td>NOV</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">VL </td><td>24</td>
</tr>

<tr>
<td valign="top">BP </td><td>1</td>
</tr>

<tr>
<td valign="top">EP </td><td>12</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.asoc.2014.06.040</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000343138500001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Miller, AE
   <br>Wolinsky, JS
   <br>Kappos, L
   <br>Comi, G
   <br>Freedman, MS
   <br>Olsson, TP
   <br>Bauer, D
   <br>Benamor, M
   <br>Truffinet, P
   <br>O'Connor, PW</td>
</tr>

<tr>
<td valign="top">AF </td><td>Miller, Aaron E.
   <br>Wolinsky, Jerry S.
   <br>Kappos, Ludwig
   <br>Comi, Giancarlo
   <br>Freedman, Mark S.
   <br>Olsson, Tomas P.
   <br>Bauer, Deborah
   <br>Benamor, Myriam
   <br>Truffinet, Philippe
   <br>O'Connor, Paul W.</td>
</tr>

<tr>
<td valign="top">CA </td><td>TOPIC Study Grp</td>
</tr>

<tr>
<td valign="top">TI </td><td>Oral teriflunomide for patients with a first clinical episode suggestive
   of multiple sclerosis (TOPIC): a randomised, double-blind,
   placebo-controlled, phase 3 trial</td>
</tr>

<tr>
<td valign="top">SO </td><td>LANCET NEUROLOGY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">ID </td><td>INTRAMUSCULAR INTERFERON BETA-1A; DEMYELINATING EVENT; FOLLOW-UP;
   DEFINITE; CONVERSION; OUTCOMES</td>
</tr>

<tr>
<td valign="top">AB </td><td>Background Teriflunomide is a once-daily oral immunomodulator approved for the treatment of relapsing-remitting multiple sclerosis. We aimed to assess the efficacy and safety of teriflunomide in patients with a first clinical episode suggestive of multiple sclerosis.
   <br>Methods In this randomised, double-blind, placebo-controlled, parallel-group study, we enrolled patients aged 18-55 years with clinically isolated syndrome (defined as a neurological event consistent with demyelination, starting within 90 days of randomisation, and two or more T2-weighted MRI lesions &gt;= 3 mm in diameter) from 112 centres (mostly hospitals) in 20 countries. Participants were randomly assigned (1:1:1) in a double-blind manner (by an interactive voice response system) to once-daily oral teriflunomide 14 mg, teriflunomide 7 mg, or placebo, for up to 108 weeks. Patients, staff administering the interventions, and outcome assessors were masked to treatment assignment. The primary endpoint was time to relapse (a new neurological abnormality separated by &gt;= 30 days from a preceding clinical event, present for &gt;= 24 h in the absence of fever or known infection), which defined conversion to clinically definite multiple sclerosis. The key secondary endpoint was time to relapse or new gadolinium-enhancing or T2 lesions on MM, whichever occurred first. The primary outcome was analysed for the modified intention-to-treat population; safety analyses included all randomised patients who were exposed to the study drug, as treated. This trial is registered with ClinicalTrials.gov, number NCT00622700.
   <br>Findings Between Feb 13,2008, and Aug 22,2012,618 patients were enrolled and randomly assigned to teriflunomide 14 mg (n=216), teriflunomide 7 mg (n=205), or placebo (n=197). Two patients in each of the teriflunomide groups did not receive the study drug, so the modified intention-to-treat population comprised 214 patients in the teriflunomide 14 mg group, 203 in the teriflunomide 7 mg group, and 197 in the placebo group. Compared with placebo, teriflunomide significantly reduced the risk of relapse defining clinically definite multiple sclerosis at the 14 mg dose (hazard ratio [HR] 0.574 [95% CI 0.379-0.869]; p=0.0087) and at the 7 mg dose (0.628 [0.416-0.949]; p=0.0271). Teriflunomide reduced the risk of relapse or a new MRI lesion compared with placebo at the 14 mg dose (HR 0.651 [95% CI 0.515-0.822]; p=0.0003) and at the 7 mg dose (0.686 [0.540-0.871]; p=0.0020). During the study, six patients who were randomly assigned to placebo accidently also received terifiunomide at some point: four received 7 mg and two received 14 mg. Therefore, the safety population comprised 216 patients on teriflunomide 14 mg, 207 on teriflunomide 7 mg, and 191 on placebo. Adverse events that occurred in at least 10% of patients in either teriflunomide group and with an incidence that was at least 2% higher than that with placebo were increased alanine aminotransferase (40 [19%] of 216 patients in the 14 mg group, 36 [17%] of 207 in the 7 mg group vs 27 [14%] of 191 in the placebo group), hair thinning (25 [12%] and 12 [6%] vs 15 [8%]), diarrhoea (23 [11%] and 28 [14%] vs 12 [6%]), paraesthesia (22 [10%] and 11 [5%] vs 10 [5%]), and upper respiratory tract infection (20 [9%] and 23 [11%] vs 14 [7%]). The most common serious adverse event was an increase in alanine aminotransferase (four [2%] and five [2%] vs three [2%]).
   <br>Interpretation TOPIC is to our knowledge the first study to report benefits of an available oral disease-modifying therapy in patients with early multiple sclerosis. These results extend the stages of multiple sclerosis in which teriflunomide shows a beneficial effect.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Miller, Aaron E.] Icahn Sch Med Mt Sinai, New York, NY 10029 USA.
   <br>[Wolinsky, Jerry S.] Univ Texas Hlth Sci Ctr Houston, Houston, TX 77030
   USA.
   <br>[Kappos, Ludwig] Univ Basel Hosp, CH-4031 Basel, Switzerland.
   <br>[Comi, Giancarlo] Univ Vita Salute San Raffaele, Milan, Italy.
   <br>[Freedman, Mark S.] Univ Ottawa, Ottawa, ON, Canada.
   <br>[Freedman, Mark S.] Ottawa Hosp, Res Inst, Ottawa, ON, Canada.
   <br>[Olsson, Tomas P.] Karolinska Inst, Stockholm, Sweden.
   <br>[Bauer, Deborah] Sanofi, Bridgewater, NJ USA.
   <br>[Benamor, Myriam; Truffinet, Philippe] Sanofi Co, Genzyme, Chilly
   Mazarin, France.
   <br>[O'Connor, Paul W.] Univ Toronto, Toronto, ON, Canada.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Miller, AE (reprint author), Icahn Sch Med Mt Sinai, Corinne Goldsmith Dickinson Ctr Multiple Sclerosi, 5 East 98th St Box 1138, New York, NY 10029 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>aaron.miller@mssm.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>112</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>115</td>
</tr>

<tr>
<td valign="top">PD </td><td>OCT</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">VL </td><td>13</td>
</tr>

<tr>
<td valign="top">IS </td><td>10</td>
</tr>

<tr>
<td valign="top">BP </td><td>977</td>
</tr>

<tr>
<td valign="top">EP </td><td>986</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/S1474-4422(14)70191-7</td>
</tr>

<tr>
<td valign="top">SC </td><td>Neurosciences &amp; Neurology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000341959700013</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wu, ZZ
   <br>Virtanen, T
   <br>Chng, ES
   <br>Li, HZ</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wu, Zhizheng
   <br>Virtanen, Tuomas
   <br>Chng, Eng Siong
   <br>Li, Haizhou</td>
</tr>

<tr>
<td valign="top">TI </td><td>Exemplar-Based Sparse Representation With Residual Compensation for
   Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Exemplar; nonnegative matrix factorization; residual compensation;
   sparse representation; voice conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>NONNEGATIVE MATRIX FACTORIZATION; ARTIFICIAL NEURAL-NETWORKS;
   LEAST-SQUARES REGRESSION; TO-SPEECH SYNTHESIS; TRANSFORMATION;
   RECOGNITION; SELECTION</td>
</tr>

<tr>
<td valign="top">AB </td><td>We propose a nonparametric framework for voice conversion, that is, exemplar-based sparse representation with residual compensation. In this framework, a spectrogram is reconstructed as a weighted linear combination of speech segments, called exemplars, which span multiple consecutive frames. The linear combination weights are constrained to be sparse to avoid over-smoothing, and high-resolution spectra are employed in the exemplars directly without dimensionality reduction to maintain spectral details. In addition, a spectral compression factor and a residual compensation technique are included in the framework to enhance the conversion performances. We conducted experiments on the VOICES database to compare the proposed method with a large set of state-of-the-art baseline methods, including the maximum likelihood Gaussian mixture model (ML-GMM) with dynamic feature constraint and the partial least squares (PLS) regression based methods. The experimental results show that the objective spectral distortion ofML-GMMis reduced from 5.19 dB to 4.92 dB, and both the subjective mean opinion score and the speaker identification rate are increased from 2.49 and 73.50% to 3.15 and 79.50%, respectively, by the proposed method. The results also show the superiority of our method over PLS-based methods. In addition, the subjective listening tests indicate that the naturalness of the converted speech by our proposed method is comparable with that by the ML-GMM method with global variance constraint.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Wu, Zhizheng; Chng, Eng Siong] Nanyang Technol Univ, Sch Comp Engn,
   Singapore 639798, Singapore.
   <br>[Wu, Zhizheng; Chng, Eng Siong] Nanyang Technol Univ, NTU, Temasek Lab,
   Singapore 639798, Singapore.
   <br>[Virtanen, Tuomas] Tampere Univ Technol, Dept Signal Proc, FI-33101
   Tampere, Finland.
   <br>[Li, Haizhou] Inst Infocomm Res, Singapore 138632, Singapore.
   <br>[Li, Haizhou] Nanyang Technol Univ, Sch Comp Engn, Singapore 639798,
   Singapore.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Wu, ZZ (reprint author), Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.</td>
</tr>

<tr>
<td valign="top">EM </td><td>wuzz@ntu.edu.sg; tuomas.virtanen@tut.fi; aseschng@ntu.edu.sg;
   hli@i2r.a-star.edu.sg</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Li, Haizhou</display_name>&nbsp;</font></td><td><font size="3">Q-6438-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Li, Haizhou</display_name>&nbsp;</font></td><td><font size="3">0000-0001-9158-9401&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>69</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>71</td>
</tr>

<tr>
<td valign="top">PD </td><td>OCT</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">VL </td><td>22</td>
</tr>

<tr>
<td valign="top">IS </td><td>10</td>
</tr>

<tr>
<td valign="top">BP </td><td>1506</td>
</tr>

<tr>
<td valign="top">EP </td><td>1521</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASLP.2014.2333242</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000340037200005</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Chen, CP
   <br>Huang, YC
   <br>Wu, CH
   <br>Lee, KD</td>
</tr>

<tr>
<td valign="top">AF </td><td>Chen, Chia-Ping
   <br>Huang, Yi-Chin
   <br>Wu, Chung-Hsien
   <br>Lee, Kuan-De</td>
</tr>

<tr>
<td valign="top">TI </td><td>Polyglot Speech Synthesis Based on Cross-Lingual Frame Selection Using
   Auditory and Articulatory Features</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Articulatory features; auditory features; cross-lingual frame selection;
   polyglot speech synthesis</td>
</tr>

<tr>
<td valign="top">ID </td><td>HIDDEN MARKOV-MODELS; VOICE CONVERSION; MAXIMUM-LIKELIHOOD; HMM;
   RECOGNITION; GENERATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, an approach for polyglot speech synthesis based on cross-lingual frame selection is proposed. This method requires only mono-lingual speech data of different speakers in different languages for building a polyglot synthesis system, thus reducing the burden of data collection. Essentially, a set of artificial utterances in the second language for a target speaker is constructed based on the proposed cross-lingual frame-selection process, and this data set is used to adapt a synthesis model in the second language to the speaker. In the cross-lingual frame-selection process, we propose to use auditory and articulatory features to improve the quality of the synthesized polyglot speech. For evaluation, a Mandarin-English polyglot system is implemented where the target speaker only speaks Mandarin. The results show that decent performance regarding voice identity and speech quality can be achieved with the proposed method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Chen, Chia-Ping] Natl Sun Yat Sen Univ, Dept Comp Sci &amp; Engn, Kaohsiung
   800, Taiwan.
   <br>[Huang, Yi-Chin; Wu, Chung-Hsien; Lee, Kuan-De] Natl Cheng Kung Univ,
   Dept Comp Sci &amp; Informat Engn, Tainan 701, Taiwan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Wu, CH (reprint author), Natl Cheng Kung Univ, Dept Comp Sci &amp; Informat Engn, Tainan 701, Taiwan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>chwu@csie.ncku.edu.tw</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Wu, Chung-Hsien</display_name>&nbsp;</font></td><td><font size="3">E-7970-2013&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>7</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>7</td>
</tr>

<tr>
<td valign="top">PD </td><td>OCT</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">VL </td><td>22</td>
</tr>

<tr>
<td valign="top">IS </td><td>10</td>
</tr>

<tr>
<td valign="top">BP </td><td>1558</td>
</tr>

<tr>
<td valign="top">EP </td><td>1570</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASLP.2014.2339738</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000340037200009</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Mills, T
   <br>Bunnell, HT
   <br>Patel, R</td>
</tr>

<tr>
<td valign="top">AF </td><td>Mills, Timothy
   <br>Bunnell, H. Timothy
   <br>Patel, Rupal</td>
</tr>

<tr>
<td valign="top">TI </td><td>Towards Personalized Speech Synthesis for Augmentative and Alternative
   Communication</td>
</tr>

<tr>
<td valign="top">SO </td><td>AUGMENTATIVE AND ALTERNATIVE COMMUNICATION</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speech synthesis; Assistive communication; Assistive technology; Speaker
   identity; Voice conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>QUESTION-STATEMENT CONTRAST; ARTIFICIAL NEURAL-NETWORKS; SEXUAL
   ORIENTATION; VOICE CONVERSION; SEVERE DYSARTHRIA; SPEAKER ADAPTATION;
   CEREBRAL-PALSY; GLOTTAL WAVE; TALKER; IDENTIFICATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Text-to-speech options on augmentative and alternative communication (AAC) devices are limited. Often, several individuals in a group setting use the same synthetic voice. This lack of customization may limit technology adoption and social integration. This paper describes our efforts to generate personalized synthesis for users with profoundly limited speech motor control. Existing voice banking and voice conversion techniques rely on recordings of clearly articulated speech from the target talker, which cannot be obtained from this population. Our VocaliD approach extracts prosodic properties from the target talker ' s source function and applies these features to a surrogate talker ' s database, generating a synthetic voice with the vocal identity of the target talker and the clarity of the surrogate talker. Promising intelligibility results suggest areas of further development for improved personalization.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Mills, Timothy; Patel, Rupal] Northeastern Univ, Dept Speech Language
   Pathol &amp; Audiol, Boston, MA 02115 USA.
   <br>[Bunnell, H. Timothy] Nemours Biomed Res, Wilmington, DE USA.
   <br>[Patel, Rupal] Northeastern Univ, Dept Speech Language Pathol &amp; Audiol &amp;
   Comp Sci, Boston, MA 02115 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Patel, R (reprint author), Northeastern Univ, Dept Speech Language Pathol &amp; Audiol, Boston, MA 02115 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>r.patel@neu.edu</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Bunnell, H Timothy</display_name>&nbsp;</font></td><td><font size="3">0000-0002-3099-7572&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>11</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>11</td>
</tr>

<tr>
<td valign="top">PD </td><td>SEP</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">VL </td><td>30</td>
</tr>

<tr>
<td valign="top">IS </td><td>3</td>
</tr>

<tr>
<td valign="top">BP </td><td>226</td>
</tr>

<tr>
<td valign="top">EP </td><td>236</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.3109/07434618.2014.924026</td>
</tr>

<tr>
<td valign="top">SC </td><td>Audiology &amp; Speech-Language Pathology; Rehabilitation</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000341302500004</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Huber, S
   <br>Roebel, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Huber, Stefan
   <br>Roebel, Axel</td>
</tr>

<tr>
<td valign="top">TI </td><td>On the use of voice descriptors for glottal source shape parameter
   estimation</td>
</tr>

<tr>
<td valign="top">SO </td><td>COMPUTER SPEECH AND LANGUAGE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Glottal source; Voice quality; R-d shape parameter; LF model; Viterbi
   smoothing</td>
</tr>

<tr>
<td valign="top">ID </td><td>WAVE-FORM ANALYSIS; OPEN QUOTIENT; AIR-FLOW; ELECTROGLOTTOGRAPHIC
   SIGNALS; SPEECH SYNTHESIS; MODEL; PHONATION; QUALITY; PARAMETRIZATION;
   ALGORITHM</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper summarizes the results of our investigations into estimating the shape of the glottal excitation source from speech signals. We employ the Liljencrants-Fant (LF) model describing the glottal flow and its derivative. The one-dimensional glottal source shape parameter R-d describes the transition in voice quality from a tense to a breathy voice. The parameter R-d has been derived from a statistical regression of the R waveshape parameters which parameterize the LF model. First, we introduce a variant of our recently proposed adaptation and range extension of the R-d parameter regression. Secondly, we discuss in detail the aspects of estimating the glottal source shape parameter R-d using the phase minimization paradigm. Based on the analysis of a large number of speech signals we describe the major conditions that are likely to result in erroneous R-d estimates. Based on these findings we investigate into means to increase the robustness of the R-d parameter estimation. We use Viterbi smoothing to suppress unnatural jumps of the estimated R-d parameter contours within short time segments. Additionally, we propose to steer the Viterbi algorithm by exploiting the covariation of other voice descriptors to improve Viterbi smoothing. The novel Viterbi steering is based on a Gaussian Mixture Model (GMM) that represents the joint density of the voice descriptors and the Open Quotient (OQ) estimated from corresponding electroglottographic (EGG) signals. A conversion function derived from the mixture model predicts OQ from the voice descriptors. Converted to R-d it defines an additional prior probability to adapt the partial probabilities of the Viterbi algorithm accordingly. Finally, we evaluate the performances of the phase minimization based methods using both variants to adapt and extent the R-d regression on one synthetic test set as well as in combination with Viterbi smoothing and each variant of the novel Viterbi steering on one test set of natural speech. The experimental findings exhibit improvements for both Viterbi approaches. (C) 2013 Elsevier Ltd. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Huber, Stefan; Roebel, Axel] IRCAM CNRS UPMC STMS, Sound Anal Synth
   Team, F-75004 Paris, France.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Huber, S (reprint author), IRCAM CNRS UPMC STMS, Sound Anal Synth Team, 1 Pl Igor Stravinsky, F-75004 Paris, France.</td>
</tr>

<tr>
<td valign="top">EM </td><td>stefan.huber@ircam.fr; axel.roebel@ircam.fr</td>
</tr>

<tr>
<td valign="top">TC </td><td>6</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>6</td>
</tr>

<tr>
<td valign="top">PD </td><td>SEP</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">VL </td><td>28</td>
</tr>

<tr>
<td valign="top">IS </td><td>5</td>
</tr>

<tr>
<td valign="top">SI </td><td>SI</td>
</tr>

<tr>
<td valign="top">BP </td><td>1170</td>
</tr>

<tr>
<td valign="top">EP </td><td>1194</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.csl.2013.09.006</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000338609100010</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>van der Mei, HC
   <br>Buijssen, KJDA
   <br>van der Laan, BFAM
   <br>Ovchinnikova, E
   <br>Geertsema-Doornbusch, GI
   <br>Atema-Smit, J
   <br>van de Belt-Gritter, B
   <br>Busscher, HJ</td>
</tr>

<tr>
<td valign="top">AF </td><td>van der Mei, Henny C.
   <br>Buijssen, Kevin J. D. A.
   <br>van der Laan, Bernard F. A. M.
   <br>Ovchinnikova, Ekatarina
   <br>Geertsema-Doornbusch, Gesinda I.
   <br>Atema-Smit, Jelly
   <br>van de Belt-Gritter, Betsy
   <br>Busscher, Henk J.</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Prosthetic Biofilm Formation and Candida Morphogenic Conversions
   in Absence and Presence of Different Bacterial Strains and Species on
   Silicone-Rubber</td>
</tr>

<tr>
<td valign="top">SO </td><td>PLOS ONE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">ID </td><td>IN-VITRO; ALBICANS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Morphogenic conversion of Candida from a yeast to hyphal morphology plays a pivotal role in the pathogenicity of Candida species. Both Candida albicans and Candida tropicalis, in combination with a variety of different bacterial strains and species, appear in biofilms on silicone-rubber voice prostheses used in laryngectomized patients. Here we study biofilm formation on silicone-rubber by C. albicans or C. tropicalis in combination with different commensal bacterial strains and lactobacillus strains. In addition, hyphal formation in C. albicans and C. tropicalis, as stimulated by Rothia dentocariosa and lactobacilli was evaluated, as clinical studies outlined that these bacterial strains have opposite results on the clinical life-time of silicone-rubber voice prostheses. Biofilms were grown during eight days in a silicone-rubber tube, while passing the biofilms through episodes of nutritional feast and famine. Biofilms consisting of combinations of C. albicans and a bacterial strain comprised significantly less viable organisms than combinations comprising C. tropicalis. High percentages of Candida were found in biofilms grown in combination with lactobacilli. Interestingly, L. casei, with demonstrated favorable effects on the clinical life-time of voice prostheses, reduced the percentage hyphal formation in Candida biofilms as compared with Candida biofilms grown in absence of bacteria or grown in combination with R. dentocariosa, a bacterial strain whose presence is associated with short clinical life-times of voice prostheses.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[van der Mei, Henny C.; Buijssen, Kevin J. D. A.; van der Laan, Bernard
   F. A. M.; Ovchinnikova, Ekatarina; Geertsema-Doornbusch, Gesinda I.;
   Atema-Smit, Jelly; van de Belt-Gritter, Betsy; Busscher, Henk J.] Univ
   Groningen, Univ Med Ctr Groningen, Dept Biomed Engn, NL-9713 AV
   Groningen, Netherlands.
   <br>[Buijssen, Kevin J. D. A.; van der Laan, Bernard F. A. M.] Univ
   Groningen, Univ Med Ctr Groningen, Dept Otorhinolaryngol &amp; Head &amp; Neck
   Surg, NL-9713 AV Groningen, Netherlands.</td>
</tr>

<tr>
<td valign="top">RP </td><td>van der Mei, HC (reprint author), Univ Groningen, Univ Med Ctr Groningen, Dept Biomed Engn, NL-9713 AV Groningen, Netherlands.</td>
</tr>

<tr>
<td valign="top">EM </td><td>h.c.van.der.mei@umcg.nl</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>van der Laan, Bernard</display_name>&nbsp;</font></td><td><font size="3">H-2420-2011&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>van der Laan, Bernard</display_name>&nbsp;</font></td><td><font size="3">O-7723-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>van der Laan, Bernard</display_name>&nbsp;</font></td><td><font size="3">0000-0002-5016-2871&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>van der Laan, Bernard</display_name>&nbsp;</font></td><td><font size="3">0000-0002-5016-2871&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>11</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>11</td>
</tr>

<tr>
<td valign="top">PD </td><td>AUG 11</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">VL </td><td>9</td>
</tr>

<tr>
<td valign="top">IS </td><td>8</td>
</tr>

<tr>
<td valign="top">AR </td><td>e104508</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1371/journal.pone.0104508</td>
</tr>

<tr>
<td valign="top">SC </td><td>Science &amp; Technology - Other Topics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000341105100063</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Gao, WX
   <br>Cao, QY</td>
</tr>

<tr>
<td valign="top">AF </td><td>Gao, Weixun
   <br>Cao, Qiying</td>
</tr>

<tr>
<td valign="top">TI </td><td>Frequency Warping for Speaker Adaptation in HMM-based Speech Synthesis</td>
</tr>

<tr>
<td valign="top">SO </td><td>JOURNAL OF INFORMATION SCIENCE AND ENGINEERING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>frequency warping; VTLN; speaker adaptation; HMM-based speech synthesis;
   MLLR</td>
</tr>

<tr>
<td valign="top">ID </td><td>TRACT LENGTH NORMALIZATION; VOICE CONVERSION; SYSTEM; RECOGNITION;
   ALGORITHMS; SELECTION; SPECTRUM; MODEL</td>
</tr>

<tr>
<td valign="top">AB </td><td>Speaker adaptation in speech synthesis transforms a source utterance to a target utterance that differs from the source in terms of voice characteristics. In this paper, we employ vocal tract length normalization, which is generally used in speech recognition to remove individual speaker characteristics, to speaker adaptation in speech synthesis. We propose a frequency warping approach based on a time-varying bilinear function to reduce the weighted spectral distance between the source speaker and the target speaker. The warped spectra of the source speaker are then converted to line spectrum pairs to train hidden Markov models (HMM). HMMs are further adapted by algorithms based on maximum likelihood linear regression with the target speaker's data. The experimental results show that our frequency warping approach can make the warped spectra of the source speaker closer to the target speaker, and the resultant adapted HMMs perform better than the HMMs trained by unwrapped spectra in terms of synthesized speech naturalness and speaker similarity.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Gao, Weixun; Cao, Qiying] Donghua Univ, Sch Informat Sci &amp; Technol,
   Shanghai 200051, Peoples R China.
   <br>[Cao, Qiying] Donghua Univ, Coll Comp Sci &amp; Technol, Shanghai 200051,
   Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Gao, WX (reprint author), Donghua Univ, Sch Informat Sci &amp; Technol, Shanghai 200051, Peoples R China.</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUL</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">VL </td><td>30</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">BP </td><td>1149</td>
</tr>

<tr>
<td valign="top">EP </td><td>1166</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000339464800013</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Muller, S
   <br>Wechsler, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Mueller, Stefan
   <br>Wechsler, Stephen</td>
</tr>

<tr>
<td valign="top">TI </td><td>Lexical approaches to argument structure</td>
</tr>

<tr>
<td valign="top">SO </td><td>THEORETICAL LINGUISTICS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>argument structure; lexicalism; valency; coordination; construction
   grammar; HPSG; GPSG; categorial grammar; neo-Davidsonianism; morphology;
   syntax; language acquisition; coercion; statistics</td>
</tr>

<tr>
<td valign="top">ID </td><td>GRAMMATICAL CONSTRUCTIONS; LANGUAGE FACULTY; ENGLISH; GERMAN; RULES;
   VERBS; ACQUISITION; CONSTRAINTS; PROJECTION; GRAMMARS</td>
</tr>

<tr>
<td valign="top">AB </td><td>In lexical approaches to argument structure, lexical items include argument structures. The argument structure represents essential information about potential argument selection and expression, but abstracts away from the actual local phrasal structure. In contrast, phrasal approaches, which are common in Construction Grammar, reject such lexical argument structures. We present evidence for lexical approaches and against phrasal ones: Lexical argument structure is necessary to explain idiosyncratic lexical selection of arguments. Abstraction from phrase structure and word order is shown by passive voice, category conversions, word-level coordination, and partial fronting. Lexical argument structure simplifies the grammar by allowing componential analysis. The phrasal alternative relies on the multiple inheritance of constructions, which is fraught with unsolved problems. Putative evidence for the phrasal approach from acquisition, psycholinguistics, and statistical distribution either fails to distinguish the two approaches, or supports the lexical approach. We conclude in favor of the lexical approach.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Mueller, Stefan] Free Univ Berlin, Inst Deutsch &amp; Niederland Philol,
   Berlin, Germany.
   <br>[Wechsler, Stephen] Univ Texas Austin, Dept Linguist, Austin, TX 78712
   USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Muller, S (reprint author), Free Univ Berlin, Inst Deutsch &amp; Niederland Philol, Berlin, Germany.</td>
</tr>

<tr>
<td valign="top">EM </td><td>Stefan.Mueller@fu-berlin.de; wechsler@austin.utexas.edu</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Muller, Stefan</display_name>&nbsp;</font></td><td><font size="3">A-3810-2010&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Muller, Stefan</display_name>&nbsp;</font></td><td><font size="3">0000-0003-4413-5313&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>16</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>16</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUL</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">VL </td><td>40</td>
</tr>

<tr>
<td valign="top">IS </td><td>1-2</td>
</tr>

<tr>
<td valign="top">BP </td><td>1</td>
</tr>

<tr>
<td valign="top">EP </td><td>76</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1515/tl-2014-0001</td>
</tr>

<tr>
<td valign="top">SC </td><td>Linguistics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000338712100001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nakashika, T
   <br>Takiguchi, T
   <br>Ariki, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nakashika, Toru
   <br>Takiguchi, Tetsuya
   <br>Ariki, Yasuo</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Conversion Based on Speaker-Dependent Restricted Boltzmann
   Machines</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEICE TRANSACTIONS ON INFORMATION AND SYSTEMS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; restricted Boltzmann machine; deep learning; speaker
   individuality</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH RECOGNITION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a voice conversion technique using speaker-dependent Restricted Boltzmann Machines (RBM) to build high-order eigen spaces of source/target speakers, where it is easier to convert the source speech to the target speech than in the traditional cepstrum space. We build a deep conversion architecture that concatenates the two speaker-dependent RBMs with neural networks, expecting that they automatically discover abstractions to express the original input features. Under this concept, if we train the RBMs using only the speech of an individual speaker that includes various phonemes while keeping the speaker individuality unchanged, it can be considered that there are fewer phonemes and relatively more speaker individuality in the output features of the hidden layer than original acoustic features. Training the RBMs for a source speaker and a target speaker, we can then connect and convert the speaker individuality abstractions using Neural Networks (NN). The converted abstraction of the source speaker is then back-propagated into the acoustic space (e.g., MFCC) using the RBM of the target speaker. We conducted speaker-voice conversion experiments and confirmed the efficacy of our method with respect to subjective and objective criteria, comparing it with the conventional Gaussian Mixture Model-based method and an ordinary NN.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nakashika, Toru; Takiguchi, Tetsuya; Ariki, Yasuo] Kobe Univ, Fac Syst
   Informat, Kobe, Hyogo 6578501, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nakashika, T (reprint author), Kobe Univ, Fac Syst Informat, Kobe, Hyogo 6578501, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>nakashika@me.cs.scitec.kobe-u.ac.jp; takigu@kobe-u.ac.jp;
   ariki@kobe-u.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>10</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>10</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">VL </td><td>E97D</td>
</tr>

<tr>
<td valign="top">IS </td><td>6</td>
</tr>

<tr>
<td valign="top">BP </td><td>1403</td>
</tr>

<tr>
<td valign="top">EP </td><td>1410</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1587/transinf.E97.D.1403</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000342784300002</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Aihara, R
   <br>Takashima, R
   <br>Takiguchi, T
   <br>Ariki, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Aihara, Ryo
   <br>Takashima, Ryoichi
   <br>Takiguchi, Tetsuya
   <br>Ariki, Yasuo</td>
</tr>

<tr>
<td valign="top">TI </td><td>Noise-Robust Voice Conversion Based on Sparse Spectral Mapping Using
   Non-negative Matrix Factorization</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEICE TRANSACTIONS ON INFORMATION AND SYSTEMS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; sparse representation; non-negative matrix
   factorization; noise robustness</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a voice conversion (VC) technique for noisy environments based on a sparse representation of speech. Sparse representation-based VC using Non-negative matrix factorization (NMF) is employed for noise-added spectral conversion between different speakers. In our previous exemplar-based VC method, source exemplars and target exemplars are extracted from parallel training data, having the same texts uttered by the source and target speakers. The input source signal is represented using the source exemplars and their weights. Then, the converted speech is constructed from the target exemplars and the weights related to the source exemplars. However, this exemplar-based approach needs to hold all training exemplars (frames), and it requires high computation times to obtain the weights of the source exemplars. In this paper, we propose a framework to train the basis matrices of the source and target exemplars so that they have a common weight matrix. By using the basis matrices instead of the exemplars, the VC is performed with lower computation times than with the exemplar-based method. The effectiveness of this method was confirmed by comparing its effectiveness (in speaker conversion experiments using noise-added speech data) with that of an exemplar-based method and a conventional Gaussian mixture model (GMM)-based method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Aihara, Ryo; Takashima, Ryoichi] Kobe Univ, Grad Sch Syst Informat,
   Kobe, Hyogo 6578501, Japan.
   <br>[Takiguchi, Tetsuya; Ariki, Yasuo] Kobe Univ, Org Adv Sci &amp; Technol,
   Kobe, Hyogo 6578501, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Aihara, R (reprint author), Kobe Univ, Grad Sch Syst Informat, Kobe, Hyogo 6578501, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>aihara@me.cs.scitec.kobe-u.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>9</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>11</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">VL </td><td>E97D</td>
</tr>

<tr>
<td valign="top">IS </td><td>6</td>
</tr>

<tr>
<td valign="top">BP </td><td>1411</td>
</tr>

<tr>
<td valign="top">EP </td><td>1418</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1587/transinf.E97.D.1411</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000342784300003</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kobayashi, K
   <br>Toda, T
   <br>Doi, H
   <br>Nakano, T
   <br>Goto, M
   <br>Neubig, G
   <br>Sakti, S
   <br>Nakamura, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kobayashi, Kazuhiro
   <br>Toda, Tomoki
   <br>Doi, Hironori
   <br>Nakano, Tomoyasu
   <br>Goto, Masataka
   <br>Neubig, Graham
   <br>Sakti, Sakriani
   <br>Nakamura, Satoshi</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Timbre Control Based on Perceived Age in Singing Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEICE TRANSACTIONS ON INFORMATION AND SYSTEMS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>singing voice; voice conversion; perceived age; spectral and prosodic
   features; subjective evaluations</td>
</tr>

<tr>
<td valign="top">ID </td><td>EXPRESSIVE SPEECH SYNTHESIS; STYLE CONTROL TECHNIQUE</td>
</tr>

<tr>
<td valign="top">AB </td><td>The perceived age of a singing voice is the age of the singer as perceived by the listener, and is one of the notable characteristics that determines perceptions of a song. In this paper, we describe an investigation of acoustic features that have an effect on the perceived age, and a novel voice timbre control technique based on the perceived age for singing voice conversion (SVC). Singers can sing expressively by controlling prosody and voice timbre, but the varieties of voices that singers can produce are limited by physical constraints. Previous work has attempted to overcome this limitation through the use of statistical voice conversion. This technique makes it possible to convert singing voice timbre of an arbitrary source singer into those of an arbitrary target singer. However, it is still difficult to intuitively control singing voice characteristics by manipulating parameters corresponding to specific physical traits, such as gender and age. In this paper, we first perform an investigation of the factors that play a part in the listener's perception of the singer's age at first. Then, we applied a multiple-regression Gaussian mixture models (MR-GMM) to SVC for the purpose of controlling voice timbre based on the perceived age and we propose SVC based on the modified MR-GMM for manipulating the perceived age while maintaining singer's individuality. The experimental results show that I) the perceived age of singing voices corresponds relatively well to the actual age of the singer, 2) prosodic features have a larger effect on the perceived age than spectral features, 3) the individuality of a singer is influenced more heavily by segmental features than prosodic features 4) the proposed voice timbre control method makes it possible to change the singer's perceived age while not having an adverse effect on the perceived individuality.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kobayashi, Kazuhiro; Toda, Tomoki; Doi, Hironori; Neubig, Graham;
   Sakti, Sakriani; Nakamura, Satoshi] Nara Inst Sci &amp; Technol NAIST, Ikoma
   6300192, Japan.
   <br>[Nakano, Tomoyasu; Goto, Masataka] Natl Inst Adv Ind Sci &amp; Technol,
   Tsukuba, Ibaraki 3058568, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kobayashi, K (reprint author), Nara Inst Sci &amp; Technol NAIST, Ikoma 6300192, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>kazuhiro-k@is.naist.jp</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Goto, Masataka</display_name>&nbsp;</font></td><td><font size="3">K-8205-2012&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Nakano, Tomoyasu</display_name>&nbsp;</font></td><td><font size="3">A-8670-2013&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Goto, Masataka</display_name>&nbsp;</font></td><td><font size="3">0000-0003-1167-0977&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Nakano, Tomoyasu</display_name>&nbsp;</font></td><td><font size="3">0000-0001-8014-2209&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>9</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>9</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">VL </td><td>E97D</td>
</tr>

<tr>
<td valign="top">IS </td><td>6</td>
</tr>

<tr>
<td valign="top">BP </td><td>1419</td>
</tr>

<tr>
<td valign="top">EP </td><td>1428</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1587/transinf.E97.D.1419</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000342784300004</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Tanaka, K
   <br>Toda, T
   <br>Neubig, G
   <br>Sakti, S
   <br>Nakamura, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Tanaka, Kou
   <br>Toda, Tomoki
   <br>Neubig, Graham
   <br>Sakti, Sakriani
   <br>Nakamura, Satoshi</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Hybrid Approach to Electrolaryngeal Speech Enhancement Based on Noise
   Reduction and Statistical Excitation Generation</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEICE TRANSACTIONS ON INFORMATION AND SYSTEMS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>speaking-aid; electrolaryngeal speech; spectral subtraction; voice
   conversion; hybrid approach</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents an electrolaryngeal (EL) speech enhancement method capable of significantly improving naturalness of EL speech while causing no degradation in its intelligibility. An electrolarynx is an external device that artificially generates excitation sounds to enable laryngectomees to produce EL speech. Although proficient laryngectomees can produce quite intelligible EL speech, it sounds very unnatural due to the mechanical excitation produced by the device. Moreover, the excitation sounds produced by the device often leak outside, adding to EL speech as noise. To address these issues, there are mainly two conventional approached to EL speech enhancement through either noise reduction or statistical voice conversion (VC). The former approach usually causes no degradation in intelligibility but yields only small improvements in naturalness as the mechanical excitation sounds remain essentially unchanged. On the other hand, the latter approach significantly improves naturalness of EL speech using spectral and excitation parameters of natural voices converted from acoustic parameters of EL speech, but it usually causes degradation in intelligibility owing to errors in conversion. We propose a hybrid approach using a noise reduction method for enhancing spectral parameters and statistical voice conversion method for predicting excitation parameters. Moreover, we further modify the prediction process of the excitation parameters to improve its prediction accuracy and reduce adverse effects caused by unvoiced/voiced prediction errors. The experimental results demonstrate the proposed method yields significant improvements in naturalness compared with EL speech while keeping intelligibility high enough.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Tanaka, Kou; Toda, Tomoki; Neubig, Graham; Sakti, Sakriani; Nakamura,
   Satoshi] Nara Inst Sci &amp; Technol NAIST, Ikoma 6300192, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Tanaka, K (reprint author), Nara Inst Sci &amp; Technol NAIST, Ikoma 6300192, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ko-t@is.naist.jp; tomoki@is.naist.jp; neubig@is.naist.jp;
   ssakti@is.naist.jp; s-nakamura@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>12</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>12</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">VL </td><td>E97D</td>
</tr>

<tr>
<td valign="top">IS </td><td>6</td>
</tr>

<tr>
<td valign="top">BP </td><td>1429</td>
</tr>

<tr>
<td valign="top">EP </td><td>1437</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1587/transinf.E97.D.1429</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000342784300005</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sarkar, S
   <br>Rao, KS</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sarkar, Sourjya
   <br>Rao, K. Sreenivasa</td>
</tr>

<tr>
<td valign="top">TI </td><td>Stochastic feature compensation methods for speaker verification in
   noisy environments</td>
</tr>

<tr>
<td valign="top">SO </td><td>APPLIED SOFT COMPUTING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speaker verification; Noisy environment; Minimum mean squared error;
   Maximum likelihood estimate; Expectation Maximization algorithm;
   Gaussian Mixture Models</td>
</tr>

<tr>
<td valign="top">ID </td><td>ROBUST SPEECH RECOGNITION; VECTOR NORMALIZATION; VOICE CONVERSION;
   ADAPTATION; IDENTIFICATION; MODELS</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper explores the significance of stereo-based stochastic feature compensation (SFC) methods for robust speaker verification (SV) in mismatched training and test environments. Gaussian Mixture Model (GMM)-based SFC methods developed in past has been solely restricted for speech recognition tasks. Application of these algorithms in a SV framework for background noise compensation is proposed in this paper. A priori knowledge about the test environment and availability of stereo training data is assumed. During the training phase, Mel frequency cepstral coefficient (MFCC) features extracted from a speaker's noisy and clean speech utterance (stereo data) are used to build front end GMMs. During the evaluation phase, noisy test utterances are transformed on the basis of a minimum mean squared error (MMSE) or maximum likelihood (MLE) estimate, using the target speaker GMMs. Experiments conducted on the NIST-2003-SRE database with clean speech utterances artificially degraded with different types of additive noises reveal that the proposed SV systems strictly outperform baseline SV systems in mismatched conditions across all noisy background environments. (C) 2014 Elsevier B.V. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Sarkar, Sourjya; Rao, K. Sreenivasa] Indian Inst Technol, Sch Informat
   Technol, Kharagpur 721302, W Bengal, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sarkar, S (reprint author), Indian Inst Technol, Sch Informat Technol, Kharagpur 721302, W Bengal, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>sourjyasarkar@gmail.com; ksrao@iitkgp.ac.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>14</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>14</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">VL </td><td>19</td>
</tr>

<tr>
<td valign="top">BP </td><td>198</td>
</tr>

<tr>
<td valign="top">EP </td><td>214</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.asoc.2014.02.016</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000334768800020</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Lee, KS</td>
</tr>

<tr>
<td valign="top">AF </td><td>Lee, Ki-Seung</td>
</tr>

<tr>
<td valign="top">TI </td><td>A unit selection approach for voice transformation</td>
</tr>

<tr>
<td valign="top">SO </td><td>SPEECH COMMUNICATION</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; Unit selection; Hidden Markov model</td>
</tr>

<tr>
<td valign="top">ID </td><td>TO-SPEECH SYNTHESIS; CONVERSION; RECOGNITION; ALGORITHM; FREQUENCY;
   NETWORKS; DESIGN; MODELS</td>
</tr>

<tr>
<td valign="top">AB </td><td>A voice transformation (VT) method that can make the utterance of a source speaker mimic that of a target speaker is described. Speaker individuality transformation is achieved by altering four feature parameters, which include the linear prediction coefficients cepstrum (LPCC), ALPCC, LP-residual and pitch period. The main objective of this study involves construction of an optimal sequence of features selected from a target speaker's database, to maximize both the correlation probabilities between the transformed and the source features and the likelihood of the transformed features with respect to the target model. A set of two-pass conversion rules is proposed, where the feature parameters are first selected from a database then the optimal sequence of the feature parameters is then constructed in the second pass. The conversion rules were developed using a statistical approach that employed a maximum likelihood criterion. In constructing an optimal sequence of the features, a hidden Markov model (HMM) with global control variables (GCV) was employed to find the most likely combination of the features with respect to the target speaker's model.
   <br>The effectiveness of the proposed transformation method was evaluated using objective tests and formal listening tests. We confirmed that the proposed method leads to perceptually more preferred results, compared with the conventional methods. (C) 2014 Elsevier B.V. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Konkuk Univ, Dept Elect Engn, Seoul 143701, South Korea.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Lee, KS (reprint author), Konkuk Univ, Dept Elect Engn, 1 Hwayang Dong, Seoul 143701, South Korea.</td>
</tr>

<tr>
<td valign="top">EM </td><td>kseung@konkuk.ac.kr</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAY</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">VL </td><td>60</td>
</tr>

<tr>
<td valign="top">BP </td><td>30</td>
</tr>

<tr>
<td valign="top">EP </td><td>43</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.specom.2014.02.002</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000335804800003</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ariwardhani, NW
   <br>Kimura, M
   <br>Iribe, Y
   <br>Katsurada, K
   <br>Nitta, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ariwardhani, Narpendyah Wisjnu
   <br>Kimura, Masashi
   <br>Iribe, Yurie
   <br>Katsurada, Kouichi
   <br>Nitta, Tsuneo</td>
</tr>

<tr>
<td valign="top">TI </td><td>Mapping Articulatory-Features to Vocal-Tract Parameters for Voice
   Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEICE TRANSACTIONS ON INFORMATION AND SYSTEMS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; articulatory feature; neural network; arbitrary
   speaker</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH RECOGNITION; FEATURE-EXTRACTION</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we propose voice conversion (VC) based on articulatory features (AF) to vocal-tract parameters (VTP) mapping. An artificial neural network (ANN) is applied to map AF to VTP and to convert a speaker's voice to a target-speaker's voice. The proposed system is not only text-independent VC, in which it does not need parallel utterances between source and target-speakers, but can also be used for an arbitrary source-speaker. This means that our approach does not require source-speaker data to build the VC model. We are also focusing on a small number of target-speaker training data. For comparison, a baseline system based on Gaussian mixture model (GMM) approach is conducted. The experimental results for a small number of training data show that the converted voice of our approach is intelligible and has speaker individuality of the target-speaker.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ariwardhani, Narpendyah Wisjnu; Kimura, Masashi; Katsurada, Kouichi;
   Nitta, Tsuneo] Toyohashi Univ Technol, Grad Sch Engn, Toyohashi, Aichi
   4418580, Japan.
   <br>[Iribe, Yurie] Aichi Prefectural Univ, Nagakute, Aichi 4801198, Japan.
   <br>[Nitta, Tsuneo] Waseda Univ, Green Comp Res Org, Tokyo 1620042, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ariwardhani, NW (reprint author), Toyohashi Univ Technol, Grad Sch Engn, Toyohashi, Aichi 4418580, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ariwardhani@gmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>APR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">VL </td><td>E97D</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">BP </td><td>911</td>
</tr>

<tr>
<td valign="top">EP </td><td>918</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1587/transinf.E97.D.911</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000342784100031</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Picart, B
   <br>Drugman, T
   <br>Dutoit, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Picart, Benjamin
   <br>Drugman, Thomas
   <br>Dutoit, Thierry</td>
</tr>

<tr>
<td valign="top">TI </td><td>Automatic Variation of the Degree of Articulation in New HMM-Based
   Voices</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Degree of articulation; expressive speech; HTS; speaking style
   transposition; speech synthesis; voice quality</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH SYNTHESIS; ADAPTATION; STYLE; CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper focuses on the automatic modification of the degree of articulation (hypo and hyperarticulation) of an existing standard neutral voice in the framework of HMM-based speech synthesis. Hypo and hyperarticulation refer to the production of speech respectively with a reduction and an increase of the articulatory efforts compared to the neutral style. Starting from a source speaker for which neutral, hypo and hyperarticulated speech data are available, statistical transformations are computed during the adaptation of the neutral speech synthesizer. These transformations are then applied to a new target speaker for which no hypo or hyperarticulated recordings are available. Four statistical methods are investigated, differing in the speaking style adaptation technique (model-space Linear Scaling LS vs. CMLLR) and in the speaking style transposition approach (phonetic vs. acoustic correspondence) they use. The efficiency of these techniques is assessed for the transposition of prosody and of filter coefficients separately. Besides we investigate which representation of the spectral envelope is the most suited for this purpose: MGC, LSP, PARCOR and LAR coefficients. Subjective evaluations are performed in order to determine which statistical transformation method achieves the highest performance in terms of segmental quality, reproduction of the articulation degree and speaker identity preservation. The most successful method is finally used for automatically modifying the degree of articulation of existing standard neutral voices.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Picart, Benjamin; Drugman, Thomas; Dutoit, Thierry] Univ Mons, TCTS
   Lab, B-7000 Mons, Belgium.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Picart, B (reprint author), Univ Mons, TCTS Lab, B-7000 Mons, Belgium.</td>
</tr>

<tr>
<td valign="top">EM </td><td>benjamin.picart@umons.ac.be; thomas.drugman@umons.ac.be;
   thierry.dutoit@umons.ac.be</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PD </td><td>APR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">VL </td><td>8</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>307</td>
</tr>

<tr>
<td valign="top">EP </td><td>322</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/JSTSP.2014.2302742</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000333100900014</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wu, CH
   <br>Huang, YC
   <br>Lee, CH
   <br>Guo, JC</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wu, Chung-Hsien
   <br>Huang, Yi-Chin
   <br>Lee, Chung-Han
   <br>Guo, Jun-Cheng</td>
</tr>

<tr>
<td valign="top">TI </td><td>Synthesis of Spontaneous Speech With Syllable Contraction Using
   State-Based Context-Dependent Voice Transformation</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Pronunciation variation; speech synthesis; transformation function</td>
</tr>

<tr>
<td valign="top">ID </td><td>MAXIMUM-LIKELIHOOD; CONVERSION; INFORMATION; ALGORITHM; SELECTION;
   SPEAKER; RECOGNITION; GENERATION; FREQUENCY; MODEL</td>
</tr>

<tr>
<td valign="top">AB </td><td>Pronunciation normally varies in spontaneous speech, and is an integral aspect of spontaneous expression. This study describes a voice transformation-based approach to generating spontaneous speech with syllable contractions for Hidden Markov Model (HMM)-based speech synthesis. A multi-dimensional linear regression model is adopted as the context-dependent, state-based transformation function to convert the feature sequence of read speech to that of spontaneous speech with syllable contraction. With insufficient number of training data, the obtained transformation functions are categorized using a decision tree based on linguistic and articulatory features for better and efficient selection of suitable transformation functions. Furthermore, to cope with the problem of small parallel corpus, cross-validation of trained transformation function is performed to ensure correct transformation functions are obtained and prevent over-fitting. Consequently, pronunciation variations of syllable contraction for the trained and the unseen syllable-contracted words are generated from the transformation function retrieved from the decision tree using linguistic and articulatory features. Objective and subjective tests were used to evaluate the performance of the proposed approach. Evaluation results demonstrate that the proposed transformation function substantially improves apparent spontaneity of the synthesized speech compared to the conventional methods.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Wu, Chung-Hsien; Huang, Yi-Chin; Lee, Chung-Han; Guo, Jun-Cheng] Natl
   Cheng Kung Univ, Dept Comp Sci &amp; Informat Engn, Tainan 701, Taiwan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Wu, CH (reprint author), Natl Cheng Kung Univ, Dept Comp Sci &amp; Informat Engn, Tainan 701, Taiwan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>chwu@csie.ncku.edu.tw; ychin.huang@gmail.com; chlee@csie.ncku.edu.tw</td>
</tr>

<tr xmlns:date="http://exslt.org/dates-and-times">
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Wu, Chung-Hsien</display_name>&nbsp;</font></td><td><font size="3">E-7970-2013&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">VL </td><td>22</td>
</tr>

<tr>
<td valign="top">IS </td><td>3</td>
</tr>

<tr>
<td valign="top">BP </td><td>585</td>
</tr>

<tr>
<td valign="top">EP </td><td>595</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASLP.2013.2297018</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000332951800001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Romak, JJ
   <br>Orbelo, DM
   <br>Maragos, NE
   <br>Ekbom, DC</td>
</tr>

<tr>
<td valign="top">AF </td><td>Romak, Jonathan J.
   <br>Orbelo, Diana M.
   <br>Maragos, Nicolas E.
   <br>Ekbom, Dale C.</td>
</tr>

<tr>
<td valign="top">TI </td><td>Correlation of the Voice Handicap Index-10 (VHI-10) and Voice-Related
   Quality of Life (V-RQOL) in Patients With Dysphonia</td>
</tr>

<tr>
<td valign="top">SO </td><td>JOURNAL OF VOICE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice Handicap Index-10; Voice; Related Quality of Life;
   Patient-reported outcome measure; Dysphonia; Presbyphonia; Muscle
   tension dysphonia</td>
</tr>

<tr>
<td valign="top">ID </td><td>VALIDATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Objectives. This study examines the correlation between two voice-specific patient-reported outcome measures: the Voice Handicap Index-10 (VHI-10) and Voice-Related Quality of Life (V-RQOL).
   <br>Study Design. Retrospective chart review.
   <br>Participants. Eight hundred four patients presenting to our voice clinic between May 2009 and August 2011. All patients completed the VHI-10 and V-RQOL in a single sitting.
   <br>Methods. Correlation between the two scales was examined using Spearman rank analysis. Calculated VHI-10 score was derived from V-RQOL score by direct conversion equation and compared with measured VHI-10 score. Receiver Operating Characteristic (ROC) curves were derived for diagnostic groups.
   <br>Results. Spearman correlation coefficient between the VHI-10 and V-RQOL was -0.91 (P &lt; 0.0001). VHI-10 and V-RQOL scores were also significantly correlated among diagnostic categories. Calculated and measured VHI-10 scores were significantly different both for individuals and overall. Area under the curve (AUC) values from ROC curves were significantly different for the presbyphonia (V-RQOL AUC = 0.586 [standard error, SE +/- 0.033]; VHI-10 AUC = 0.530 [SE +/- 0.031]; P = 0.0014) and muscle tension dysphonia (V-RQOL AUC 0.536 [SE +/- 0.026]; VHI-10 AUC = 0.508 [SE +/- 0.26]; P = 0.018) groups, with the V-RQOL showing relatively greater sensitivity.
   <br>Conclusions. The VHI-10 and V-RQOL are highly correlated. However, VHI-10 score cannot be calculated from V-RQOL score using the tested equation. The V-RQOL may be more sensitive than the VHI-10 in detecting the impact of presbyphonia and muscle tension dysphonia.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Romak, Jonathan J.; Orbelo, Diana M.; Maragos, Nicolas E.; Ekbom, Dale
   C.] Mayo Clin, Dept Otorhinolaryngol Head &amp; Neck Surg, Rochester, MN
   55905 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Orbelo, DM (reprint author), Mayo Clin, Dept Otorhinolaryngol Head &amp; Neck Surg, 200 First St SW, Rochester, MN 55905 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>dmorbelo@gmail.com</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Orbelo, Diana</display_name>&nbsp;</font></td><td><font size="3">K-9319-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Orbelo, Diana</display_name>&nbsp;</font></td><td><font size="3">0000-0003-3035-2077&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>17</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>17</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">VL </td><td>28</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>237</td>
</tr>

<tr>
<td valign="top">EP </td><td>240</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.jvoice.2013.07.015</td>
</tr>

<tr>
<td valign="top">SC </td><td>Audiology &amp; Speech-Language Pathology; Otorhinolaryngology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000332399000015</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Xu, N
   <br>Tang, YB
   <br>Bao, JY
   <br>Jiang, AM
   <br>Liu, XF
   <br>Yang, Z</td>
</tr>

<tr>
<td valign="top">AF </td><td>Xu, Ning
   <br>Tang, Yibing
   <br>Bao, Jingyi
   <br>Jiang, Aiming
   <br>Liu, Xiaofeng
   <br>Yang, Zhen</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice conversion based on Gaussian processes by coherent and asymmetric
   training with limited training data</td>
</tr>

<tr>
<td valign="top">SO </td><td>SPEECH COMMUNICATION</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Asymmetric training; Coherent training; Gaussian processes; Gaussian
   mixture model; Voice conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>ARTIFICIAL NEURAL-NETWORKS; TRANSFORMATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion (VC) is a technique aiming to mapping the individuality of a source speaker to that of a target speaker, wherein Gaussian mixture model (GMM) based methods are evidently prevalent. Despite their wide use, two major problems remains to be resolved, i.e., over-smoothing and over-fitting. The latter one arises naturally when the structure of model is too complicated given limited amount of training data.
   <br>Recently, a new voice conversion method based on Gaussian processes (GPs) was proposed, whose nonparametric nature ensures that the over-fitting problem can be alleviated significantly. Meanwhile, it is flexible to perform non-linear mapping under the framework of GPs by introducing sophisticated kernel functions. Thus this kind of method deserves to be explored thoroughly in this paper. To further improve the performance of the GP-based method, a strategy for mapping prosodic and spectral features coherently is adopted, making the best use of the intercorrelations embedded among both excitation and vocal tract features. Moreover, the accuracy in computing the kernel functions of GP can be improved by resorting to an asymmetric training strategy that allows the dimensionality of input vectors being reasonably higher than that of the output vectors without additional computational costs. Experiments have been conducted to confirm the effectiveness of the proposed method both objectively and subjectively, which have demonstrated that improvements can be obtained by GP-based method compared to the traditional GMM-based approach. (C) 2013 Elsevier B.V. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Xu, Ning; Tang, Yibing; Jiang, Aiming; Liu, Xiaofeng] Hohai Univ, Coll
   IoT Engn, Changzhou, Peoples R China.
   <br>[Xu, Ning; Jiang, Aiming; Liu, Xiaofeng] Hohai Univ, Changzhou Key Lab
   Robot &amp; Intelligent Technol, Changzhou, Peoples R China.
   <br>[Xu, Ning] Nanjing Univ Posts &amp; Telecommun, Minist Educ, Key Lab
   Broadband Wireless Commun &amp; Sensor Networ, Nanjing, Jiangsu, Peoples R
   China.
   <br>[Bao, Jingyi] Changzhou Inst Technol, Sch Elect Informat &amp; Elect Engn,
   Changzhou, Peoples R China.
   <br>[Yang, Zhen] Nanjing Univ Posts &amp; Telecommun, Coll Telecommun &amp; Informat
   Engn, Nanjing, Jiangsu, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Xu, N (reprint author), Hohai Univ, Coll IoT Engn, Changzhou, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>xuningdlts@gmail.com; tangyb@hhuc.e-du.cn; baojy@czu.cn;
   jiangam@hhuc.edu.cn; liuxf@hhuc.edu.cn; yangz@njupt.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>16</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>20</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">VL </td><td>58</td>
</tr>

<tr>
<td valign="top">BP </td><td>124</td>
</tr>

<tr>
<td valign="top">EP </td><td>138</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.specom.2013.11.005</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000329956000010</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Aihara, R
   <br>Takashima, R
   <br>Takiguchi, T
   <br>Ariki, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Aihara, Ryo
   <br>Takashima, Ryoichi
   <br>Takiguchi, Tetsuya
   <br>Ariki, Yasuo</td>
</tr>

<tr>
<td valign="top">TI </td><td>A preliminary demonstration of exemplar-based voice conversion for
   articulation disorders using an individuality-preserving dictionary</td>
</tr>

<tr>
<td valign="top">SO </td><td>EURASIP JOURNAL ON AUDIO SPEECH AND MUSIC PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; NMF; Articulation disorders; Voice reconstruction;
   Assistive technologies</td>
</tr>

<tr>
<td valign="top">ID </td><td>NONNEGATIVE MATRIX FACTORIZATION; SPEECH; RECOGNITION</td>
</tr>

<tr>
<td valign="top">AB </td><td>We present in this paper a voice conversion (VC) method for a person with an articulation disorder resulting from athetoid cerebral palsy. The movement of such speakers is limited by their athetoid symptoms, and their consonants are often unstable or unclear, which makes it difficult for them to communicate. In this paper, exemplar-based spectral conversion using nonnegative matrix factorization (NMF) is applied to a voice with an articulation disorder. To preserve the speaker's individuality, we used an individuality-preserving dictionary that is constructed from the source speaker's vowels and target speaker's consonants. Using this dictionary, we can create a natural and clear voice preserving their voice's individuality. Experimental results indicate that the performance of NMF-based VC is considerably better than conventional GMM-based VC.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Aihara, Ryo; Takashima, Ryoichi] Kobe Univ, Grad Sch Syst Informat,
   Nada Ku, Kobe, Hyogo 6578501, Japan.
   <br>[Takiguchi, Tetsuya; Ariki, Yasuo] Kobe Univ, Org Adv Sci &amp; Technol,
   Nada Ku, Kobe, Hyogo 6578501, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Aihara, R (reprint author), Kobe Univ, Grad Sch Syst Informat, Nada Ku, 1-1 Rokkodai, Kobe, Hyogo 6578501, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>aihara@me.cs.scitec.kobe-u.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>11</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>11</td>
</tr>

<tr>
<td valign="top">PD </td><td>FEB 1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">AR </td><td>5</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1186/1687-4722-2014-5</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000332015300002</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Liu, FF
   <br>Hossein-Zadeh, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Liu, Fenfei
   <br>Hossein-Zadeh, Mani</td>
</tr>

<tr>
<td valign="top">TI </td><td>Characterization of Optomechanical RF frequency Mixing/Down-Conversion
   and its Application in Photonic RF Receivers</td>
</tr>

<tr>
<td valign="top">SO </td><td>JOURNAL OF LIGHTWAVE TECHNOLOGY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Optomechanical oscillation; optical resonators; oscillators; RF
   Frequency conversion; RF-Photonics</td>
</tr>

<tr>
<td valign="top">ID </td><td>RADIATION-PRESSURE-DRIVEN; MICROWAVE RECEIVER; OSCILLATOR; MODULATOR;
   CAVITY; STATE</td>
</tr>

<tr>
<td valign="top">AB </td><td>We have characterized the process of radio-frequency (RF) frequency mixing and signal down-conversion in radiation pressure driven optomechanical RF oscillators. We have studied the dependence of the optomechanical mixing process on RF frequency, optical pump power and wavelength detuning and verified the existence of a linear regime where down-converted power is proportional to the RF input power. These outcomes show that optomechanical oscillator (OMO) has the potential to be used as frequency down-converter in RF over fiber links and photonic RF receivers. We have verified the fidelity of the optomechanical down-conversion process and demonstrated the first optomechanical voice down-conversion from an RF carrier.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Liu, Fenfei; Hossein-Zadeh, Mani] Univ New Mexico, Ctr High Technol
   Mat, Albuquerque, NM 87106 USA.
   <br>[Liu, Fenfei] Univ New Mexico, Dept Phys, Albuquerque, NM 87106 USA.
   <br>[Hossein-Zadeh, Mani] Univ New Mexico, Elect &amp; Comp Engn Dept,
   Albuquerque, NM 87106 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Liu, FF (reprint author), Univ New Mexico, Ctr High Technol Mat, Albuquerque, NM 87106 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>fliu@unm.edu; mhz@chtm.unm.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>7</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>7</td>
</tr>

<tr>
<td valign="top">PD </td><td>JAN 15</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">VL </td><td>32</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>309</td>
</tr>

<tr>
<td valign="top">EP </td><td>317</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/JLT.2013.2286329</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Optics; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000328967700001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Saito, D
   <br>Doi, H
   <br>Minematsu, N
   <br>Hirose, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Saito, Daisuke
   <br>Doi, Hidenobu
   <br>Minematsu, Nobuaki
   <br>Hirose, Keikichi</td>
</tr>

<tr>
<td valign="top">BE </td><td>Yuan, B
   <br>Ruan, Q
   <br>Tang, X</td>
</tr>

<tr>
<td valign="top">TI </td><td>VOICE CONVERSION BASED ON MATRIX VARIATE GAUSSIAN MIXTURE MODEL</td>
</tr>

<tr>
<td valign="top">SO </td><td>2014 12TH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING (ICSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Signal Processing</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>12th IEEE International Conference on Signal Processing (ICSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 19-23, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>HangZhou, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; Gaussian mixture model; matrix variate distribution;
   matrix variate normal; matrix variate Gaussian mixture model</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH RECOGNITION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper describes a novel approach to construct a mapping function between a given speaker pair using probability density functions (PDF) of matrix variate. In voice conversion studies, two important functions should be realized: 1) precise modeling of both the source and target feature spaces, and 2) construction of a proper transform function between these spaces. Voice conversion based on Gaussian mixture model (GMM) is widely used because of their flexibility and easiness in handling. In GMM-based approaches, a joint vector space of the source and target is first constructed, and the joint PDF of the two vectors is modeled as GMM in the joint vector space. The joint vector approach mainly focuses on precise modeling of the 'joint' feature space, and does not always construct a proper transform between two feature spaces. In contrast, the proposed method constructs the joint PDF as GMM in a matrix variate space whose row and column respectively correspond to the two functions, and it has potential to precisely model both the characteristics of the feature spaces and the relation between the source and target spaces. Experimental results show that the proposed method contributes to improve the performance of voice conversion.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Saito, Daisuke; Doi, Hidenobu; Minematsu, Nobuaki; Hirose, Keikichi]
   Univ Tokyo, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Saito, D (reprint author), Univ Tokyo, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>dsk_saito@gavo.t.u-tokyo.ac.jp; hdoi@gavo.t.u-tokyo.ac.jp;
   mine@gavo.t.u-tokyo.ac.jp; hirose@gavo.t.u-tokyo.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">BP </td><td>567</td>
</tr>

<tr>
<td valign="top">EP </td><td>571</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000365529500110</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Che, YX
   <br>Yu, YB</td>
</tr>

<tr>
<td valign="top">AF </td><td>Che, Yingxia
   <br>Yu, Yibiao</td>
</tr>

<tr>
<td valign="top">BE </td><td>Xu, G
   <br>Qiao, Y
   <br>Wu, X</td>
</tr>

<tr>
<td valign="top">TI </td><td>Fast Model Alignment for Structured Statistical Approach of Non-parallel
   Corpora Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>2014 4TH IEEE INTERNATIONAL CONFERENCE ON INFORMATION SCIENCE AND
   TECHNOLOGY (ICIST)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Information Science and Technology</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>4th IEEE International Conference on Information Science and Technology
   (ICIST)</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 26-28, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Shenzhen, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; Model alignment; Structured Gaussian Mixture Model;
   Hill climbing algorithm</td>
</tr>

<tr>
<td valign="top">AB </td><td>This study proposes a fast model matching algorithm of structured approach in Non-parallel corpora voice conversion. Most of conventional non-parallel corpus-based voice conversion method requires joint training which is computationally intensive and extremely inconvenient in system expansion. Existing structured approach of Non-parallel corpora voice conversion without joint training suffers from the imprecision in model alignment because of the simplified model matching algorithm, so we proposed a fast matching algorithm between statistical acoustic models of source-target speaker in structured approach of Non-parallel corpora voice conversion in this paper. In the proposed method, a Structured Gaussian mixture model (SGMM) is used to describe distribution of Linear Predication Cepstrum Coefficients (LPCC) and distribution structure of voices, then the structured distributions of source and target speaker are matched through Hill Climbing algorithm so that the conversion function is derived.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Che, Yingxia; Yu, Yibiao] Soochow Univ, Sch Elect &amp; Informat Engn,
   Suzhou, Jiangsu, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Che, YX (reprint author), Soochow Univ, Sch Elect &amp; Informat Engn, Suzhou, Jiangsu, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>cheyxf@gmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">BP </td><td>88</td>
</tr>

<tr>
<td valign="top">EP </td><td>92</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000364968700021</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Benisty, H
   <br>Malah, D
   <br>Crammer, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Benisty, Hadas
   <br>Malah, David
   <br>Crammer, Koby</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Sequential Voice Conversion Using Grid-Based Approximation</td>
</tr>

<tr>
<td valign="top">SO </td><td>2014 IEEE 28TH CONVENTION OF ELECTRICAL &amp; ELECTRONICS ENGINEERS IN
   ISRAEL (IEEEI)</td>
</tr>

<tr>
<td valign="top">SE </td><td>IEEE Convention of Electrical and Electronics Engineers in Israel</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE 28th Convention of Electrical and Electronics Engineers in Israel
   (IEEEI)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 03-05, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Eilat, ISRAEL</td>
</tr>

<tr>
<td valign="top">ID </td><td>MAXIMUM-LIKELIHOOD-ESTIMATION; PARAMETER</td>
</tr>

<tr>
<td valign="top">AB </td><td>Common voice conversion methods are based on Gaussian Mixture Modeling (GMM), which requires exhaustive training (typically lasting hours), often leading to ill-conditioning if the dataset used is too small. We propose a new conversion method that is trained in seconds, using either small or large scale datasets. The proposed Grid-Based (GB) method is based on sequential Bayesian tracking, by which the conversion process is expressed as a sequential estimation problem of tracking the target spectrum based on the observed source spectrum. The converted MFCC vectors are sequentially evaluated using a weighted sum of the target training set used as grid-points.
   <br>To improve the perceived quality of the synthesized signals, we use a post-processing block for enhancing the global variance. Objective and subjective evaluations show that the enhanced-GB method is comparable to classic GMM-based methods, in terms of quality, and comparable to their enhanced versions, in terms of individuality.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Benisty, Hadas; Malah, David; Crammer, Koby] Technion Israel Inst
   Technol, Dept Elect Engn, IL-32000 Haifa, Israel.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Benisty, H (reprint author), Technion Israel Inst Technol, Dept Elect Engn, IL-32000 Haifa, Israel.</td>
</tr>

<tr>
<td valign="top">EM </td><td>hadasbe@tx.technion.ac.il; malah@ee.technion.ac.il;
   koby@ee.technion.ac.il</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000360307100144</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Bruneau, P
   <br>Parisot, O
   <br>Mohammadi, A
   <br>Demiroglu, C
   <br>Ghoniem, M
   <br>Tamisier, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Bruneau, Pierrick
   <br>Parisot, Olivier
   <br>Mohammadi, Amir
   <br>Demiroglu, Cenk
   <br>Ghoniem, Mohammad
   <br>Tamisier, Thomas</td>
</tr>

<tr>
<td valign="top">BE </td><td>Calzolari, N
   <br>Choukri, K
   <br>Declerck, T
   <br>Loftsson, H
   <br>Maegaard, B
   <br>Mariani, J
   <br>Moreno, A
   <br>Odijk, J
   <br>Piperidis, S</td>
</tr>

<tr>
<td valign="top">TI </td><td>Finding Relevant Features for Statistical Speech Synthesis Adaptation</td>
</tr>

<tr>
<td valign="top">SO </td><td>LREC 2014 - NINTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND
   EVALUATION</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th International Conference on Language Resources and Evaluation (LREC)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 26-31, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Reykjavik, ICELAND</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speech Synthesis; Speaker Adaptation; Feature Selection; Visual
   Analytics</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEAKER ADAPTATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Statistical speech synthesis (SSS) models typically lie in a very high-dimensional space. They can be used to allow speech synthesis on digital devices, using only few sentences of input by the user. However, the adaptation algorithms of such weakly trained models suffer from the high dimensionality of the feature space. Because creating new voices is easy with the SSS approach, thousands of voices can be trained and a Nearest-Neighbor (NN) algorithm can be used to obtain better speaker similarity in those limited-data cases. NN methods require good distance measures that correlate well with human perception. This paper investigates the problem of finding good low-cost metrics, i.e. simple functions of feature values that map with objective signal quality metrics. We show this is a ill-posed problem, and study its conversion to a tractable form. Tentative solutions are found using statistical analyzes. With a performance index improved by 36% w.r.t. a naive solution, while using only 0.77% of the respective amount of features, our results are promising. Deeper insights in our results are then unveiled using visual methods, namely high-dimensional data visualization and dimensionality reduction techniques. Perspectives on new adaptation algorithms, and tighter integration of data mining and visualization principles are eventually given.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Bruneau, Pierrick; Parisot, Olivier; Ghoniem, Mohammad; Tamisier,
   Thomas] Gabriel Lippmann Informat, Ctr Rech Publ, Syst &amp; Collaborat
   Dept, L-4422 Belvaux, Luxembourg.
   <br>[Mohammadi, Amir; Demiroglu, Cenk] Ozyegin Univ, Elect &amp; Elect Engn
   Dept, Istanbul, Turkey.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Bruneau, P (reprint author), Gabriel Lippmann Informat, Ctr Rech Publ, Syst &amp; Collaborat Dept, 41 Rue Brill, L-4422 Belvaux, Luxembourg.</td>
</tr>

<tr>
<td valign="top">EM </td><td>bruneau@lippmann.lu; parisot@lippmann.lu; amir.mohammadi@ozu.edu.tr;
   cenk.demiroglu@ozyegin.edu.tr; ghoniem@lippmann.lu; tamisier@lippmann.lu</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">SC </td><td>Linguistics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000355611001083</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Chadha, AN
   <br>Nirmal, JH
   <br>Zaveri, MA</td>
</tr>

<tr>
<td valign="top">AF </td><td>Chadha, Ankita N.
   <br>Nirmal, Jagannath H.
   <br>Zaveri, Mukesh A.</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Comparison of Multi-layer Perceptron and Radial Basis Function Neural
   Network in the Voice Conversion Framework</td>
</tr>

<tr>
<td valign="top">SO </td><td>2014 INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTING, COMMUNICATIONS
   AND INFORMATICS (ICACCI)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>3rd International Conference on Advances in Computing, Communications
   and Informatics (ICACCI)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 24-27, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>New Delhi, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>dynamic time warping; line spectral frequencies; multi-layer perceptron;
   radial basis function; residual selection; voice conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEAKER; ALGORITHM</td>
</tr>

<tr>
<td valign="top">AB </td><td>The voice conversion system modifies the speaker specific features of the source speaker so that it sounds like a target speaker speech. The voice individuality of the speech signal is characterized at various levels such as shape of the glottal excitation, shape of the vocal tract and the long term prosodic features. In this work, Line Spectral Frequencies (LSF) are used to represent the shape of the vocal tract and Linear Predictive (LP) residual represents the shape of the glottal excitation of a particular speaker. A Multi Layer Perceptron (MLP) and Radial Basis Function (RBF) based neural network are explored to formulate the nonlinear mapping for modifying the LSFs. The baseline residual selection method is used to modify the LP-residual of one speaker to that of another speaker. A relative comparison between MLP and RBF are carried out using various objective and subjective measures for inter-gender and intra-gender voice conversion. The results reveal that an optimized RBF performs slightly better than baseline MLP based voice conversion.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Chadha, Ankita N.; Nirmal, Jagannath H.] KJ Somaiya Coll, Dept Elect
   Engn, Bombay, Maharashtra, India.
   <br>[Zaveri, Mukesh A.] Sardar Vallabhbahi Natl Inst Technol, Dept Comp
   Engn, Surat, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Chadha, AN (reprint author), KJ Somaiya Coll, Dept Elect Engn, Bombay, Maharashtra, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ankita.chadha@somaiya.edu; jhnirmal@somaiya.edu;
   mazaveri.coed@svnit.ac.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">BP </td><td>1045</td>
</tr>

<tr>
<td valign="top">EP </td><td>1052</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000358999700174</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Chen, XQ
   <br>Yu, YB
   <br>Zhao, HM</td>
</tr>

<tr>
<td valign="top">AF </td><td>Chen, Xueqin
   <br>Yu, Yibiao
   <br>Zhao, Heming</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>F0 Prediction from Linear Predictive Cepstral Coefficient</td>
</tr>

<tr>
<td valign="top">SO </td><td>2014 SIXTH INTERNATIONAL CONFERENCE ON WIRELESS COMMUNICATIONS AND
   SIGNAL PROCESSING (WCSP)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>6th International Conference on Wireless Communications and Signal
   Processing (WCSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 23-25, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hefei, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>F0 prediction; Spectrum envelope; Gaussian mixture model</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we proposed a fundamental frequency prediction method which is used primarily in the voice conversion system. This paper establishes a Gaussian Mixture Model (GMM) to predict the fundamental frequency based on the Linear Predictive Cepstral Coefficient (LPCC). The model may be the speaker-dependent Gaussian mixture model or the speaker-independent universal background model that is decided by the number of speakers in the training data set. The gross pitch error and the tone correct rate of the predicted F0 are reviewed and analyzed separately. The experimental results show that there is a relatively stable mapping relationship between LPCCs and fundamental. The gross pitch error in the GMM and UBM is about 7.52% and 14.03%. Subjective tests certify that the tone could be understood well. This F0 prediction method could be utilized to predict pitch in whispered speech conversion system and voice conversion system.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Chen, Xueqin; Yu, Yibiao; Zhao, Heming] Soochow Univ, Sch Elect &amp;
   Informat Engn, Suzhou, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Chen, XQ (reprint author), Soochow Univ, Sch Elect &amp; Informat Engn, Suzhou, Peoples R China.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000358694800058</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nose, T
   <br>Kobayashi, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nose, Takashi
   <br>Kobayashi, Takao</td>
</tr>

<tr>
<td valign="top">BE </td><td>Watada, J
   <br>Ito, A
   <br>Pan, JS
   <br>Chao, HC
   <br>Chen, CM</td>
</tr>

<tr>
<td valign="top">TI </td><td>Quantized F0 Context and Its Applications to Speech Synthesis, Speech
   Coding and Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>2014 TENTH INTERNATIONAL CONFERENCE ON INTELLIGENT INFORMATION HIDING
   AND MULTIMEDIA SIGNAL PROCESSING (IIH-MSP 2014)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>10th International Conference on Intelligent Information Hiding and
   Multimedia Signal Processing (IIH-MSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 27-29, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Waseda Univ, Kitakyushu, JAPAN</td>
</tr>

<tr>
<td valign="top">HO </td><td>Waseda Univ</td>
</tr>

<tr>
<td valign="top">DE </td><td>HMM-based speech synthesis; quantized F0 context; low bit-rate speech
   coding; voice conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>HMM</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper describes a technique for language-independent prosody modeling using unsupervised prosodic labeling in HMM-based speech synthesis and shows its applications to low bit-rate speech coding and speaker-independent voice conversion. In the proposed technique, sequences of prosodic features are roughly quantized at a phone level and the resultant indexes are used as the prosodic context for the model training. The conventional HMM-based speech synthesis requires accurate prosodic labels corresponding to the speech samples where manual modification is necessary to improve the modeling accuracy, which sometimes takes extra costs and limits its application. In contrast, the proposed technique creates the prosodic label from the training data itself and can apply not only to the speech synthesis but also to the speech coding and voice conversion. Subjective experimental results show the effectiveness of the use of the quantized F0 context without manual prosodic labeling.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nose, Takashi] Tohoku Univ, Grad Sch Engn, Sendai, Miyagi 980, Japan.
   <br>[Kobayashi, Takao] Tokyo Inst Technol, Interdisciplinary Grad Sch Sci &amp;
   Engn, Yokohama, Kanagawa 227, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nose, T (reprint author), Tohoku Univ, Grad Sch Engn, Sendai, Miyagi 980, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>tnose@m.tohoku.ac.jp; takao.kobayashi@ip.titech.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">BP </td><td>578</td>
</tr>

<tr>
<td valign="top">EP </td><td>581</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/IIH-MSP.2014.149</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000358743800143</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Lin, YJ
   <br>Chen, XJ</td>
</tr>

<tr>
<td valign="top">AF </td><td>Lin, Yingjian
   <br>Chen, Xiaoji</td>
</tr>

<tr>
<td valign="top">BE </td><td>Liu, DL
   <br>Zhu, XB
   <br>Xu, KL
   <br>Fang, DM</td>
</tr>

<tr>
<td valign="top">TI </td><td>BP Neural Network Learning Algorithm and Its Software Implementation</td>
</tr>

<tr>
<td valign="top">SO </td><td>APPLIED SCIENCE, MATERIALS SCIENCE AND INFORMATION TECHNOLOGIES IN
   INDUSTRY</td>
</tr>

<tr>
<td valign="top">SE </td><td>Applied Mechanics and Materials</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Advances in Materials Science and
   Information Technologies in Industry (AMSITI)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JAN 11-12, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Xian, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>BP neural network; BP learning algorithm; software implementation; data
   structure</td>
</tr>

<tr>
<td valign="top">AB </td><td>BP neural network in character recognition, pattern classification, text and voice conversion, image compression, decision support and so on aspects has the widespread application, in view of the problems existing in the actual application, this paper researches learning algorithm and software implementation. Learning algorithm studies include three aspects, illustrates the basic thoughts of the BP algorithm, designed the three layers BP network structure, the mathematical model for the accurate description of algorithm. Software implementation studies include two aspects, the network model of all neurons become linked list structure and storage structure is designed, the design of the software process and will implement the process into four steps. BP algorithm of the software implementation is a basic work for the application of BP neural network, using the research results of this paper, the user can easily neural network design and simulation.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Bohai Univ, Teaching &amp; Res Inst Coll Comp, Jinzhou, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Lin, YJ (reprint author), Bohai Univ, Teaching &amp; Res Inst Coll Comp, Jinzhou, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>76028245@qq.com; 1213552916@qq.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">VL </td><td>513-517</td>
</tr>

<tr>
<td valign="top">BP </td><td>738</td>
</tr>

<tr>
<td valign="top">EP </td><td>741</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.4028/www.scientific.net/AMM.513-517.738</td>
</tr>

<tr>
<td valign="top">SC </td><td>Construction &amp; Building Technology; Engineering; Mechanics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000349668901019</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Anil, MC
   <br>Shirbahadurkar, SD</td>
</tr>

<tr>
<td valign="top">AF </td><td>Anil, Manjare Chandraprabha
   <br>Shirbahadurkar, S. D.</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Speech Modification for Prosody Conversion in Expressive Marathi
   Text-to-Speech Synthesis</td>
</tr>

<tr>
<td valign="top">SO </td><td>2014 INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING AND INTEGRATED
   NETWORKS (SPIN)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>1st International Conference on Signal Processing and Integrated
   Networks (SPIN)</td>
</tr>

<tr>
<td valign="top">CY </td><td>FEB 20-21, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Amity Univ Campus, Amity Sch Engn &amp; Technol, Noida, INDIA</td>
</tr>

<tr>
<td valign="top">HO </td><td>Amity Univ Campus, Amity Sch Engn &amp; Technol</td>
</tr>

<tr>
<td valign="top">DE </td><td>Prosody; Pitch Contour; Expressive Text-to-Speech synthesis</td>
</tr>

<tr>
<td valign="top">AB </td><td>During Expressive speech, the voice conveys intended message as well as basic emotions of the speaker. This work focuses on the development of expressive Text-to-Speech synthesis techniques for a Marathi (spoken in Maharashtra, India) language. The Pitch contour is one of the important properties of speech that is affected by emotional speech.
   <br>This work presents analysis of Emotional and Neutral Marathi speech from pitch contour. Pitch features derived from emotional speech samples with punctuation marks. Presently, question mark and exclamation mark in text are studied to find prosodic information. First, pitch features derived from emotional speech samples are compared with the features derived from neutral speech. Speaker usually tends to put more emphasis on one particular syllable within a prosodic word. We identify such syllable as the core syllable that can be derived with the punctuation marks. The experimental results showed that the proposed prosody conversion, based on pitch modification and prosodic word detection with punctuation mark, can improve speech quality.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Anil, Manjare Chandraprabha] JSPMs Rajarshi Shahu Coll Engn, Dept Elect
   &amp; Telecommun Engn, Pune 411033, Maharashtra, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Anil, MC (reprint author), JSPMs Rajarshi Shahu Coll Engn, Dept Elect &amp; Telecommun Engn, Pune 411033, Maharashtra, India.</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">BP </td><td>56</td>
</tr>

<tr>
<td valign="top">EP </td><td>58</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000349853100012</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Xie, FL
   <br>Qian, Y
   <br>Soong, FK
   <br>Li, HF</td>
</tr>

<tr>
<td valign="top">AF </td><td>Xie, Feng-Long
   <br>Qian, Yao
   <br>Soong, Frank K.
   <br>Li, Haifeng</td>
</tr>

<tr>
<td valign="top">BE </td><td>Dong, M
   <br>Tao, J
   <br>Li, H
   <br>Zheng, TF
   <br>Lu, Y</td>
</tr>

<tr>
<td valign="top">TI </td><td>Pitch Transformation in Neural Network based Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>2014 9TH INTERNATIONAL SYMPOSIUM ON CHINESE SPOKEN LANGUAGE PROCESSING
   (ISCSLP)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th International Symposium on Chinese Spoken Language Processing
   (ISCSLP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 12-14, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>SINGAPORE</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; pitch; neural network</td>
</tr>

<tr>
<td valign="top">AB </td><td>In voice conversion task, prosody conversion especially pitch conversion is a very challenging research topic because of the discontinuity property of pitch. Conventionally pitch conversion is always achieved by adjusting the mean and variance of the source pitch distribution to the target pitch distribution. This method removes most of the detailed information of the speaker's prosody and only maintains the global F0 contour. In this paper, we propose a neural network based pitch conversion system which converts F0 and spectral features all together frame by frame. Experimental results show that neural network based pitch conversion can significantly reduce the Unvoiced/Voiced error and RMSE of F0 between converted pitch and target pitch compared with the conventional Gaussian normalized transformation method. Wavelet decomposition for F0 can further improve the performance of voice conversion.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Xie, Feng-Long; Li, Haifeng] Harbin Inst Technol, Harbin 150006,
   Peoples R China.
   <br>[Xie, Feng-Long; Qian, Yao; Soong, Frank K.] Microsoft Res Asia,
   Beijing, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Xie, FL (reprint author), Harbin Inst Technol, Harbin 150006, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>v-fxie@microsoft.com; yaoqian@microsoft.com; frankkps@microsoft.com;
   lihaifeng@hit.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">BP </td><td>197</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000349765600041</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Tian, XH
   <br>Wu, ZZ
   <br>Lee, SW
   <br>Chng, ES</td>
</tr>

<tr>
<td valign="top">AF </td><td>Tian, Xiaohai
   <br>Wu, Zhizheng
   <br>Lee, S. W.
   <br>Chng, Eng Siong</td>
</tr>

<tr>
<td valign="top">BE </td><td>Dong, M
   <br>Tao, J
   <br>Li, H
   <br>Zheng, TF
   <br>Lu, Y</td>
</tr>

<tr>
<td valign="top">TI </td><td>Correlation-based Frequency Warping for Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>2014 9TH INTERNATIONAL SYMPOSIUM ON CHINESE SPOKEN LANGUAGE PROCESSING
   (ISCSLP)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th International Symposium on Chinese Spoken Language Processing
   (ISCSLP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 12-14, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>SINGAPORE</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speech synthesis; Voice conversion; Frequency warping; Correlation</td>
</tr>

<tr>
<td valign="top">ID </td><td>WORD RECOGNITION; ALGORITHM; TRANSFORMATION; EXTRACTION; SPECTRUM</td>
</tr>

<tr>
<td valign="top">AB </td><td>Frequency warping (FW) based voice conversion aims to modify the frequency axis of source spectra towards that of the target. In previous works, the optimal warping function was calculated by minimizing the spectral distance of converted and target spectra without considering the spectral shape. Nevertheless, speaker timbre and identity greatly depend on vocal tract peaks and valleys of spectrum. In this paper, we propose a method to define the warping function by maximizing the correlation between the converted and target spectra. Different from the conventional warping methods, the correlation-based optimization is not determined by the magnitude of the spectra. Instead, both spectral peaks and valleys are considered in the optimization process, which also improves the performance of amplitude scaling. Experiments were conducted on VOICES database, and the results show that after amplitude scaling our proposed method reduced the mel-spectral distortion from 5.85 dB to 5.60 dB. The subjective listening tests also confirmed the effectiveness of the proposed method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Tian, Xiaohai; Chng, Eng Siong] Nanyang Technol Univ, Sch Comp Engn,
   Singapore 639798, Singapore.
   <br>[Tian, Xiaohai; Chng, Eng Siong] Nanyang Technol Univ, Joint NTU UBC Res
   Ctr Excellence Act Living Elder, Singapore, Singapore.
   <br>[Wu, Zhizheng] Univ Edinburgh, Ctr Speech Technol Res, Edinburgh,
   Midlothian, Scotland.
   <br>[Lee, S. W.] Inst Infocomm Res, Huma Language Technol, Singapore,
   Singapore.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Tian, XH (reprint author), Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.</td>
</tr>

<tr>
<td valign="top">EM </td><td>xhtian@ntu.edu.sg; zhizheng.wu@ed.ac.uk; swylee@i2r.a-star.edu.sg;
   aseschng@ntu.edu.sg</td>
</tr>

<tr>
<td valign="top">TC </td><td>9</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>9</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">BP </td><td>211</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000349765600044</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Li, X
   <br>Wang, ZF</td>
</tr>

<tr>
<td valign="top">AF </td><td>Li, Xian
   <br>Wang, Zeng-fu</td>
</tr>

<tr>
<td valign="top">BE </td><td>Dong, M
   <br>Tao, J
   <br>Li, H
   <br>Zheng, TF
   <br>Lu, Y</td>
</tr>

<tr>
<td valign="top">TI </td><td>Frame Correlation Based Autoregressive GMM Method for Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>2014 9TH INTERNATIONAL SYMPOSIUM ON CHINESE SPOKEN LANGUAGE PROCESSING
   (ISCSLP)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th International Symposium on Chinese Spoken Language Processing
   (ISCSLP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 12-14, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>SINGAPORE</td>
</tr>

<tr>
<td valign="top">DE </td><td>speech synthesis; voice conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>MAXIMUM-LIKELIHOOD</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we present a frame correlation based autoregressive GMM method for voice conversion. In our system, the cross-frame correlation of the source feature is modeled with augmented delta features, and the cross-frame correlation of target feature is modeled by autoregressive models. The expectation maximization (EM) algorithm is used for the model training, and a maximum likelihood parameter conversion algorithm is then employed to convert the feature of a source speaker into the one of a target speaker frame by frame. This method is consistent in training and conversion by using target feature's cross-frame correlation explicitly at both stage. The experimental results show that the proposed method has excellent performance. The test set log probability of it is higher than the GMM-DYN (GMM with dynamic features) method, and the subjective evaluation results of it are also comparable to the GMM-DYN method. Furthermore, it is much more suitable for low-latency application.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Li, Xian; Wang, Zeng-fu] Univ Sci &amp; Technol China, Dept Automat, Hefei
   230027, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Li, X (reprint author), Univ Sci &amp; Technol China, Dept Automat, Hefei 230027, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>shysian@mail.ustc.edu.cn; zfwang@ustc.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">BP </td><td>221</td>
</tr>

<tr>
<td valign="top">EP </td><td>225</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000349765600046</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wang, SS
   <br>Lin, P
   <br>Lyu, DC
   <br>Tsao, Y
   <br>Hwang, HT
   <br>Su, B</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wang, Syu-Siang
   <br>Lin, Payton
   <br>Lyu, Dau-Cheng
   <br>Tsao, Yu
   <br>Hwang, Hsin-Te
   <br>Su, Borching</td>
</tr>

<tr>
<td valign="top">BE </td><td>Dong, M
   <br>Tao, J
   <br>Li, H
   <br>Zheng, TF
   <br>Lu, Y</td>
</tr>

<tr>
<td valign="top">TI </td><td>Acoustic Feature Conversion Using a Polynomial Based Feature
   Transferring Algorithm</td>
</tr>

<tr>
<td valign="top">SO </td><td>2014 9TH INTERNATIONAL SYMPOSIUM ON CHINESE SPOKEN LANGUAGE PROCESSING
   (ISCSLP)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th International Symposium on Chinese Spoken Language Processing
   (ISCSLP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 12-14, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>SINGAPORE</td>
</tr>

<tr>
<td valign="top">DE </td><td>acoustic feature conversion; feature transformation; robust feature
   extraction; robust speech recognition</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH RECOGNITION; GENRE CLASSIFICATION; VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This study proposes a polynomial based feature transferring (PFT) algorithm for acoustic feature conversion. The PFT process consists of estimation and conversion phases. The estimation phase aims to compute a polynomial based transfer function using only a small set of parallel source and target features. With the estimated transfer function, the conversion phase converts large sets of source features to target ones. This study evaluates the proposed PFT algorithm using a robust automatic speech recognition (ASR) task on the Aurora-2 database. The source features were MFCCs with cepstral mean and variance normalization (CMVN), and the target features were advanced front end features (AFE). Compared to CMVN, AFE provides better robust speech recognition performance but requires more complicated and expensive cost for feature extraction. By PFT, we intend to use a simple transfer function to obtain AFE-like acoustic features from the source CMVN features. Experimental results on Aurora-2 demonstrate that the PFT generated AFE-like features that can notably improve the CMVN performance and approach results achieved by AFE. Furthermore, the recognition accuracy of PFT was better than that of histogram equalization (HEQ) and polynomial based histogram equalization (PHEQ). The results confirm the effectiveness of PFT with just a few sets of parallel features.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Wang, Syu-Siang; Su, Borching] Natl Taiwan Univ, Grad Inst Commun Engn,
   Taipei 10764, Taiwan.
   <br>[Lin, Payton; Tsao, Yu] Acad Sinica, Res Ctr informat Technol Innovat,
   Taipei, Taiwan.
   <br>[Lyu, Dau-Cheng] ASUSTeK Comp Inc, Suzhou, Peoples R China.
   <br>[Hwang, Hsin-Te] Acad Sinica, Inst Informat Sci, Taipei, Taiwan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Wang, SS (reprint author), Natl Taiwan Univ, Grad Inst Commun Engn, Taipei 10764, Taiwan.</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Su, Borching</display_name>&nbsp;</font></td><td><font size="3">0000-0001-8617-2601&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">BP </td><td>454</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000349765600117</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Gu, HY
   <br>Tsai, SF</td>
</tr>

<tr>
<td valign="top">AF </td><td>Gu, Hung-Yan
   <br>Tsai, Sung-Fung</td>
</tr>

<tr>
<td valign="top">BE </td><td>Dong, M
   <br>Tao, J
   <br>Li, H
   <br>Zheng, TF
   <br>Lu, Y</td>
</tr>

<tr>
<td valign="top">TI </td><td>Improving Segmental GMM Based Voice Conversion Method with Target Frame
   Selection</td>
</tr>

<tr>
<td valign="top">SO </td><td>2014 9TH INTERNATIONAL SYMPOSIUM ON CHINESE SPOKEN LANGUAGE PROCESSING
   (ISCSLP)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th International Symposium on Chinese Spoken Language Processing
   (ISCSLP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 12-14, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>SINGAPORE</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; GMM; frame selection; discrete cepstral coefficient;
   variance ratio</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, the voice conversion method based on segmental Gaussian mixture models (GMMs) is further improved by adding the module of target frame selection (TFS). Segmental GMMs are meant to replace a single GMM of a large number of mixture components with several voice-content specific GMMs each consisting of much fewer mixture components. In addition, TFS is used to find a frame, of spectral features near to the mapped feature vector, from the target-speaker frame pool corresponding to the segment class as the input frame belongs to. Both ideas are intended to alleviate the problem that the converted spectral envelopes are often over smoothed. To evaluate the performance of the two ideas mentioned, three voice conversion systems are constructed, and used to conduct listening tests. The results of the tests show that the system using the two ideas together can obtain much improved voice quality. In addition, the measured variance ratio (VR) values show that the system with the two ideas adopted also obtains the highest VR value.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Gu, Hung-Yan; Tsai, Sung-Fung] Natl Taiwan Univ Sci &amp; Technol, Taipei,
   Taiwan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Gu, HY (reprint author), Natl Taiwan Univ Sci &amp; Technol, Taipei, Taiwan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>guhy@mail.ntust.edu.tw; m9615069@mail.ntust.edu.tw</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">BP </td><td>483</td>
</tr>

<tr>
<td valign="top">EP </td><td>487</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000349765600123</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Vira, I
   <br>Vasiljevs, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Vira, Inese
   <br>Vasiljevs, Andrejs</td>
</tr>

<tr>
<td valign="top">BE </td><td>Utka, A
   <br>Grigonyte, G
   <br>KapociuteDzikiene, J
   <br>Vaicenoniene, J</td>
</tr>

<tr>
<td valign="top">TI </td><td>The Development of Conversational Agent Based Interface</td>
</tr>

<tr>
<td valign="top">SO </td><td>HUMAN LANGUAGE TECHNOLOGIES - THE BALTIC PERSPECTIVE, BALTIC HLT 2014</td>
</tr>

<tr>
<td valign="top">SE </td><td>Frontiers in Artificial Intelligence and Applications</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>6th International Conference on Human Language Technologies - The Baltic
   Perspective (Baltic HLT)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 26-27, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Kaunas, LITHUANIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Conversational agent; speech interfaces; TTS; ASR; chatbot; question
   answering; multilingual information systems; AIML; Latvian</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper we present two prototypes of 3D based virtual agents: one chatbot which in addition to the ability to hold a conversation can perform translation from English into Spanish, Russian, and French; and another which supplies currency conversion (lats to euro and euro to lats) in the Latvian language. Both chatbots are voice controlled, with natural mimicry and representations of human-like emotions. We describe the motivation, development process, design and architecture of these mobile applications. The evaluation of both applications and their usage in selected scenarios is also presented.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Vira, Inese; Vasiljevs, Andrejs] Tilde, LV-1004 Riga, Latvia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Vira, I (reprint author), Tilde, Vienibas Gatve 75a, LV-1004 Riga, Latvia.</td>
</tr>

<tr>
<td valign="top">EM </td><td>inese.vira@tilde.lv; andrejs@tilde.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">VL </td><td>268</td>
</tr>

<tr>
<td valign="top">BP </td><td>46</td>
</tr>

<tr>
<td valign="top">EP </td><td>53</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.3233/978-1-61499-442-8-46</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000349540000007</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>d'Alessandro, N
   <br>Tilmanne, J
   <br>Astrinaki, M
   <br>Hueber, T
   <br>Dall, R
   <br>Ravet, T
   <br>Moinet, A
   <br>Cakmak, H
   <br>Babacan, O
   <br>Barbulescu, A
   <br>Parfait, V
   <br>Huguenin, V
   <br>Kalayci, ES
   <br>Hu, QO</td>
</tr>

<tr>
<td valign="top">AF </td><td>d'Alessandro, Nicolas
   <br>Tilmanne, Joelle
   <br>Astrinaki, Maria
   <br>Hueber, Thomas
   <br>Dall, Rasmus
   <br>Ravet, Thierry
   <br>Moinet, Alexis
   <br>Cakmak, Huseyin
   <br>Babacan, Onur
   <br>Barbulescu, Adela
   <br>Parfait, Valentin
   <br>Huguenin, Victor
   <br>Kalayci, Emine Sumeyye
   <br>Hu, Qiong</td>
</tr>

<tr>
<td valign="top">BE </td><td>Rybarczyk, Y
   <br>Cardoso, T
   <br>Rosas, J
   <br>CamarinhaMatos, LM</td>
</tr>

<tr>
<td valign="top">TI </td><td>Reactive Statistical Mapping: Towards the Sketching of Performative
   Control with Data</td>
</tr>

<tr>
<td valign="top">SO </td><td>INNOVATIVE AND CREATIVE DEVELOPMENTS IN MULTIMODAL INTERACTION SYSTEMS</td>
</tr>

<tr>
<td valign="top">SE </td><td>IFIP Advances in Information and Communication Technology</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th IFIP WG 5.5 International Summer Workshop on Multimodal Interfaces
   (eNTERFACE)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUL 15-AUG 09, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Lisbon, PORTUGAL</td>
</tr>

<tr>
<td valign="top">DE </td><td>Statistical Modelling; Hidden Markov Models; Motion Capture; Speech;
   Singing; Laughter; Realtime Systems; Mapping</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents the results of our participation to the ninth eNTERFACE workshop on multimodal user interfaces. Our target for this workshop was to bring some technologies currently used in speech recognition and synthesis to a new level, i.e. being the core of a new HMM-based mapping system. The idea of statistical mapping has been investigated, more precisely how to use Gaussian Mixture Models and Hidden Markov Models for realtime and reactive generation of new trajectories from inputted labels and for realtime regression in a continuous-to-continuous use case. As a result, we have developed several proofs of concept, including an incremental speech synthesiser, a software for exploring stylistic spaces for gait and facial motion in realtime, a reactive audiovisual laughter and a prototype demonstrating the realtime reconstruction of lower body gait motion strictly from upper body motion, with conservation of the stylistic properties. This project has been the opportunity to formalise HMM-based mapping, integrate various of these innovations into the Mage library and explore the development of a realtime gesture recognition tool.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[d'Alessandro, Nicolas; Tilmanne, Joelle; Astrinaki, Maria; Ravet,
   Thierry; Moinet, Alexis; Cakmak, Huseyin; Babacan, Onur; Parfait,
   Valentin; Huguenin, Victor; Kalayci, Emine Sumeyye] Univ Mons, Numediart
   Inst New Media Art Technol, B-7000 Mons, Belgium.
   <br>[Hueber, Thomas; Barbulescu, Adela] Univ Grenoble 3, UJF, CNRS, GIPSA
   Lab,INP,UMR 5216, Grenoble, France.
   <br>[Dall, Rasmus; Hu, Qiong] Univ Edinburgh, Ctr Speech Technol Res,
   Edinburgh EH8 9YL, Midlothian, Scotland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>d'Alessandro, N (reprint author), Univ Mons, Numediart Inst New Media Art Technol, B-7000 Mons, Belgium.</td>
</tr>

<tr>
<td valign="top">EM </td><td>nicolas.dalessandro@umons.ac.be; joelle.tilmanne@umons.ac.be;
   maria.astrinaki@umons.ac.be; thomas.hueber@gipsa-lab.grenoble-inp.fr;
   r.dall@sms.ed.ac.uk; thierry.ravet@umons.ac.be;
   alexis.moinet@umons.ac.be; huseyin.cakmak@umons.ac.be;
   onur.babacan@umons.ac.be; adela.barbulescu@gipsa-lab.grenoble-inp.fr;
   qiong.hu@ed.ac.uk</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">VL </td><td>425</td>
</tr>

<tr>
<td valign="top">BP </td><td>20</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000349440300002</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Okajima, H
   <br>Honda, M
   <br>Yoshino, R
   <br>Matsunaga, N</td>
</tr>

<tr>
<td valign="top">AF </td><td>Okajima, Hiroshi
   <br>Honda, Maho
   <br>Yoshino, Rei
   <br>Matsunaga, Nobutomo</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Design Method of Delta-Sigma Data Conversion System with Pre-Filter</td>
</tr>

<tr>
<td valign="top">SO </td><td>2014 Proceedings of the SICE Annual Conference (SICE)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Proceedings of the SICE Annual Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>SICE Annual Conference (SICE)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 09-12, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hokkaido Univ, Sapporo, JAPAN</td>
</tr>

<tr>
<td valign="top">HO </td><td>Hokkaido Univ</td>
</tr>

<tr>
<td valign="top">DE </td><td>AD/DA converter; Delta-Sigma modulator; Data compression; Noise
   reduction</td>
</tr>

<tr>
<td valign="top">AB </td><td>AD/DA conversion has become a core technology in digital signal processing. Signal compression is one of the important matters for AD/DA conversion system. Delta-sigma modulator (DSM) is well known as a effective method for encoding analog signals into digital signals. The data conversion systems are composed of the post-filter and the DSM. It is required to satisfy the low quantization noise and the low distortion characteristics by appropriate design of the filter and DSM. In our studies, we proposed a new design method of data conversion system that includes pre-filter, in addition to the traditional data conversion system. To design easier, an evaluation framework of the data conversion system is proposed. Then, a design algorithm based on the evaluation framework is proposed by using particle swarm optimization algorithm. Post and pre-filters are designed towards the original signal, so as to minimize the effect of noise and distortion due to quantization. In this paper, we verify the AD/DA systems with post and pre-filters by using the voice signal compression system. We evaluate the effectiveness by the simulation.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Okajima, Hiroshi; Honda, Maho; Yoshino, Rei; Matsunaga, Nobutomo]
   Kumamoto Univ, Grad Sch Sci &amp; Technol, Kumamoto 8608555, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Okajima, H (reprint author), Kumamoto Univ, Grad Sch Sci &amp; Technol, 2-39-1 Kurokami, Kumamoto 8608555, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>okajima@cs.kumamoto-u.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">BP </td><td>1388</td>
</tr>

<tr>
<td valign="top">EP </td><td>1394</td>
</tr>

<tr>
<td valign="top">SC </td><td>Automation &amp; Control Systems; Instruments &amp; Instrumentation</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000348729200096</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Correia, MJ
   <br>Abad, A
   <br>Trancoso, I</td>
</tr>

<tr>
<td valign="top">AF </td><td>Correia, M. J.
   <br>Abad, A.
   <br>Trancoso, I.</td>
</tr>

<tr>
<td valign="top">BE </td><td>Biljanovic, P
   <br>Butkovic, Z
   <br>Skala, K
   <br>Golubic, S
   <br>CicinSain, M
   <br>Sruk, V
   <br>Ribaric, S
   <br>Gros, S
   <br>Vrdoljak, B
   <br>Mauher, M
   <br>Cetusic, G</td>
</tr>

<tr>
<td valign="top">TI </td><td>Preventing converted speech spoofing attacks in speaker verification</td>
</tr>

<tr>
<td valign="top">SO </td><td>2014 37TH INTERNATIONAL CONVENTION ON INFORMATION AND COMMUNICATION
   TECHNOLOGY, ELECTRONICS AND MICROELECTRONICS (MIPRO)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>37th International Convention on Information and Communication
   Technology, Electronics and Microelectronics (MIPRO)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 26-30, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Opatija, CROATIA</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion (VC) techniques, which modify a speaker's voice to sound like another's, present a threat to automatic speaker verification (SV) systems. In this paper, we evaluate the vulnerability of a state-of-the-art SV system against a converted speech spoofing attack. To overcome the spoofing attack, we implement state-of-the-art converted speech detectors based on short-and long-term features. We propose a new converted speech detector using a compact feature representation and a discriminative modeling approach. We experiment pairing converted speech detectors based on short-and long-term features to improve converted speech detection. The results indicate that the proposed converted speech detector pair outperforms state-of-the-art ones, achieving a detection accuracy of 97.9% for natural utterances and 98.0% for converted utterances. We include the anti-spoofing mechanism in our SV system as a post-processing module for accepted trials and reevaluate its performance, comparing it with the performance of an ideal system. Our results show that the SV system's performance returns to acceptable values, with less than 1.6% equal error rate (EER) change.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Correia, M. J.; Abad, A.; Trancoso, I.] Inst Super Tecn, Lisbon,
   Portugal.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Correia, MJ (reprint author), Inst Super Tecn, Lisbon, Portugal.</td>
</tr>

<tr>
<td valign="top">EM </td><td>joana.correia@12f.inesc-id.pt; alberto.abad@12f.inesc-id.pt;
   isabel.trancoso@12f.inesc-id.pt</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Abad, Alberto</display_name>&nbsp;</font></td><td><font size="3">O-1351-2019&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Trancoso, Isabel</display_name>&nbsp;</font></td><td><font size="3">C-5965-2008&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Abad, Alberto</display_name>&nbsp;</font></td><td><font size="3">0000-0003-2122-5148&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Trancoso, Isabel</display_name>&nbsp;</font></td><td><font size="3">0000-0001-5874-6313&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">BP </td><td>1320</td>
</tr>

<tr>
<td valign="top">EP </td><td>1325</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000346438700252</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Aihara, R
   <br>Nakashika, T
   <br>Takiguchi, T
   <br>Ariki, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Aihara, Ryo
   <br>Nakashika, Toru
   <br>Takiguchi, Tetsuya
   <br>Ariki, Yasuo</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>VOICE CONVERSION BASED ON NON-NEGATIVE MATRIX FACTORIZATION USING
   PHONEME-CATEGORIZED DICTIONARY</td>
</tr>

<tr>
<td valign="top">SO </td><td>2014 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech and Signal Processing
   (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 04-09, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Florence, ITALY</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; sparse representation; non-negative matrix
   factorization; sub-dictionary</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH</td>
</tr>

<tr>
<td valign="top">AB </td><td>We present in this paper an exemplar-based voice conversion (VC) method using a phoneme-categorized dictionary. Sparse representation-based VC using Non-negative matrix factorization (NMF) is employed for spectral conversion between different speakers. In our previous NMF-based VC method, source exemplars and target exemplars are extracted from parallel training data, having the same texts uttered by the source and target speakers. The input source signal is represented using the source exemplars and their weights. Then, the converted speech is constructed from the target exemplars and the weights related to the source exemplars. However, this exemplar-based approach needs to hold all the training exemplars (frames), and it may cause mismatching of phonemes between input signals and selected exemplars. In this paper, in order to reduce the mismatching of phoneme alignment, we propose a phoneme-categorized sub-dictionary and a dictionary selection method using NMF. By using the sub-dictionary, the performance of VC is improved compared to a conventional NMF-based VC. The effectiveness of this method was confirmed by comparing its effectiveness with that of a conventional Gaussian Mixture Model (GMM)-based method and a conventional NMF-based method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Aihara, Ryo; Nakashika, Toru; Takiguchi, Tetsuya; Ariki, Yasuo] Kobe
   Univ, Grad Sch Syst Informat, Nada Ku, Kobe, Hyogo 6578501, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Aihara, R (reprint author), Kobe Univ, Grad Sch Syst Informat, Nada Ku, 1-1 Rokkodai, Kobe, Hyogo 6578501, Japan.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000343655307187</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Aryal, S
   <br>Gutierrez-Osuna, R</td>
</tr>

<tr>
<td valign="top">AF </td><td>Aryal, Sandesh
   <br>Gutierrez-Osuna, Ricardo</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>CAN VOICE CONVERSION BE USED TO REDUCE NON-NATIVE ACCENTS?</td>
</tr>

<tr>
<td valign="top">SO </td><td>2014 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech and Signal Processing
   (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 04-09, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Florence, ITALY</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; accent conversion; non-native speech; Spanish accent</td>
</tr>

<tr>
<td valign="top">ID </td><td>TRANSFORMATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice-conversion (VC) techniques aim to transform utterances from a source speaker to sound as if a target speaker had produced them. For this reason, VC is generally ill-suited for accent-conversion (AC) purposes, where the goal is to capture the regional accent of the source while preserving the voice quality of the target. In this paper, we propose a modification of the conventional training process for VC that allows it to perform as an AC transform. Namely, we pair source and target vectors based not on their ordering within a parallel corpus, as is commonly done in VC, but based on their linguistic similarity. We validate the approach on a corpus containing native-accented and Spanish-accented utterances, and compare it against conventional VC through a series of listening tests. We also analyze whether phonological differences between the two languages (Spanish and American English) help predict the performance of the two methods.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Aryal, Sandesh; Gutierrez-Osuna, Ricardo] Texas A&amp;M Univ, Dept Comp Sci
   &amp; Engn, College Stn, TX 77843 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Aryal, S (reprint author), Texas A&amp;M Univ, Dept Comp Sci &amp; Engn, College Stn, TX 77843 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>sandesh@cse.tamu.edu; rgutier@cse.tamu.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000343655307184</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Aryal, S
   <br>Gutierrez-Osuna, R</td>
</tr>

<tr>
<td valign="top">AF </td><td>Aryal, Sandesh
   <br>Gutierrez-Osuna, Ricardo</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>ACCENT CONVERSION THROUGH CROSS-SPEAKER ARTICULATORY SYNTHESIS</td>
</tr>

<tr>
<td valign="top">SO </td><td>2014 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech and Signal Processing
   (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 04-09, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Florence, ITALY</td>
</tr>

<tr>
<td valign="top">DE </td><td>Data-driven articulatory synthesis; accent conversion; voice conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>MODEL; TRANSFORMATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Accent conversion (AC) seeks to transform second-language (L2) utterances to appear as if produced with a native (L1) accent. In the acoustic domain, AC is difficult due to the complex interaction between linguistic content and voice quality. Alternatively, AC can be performed in the articulatory domain by building a mapping from L2 articulators to L2 acoustics, and then driving the model with L1 articulators. However, collecting articulatory data for each L2 learner is impractical. Here we propose an approach that avoids this expensive step. Our method builds a cross-speaker forward mapping (CSFM) to generate L2 acoustic observations directly from L1 articulatory trajectories. We evaluated the CSFM against a baseline articulatory synthesizer trained with L2 articulators. Subjective listening tests show that both methods perform comparably in terms of accent reduction and ability to preserve the voice quality of the L2 speaker, with only a small impact in acoustic quality.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Aryal, Sandesh; Gutierrez-Osuna, Ricardo] Texas A&amp;M Univ, Dept Comp Sci
   &amp; Engn, College Stn, TX 77843 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Aryal, S (reprint author), Texas A&amp;M Univ, Dept Comp Sci &amp; Engn, College Stn, TX 77843 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>sandesh@cse.tamu.edu; rgutier@cse.tamu.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000343655307147</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Benisty, H
   <br>Malah, D
   <br>Crammer, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Benisty, H.
   <br>Malah, D.
   <br>Crammer, K.</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>NON-PARALLEL VOICE CONVERSION USING JOINT OPTIMIZATION OF ALIGNMENT BY
   TEMPORAL CONTEXT AND SPECTRAL DISTORTION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2014 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech and Signal Processing
   (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 04-09, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Florence, ITALY</td>
</tr>

<tr>
<td valign="top">DE </td><td>Non-Parallel Voice Conversion; INCA; Gaussian Mixture Model (GMM);
   Spectral Distance</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH SYNTHESIS; MODEL</td>
</tr>

<tr>
<td valign="top">AB </td><td>Many voice conversion systems require parallel training sets of the source and target speakers. Non-parallel training is more complicated as it involves evaluation of source-target correspondence along with the conversion function itself. INCA is a recently proposed method for non-parallel training, based on iterative estimation of alignment and conversion function. The alignment is evaluated using a simple nearest-neighbor search, which often leads to phonetic miss-matched source-target pairs. We propose here a generalized approach, denoted as Temporal-Context INCA (TC-INCA), based on matching temporal context vectors. We formulate the training stage as a minimization problem of a joint cost, considering both context-based alignment and conversion function. We show that TC-INCA reduces the joint cost and prove its convergence. Experimental results indicate that TC-INCA significantly improves the alignment accuracy, compared to INCA. Moreover, subjective evaluations show that TC-INCA leads to improved quality of the synthesized output signals, when small training sets are used.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Benisty, H.; Malah, D.; Crammer, K.] Technion Israel Inst Technol, Dept
   Elect Engn, IL-32000 Haifa, Israel.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Benisty, H (reprint author), Technion Israel Inst Technol, Dept Elect Engn, IL-32000 Haifa, Israel.</td>
</tr>

<tr>
<td valign="top">EM </td><td>hadasbe@tx.technion.ac.il; malah@ee.technion.ac.il;
   koby@ee.technion.ac.il</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000343655307190</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Janke, M
   <br>Wand, M
   <br>Heistermann, T
   <br>Schultz, T
   <br>Prahallad, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Janke, M.
   <br>Wand, M.
   <br>Heistermann, T.
   <br>Schultz, T.
   <br>Prahallad, K.</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>FUNDAMENTAL FREQUENCY GENERATION FOR WHISPER-TO-AUDIBLE SPEECH
   CONVERSION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2014 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech and Signal Processing
   (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 04-09, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Florence, ITALY</td>
</tr>

<tr>
<td valign="top">DE </td><td>Silent speech interface; whisper-to-speech conversion; voice conversion;
   F0 generation</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this work, we address the issues involved in whisper-to-audible speech conversion. Spectral mapping techniques using Gaussian mixture models or Artificial Neural Networks borrowed from voice conversion have been applied to transform whisper spectral features to normally phonated audible speech. However, the modeling and generation of fundamental frequency (F0) and its contour in the converted speech is a major issue. Whispered speech does not contain explicit voicing characteristics and hence it is hard to derive a suitable F0, making it difficult to generate a natural prosody after conversion. Our work addresses the F0 modeling in whisper-tospeech conversion. We show that F0 contours can be derived from the mapped spectral vectors, which can be used for the synthesis of a speech signal. We also present a hybrid unit selection approach for whisper-to-speech conversion. Unit selection is performed on the spectral vectors, where F0 and its contour can be obtained as a byproduct without any additional modeling.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Janke, M.; Wand, M.; Heistermann, T.; Schultz, T.] Karlsruhe Inst
   Technol, Cognit Syst Lab, D-76021 Karlsruhe, Germany.
   <br>[Prahallad, K.] Int Inst Informat, Hyderabad, Andhra Pradesh, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Janke, M (reprint author), Karlsruhe Inst Technol, Cognit Syst Lab, D-76021 Karlsruhe, Germany.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000343655302123</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Jiao, YS
   <br>Xie, X
   <br>Na, XY
   <br>Tu, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Jiao, Yishan
   <br>Xie, Xiang
   <br>Na, Xingyu
   <br>Tu, Ming</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>IMPROVING VOICE QUALITY OF HMM-BASED SPEECH SYNTHESIS USING VOICE
   CONVERSION METHOD</td>
</tr>

<tr>
<td valign="top">SO </td><td>2014 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech and Signal Processing
   (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 04-09, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Florence, ITALY</td>
</tr>

<tr>
<td valign="top">DE </td><td>HMM-based speech synthesis; voice conversion; local linear
   transformation; temporal decomposition</td>
</tr>

<tr>
<td valign="top">AB </td><td>HMM-based speech synthesis system (HTS) often generates buzzy and muffled speech. Such degradation of voice quality makes synthetic speech sound robotically rather than naturally. From this point, we suppose that synthetic speech is in a different speaker space apart from the original. We propose to use voice conversion method to transform synthetic speech toward the original so as to improve its quality. Local linear transformation (LLT) combined with temporal decomposition (TD) is proposed as the conversion method. It can not only ensure smooth spectral conversion but also avoid over-smoothing problem. Moreover, we design a robust spectral selection and modification strategy to make the modified spectra stable. Preference test shows that the proposed method can improve the quality of HMM-based speech synthesis.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Jiao, Yishan; Xie, Xiang; Na, Xingyu; Tu, Ming] Beijing Inst Technol,
   Sch Informat &amp; Elect, Beijing 100081, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Jiao, YS (reprint author), Beijing Inst Technol, Sch Informat &amp; Elect, Beijing 100081, Peoples R China.</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000343655307191</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kobayashi, K
   <br>Toda, T
   <br>Nakano, T
   <br>Goto, M
   <br>Neubig, G
   <br>Sakti, S
   <br>Nakamura, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kobayashi, Kazuhiro
   <br>Toda, Tomoki
   <br>Nakano, Tomoyasu
   <br>Goto, Masataka
   <br>Neubig, Graham
   <br>Sakti, Sakriani
   <br>Nakamura, Satoshi</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>REGRESSION APPROACHES TO PERCEPTUAL AGE CONTROL IN SINGING VOICE
   CONVERSION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2014 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech and Signal Processing
   (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 04-09, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Florence, ITALY</td>
</tr>

<tr>
<td valign="top">DE </td><td>singing voice conversion; perceptual age; voice timbre control;
   regression approaches; singer's individuality</td>
</tr>

<tr>
<td valign="top">AB </td><td>The perceptual age of a singing voice is the age of the singer as perceived by the listener, and is one of the notable characteristics that determines perceptions of a song. In this paper, we describe a novel voice timbre control technique based on the perceptual age for singing voice conversion (SVC). Singers can sing expressively by controlling prosody and voice timbre, but the varieties of voices that singers can produce are limited by physical constraints. Previous work has attempted to overcome the limitation through the use of statistical voice conversion. This technique makes it possible to convert singing voice timbre of an arbitrary source singer into that of an arbitrary target singer. However, it is still difficult to intuitively control singing voice characteristics by manipulating parameters corresponding to specific physical traits, such as gender and age. In this paper, we develop a technique for controlling the voice timbre based on perceptual age that maintains the singer's individuality. The experimental results show that the proposed voice timbre control method makes it possible to change the singer's perceptual age while not having an adverse effect on the perceived individuality.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kobayashi, Kazuhiro; Toda, Tomoki; Neubig, Graham; Sakti, Sakriani;
   Nakamura, Satoshi] Nara Inst Sci &amp; Technol NAIST, Grad Sch Informat Sci,
   Nara, Japan.
   <br>[Goto, Masataka; Neubig, Graham] AIST, Natl Inst Adv Ind Sci &amp; Technol,
   Nara, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kobayashi, K (reprint author), Nara Inst Sci &amp; Technol NAIST, Grad Sch Informat Sci, Nara, Japan.</td>
</tr>

<tr xmlns:date="http://exslt.org/dates-and-times">
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Goto, Masataka</display_name>&nbsp;</font></td><td><font size="3">K-8205-2012&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Nakano, Tomoyasu</display_name>&nbsp;</font></td><td><font size="3">A-8670-2013&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Goto, Masataka</display_name>&nbsp;</font></td><td><font size="3">0000-0003-1167-0977&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Nakano, Tomoyasu</display_name>&nbsp;</font></td><td><font size="3">0000-0001-8014-2209&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000343655307189</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Liu, LJ
   <br>Chen, LH
   <br>Ling, ZH
   <br>Dai, LR</td>
</tr>

<tr>
<td valign="top">AF </td><td>Liu, Li-Juan
   <br>Chen, Ling-Hui
   <br>Ling, Zhen-Hua
   <br>Dai, Li-Rong</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>USING BIDIRECTIONAL ASSOCIATIVE MEMORIES FOR JOINT SPECTRAL ENVELOPE
   MODELING IN VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2014 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech and Signal Processing
   (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 04-09, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Florence, ITALY</td>
</tr>

<tr>
<td valign="top">DE </td><td>Spectral envelope modeling; bidirectional associative memory;
   contrastive divergence; voice conversion</td>
</tr>

<tr>
<td valign="top">AB </td><td>The spectral envelope is the most natural representation of speech signal. But in voice conversion, it is difficult to directly model the raw spectral envelope space, which is high dimensional and strongly cross-dimensional correlated, with conventional Gaussian distributions. Bidirectional associative memory (BAM) is a two-layer feedback neural network that can better model the cross-dimensional correlations in high dimensional vectors. In this paper, we propose to reformulate BAMs as Gaussian distributions in order to model the spectral envelope space. The parameters of BAMs are estimated using the contrastive divergence algorithm. The evaluations on likelihood show that BAMs have better modeling ability than Gaussians with diagonal covariance. And the subjective tests on voice conversion indicate that the performance of the proposed method is significantly improved comparing with the conventional GMM based method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Liu, Li-Juan; Chen, Ling-Hui; Ling, Zhen-Hua; Dai, Li-Rong] Univ Sci &amp;
   Technol China, Natl Engn Lab Speech &amp; Language Informat Proc, Hefei
   230026, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Liu, LJ (reprint author), Univ Sci &amp; Technol China, Natl Engn Lab Speech &amp; Language Informat Proc, Hefei 230026, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ljliu037@mail.ustc.edu.cn; chenlh@mail.ustc.edu.cn; zhling@ustc.edu.cn;
   lrdai@ustc.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000343655307185</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Luo, CW
   <br>Yu, J
   <br>Wang, ZF</td>
</tr>

<tr>
<td valign="top">AF </td><td>Luo, Changwei
   <br>Yu, Jun
   <br>Wang, Zengfu</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>SYNTHESIZING REAL-TIME SPEECH-DRIVEN FACIAL ANIMATION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2014 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech and Signal Processing
   (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 04-09, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Florence, ITALY</td>
</tr>

<tr>
<td valign="top">DE </td><td>audio-to-visual conversion; GMM; blendshape; facial animation</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>We present a real-time speech-driven facial animation system. In this system, Gaussian Mixture Models (GMM) are employed to perform the audio-to-visual conversion. The conventional GMM-based method performs the conversion frame by frame using minimum mean square error (MMSE) estimation. The method is reasonably effective. However, discontinuities often appear in the sequences of estimated visual features. To solve this problem, we incorporate previous visual features into the conversion so that the conversion procedure is performed in the manner of a Markov chain. After audio-to-visual conversion, the estimated visual features are transformed to blendshape weights to synthesize facial animation. Experiments show that our system can accurately convert audio features into visual features. The conversion accuracy is comparable to a current state-of-the-art trajectory-based approach. Moreover, our system runs in real time and outputs high quality lip-sync animations.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Luo, Changwei; Yu, Jun; Wang, Zengfu] Univ Sci &amp; Technol China, Dept
   Automat, Hefei, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Luo, CW (reprint author), Univ Sci &amp; Technol China, Dept Automat, Hefei, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>luocw@mail.ustc.edu.cn; harryjun@ustc.edu.cn; zfwang@ustc.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000343655304119</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Masaka, K
   <br>Aihara, R
   <br>Takiguchi, T
   <br>Ariki, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Masaka, Kenta
   <br>Aihara, Ryo
   <br>Takiguchi, Tetsuya
   <br>Ariki, Yasuo</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>MULTIMODAL VOICE CONVERSION USING NON-NEGATIVE MATRIX FACTORIZATION IN
   NOISY ENVIRONMENTS</td>
</tr>

<tr>
<td valign="top">SO </td><td>2014 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech and Signal Processing
   (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 04-09, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Florence, ITALY</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; multimodal; image features; non-negative matrix
   factorization; noise robustness</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH RECOGNITION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a multimodal voice conversion (VC) method for noisy environments. In our previous NMF-based VC method, source exemplars and target exemplars are extracted from parallel training data, in which the same texts are uttered by the source and target speakers. The input source signal is then decomposed into source exemplars, noise exemplars obtained from the input signal, and their weights. Then, the converted speech is constructed from the target exemplars and the weights related to the source exemplars. In this paper, we propose a multimodal VC that improves the noise robustness in our NMF-based VC method. By using the joint audio-visual features as source features, the performance of VC is improved compared to a previous audio-input NMF-based VC method. The effectiveness of this method was confirmed by comparing its effectiveness with that of a conventional Gaussian Mixture Model (GMM)-based method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Masaka, Kenta; Aihara, Ryo; Takiguchi, Tetsuya; Ariki, Yasuo] Kobe
   Univ, Grad Sch Syst Informat, Nada Ku, Kobe, Hyogo 6578501, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Masaka, K (reprint author), Kobe Univ, Grad Sch Syst Informat, Nada Ku, 1-1 Rokkodai, Kobe, Hyogo 6578501, Japan.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000343655301114</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nakashika, T
   <br>Takiguchi, T
   <br>Ariki, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nakashika, Toru
   <br>Takiguchi, Tetsuya
   <br>Ariki, Yasuo</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>VOICE CONVERSION IN TIME-INVARIANT SPEAKER-INDEPENDENT SPACE</td>
</tr>

<tr>
<td valign="top">SO </td><td>2014 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech and Signal Processing
   (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 04-09, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Florence, ITALY</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; conditional restricted Boltzmann machine; deep
   learning; speaker specific features</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH RECOGNITION</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we present a voice conversion (VC) method that utilizes conditional restricted Boltzmann machines (CRBMs) for each speaker to obtain time-invariant speaker-independent spaces where voice features are converted more easily than those in an original acoustic feature space. First, we train two CRBMs for a source and target speaker independently using speaker-dependent training data (without the need to parallelize the training data). Then, a small number of parallel data are fed into each CRBM and the high-order features produced by the CRBMs are used to train a concatenating neural network (NN) between the two CRBMs. Finally, the entire network (the two CRBMs and the NN) is fine-tuned using the acoustic parallel data. Through voice-conversion experiments, we confirmed the high performance of our method in terms of objective and subjective evaluations, comparing it with conventional GMM, NN, and speaker-dependent DBN approaches.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nakashika, Toru; Takiguchi, Tetsuya; Ariki, Yasuo] Kobe Univ, Grad Sch
   Syst Informat, Nada Ku, Kobe, Hyogo 6578501, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nakashika, T (reprint author), Kobe Univ, Grad Sch Syst Informat, Nada Ku, 1-1 Rokkodai, Kobe, Hyogo 6578501, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>nakashika@me.cs.scitec.kobe-u.ac.jp; takigu@kobe-u.ac.jp;
   ariki@kobe-u.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000343655307186</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sawada, K
   <br>Takehara, M
   <br>Tamura, S
   <br>Hayamizu, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sawada, Kohei
   <br>Takehara, Masanori
   <br>Tamura, Satoshi
   <br>Hayamizu, Satoru</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>AUDIO-VISUAL VOICE CONVERSION USING NOISE-ROBUST FEATURES</td>
</tr>

<tr>
<td valign="top">SO </td><td>2014 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech and Signal Processing
   (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 04-09, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Florence, ITALY</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; audio-visual processing; noise robustness; feature
   selection</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice Conversion (VC) is a technique to convert speech data of source speaker into ones of target speaker. VC has been investigated and statistical VC is used for various purposes. Conventional VC uses acoustic features, however, the audio-only VC has suffered from the degradation in noisy or real environments. This paper proposes an Audio-Visual VC (AVVC) method using not only audio features but also visual information, i.e. lip images. Eigenlip feature is employed in our scheme as visual feature. We also propose a feature selection approach for audio-visual features. Experiments were conducted to evaluate our AVVC scheme comparing with audio-only VC, using noisy data. The results show that AVVC can improve the performance even in noisy environments, by properly selecting audio and visual parameters. It is also found that visual VC is also successful. Furthermore, it is observed that visual dynamic features are more effective than visual static information.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Sawada, Kohei; Takehara, Masanori; Tamura, Satoshi; Hayamizu, Satoru]
   Gifu Univ, Dept Engn, Gifu 5011193, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sawada, K (reprint author), Gifu Univ, Dept Engn, 1-1 Yanagido, Gifu 5011193, Japan.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000343655307188</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Tanaka, K
   <br>Toda, T
   <br>Neubig, G
   <br>Sakti, S
   <br>Nakamura, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Tanaka, Kou
   <br>Toda, Tomoki
   <br>Neubig, Graham
   <br>Sakti, Sakriani
   <br>Nakamura, Satoshi</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>AN EVALUATION OF EXCITATION FEATURE PREDICTION IN A HYBRID APPROACH TO
   ELECTROLARYNGEAL SPEECH ENHANCEMENT</td>
</tr>

<tr>
<td valign="top">SO </td><td>2014 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech and Signal Processing
   (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 04-09, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Florence, ITALY</td>
</tr>

<tr>
<td valign="top">DE </td><td>speaking aid; electrolaryngeal speech; hybrid approach; statistical
   excitation prediction; unvoiced/voiced information</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION; SPECTRAL SUBTRACTION</td>
</tr>

<tr>
<td valign="top">AB </td><td>We implement removing micro-prosody with low-pass filtering and avoiding Unvoiced/Voiced (U/V) prediction as part of a hybrid approach to improve statistical excitation prediction in electrolaryngeal (EL) speech enhancement. An electrolarynx is a device that artificially generates excitation sounds to enable laryngectomees to produce EL speech. Although proficient laryngectomees can produce quite intelligible EL speech, it sounds very unnatural due to the mechanical excitation produced by the device. Moreover, the excitation sounds produced by the device often leak outside, adding noise to EL speech. To address these issues, in our previous work, we proposed a hybrid method using a noise reduction method for enhancing spectral parameters and voice conversion method for predicting excitation parameters. In this paper, we evaluate the effect of removing micro-prosody with low-pass filtering and avoiding U/V prediction in the hybrid enhancement process.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Tanaka, Kou; Toda, Tomoki; Neubig, Graham; Sakti, Sakriani; Nakamura,
   Satoshi] Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Tanaka, K (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000343655304103</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Petrovsky, A
   <br>Azarov, E</td>
</tr>

<tr>
<td valign="top">AF </td><td>Petrovsky, Alexander
   <br>Azarov, Elias</td>
</tr>

<tr>
<td valign="top">BE </td><td>Ronzhin, A
   <br>Potapova, R
   <br>Delic, V</td>
</tr>

<tr>
<td valign="top">TI </td><td>Instantaneous Harmonic Analysis: Techniques and Applications to Speech
   Signal Processing</td>
</tr>

<tr>
<td valign="top">SO </td><td>SPEECH AND COMPUTER</td>
</tr>

<tr>
<td valign="top">SE </td><td>Lecture Notes in Artificial Intelligence</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>16th International Conference on Speech and Computer (SPECOM)</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 05-09, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Novi Sad, SERBIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speech processing; instantaneous frequency; harmonic model</td>
</tr>

<tr>
<td valign="top">ID </td><td>REPRESENTATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Parametric speech modeling is a key issue in various processing applications such as text to speech synthesis, voice morphing, voice conversion and other. Building an adequate parametric model is a complicated problem considering time-varying nature of speech. This paper gives an overview of tools for instantaneous harmonic analysis and shows how it can be applied to stationary, frequency-modulated and quasiperiodic signals in order to extract and manipulate instantaneous pitch, excitation and spectrum envelope.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Petrovsky, Alexander; Azarov, Elias] Belarusian State Univ Informat &amp;
   Radioelect, Dept Comp Engn, Minsk, BELARUS.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Petrovsky, A (reprint author), Belarusian State Univ Informat &amp; Radioelect, Dept Comp Engn, Minsk, BELARUS.</td>
</tr>

<tr>
<td valign="top">EM </td><td>palex@bsuir.by; azarov@bsuir.by</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">VL </td><td>8773</td>
</tr>

<tr>
<td valign="top">BP </td><td>24</td>
</tr>

<tr>
<td valign="top">EP </td><td>33</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000345576400003</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Doi, H
   <br>Toda, T
   <br>Nakamura, K
   <br>Saruwatari, H
   <br>Shikano, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Doi, Hironori
   <br>Toda, Tomoki
   <br>Nakamura, Keigo
   <br>Saruwatari, Hiroshi
   <br>Shikano, Kiyohiro</td>
</tr>

<tr>
<td valign="top">TI </td><td>Alaryngeal Speech Enhancement Based on One-to-Many Eigenvoice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Alaryngeal speech; eigenvoice conversion; laryngectomees; speech
   enhancement; voice conversion.</td>
</tr>

<tr>
<td valign="top">ID </td><td>FREQUENCY CEPSTRAL COEFFICIENTS; STATISTICAL VOICE CONVERSION;
   PREDICTION</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we present novel speaking- aid systems based on one-to- many eigenvoice conversion (EVC) to enhance three types of alaryngeal speech: esophageal speech, electrolaryngeal speech, and body-conducted silent electrolaryngeal speech. Although alaryngeal speech allows laryngectomees to utter speech sounds, it suffers from the lack of speech quality and speaker individuality. To improve the speech quality of alaryngeal speech, alaryngeal-speech- to-speech (AL-to- Speech) methods based on statistical voice conversion have been proposed. In this paper, one-to- many EVC capable of flexibly controlling the converted voice quality by adapting the conversion model to given target natural voices is further implemented for the AL-to-Speech methods to effectively recover speaker individuality of each type of alaryngeal speech. These proposed systems are compared with each other from various perspectives. The experimental results demonstrate that our proposed systems are capable of effectively addressing the issues of alaryngeal speech, e. g., yielding significant improvements in speech quality of each type of alaryngeal speech.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Doi, Hironori; Toda, Tomoki; Nakamura, Keigo; Saruwatari, Hiroshi;
   Shikano, Kiyohiro] Nara Inst Sci &amp; Technol, Grad Sch Informat Sci,
   Ikoma, Nara 6300192, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Doi, H (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Ikoma, Nara 6300192, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>hironori-d@is.naist.jp; tomoki@is.naist.jp; sawatari@is.naist.jp;
   shikano@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>19</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>22</td>
</tr>

<tr>
<td valign="top">PD </td><td>JAN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">VL </td><td>22</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>172</td>
</tr>

<tr>
<td valign="top">EP </td><td>183</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASLP.2013.2286917</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000332614400015</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kuo, CFJ
   <br>Wang, HW
   <br>Hsiao, SW
   <br>Peng, KC
   <br>Chou, YL
   <br>Lai, CY
   <br>Hsu, CTM</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kuo, Chung-Feng Jeffrey
   <br>Wang, Hsing-Won
   <br>Hsiao, Shang-Wun
   <br>Peng, Kai-Ching
   <br>Chou, Ying-Liang
   <br>Lai, Chun-Yu
   <br>Hsu, Chien-Tung Max</td>
</tr>

<tr>
<td valign="top">TI </td><td>Development of laryngeal video stroboscope with laser marking module for
   dynamic glottis measurement</td>
</tr>

<tr>
<td valign="top">SO </td><td>COMPUTERIZED MEDICAL IMAGING AND GRAPHICS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Laryngeal video stroboscope; Glottis physiological parameters; Digital
   image processing; Laser projection marking module; The area of glottis</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOCAL FOLD VIBRATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Physicians clinically use laryngeal video stroboscope as an auxiliary instrument to test glottal diseases, and read vocal fold images and voice quality for diagnosis. As the position of vocal fold varies in each person, the proportion of the vocal fold size as presented in the vocal fold image is different, making it impossible to directly estimate relevant glottis physiological parameters, such as the length, area, perimeter, and opening angle of the glottis. Hence, this study designs an innovative laser projection marking module for the laryngeal video stroboscope to provide reference parameters for image scaling conversion. This innovative laser projection marking module to be installed on the laryngeal video stroboscope using laser beams to project onto the glottis plane, in order to provide reference parameters for scaling conversion of images of laryngeal video stroboscope. (C) 2013 Elsevier Ltd. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kuo, Chung-Feng Jeffrey; Peng, Kai-Ching; Lai, Chun-Yu; Hsu, Chien-Tung
   Max] Natl Taiwan Univ Sci &amp; Technol, Grad Inst Automat &amp; Control, Taipei
   106, Taiwan.
   <br>[Wang, Hsing-Won] Taipei Med Univ, Shuang Ho Hosp, Coll Med, Grad Inst
   Clin Med, Taipei, Taiwan.
   <br>[Wang, Hsing-Won] Taipei Med Univ, Shuang Ho Hosp, Coll Med, Dept
   Otolaryngol, New Taipei City, Taiwan.
   <br>[Hsiao, Shang-Wun] Natl Taiwan Univ Sci &amp; Technol, Dept Mat Sci &amp; Engn,
   Taipei 106, Taiwan.
   <br>[Chou, Ying-Liang] Taichung Armed Forces Gen Hosp, Dept
   Otorhinolaryngol, Taichung, Taiwan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kuo, CFJ (reprint author), Natl Taiwan Univ Sci &amp; Technol, Grad Inst Automat &amp; Control, Taipei 106, Taiwan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jeffreykuo@mail.ntust.edu.tw</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PD </td><td>JAN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">VL </td><td>38</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>34</td>
</tr>

<tr>
<td valign="top">EP </td><td>41</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.compmedimag.2013.10.004</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Radiology, Nuclear Medicine &amp; Medical Imaging</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000329953900004</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Tanaka, K
   <br>Toda, T
   <br>Neubig, G
   <br>Sakti, S
   <br>Nakamura, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Tanaka, Kou
   <br>Toda, Tomoki
   <br>Neubig, Graham
   <br>Sakti, Sakriani
   <br>Nakamura, Satoshi</td>
</tr>

<tr>
<td valign="top">BE </td><td>Li, H
   <br>Ching, P</td>
</tr>

<tr>
<td valign="top">TI </td><td>Direct F0 Control of an Electrolarynx based on Statistical Excitation
   Feature Prediction and its Evaluation through Simulation</td>
</tr>

<tr>
<td valign="top">SO </td><td>15TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2014), VOLS 1-4</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>15th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2014)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 14-18, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Singapore, SINGAPORE</td>
</tr>

<tr>
<td valign="top">DE </td><td>laryngectomee; electrolarynx; electrolaryngeal speech; statistical
   excitation prediction; simulation evaluation</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION; SPEECH SYNTHESIS; ENHANCEMENT</td>
</tr>

<tr>
<td valign="top">AB </td><td>An electrolarynx is a device that artificially generates excitation sounds to enable laryngectomees to produce electrolaryngeal (EL) speech. Although proficient laryngectomees can produce quite intelligible EL speech, it sounds very unnatural due to the mechanical excitation produced by the device. To address this issue, we have proposed several EL speech enhancement methods using statistical voice conversion and showed that statistical prediction of excitation parameters, such as F-0 patterns, was essential to significantly improve naturalness of EL speech. In these methods, the original EL speech is recorded with a microphone and the enhanced EL speech is presented from a loudspeaker in real time. This framework is effective for telecommunication but it is not suitable to face-to-face conversation because both the original EL speech and the enhanced EL speech are presented to listeners. In this paper, we propose direct F-0 control of the electrolarynx based on statistical excitation prediction to develop an EL speech enhancement technique also effective for face-to-face conversation. F-0 patterns of excitation signals produced by the electrolarynx are predicted in real time from the EL speech produced by the laryngectomee's articulation of the excitation signals with previously predicted F-0 values. A simulation experiment is conducted to evaluate the effectiveness of the proposed method. The experimental results demonstrate that the proposed method yields significant improvements in naturalness of EL speech while keeping its intelligibility high enough.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Tanaka, Kou; Toda, Tomoki; Neubig, Graham; Sakti, Sakriani; Nakamura,
   Satoshi] Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Ikoma, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Tanaka, K (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Ikoma, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ko-t@is.naist.jp; tomoki@is.naist.jp; neubig@is.naist.jp;
   ssakti@is.naist.jp; s-nakamura@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">BP </td><td>31</td>
</tr>

<tr>
<td valign="top">EP </td><td>35</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000395050100007</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Khoury, E
   <br>Kinnunen, T
   <br>Sizov, A
   <br>Wu, ZZ
   <br>Marcel, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Khoury, Elie
   <br>Kinnunen, Tomi
   <br>Sizov, Aleksandr
   <br>Wu, Zhizheng
   <br>Marcel, Sebastien</td>
</tr>

<tr>
<td valign="top">BE </td><td>Li, H
   <br>Ching, P</td>
</tr>

<tr>
<td valign="top">TI </td><td>Introducing I-Vectors for Joint Anti-spoofing and Speaker Verification</td>
</tr>

<tr>
<td valign="top">SO </td><td>15TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2014), VOLS 1-4</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>15th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2014)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 14-18, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Singapore, SINGAPORE</td>
</tr>

<tr>
<td valign="top">DE </td><td>speaker recognition; spoofing; voice conversion; attack; i-vector; joint
   verification and anti-spoofing</td>
</tr>

<tr>
<td valign="top">ID </td><td>BIOMETRICS; SECURITY; SYSTEMS; FUSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Any biometric recognizer is vulnerable to direct spoofing attacks and automatic speaker verification (ASV) is no exception; replay, synthesis and conversion attacks all provoke false acceptances unless countermeasures are used. We focus on voice conversion (VC) attacks. Most existing countermeasures use full knowledge of a particular VC system to detect spoofing. We study a potentially more universal approach involving generative modeling perspective. Specifically, we adopt standard i-vector representation and probabilistic linear discriminant analysis (PLDA) back-end for joint operation of spoofing attack detector and ASV system. As a proof of concept, we study a vocoder-mismatched ASV and VC attack detection approach on the NIST 2006 speaker recognition evaluation corpus. We report stand-alone accuracy of both the ASV and countermeasure systems as well as their combination using score fusion and joint approach. The method holds promise.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Khoury, Elie; Marcel, Sebastien] Idiap Res Inst, Martigny, Switzerland.
   <br>[Kinnunen, Tomi; Sizov, Aleksandr] Univ Eastern Finland, Sch Comp,
   Joensuu, Finland.
   <br>[Wu, Zhizheng] Nanyang Technol Univ, Sch Comp Engn, Singapore, Singapore.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Khoury, E (reprint author), Idiap Res Inst, Martigny, Switzerland.</td>
</tr>

<tr>
<td valign="top">TC </td><td>9</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>9</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">BP </td><td>61</td>
</tr>

<tr>
<td valign="top">EP </td><td>65</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000395050100013</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ramani, B
   <br>Jeeva, MPA
   <br>Vijayalakshmi, P
   <br>Nagarajan, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ramani, B.
   <br>Jeeva, Actlin M. P.
   <br>Vijayalakshmi, P.
   <br>Nagarajan, T.</td>
</tr>

<tr>
<td valign="top">BE </td><td>Li, H
   <br>Ching, P</td>
</tr>

<tr>
<td valign="top">TI </td><td>Cross-Lingual Voice Conversion-Based Polyglot Speech Synthesizer for
   Indian Languages</td>
</tr>

<tr>
<td valign="top">SO </td><td>15TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2014), VOLS 1-4</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>15th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2014)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 14-18, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Singapore, SINGAPORE</td>
</tr>

<tr>
<td valign="top">DE </td><td>polyglot; GMM; cross-lingual voice conversion</td>
</tr>

<tr>
<td valign="top">AB </td><td>A polyglot speech synthesizer, synthesizes speech for any given monolingual or multilingual text, in a single speaker's voice. In this regard, a polyglot speech corpus is required. It is difficult to find a speaker proficient in multiple languages. Therefore, in the current work, by exploiting the acoustic similarity of phonemes across Indian languages, a polyglot speech corpus is obtained for four Indian languages and Indian English, using GMM-based cross-lingual voice conversion. The optimum target speaker and GMM topology is chosen based on the performance of a speaker identification system. It is observed that, the language that shares the most number of phonemes with the other languages, serves as the best target. A polyglot speech corpus derived in this target speaker's voice, is further used to develop an HMM-based polyglot speech synthesizer. The performance of this synthesizer is evaluated in terms of speaker identity using ABX listening test, quality using mean opinion score (MOS) and speaker switching using subjective listening test.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ramani, B.; Jeeva, Actlin M. P.; Vijayalakshmi, P.; Nagarajan, T.] SSN
   Coll Engn, Madras, Tamil Nadu, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ramani, B (reprint author), SSN Coll Engn, Madras, Tamil Nadu, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>vijayalakshmip@ssn.edu.in; nagarajant@ssn.edu.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">BP </td><td>775</td>
</tr>

<tr>
<td valign="top">EP </td><td>779</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000395050100158</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Masaka, K
   <br>Aihara, R
   <br>Takiguchi, T
   <br>Ariki, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Masaka, Kenta
   <br>Aihara, Ryo
   <br>Takiguchi, Tetsuya
   <br>Ariki, Yasuo</td>
</tr>

<tr>
<td valign="top">BE </td><td>Li, H
   <br>Ching, P</td>
</tr>

<tr>
<td valign="top">TI </td><td>Multimodal Exemplar-based Voice Conversion using Lip Features in Noisy
   Environments</td>
</tr>

<tr>
<td valign="top">SO </td><td>15TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2014), VOLS 1-4</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>15th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2014)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 14-18, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Singapore, SINGAPORE</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; multimodal; image features; non-negative matrix
   factorization; noise robustness</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH RECOGNITION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a multimodal voice conversion (VC) method for noisy environments. In our previous exemplar based VC method, source exemplars and target exemplars are extracted from parallel training data, in which the same texts are uttered by the source and target speakers. The input source signal is then decomposed into source exemplars, noise exemplars obtained from the input signal, and their weights. Then, the converted speech is constructed from the target exemplars and the weights related to the source exemplars. In this paper, we propose a multimodal VC method that improves the noise robustness of our previous exemplar-based VC method. As visual features, we use not only conventional DCT but also the features extracted from Active Appearance Model (AAM) applied to the lip area of a face image. Furthermore, we introduce the combination weight between audio and visual features and formulate a new cost function in order to estimate the audiovisual exemplars. By using the joint audio-visual features as source features, the VC performance is improved compared to a previous audio-input exemplar-based VC method. The effectiveness of this method was confirmed by comparing its effectiveness with that of a conventional Gaussian Mixture Model (GMM)-based method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Masaka, Kenta; Aihara, Ryo; Takiguchi, Tetsuya; Ariki, Yasuo] Kobe
   Univ, Grad Sch Syst Informat, Nada Ku, 1-1 Rokkodai, Kobe, Hyogo
   6578501, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Masaka, K (reprint author), Kobe Univ, Grad Sch Syst Informat, Nada Ku, 1-1 Rokkodai, Kobe, Hyogo 6578501, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>makka@me.cs.scitec.kobeu.ac.jp; aihara@me.cs.scitec.kobeu.ac.jp;
   takigu@kobeu.ac.jp; ariki@kobeu.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>4</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>4</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">BP </td><td>1159</td>
</tr>

<tr>
<td valign="top">EP </td><td>1163</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000395050100236</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Zahner, M
   <br>Janke, M
   <br>Wand, M
   <br>Schultz, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Zahner, Marlene
   <br>Janke, Matthias
   <br>Wand, Michael
   <br>Schultz, Tanja</td>
</tr>

<tr>
<td valign="top">BE </td><td>Li, H
   <br>Ching, P</td>
</tr>

<tr>
<td valign="top">TI </td><td>Conversion from Facial Myoelectric Signals to Speech: A Unit Selection
   Approach</td>
</tr>

<tr>
<td valign="top">SO </td><td>15TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2014), VOLS 1-4</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>15th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2014)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 14-18, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Singapore, SINGAPORE</td>
</tr>

<tr>
<td valign="top">DE </td><td>electromyography; silent speech interface; unit selection</td>
</tr>

<tr>
<td valign="top">ID </td><td>CLASSIFICATION; ALGORITHM; MODEL</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper reports on our recent research on surface electromyographic (EMG) speech synthesis: a direct conversion of the EMG signals of the articulatory muscle movements to the acoustic speech signal. In this work we introduce a unit selection approach which compares segments of the input EMG signal to a database of simultaneously recorded EMG/audio unit pairs and selects the best matching audio unit based on target and concatenation cost, which will be concatenated to synthesize an acoustic speech output. We show that this approach is feasible to generate a proper speech output from the input EMG signal. We evaluate different properties of the units and investigate what amount of data is necessary for an initial transformation. Prior work on EMG-to-speech conversion used a frame based approach from the voice conversion domain, which struggles with the generation of a natural F-0 contour. This problem may also be tackled by our unit selection approach.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Zahner, Marlene; Janke, Matthias; Wand, Michael; Schultz, Tanja]
   Karlsruhe Inst Technol, Cognit Syst Lab, Karlsruhe, Germany.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Zahner, M (reprint author), Karlsruhe Inst Technol, Cognit Syst Lab, Karlsruhe, Germany.</td>
</tr>

<tr>
<td valign="top">EM </td><td>matthias.janke@kit.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>4</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>4</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">BP </td><td>1184</td>
</tr>

<tr>
<td valign="top">EP </td><td>1188</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000395050100241</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Qavi, A
   <br>Khan, SA
   <br>Basir, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Qavi, Abdul
   <br>Khan, Shoab Ahmad
   <br>Basir, Kashif</td>
</tr>

<tr>
<td valign="top">BE </td><td>Rasheed, H
   <br>Lakhani, F
   <br>Maheshwari, MK</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Morphing Based on Spectral Features and Prosodic Modification</td>
</tr>

<tr>
<td valign="top">SO </td><td>17TH IEEE INTERNATIONAL MULTI TOPIC CONFERENCE 2014</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>17th IEEE International Multi Topic Conference (INMIC)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 08-10, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Karachi, PAKISTAN</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper is aimed at morphing the speech uttered by a source speaker in a manner that it seems to be spoken by another target speaker - a new identity is given while preserving the original content. The proposed method transforms the vocal tract parameters and glottal excitation of the source speaker into target speaker's acoustic characteristics. It relates to the development of appropriate vocal tract models that can capture information specific to the speaker and estimate the model parameters that closely relate to the model of the target speaker. It detects the pitch, separates the glottal excitation and vocal tract spectral features. The glottal excitation of the source is taken, voice/ un- voice decision is made, the prosody information is found, PSOLA (Pitch Synchronous OverLap Add) is used to modify the pitch, the spectral features are found, and finally speech is modified using target spectral features and prosody. The subjective experiment shows that the proposed method improves the quality of conversion and contains the original vocal and glottal characteristics of the target speaker.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Qavi, Abdul] Ctr Adv Studies Engn, Dept Elect &amp; Comp Engn, Islamabad,
   Pakistan.
   <br>Natl Univ Sci &amp; Technol, CEME, Islamabad, Pakistan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Qavi, A (reprint author), Ctr Adv Studies Engn, Dept Elect &amp; Comp Engn, Islamabad, Pakistan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>aqavi_paracha@yahoo.com; shoab@case.edu.pk; kashifbasir@yahoo.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">BP </td><td>401</td>
</tr>

<tr>
<td valign="top">EP </td><td>405</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380411400074</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sultana, R
   <br>Palit, R</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sultana, Rumia
   <br>Palit, Ratesh</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Survey on Bengali Speech-to-Text Recognition Techniques</td>
</tr>

<tr>
<td valign="top">SO </td><td>2014 9TH INTERNATIONAL FORUM ON STRATEGIC TECHNOLOGY (IFOST)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th International Forum on Strategic Technology (IFOST)</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 21-23, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Coxs Bazar, BANGLADESH</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speech Recognition; Speech-To-Text; Bengali Speech</td>
</tr>

<tr>
<td valign="top">AB </td><td>With the advancement of science and technology, automated speech-to-text (STT) conversion system has been developed to make a visual text format of the speech. This technology enables people with listening disability to communicate in an alternative way to understand voice communication, and follow instructions using their visual ability. Sometimes visual ability becomes more powerful than the listening ability specially in distant communication, and speech-to-text conversion fits as an important tool in such cases. Speech-to-text research has found new idea to help the handicap people with the voice prompted writing tools. Research in Bengali speech recognition field is still in primary stage. A large amount of progressive work are required in the field of Bengali speech-to-text conversion. This paper can be considered as a reference for interested researchers working in Bengali speech-to-text conversion system as it provides sufficient information to develop primary understanding about the Bengali speech-to-text literature.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Sultana, Rumia; Palit, Ratesh] North South Univ, Dept Elect &amp; Comp
   Engn, Dhaka, Bangladesh.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sultana, R (reprint author), North South Univ, Dept Elect &amp; Comp Engn, Dhaka, Bangladesh.</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Palit, Rajesh</display_name>&nbsp;</font></td><td><font size="3">0000-0003-3952-3653&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">BP </td><td>26</td>
</tr>

<tr>
<td valign="top">EP </td><td>29</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000392872100007</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Aihara, R
   <br>Ueda, R
   <br>Takiguchi, T
   <br>Ariki, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Aihara, Ryo
   <br>Ueda, Reina
   <br>Takiguchi, Tetsuya
   <br>Ariki, Yasuo</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Exemplar-based Emotional Voice Conversion Using Non-negative Matrix
   Factorization</td>
</tr>

<tr>
<td valign="top">SO </td><td>2014 ASIA-PACIFIC SIGNAL AND INFORMATION PROCESSING ASSOCIATION ANNUAL
   SUMMIT AND CONFERENCE (APSIPA)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Asia-Pacific Signal and Information Processing Association Annual Summit
   and Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Annual Summit and Conference of
   Asia-Pacific-Signal-and-Information-Processing-Association (APSIPA)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 09-12, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Angkor, CAMBODIA</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH SYNTHESIS</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents an emotional voice conversion (VC) technology using non-negative matrix factorization, where parallel exemplars are introduced to encode the source speech signal and synthesize the target speech signal. The input source spectrum is decomposed into the source spectrum exemplars and their weights. By replacing source exemplars with target exemplars, the converted spectrum and F0 are constructed from the target exemplars and the target F0, which is paired with exemplars. In order to reduce the computational time, we adopted non-negative matrix factorization using active Newton set algorithms to our VC method. We carried out emotional voice conversion tasks, which convert an emotional voice into a neutral voice. The effectiveness of this method was confirmed with objective and subjective evaluations.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Aihara, Ryo; Ueda, Reina; Takiguchi, Tetsuya; Ariki, Yasuo] Kobe Univ,
   Grad Sch Syst Informat, Kobe, Hyogo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Aihara, R (reprint author), Kobe Univ, Grad Sch Syst Informat, Kobe, Hyogo, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>aihara@me.cs.scitec.kobe-u.ac.jp; reina_1102@me.cs.scitec.kobe-u.ac.jp;
   takigu@kobe-u.ac.jp; ariki@kobe-u.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000392861900128</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kobayashi, K
   <br>Toda, T
   <br>Nakano, T
   <br>Goto, M
   <br>Neubig, G
   <br>Sakti, S
   <br>Nakamura, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kobayashi, Kazuhiro
   <br>Toda, Tomoki
   <br>Nakano, Tomoyasu
   <br>Goto, Masataka
   <br>Neubig, Graham
   <br>Sakti, Sakriani
   <br>Nakamura, Satoshi</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Gender-dependent Spectrum Differential Models for Perceived Age Control
   based on Direct Waveform Modification in Singing Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>2014 ASIA-PACIFIC SIGNAL AND INFORMATION PROCESSING ASSOCIATION ANNUAL
   SUMMIT AND CONFERENCE (APSIPA)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Asia-Pacific Signal and Information Processing Association Annual Summit
   and Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Annual Summit and Conference of
   Asia-Pacific-Signal-and-Information-Processing-Association (APSIPA)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 09-12, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Angkor, CAMBODIA</td>
</tr>

<tr>
<td valign="top">AB </td><td>The perceived age of a singing voice, which is the age of the singer as perceived by the listener, is one of the intuitively understandable measures to describe voice characteristics of the singing voice. Singers can sing expressively by controlling voice timbre to some extent but the varieties of voice timbre that singers can produce are limited by physical constraints. To overcome this limitation, previous work has proposed statistical voice timbre control technique based on the perceived age. This technique makes it possible to control the perceived age of singing voice while retaining singer individuality by the use of statistical voice conversion (SVC) with a multiple-regression Gaussian mixture model (MR-GMM). However, the range of controllable perceived age is limited and speech quality of the converted singing voice is significantly degraded compared to that of a natural singing voice. In this paper, we propose a method for perceived age control using direct waveform modification based on spectrum differential and gender-dependent modeling. The experimental results show that the proposed method makes the range of controllable perceived age wider and quality of converted singing voice higher compared to the conventional method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kobayashi, Kazuhiro; Toda, Tomoki; Neubig, Graham; Sakti, Sakriani;
   Nakamura, Satoshi] Nara Inst Sci &amp; Technol, Grad Sch Informat Sci,
   Takayama 8916-5, Ikoma, Nara 6300192, Japan.
   <br>[Nakano, Tomoyasu; Goto, Masataka] Natl Inst Adv Ind Sci &amp; Technol,
   Tsukuba, Ibaraki 3058568, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kobayashi, K (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Takayama 8916-5, Ikoma, Nara 6300192, Japan.</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Nakano, Tomoyasu</display_name>&nbsp;</font></td><td><font size="3">A-8670-2013&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Goto, Masataka</display_name>&nbsp;</font></td><td><font size="3">K-8205-2012&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Nakano, Tomoyasu</display_name>&nbsp;</font></td><td><font size="3">0000-0001-8014-2209&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Goto, Masataka</display_name>&nbsp;</font></td><td><font size="3">0000-0003-1167-0977&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000392861900078</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Takamichi, S
   <br>Toda, T
   <br>Black, AW
   <br>Nakamura, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Takamichi, Shinnosuke
   <br>Toda, Tomoki
   <br>Black, Alan W.
   <br>Nakamura, Satoshi</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Modulation Spectrum-Based Post-Filter for GMM-Based Voice Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>2014 ASIA-PACIFIC SIGNAL AND INFORMATION PROCESSING ASSOCIATION ANNUAL
   SUMMIT AND CONFERENCE (APSIPA)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Asia-Pacific Signal and Information Processing Association Annual Summit
   and Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Annual Summit and Conference of
   Asia-Pacific-Signal-and-Information-Processing-Association (APSIPA)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 09-12, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Angkor, CAMBODIA</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper addresses an over-smoothing effect in Gaussian Mixture Model (GMM)-based Voice Conversion (VC). The flexible use of the statistical approach is one of the major reason why this approach is widely applied to the speech based systems. However, quality degradation by over-smoothed speech parameter converted is unavoidable problem of statistical modeling. One of common approaches to this over-smoothness in conversion step is to compensate generated features, such as Global Variance (GV), that explicitly express the over-smoothing effect. In statistical Text-To-Speech (TTS) synthesis, we have recently introduced a Modulation Spectrum (MS) which is an extended form of GV, and have proposed MS-based Post Filter (MSPF) in Hidden Markov Model (HMM)-based TTS synthesis. In this paper, we apply the MSPF to GMM-based VC. Because the MS of speech parameters is degraded through GMM-based conversion process, we perform the post-filter due to MS modification of converted parameters. The experimental evaluation yields the quality benefits by the proposed post-filter.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Takamichi, Shinnosuke; Toda, Tomoki; Nakamura, Satoshi] Nara Inst Sci &amp;
   Technol NAIST, Grad Sch Informat Sci, Ikoma, Nara, Japan.
   <br>[Takamichi, Shinnosuke; Black, Alan W.] Carnegie Mellon Univ, Language
   Technol Inst, Pittsburgh, PA 15213 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Takamichi, S (reprint author), Nara Inst Sci &amp; Technol NAIST, Grad Sch Informat Sci, Ikoma, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>shinnosuke-t@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000392861900028</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Tsuruta, S
   <br>Tanaka, K
   <br>Toda, T
   <br>Neubig, G
   <br>Sakti, S
   <br>Nakamura, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Tsuruta, Sakura
   <br>Tanaka, Kou
   <br>Toda, Tomoki
   <br>Neubig, Graham
   <br>Sakti, Sakriani
   <br>Nakamura, Satoshi</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>An Evaluation of Target Speech for a Nonaudible Murmur Enhancement
   System in Noisy Environments</td>
</tr>

<tr>
<td valign="top">SO </td><td>2014 ASIA-PACIFIC SIGNAL AND INFORMATION PROCESSING ASSOCIATION ANNUAL
   SUMMIT AND CONFERENCE (APSIPA)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Asia-Pacific Signal and Information Processing Association Annual Summit
   and Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Annual Summit and Conference of
   Asia-Pacific-Signal-and-Information-Processing-Association (APSIPA)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 09-12, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Angkor, CAMBODIA</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Nonaudible murmur (NAM) is a soft whispered voice recorded with NAM microphone through body conduction. NAM allows for silent speech communication as it makes it possible for the speaker to convey their message in a nonaudible voice. However, its intelligibility and naturalness are significantly degraded compared to those of natural speech owing to acoustic changes caused by body conduction. To address this issue, statistical voice conversion (VC) methods from NAM to normal speech (NAM-to-Speech) and to a whispered voice (NAM-to Whisper) have been proposed. It has been reported that these NAM enhancement methods significantly improve speech quality and intelligibility of NAM, and NAM-to-Whisper is more effective than NAM-to-Speech. However, it is still not obvious which method is more effective if a listener listens to the enhanced speech in noisy environments, a situation that often happens in silent speech communication. In this paper, assuming a typical situation in which NAM is uttered by a speaker in a quiet environment and conveyed to a listener in noisy environments, we investigate what kinds of target speech are more effective for NAM enhancement. We also propose NAM enhancement methods for converting NAM to other types of target voiced speech. Experiments show that the conversion process into voiced speech is more effective than that into unvoiced speech for generating more intelligible speech in noisy environments.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Tsuruta, Sakura; Tanaka, Kou; Toda, Tomoki; Neubig, Graham; Sakti,
   Sakriani; Nakamura, Satoshi] Nara Inst Sci &amp; Technol NAIST, Grad Sch
   Informat Sci, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Tsuruta, S (reprint author), Nara Inst Sci &amp; Technol NAIST, Grad Sch Informat Sci, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>tsumta.sakura.t10@is.naist.jp; ko-t@is.naist.jp; tomoki@is.naist.jp;
   neubig@is.naist.jp; ssakti@is.naist.jp; s-nakamura@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000392861900106</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Zhu, FY
   <br>Fan, ZY
   <br>Wu, XH</td>
</tr>

<tr>
<td valign="top">AF </td><td>Zhu, Fengyun
   <br>Fan, Ziye
   <br>Wu, Xihong</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>VOICE CONVERSION USING CONDITIONAL RESTRICTED BOLTZMANN MACHINE</td>
</tr>

<tr>
<td valign="top">SO </td><td>2014 IEEE CHINA SUMMIT &amp; INTERNATIONAL CONFERENCE ON SIGNAL AND
   INFORMATION PROCESSING (CHINASIP)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>2nd IEEE China Summit / International Conference on Signal and
   Information Processing (IEEE ChinaSIP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUL 09-13, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Xian, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; Conditional restricted Boltzmann machine</td>
</tr>

<tr>
<td valign="top">ID </td><td>TRANSFORMATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we proposed a new method for voice conversion using conditional restricted Boltzmann machine (Conditional RBM, CRBM). The joint distribution of source and target acoustic features are modeled by the RBM part of the model. Short-term temporal constraints are introduced by conditioning on contextual frames, say, the past and future frames of the source speaker. In contrast to conventional methods, temporal structure of the data could be modeled without using dynamic features. Objective and subjective experiments were conducted to evaluate the method. Experimental results show that short-term temporal structure could be modeled well by CRBM, and the proposed method outperforms conventional joint density Gaussian mixture models based method significantly.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Zhu, Fengyun; Fan, Ziye; Wu, Xihong] Peking Univ, Sch Elect Engn &amp; Comp
   Sci, Minist Educ, Speech &amp; Hearing Res Ctr,Key Lab Machine Percept,
   Beijing 100871, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Zhu, FY (reprint author), Peking Univ, Sch Elect Engn &amp; Comp Sci, Minist Educ, Speech &amp; Hearing Res Ctr,Key Lab Machine Percept, Beijing 100871, Peoples R China.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">BP </td><td>110</td>
</tr>

<tr>
<td valign="top">EP </td><td>114</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000366612600023</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Toda, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Toda, Tomoki</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Augmented Speech Production based on Real-Time Statistical Voice
   Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>2014 IEEE GLOBAL CONFERENCE ON SIGNAL AND INFORMATION PROCESSING
   (GLOBALSIP)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE Global Conference on Signal and Information Processing (GlobalSIP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 03-05, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Atlanta, GA</td>
</tr>

<tr>
<td valign="top">DE </td><td>human-to-human speech communication enhancement; augmented speech
   production; statistical voice conversion; real-time processing</td>
</tr>

<tr>
<td valign="top">ID </td><td>ONE-TO-MANY; SPARSE REPRESENTATION; SPEAKER ADAPTATION; PROSODY
   CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>In human-to-human speech communication, various barriers are caused by some constraints, such as physical constraints causing vocal disorders and environmental constraints making it hard to produce intelligible speech. These barriers would be overcome if our speech production was augmented so that we could produce speech sounds as we want beyond these constraints. Voice conversion (VC) is a technique for modifying speech acoustics, converting non-/para-linguistic information to any form we want while preserving the linguistic content. One of the most popular approaches to VC is based on statistical processing, which is capable of extracting a complex conversion function in a data-driven manner. Although this technique was originally studied in the context of speaker conversion, which converts the voice of a certain speaker to sound like that of another specific speaker, it has great potential to achieve various applications beyond speaker conversion. This paper briefly reviews a trajectory-based conversion method that is capable of effectively reproducing natural speech parameter trajectories utterance by utterance and highlights several techniques that extend this trajectory-based conversion method to achieve real-time conversion processing. Finally this paper shows some examples of real-time VC applications to enhance human-to-human speech communication, such as speaking-aid, silent speech communication, and voice changer/vocal effector.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Toda, Tomoki] Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Takayama
   8916-5, Ikoma, Nara 6300192, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Toda, T (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Takayama 8916-5, Ikoma, Nara 6300192, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>tomoki@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">BP </td><td>592</td>
</tr>

<tr>
<td valign="top">EP </td><td>596</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000382032100121</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nematollahi, MA
   <br>Al-Haddad, SAR
   <br>Doraisamy, S
   <br>Ranjbari, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nematollahi, M. A.
   <br>Al-Haddad, S. A. R.
   <br>Doraisamy, Shyamala
   <br>Ranjbari, M.</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Digital Speech Watermarking for Anti-Spoofing Attack in Speaker
   Recognition</td>
</tr>

<tr>
<td valign="top">SO </td><td>2014 IEEE REGION 10 SYMPOSIUM</td>
</tr>

<tr>
<td valign="top">SE </td><td>IEEE Region 10 Symposium</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE Region 10 Symposium</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 14-16, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Kuala Lumpur, MALAYSIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Digital speech watermarking; speaker recognition; spoofing attack</td>
</tr>

<tr>
<td valign="top">ID </td><td>SECURITY IMPROVEMENT; VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents new method for improving the security of speaker recognition in case of spoofing attack. In the proposed technique, digital speech watermarking has been applied on speech signal to increase robustness. To achieve this purpose, watermark is embedded in claimed signal at transmission side and then it is sent through the unsecure channel. In receiver side, watermark is extracted as proof of authentication. The results shows that digital speech watermarking can successfully apply for anti-spoofing attack purposes because the quality of the speech signal is not significantly degraded by watermark.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nematollahi, M. A.; Al-Haddad, S. A. R.; Doraisamy, Shyamala; Ranjbari,
   M.] Univ Putra Malaysia, Fac Engn, Dept Comp &amp; Commun Syst Engn, Upm
   Serdang 43400, Selangor Darule, Malaysia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nematollahi, MA (reprint author), Univ Putra Malaysia, Fac Engn, Dept Comp &amp; Commun Syst Engn, Upm Serdang 43400, Selangor Darule, Malaysia.</td>
</tr>

<tr>
<td valign="top">EM </td><td>Greencomputinguae@gmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">BP </td><td>476</td>
</tr>

<tr>
<td valign="top">EP </td><td>479</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000393381300093</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Mohammadi, SH
   <br>Kain, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Mohammadi, Seyed Hamidreza
   <br>Kain, Alexander</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>VOICE CONVERSION USING DEEP NEURAL NETWORKS WITH SPEAKER-INDEPENDENT
   PRE-TRAINING</td>
</tr>

<tr>
<td valign="top">SO </td><td>2014 IEEE WORKSHOP ON SPOKEN LANGUAGE TECHNOLOGY SLT 2014</td>
</tr>

<tr>
<td valign="top">SE </td><td>IEEE Workshop on Spoken Language Technology</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE Workshop on Spoken Language Technology (SLT 2014)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 07-10, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>south lake, NV</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; pre-training; deep neural network; autoencoder</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this study, we trained a deep autoencoder to build compact representations of short-term spectra of multiple speakers. Using this compact representation as mapping features, we then trained an artificial neural network to predict target voice features from source voice features. Finally, we constructed a deep neural network from the trained deep autoencoder and artificial neural network weights, which were then fine-tuned using back-propagation. We compared the proposed method to existing methods using Gaussian mixture models and frame-selection. We evaluated the methods objectively, and also conducted perceptual experiments to measure both the conversion accuracy and speech quality of selected systems. The results showed that, for 70 training sentences, frame-selection performed best, regarding both accuracy and quality. When using only two training sentences, the pre-trained deep neural network performed best, regarding both accuracy and quality.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Mohammadi, Seyed Hamidreza; Kain, Alexander] Oregon Hlth &amp; Sci Univ,
   Ctr Spoken Language Understanding, Portland, OR 97201 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Mohammadi, SH (reprint author), Oregon Hlth &amp; Sci Univ, Ctr Spoken Language Understanding, Portland, OR 97201 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>mohammah@ohsu.edu; kaina@ohsu.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>15</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>15</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">BP </td><td>19</td>
</tr>

<tr>
<td valign="top">EP </td><td>23</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380375100003</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Correia, MJ
   <br>Abad, A
   <br>Trancoso, I</td>
</tr>

<tr>
<td valign="top">AF </td><td>Correia, Maria Joana
   <br>Abad, Alberto
   <br>Trancoso, Isabel</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>EXPLOITING MAGNITUDE AND PHASE SPECTRAL INFORMATION FOR CONVERTED SPEECH
   DETECTION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2014 IEEE WORKSHOP ON SPOKEN LANGUAGE TECHNOLOGY SLT 2014</td>
</tr>

<tr>
<td valign="top">SE </td><td>IEEE Workshop on Spoken Language Technology</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE Workshop on Spoken Language Technology (SLT 2014)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 07-10, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>south lake, NV</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; Speaker verification; Converted speech detection;
   Spoofing</td>
</tr>

<tr>
<td valign="top">AB </td><td>Speaker verification systems have been shown to be vulnerable in situations where voice conversion techniques are used to try to fool them, evidencing an important security breach in these applications.
   <br>This work focuses on the development of a new converted speech detector able to robustly address this problem. The proposed detector uses four spectral features extracted from the magnitude and the phase spectrum of the speech signal. To evaluate the performance of the detector we use a subset of the core task of the NIST SRE2006 corpus as the natural data. The converted data was produced with two different voice conversion methods: Gaussian mixture model and unit selection, from other NIST SRE2006 conditions. The converted speech detector achieved a detection accuracy of 99.1% and 98.5% for natural and converted utterances, respectively.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Correia, Maria Joana; Abad, Alberto; Trancoso, Isabel] INESC ID Spoken
   Language Syst Lab, Lisbon, Portugal.
   <br>[Abad, Alberto; Trancoso, Isabel] Univ Lisbon, Inst Super Tecn, P-1699
   Lisbon, Portugal.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Correia, MJ (reprint author), INESC ID Spoken Language Syst Lab, Lisbon, Portugal.</td>
</tr>

<tr xmlns:date="http://exslt.org/dates-and-times">
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Trancoso, Isabel</display_name>&nbsp;</font></td><td><font size="3">C-5965-2008&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Abad, Alberto</display_name>&nbsp;</font></td><td><font size="3">J-3281-2013&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Trancoso, Isabel</display_name>&nbsp;</font></td><td><font size="3">0000-0001-5874-6313&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Abad, Alberto</display_name>&nbsp;</font></td><td><font size="3">0000-0003-2122-5148&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">BP </td><td>396</td>
</tr>

<tr>
<td valign="top">EP </td><td>401</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380375100067</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Alegre, F
   <br>Janicki, A
   <br>Evans, N</td>
</tr>

<tr>
<td valign="top">AF </td><td>Alegre, Federico
   <br>Janicki, Artur
   <br>Evans, Nicholas</td>
</tr>

<tr>
<td valign="top">BE </td><td>Bromme, A
   <br>Busch, C</td>
</tr>

<tr>
<td valign="top">TI </td><td>Re-assessing the threat of replay spoofing attacks against automatic
   speaker verification</td>
</tr>

<tr>
<td valign="top">SO </td><td>2014 INTERNATIONAL CONFERENCE OF THE BIOMETRICS SPECIAL INTEREST GROUP
   (BIOSIG)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference of the Biometrics-Special-Interest-Group
   (BIOSIG) of the Gesellschaft fur Informatik (GI) e.V.</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 10-12, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Darmstadt, GERMANY</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH SYNTHESIS</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper re-examines the threat of spoofing or presentation attacks in the context of automatic speaker verification (ASV). While voice conversion and speech synthesis attacks present a serious threat, and have accordingly received a great deal of attention in the recent literature, they can only be implemented with a high level of technical know-how. In contrast, the implementation of replay attacks require no specific expertise nor any sophisticated equipment and thus they arguably present a greater risk. The comparative threat of each attack is re-examined in this paper against six different ASV systems including a state-of-the-art iVector-PLDA system. Despite the lack of attention in the literature, experiments show that low-effort replay attacks provoke higher levels of false acceptance than comparatively higher-effort spoofing attacks such as voice conversion and speech synthesis. Results therefore show the need to refocus research effort and to develop countermeasures against replay attacks in future work.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Alegre, Federico; Evans, Nicholas] EURECOM, Sophia Antipolis, France.
   <br>[Janicki, Artur] Warsaw Univ Technol, Warsaw, Poland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Alegre, F (reprint author), EURECOM, Sophia Antipolis, France.</td>
</tr>

<tr>
<td valign="top">EM </td><td>alegre@eurecom.fr; A.Janicki@tele.pw.edu.pl; evans@eurecom.fr</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000412427900013</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kumar, CS
   <br>Julian, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kumar, Suresh C.
   <br>Julian, Anitha</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Wearable Messaging Device for Visually Impaired Person</td>
</tr>

<tr>
<td valign="top">SO </td><td>2014 INTERNATIONAL CONFERENCE ON ADVANCED COMMUNICATION CONTROL AND
   COMPUTING TECHNOLOGIES (ICACCCT)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Advanced Communication Control and Computing
   Technologies*</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 08-10, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Syed Ammal Eng Coll, Ramanathapuram, INDIA</td>
</tr>

<tr>
<td valign="top">HO </td><td>Syed Ammal Eng Coll</td>
</tr>

<tr>
<td valign="top">DE </td><td>Wearable input device; embedded technology; key gloves; alphanumeric
   keypad; Text to Voice Conversion; GSM</td>
</tr>

<tr>
<td valign="top">AB </td><td>Today's technology has innumerable wearable solutions to alleviate the problems of disabled persons. The solution presented here is for the visually impaired persons and proposes to develop a wearable device that uses embedded system technology for data input functionalities in a hand glove based keyboard. The hand mounted keypad is easily adaptable by the user and provides flexibility as no prior knowledge is required to handle the hand mounted wearable device. The key glove acts as a prototype which enables the user to have the total functionality of a keyboard in one hand. Key glove is attached with alpha numeric keypad buttons that are mounted on the gloves. The character or number is generated by using mode Selection. (Alpha/Num). The hand mounted keypad can be interfaced with any kind of display device. It can be used for the visually impaired person to send message/call from one person to another using GSM modem. The GSM modem is attached with the gloves. Reply Message from others can be converted into text to voice format using Robo voice shield. The efficiency of the proposed work is evaluated with reference to decreased error rate and increased speed of operation. Voice conversion is also added to this device.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kumar, Suresh C.; Julian, Anitha] Anna Univ, Velammal Engn Coll, TIFAC
   CORE Pervas Comp Technol, Chennai, Tamil Nadu, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kumar, CS (reprint author), Anna Univ, Velammal Engn Coll, TIFAC CORE Pervas Comp Technol, Chennai, Tamil Nadu, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>sureshece134@gmail.com; tifacore.anithajulian@velammal.edu.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">BP </td><td>994</td>
</tr>

<tr>
<td valign="top">EP </td><td>998</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380467200109</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sawant, SN
   <br>Kumbhar, MS</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sawant, Shreyashi Narayan
   <br>Kumbhar, M. S.</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Real Time Sign Language Recognition using PCA</td>
</tr>

<tr>
<td valign="top">SO </td><td>2014 INTERNATIONAL CONFERENCE ON ADVANCED COMMUNICATION CONTROL AND
   COMPUTING TECHNOLOGIES (ICACCCT)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Advanced Communication Control and Computing
   Technologies*</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 08-10, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Syed Ammal Eng Coll, Ramanathapuram, INDIA</td>
</tr>

<tr>
<td valign="top">HO </td><td>Syed Ammal Eng Coll</td>
</tr>

<tr>
<td valign="top">DE </td><td>Sign Language; Feature Extraction; Sign Recognition; PCA</td>
</tr>

<tr>
<td valign="top">AB </td><td>The Sign Language is a method of communication for deaf-dumb people. This paper presents the Sign Language Recognition system capable of recognizing 26 gestures from the Indian Sign Language by using MATLAB. The proposed system having four modules such as: pre-processing and hand segmentation, feature extraction, sign recognition and sign to text and voice conversion. Segmentation is done by using image processing. Different features are extracted such as Eigen values and Eigen vectors which are used in recognition. The Principle Component Analysis (PCA) algorithm was used for gesture recognition and recognized gesture is converted into text and voice format. The proposed system helps to minimize communication barrier between deaf-dumb people and normal people.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Sawant, Shreyashi Narayan; Kumbhar, M. S.] Rajarambapu Inst Technol,
   Dept Elect &amp; Telecommun, Rajaramnagar, Islampur, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sawant, SN (reprint author), Rajarambapu Inst Technol, Dept Elect &amp; Telecommun, Rajaramnagar, Islampur, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>shreysawant@gmail.com; mahesh.kumbhar@ritindia.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>3</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">BP </td><td>1412</td>
</tr>

<tr>
<td valign="top">EP </td><td>1415</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380467200199</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Chen, XT
   <br>Zhang, LH</td>
</tr>

<tr>
<td valign="top">AF </td><td>Chen Xiantong
   <br>Zhang Linghua</td>
</tr>

<tr>
<td valign="top">BE </td><td>Wan, WG
   <br>Luo, FL
   <br>Yu, XQ</td>
</tr>

<tr>
<td valign="top">TI </td><td>An Improved ANN Method Based on Clustering Optimization for Voice
   Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>2014 INTERNATIONAL CONFERENCE ON AUDIO, LANGUAGE AND IMAGE PROCESSING
   (ICALIP), VOLS 1-2</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Audio, Language and Image Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUL 07-09, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Shanghai, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; STRAIGHT; RBF; K-means; PSO</td>
</tr>

<tr>
<td valign="top">ID </td><td>TRANSFORMATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Artificial neural network is a commonly used conversion model in voice conversion system, in which RBF is known for its concise convergence and fast learning. Based on optimizing the centers of RBF network, this article presents a method of using K-means algorithm to cluster and form centers and PSO algorithm to optimize the clustering number to improve the property of RBF, thus to enhance the transformation of speech parameters. Firstly, STRAIGHT model is used to extract linear prediction coefficients and pitch frequencies. Then the parameters are sent to RBF network, K-means and PSO algorithms are used to optimize the centers of RBF network until the fitness value is lowest. Experiment shows that, this method not only eliminates the trouble of finding the best clustering number one-by-one, but also effectively improves the performance of neural network, and the converted speeches are closer to the target one.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Chen Xiantong; Zhang Linghua] Nanjing Univ Posts &amp; Telecommun, Coll
   Telecommun &amp; Informat Engn, Nanjing, Jiangsu, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Chen, XT (reprint author), Nanjing Univ Posts &amp; Telecommun, Coll Telecommun &amp; Informat Engn, Nanjing, Jiangsu, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>chenxt0524@126.com; zhanglh@njupt.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">BP </td><td>464</td>
</tr>

<tr>
<td valign="top">EP </td><td>469</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380438100093</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Pan, H
   <br>Wei, YJ
   <br>Guan, N
   <br>Wang, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Pan, He
   <br>Wei, Yangjie
   <br>Guan, Nan
   <br>Wang, Yi</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">BE </td><td>Aldabass, D
   <br>Kuo, JY
   <br>Liu, CH
   <br>Ma, SP
   <br>Ibrahim, Z</td>
</tr>

<tr>
<td valign="top">TI </td><td>Comprehensive Voice Conversion Analysis Based on D_GMM and Feature
   Combination</td>
</tr>

<tr>
<td valign="top">SO </td><td>ASIA MODELLING SYMPOSIUM 2014 (AMS 2014)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Asia Modelling Symposium AMS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Asia Modelling Symposium 2014 8th International Conference Mathematical
   Modelling Computer Simulation</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 23-25, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Taipei, TAIWAN</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; feature combination; D_GMM; STRAIGHT synthesis;
   speaker recognition</td>
</tr>

<tr>
<td valign="top">ID </td><td>QUALITY</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion system modifies a speaker's voice to be perceived as another speaker uttered, and now it is widely used in many real applications. However, most research only focuses on one aspect performance of voice conversion system, rare theoretical analysis and experimental comparison on the whole source-target speaker voice conversion process has been introduced. Therefore, in this paper, a comprehensive analysis on source-target speaker voice conversion is conducted based on three key steps, including acoustic features selection and extraction, voice conversion model construction, and target speech synthesis, and a complete and optimal source-target speaker voice conversion is proposed. First, a comprehensive feature combination form consisting of prosodic feature, spectrum parameter and spectral envelope characteristic, is proposed. Then, to void the discontinuity and spectrum distortion of a converted speech, D_ GMM ( Dynamic Gaussian Mixture Model) considering dynamic information between frames is presented. Subsequently, for speech synthesis, STRAIGHT algorithm synthesizer with feature combination is modified. Finally, the objective contrast experiment shows that our new source-target voice conversion process achieves better performance than the conventional methods. In addition, the speaker recognition system is also used to evaluate the quality of converted speech, and experimental result shows that the converted speech has higher target speaker individuality and speech quality.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Pan, He; Wei, Yangjie; Guan, Nan] Northeastern Univ, Shenyang, Peoples
   R China.
   <br>[Wang, Yi] Uppsala Univ, Uppsala, Sweden.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Pan, H (reprint author), Northeastern Univ, Shenyang, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>panhe_sy@126.com; weiyangjie@ise.neu.edu.cn; guannan@ise.neu.edu.cn;
   wangyi@ise.neu.edu.cn</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Guan, Nan</display_name>&nbsp;</font></td><td><font size="3">0000-0003-3775-911X&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">BP </td><td>159</td>
</tr>

<tr>
<td valign="top">EP </td><td>164</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/AMS.2014.39</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380455400027</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ibrahim, AAA
   <br>Embug, AJ</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ibrahim, Ag. Asri Ag
   <br>Embug, Alter Jimat</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Sonification of 3D Body Movement Using Parameter Mapping Technique</td>
</tr>

<tr>
<td valign="top">SO </td><td>PROCEEDINGS OF THE 2014 6TH INTERNATIONAL CONFERENCE ON INFORMATION
   TECHNOLOGY AND MULTIMEDIA (ICIM)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Information Technology &amp; Multimedia</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>6th International Conference on Information Technology and Multimedia
   (ICIM)</td>
</tr>

<tr>
<td valign="top">CY </td><td>NOV 18-20, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Univ Tenaga Nas, Putrajaya, MALAYSIA</td>
</tr>

<tr>
<td valign="top">HO </td><td>Univ Tenaga Nas</td>
</tr>

<tr>
<td valign="top">DE </td><td>Sonification; kinematics; 3d movements; parameter mapping</td>
</tr>

<tr>
<td valign="top">AB </td><td>Most of instructions by trainers or therapists in body movements, such as walking, turning, rising arms or legs are mostly done through voice instructions or touches. This does not have much problem to normal people as they can see it at the same time. To follow these instructions without seeing it can cause confusion in terms of actions and directions. Unfortunately, there is no other option for people who are blind. Thus, this research will try to replace those voice and touches form of instructions into non-speech sound instructions. The method involves transforming 3-dimensional data of body movements (kinematics) into sounds. It is hoped by only listening to the sounds, a person should be able to follow the exact movements of the body of another person or instructor without any voice commands or instructions. The novel contribution of this research is to produce an effective and efficient sonification technique (converting the data into sound) to represent the actions and directions of the body movements in 3-dimensional space. The conversion approach to be used is Parameter Mapping, where the movement properties will be mapped to sound properties. The parameter mapping will involve at least 3 transformation processes - data, acoustics parameters and sound representations. The effectiveness and efficiency of this approach is depending on all of these transformation processes. This research will use Kinect (a device used to play game with Microsoft Xbox game console) as the live 3D movements input data stream. This device is intentionally used in this research, as it is readily available and cheap. Thus, the potential applications or products from this research later on can also use this device.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ibrahim, Ag. Asri Ag; Embug, Alter Jimat] Univ Malaysia Sabah, Fac Comp
   &amp; Informat, Labuan, Malaysia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ibrahim, AAA (reprint author), Univ Malaysia Sabah, Fac Comp &amp; Informat, Labuan, Malaysia.</td>
</tr>

<tr>
<td valign="top">EM </td><td>awgasri@ums.edu.my; alterjimat@gmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">BP </td><td>385</td>
</tr>

<tr>
<td valign="top">EP </td><td>389</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000410571300072</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nirmal, JH
   <br>Zaveri, MA
   <br>Patnaik, S
   <br>Kachare, PH</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nirmal, Jagannath H.
   <br>Zaveri, Mukesh A.
   <br>Patnaik, Suprava
   <br>Kachare, Pramod H.</td>
</tr>

<tr>
<td valign="top">TI </td><td>A novel voice conversion approach using admissible wavelet packet
   decomposition</td>
</tr>

<tr>
<td valign="top">SO </td><td>EURASIP JOURNAL ON AUDIO SPEECH AND MUSIC PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Admissible wavelet packet; Dynamic time warping; Radial basis function;
   Speaker-specific features; Wavelet-based filter bank</td>
</tr>

<tr>
<td valign="top">ID </td><td>INDEPENDENT SPEAKER IDENTIFICATION; NEURAL-NETWORKS; TRANSFORMATION;
   INDIVIDUALITY; SPECTRUM; FEATURES</td>
</tr>

<tr>
<td valign="top">AB </td><td>The framework of voice conversion system is expected to emphasize both the static and dynamic characteristics of the speech signal. The conventional approaches like Mel frequency cepstrum coefficients and linear predictive coefficients focus on spectral features limited to lower frequency bands. This paper presents a novel wavelet packet filter bank approach to identify non-uniformly distributed dynamic characteristics of the speaker. Contribution of this paper is threefold. First, in the feature extraction stage, dyadic wavelet packet tree structure is optimized to involve less computation while preserving the speaker-specific features. Second, in the feature representation step, magnitude and phase attributes are treated separately to rule out on the fact that raw time-frequency traits are highly correlated but carry intelligent speech information. Finally, the RBF mapping function is established to transform the speaker-specific features from the source to the target speakers. The results obtained by the proposed filter bank-based voice conversion system are compared to the baseline multiscale voice morphing results by using subjective and objective measures. Evaluation results reveal that the proposed method outperforms by incorporating the speaker-specific dynamic characteristics and phase information of the speech signal.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nirmal, Jagannath H.; Patnaik, Suprava] SV Natl Inst Technol, Dept
   Elect Engn, Surat 395007, India.
   <br>[Zaveri, Mukesh A.] SV Natl Inst Technol, Dept Comp Engn, Surat 395007,
   India.
   <br>[Kachare, Pramod H.] Veermata Jeejabai Inst Technol, Dept Elect Engn,
   Bombay 400031, Maharashtra, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nirmal, JH (reprint author), SV Natl Inst Technol, Dept Elect Engn, Surat 395007, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jhnirmal1975@gmail.com</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Patnaik, Suprava</display_name>&nbsp;</font></td><td><font size="3">0000-0002-7068-5960&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>4</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>4</td>
</tr>

<tr>
<td valign="top">PD </td><td>DEC 10</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">AR </td><td>28</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1186/1687-4722-2013-28</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000328840400001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Defez, AC
   <br>Carrie, JCDS</td>
</tr>

<tr>
<td valign="top">AF </td><td>Calzada Defez, Angel
   <br>Socoro Carrie, Joan Claudi Dr</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Quality Modification Using a Harmonics Plus Noise Model
   Transferring Vocal Effort with Parallel Corpora</td>
</tr>

<tr>
<td valign="top">SO </td><td>COGNITIVE COMPUTATION</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice quality; Harmonics plus noise model; Speech synthesis; Vocal
   effort; Speech conversion; Expressive speech</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH SYNTHESIS</td>
</tr>

<tr>
<td valign="top">AB </td><td>The harmonics plus noise model (HNM) has been used for prosodic speech signal modifications in high-quality environments in recent decades. Such speech modification techniques allow Text-To-Speech systems to generate more expressive synthesis without requiring extensive corpora resources. A more expressive synthesis can improve the user experience with Human-Machine-Interfaces. In this paper, an adaptation of the adaptive pre-emphasis linear prediction technique to the HNM for modifying vocal effort is presented. The proposed transformation methodology is validated using a copy re-synthesis strategy on a speech corpora specifically designed for vocal effort research. The perceptual tests demonstrate the effectiveness of the proposed technique in performing various types of vocal effort conversions for the given corpus.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Calzada Defez, Angel; Socoro Carrie, Joan Claudi Dr] La Salle Univ
   Ramon Llull, GTM, Barcelona 08022, Spain.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Defez, AC (reprint author), La Salle Univ Ramon Llull, GTM, C Quatre Camins 30, Barcelona 08022, Spain.</td>
</tr>

<tr>
<td valign="top">EM </td><td>acalzada@salle.url.edu; jclaudi@salle.url.edu</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Socoro, Joan Claudi</display_name>&nbsp;</font></td><td><font size="3">0000-0002-7348-6916&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PD </td><td>DEC</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">VL </td><td>5</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">SI </td><td>SI</td>
</tr>

<tr>
<td valign="top">BP </td><td>473</td>
</tr>

<tr>
<td valign="top">EP </td><td>482</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/s12559-012-9193-9</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Neurosciences &amp; Neurology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000328221100007</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Xu, N
   <br>Bao, JY
   <br>Liu, XF
   <br>Jiang, AM
   <br>Tang, YB</td>
</tr>

<tr>
<td valign="top">AF </td><td>Xu Ning
   <br>Bao JingYi
   <br>Liu XiaoFeng
   <br>Jiang AiMing
   <br>Tang YiBing</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice conversion towards modeling dynamic characteristics using
   switching state space model</td>
</tr>

<tr>
<td valign="top">SO </td><td>SCIENCE CHINA-INFORMATION SCIENCES</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>discontinuity problem; dynamic characteristics; Gaussian mixture model;
   switching state space model; voice conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>TRANSFORMATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>In the literature of voice conversion (VC), the method based on statistical Gaussian mixture model (GMM) serves as a benchmark. However, one of the inherent drawbacks of GMM is well-known as discontinuity problem, which is caused by transforming features on a frame-by-frame basis, thus ignoring the dynamics between adjacent frames and finally resulting in degraded quality of the converted speech. A variety of algorithms have been proposed to overcome this deficiency, among which the state space model (SSM) based method provides some promising results. In this paper, we proceed by presenting an enhanced version of the traditional SSM, namely, the switching SSM (SSSM). This new structure is more flexible than the conventional one in that it allows using mixture of components to account for the rapid transitions between neighboring frames. Moreover, physical meaning of the model parameters of SSSM has been examined in depth, leading to efficient application-specific training and transforming procedures of VC. Experiments including both objective and subjective measurements were conducted to compare the performances of the conventional and the proposed SSM-based methods, which have convinced that obvious improvements in both aspects of similarity and quality can be obtained by SSSM.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Xu Ning; Liu XiaoFeng; Jiang AiMing; Tang YiBing] Hohai Univ, Coll Comp
   &amp; Informat Engn, Changzhou 213022, Peoples R China.
   <br>[Xu Ning] Nanjing Univ Posts &amp; Telecommun, Minist Educ, Key Lab
   Broadband Wireless Commun &amp; Sensor Networ, Nanjing 210003, Jiangsu,
   Peoples R China.
   <br>[Bao JingYi] Changzhou Inst Technol, Sch Elect Informat &amp; Elect Engn,
   Changzhou 213002, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Xu, N (reprint author), Hohai Univ, Coll Comp &amp; Informat Engn, Changzhou 213022, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>xuningdlts@gmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>DEC</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">VL </td><td>56</td>
</tr>

<tr>
<td valign="top">IS </td><td>12</td>
</tr>

<tr>
<td valign="top">AR </td><td>122308</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/s11432-013-4799-4</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000328295500023</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Berkhout, M
   <br>Dooper, L
   <br>Krabbenborg, B</td>
</tr>

<tr>
<td valign="top">AF </td><td>Berkhout, Marco
   <br>Dooper, Lutsen
   <br>Krabbenborg, Benno</td>
</tr>

<tr>
<td valign="top">TI </td><td>A 4 Omega 2.65W Class-D Audio Amplifier With Embedded DC-DC Boost
   Converter, Current Sensing ADC and DSP for Adaptive Speaker Protection</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE JOURNAL OF SOLID-STATE CIRCUITS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article; Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Solid-State Circuits Conference (ISSCC)</td>
</tr>

<tr>
<td valign="top">CY </td><td>FEB 05-09, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>San Francisco, CA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Audio amplifier; boost converter; class-D; current sensing; DC-DC
   conversion; pulse width modulation; speaker driver; speaker protection;
   switching amplifier</td>
</tr>

<tr>
<td valign="top">ID </td><td>CMOS</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper a class-D smart speaker driver is presented that can deliver 2.65Wat 1% THD into a 4 Omega load. Maximal output power is maintained at low battery voltage by supplying the class-D amplifier from a DC-DC boost converter. Speaker damage is avoided by a speaker protection algorithm that runs on an embedded DSP. The protection algorithm estimates the membrane excursion and voice coil temperature using a speaker model that tracks the speaker impedance which is determined by measuring the speaker current with less than 2% relative error. A sample &amp; hold technique is presented that rejects the load current ripple by sampling at the moments where the instantaneous load current equals the average current. At full output power the combined efficiency of the class-D amplifier and DC-DC boost converter is higher than 80%. The complete system is implemented on a single chip that measures 6.6 mm(2) and is fabricated in a 0.14 mu m CMOS technology with a 5 V gate-oxide option.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Berkhout, Marco; Dooper, Lutsen; Krabbenborg, Benno] NXP Semicond,
   NL-6534 AE Nijmegen, Netherlands.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Berkhout, M (reprint author), NXP Semicond, NL-6534 AE Nijmegen, Netherlands.</td>
</tr>

<tr>
<td valign="top">EM </td><td>marco.berkhout@nxp.com; lutsen.dooper@nxp.com; benno.krabbenborg@nxp.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>7</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>7</td>
</tr>

<tr>
<td valign="top">PD </td><td>DEC</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">VL </td><td>48</td>
</tr>

<tr>
<td valign="top">IS </td><td>12</td>
</tr>

<tr>
<td valign="top">SI </td><td>SI</td>
</tr>

<tr>
<td valign="top">BP </td><td>2952</td>
</tr>

<tr>
<td valign="top">EP </td><td>2961</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/JSSC.2013.2284692</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000327548900002</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Dekens, T
   <br>Verhelst, W</td>
</tr>

<tr>
<td valign="top">AF </td><td>Dekens, Tomas
   <br>Verhelst, Werner</td>
</tr>

<tr>
<td valign="top">TI </td><td>Body Conducted Speech Enhancement by Equalization and Signal Fusion</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Non-conventional microphones; speech enhancement</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION; RECOGNITION; MICROPHONES; ESTIMATOR</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper studies body-conducted speech for noise robust speech processing purposes. As body-conducted speech is typically limited in bandwidth, signal processing is required to obtain a signal that is both high in quality and low in noise. We propose an algorithm that first equalizes the body-conducted speech using filters obtained from a pre-defined filter set and subsequently fuses this equalized signal with a noisy conventional microphone signal using an optimal clean speech amplitude and phase estimator. We evaluated the proposed equalization and fusion technique using a combination of a conventional close-talk and a throat microphone. Subjective listening tests show that the proposed method successfully fuses the speech quality of the conventional signal and the noise robustness of the throat microphone signal. The listening tests also indicate that the inclusion of the body-conducted signal can improve single-channel speech enhancement methods, while a calculated set of objective signal quality measures confirm these observations.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Dekens, Tomas; Verhelst, Werner] Vrije Univ Brussel, Dept ETRO, B-1050
   Brussels, Belgium.
   <br>[Verhelst, Werner] iMinds, Dept FMI, B-9050 Ghent, Belgium.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Dekens, T (reprint author), Vrije Univ Brussel, Dept ETRO, B-1050 Brussels, Belgium.</td>
</tr>

<tr>
<td valign="top">EM </td><td>tdekens@etro.vub.ac.be; wverhels@etro.vub.ac.be</td>
</tr>

<tr>
<td valign="top">TC </td><td>5</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>6</td>
</tr>

<tr>
<td valign="top">PD </td><td>DEC</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">VL </td><td>21</td>
</tr>

<tr>
<td valign="top">IS </td><td>12</td>
</tr>

<tr>
<td valign="top">BP </td><td>2481</td>
</tr>

<tr>
<td valign="top">EP </td><td>2492</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASL.2013.2274696</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000327653500002</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Baker, J
   <br>Ben-Tovim, D
   <br>Butcher, A
   <br>Esterman, A
   <br>McLaughlin, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Baker, Janet
   <br>Ben-Tovim, David
   <br>Butcher, Andrew
   <br>Esterman, Adrian
   <br>McLaughlin, Kristin</td>
</tr>

<tr>
<td valign="top">TI </td><td>Psychosocial risk factors which may differentiate between women with
   Functional Voice Disorder, Organic Voice Disorder and a Control group</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERNATIONAL JOURNAL OF SPEECH-LANGUAGE PATHOLOGY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice disorders; emotion; psychosocial</td>
</tr>

<tr>
<td valign="top">ID </td><td>LIFE EVENTS; PERSONALITY-TRAITS; SPEECH-THERAPY; DYSPHONIA; APHONIA;
   DEPRESSION; ANXIETY; STRESS; DIFFICULTIES; CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This study aimed to explore psychosocial factors contributing to the development of functional voice disorders (FVD) and those differentiating between organic voice disorders (OVD) and a non-voice-disordered control group. A case-control study was undertaken of 194 women aged 18-80 years diagnosed with FVD (n = 73), OVD (n = 55), and controls (n = 66). FVD women were allocated into psychogenic voice disorder (PVD) (n = 37) and muscle tension voice disorder (MTVD) (n = 36) for sub-group analysis. Dependent variables included biographical and voice assessment data, the number and severity of life events and difficulties and conflict over speaking out (COSO) situations derived from the Life Events and Difficulties Schedule (LEDS), and psychological traits including emotional expressiveness scales. Four psychosocial components differentiated between the FVD and control group accounting for 84.9% of the variance: severe events, moderate events, severe COSO, and mild COSO difficulties. Severe events, severe and mild COSO difficulties differentiated between FVD and OVD groups, accounting for 80.5% of the variance. Moderate events differentiated between PVD and MTVD sub-groups, accounting for 58.9% of the variance. Psychological traits did not differentiate between groups. Stressful life events and COSO situations best differentiated FVD from OVD and control groups. More refined aetiological studies are needed to differentiate between PVD and MTVD.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Baker, Janet; Butcher, Andrew] Flinders Univ S Australia, Adelaide, SA
   5001, Australia.
   <br>[Ben-Tovim, David] Flinders Med Ctr, Adelaide, SA, Australia.
   <br>[Esterman, Adrian] Univ S Australia, Adelaide, SA 5001, Australia.
   <br>[McLaughlin, Kristin] Lyell McEwin Hosp, Elizabeth Vale, SA, Australia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Baker, J (reprint author), 22 Howard Terrace, Hazelwood Pk, SA 5066, Australia.</td>
</tr>

<tr>
<td valign="top">EM </td><td>janet.baker@flinders.edu.au</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Esterman, Adrian</display_name>&nbsp;</font></td><td><font size="3">D-1042-2009&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Esterman, Adrian</display_name>&nbsp;</font></td><td><font size="3">0000-0001-7324-9171&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Butcher, Andy</display_name>&nbsp;</font></td><td><font size="3">0000-0002-2209-4250&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>11</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>11</td>
</tr>

<tr>
<td valign="top">PD </td><td>DEC</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">VL </td><td>15</td>
</tr>

<tr>
<td valign="top">IS </td><td>6</td>
</tr>

<tr>
<td valign="top">BP </td><td>547</td>
</tr>

<tr>
<td valign="top">EP </td><td>563</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.3109/17549507.2012.721397</td>
</tr>

<tr>
<td valign="top">SC </td><td>Audiology &amp; Speech-Language Pathology; Linguistics; Rehabilitation</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000326970100001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Rajam, PS
   <br>Balakrishnan, G</td>
</tr>

<tr>
<td valign="top">AF </td><td>Rajam, P. Subha
   <br>Balakrishnan, G.</td>
</tr>

<tr>
<td valign="top">TI </td><td>Design and development of tamil sign alphabets using image processing
   with right hand palm to aid deaf-dumb people</td>
</tr>

<tr>
<td valign="top">SO </td><td>IETE JOURNAL OF RESEARCH</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Feature extraction point; Human-computer interaction; Image processing
   technique; Pattern recognition; Sign detection; and Tamil sign alphabet
   recognition system</td>
</tr>

<tr>
<td valign="top">AB </td><td>Hand recognition is a recent active area of research in the computer vision for the purpose of Human - Computer Interaction. This paper mainly concentrates on Tamil sign alphabets (TSL) into speech which could be helpful for deaf-dumb people. In this paper, a set of 32 (2 (5) ) combinations of binary number sign images are introduced to propose a system to recognize Tamil sign alphabets. These Tamil alphabets have 12 vowels, 18 consonants, and one Aayutha Ezhuthu. The proposed system is based on four main stages: Pre-processing method, Training phase, Sign detection, and Conversion of Binary to voice. The binary sign images are loaded at a run time or static as 310 images which are taken ten times in different distances at the same position. The five fingertip positions represent (1 or 0) and are identified by using image processing techniques with proposed right hand palm angular-based analysis. Then, the binary values are assigned to the corresponding Tamil letters and voice. The experiments were performed with ten different signer palms and the results demonstrated that the system could successfully recognize Tamil sign alphabets with better accuracy with 99.35% of static images and 98.36% of dynamic images (runtime).</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Rajam, P. Subha] JJ Coll Engn &amp; Technol, Dept IT, Tiruchchirappalli,
   Tamil Nadu, India.
   <br>[Balakrishnan, G.] Indra Ganesan Coll Engn, Dept CSE, Tiruchchirappalli,
   Tamil Nadu, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Rajam, PS (reprint author), JJ Coll Engn &amp; Technol, Dept IT, Tiruchchirappalli, Tamil Nadu, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>subha8892@yahoo.co.in; balakrishnan.g@gmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PD </td><td>NOV-DEC</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">VL </td><td>59</td>
</tr>

<tr>
<td valign="top">IS </td><td>6</td>
</tr>

<tr>
<td valign="top">BP </td><td>709</td>
</tr>

<tr>
<td valign="top">EP </td><td>718</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.4103/0377-2063.126969</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000331384700010</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Pan, J
   <br>Yan, M
   <br>Laubrock, J
   <br>Shu, H
   <br>Kliegl, R</td>
</tr>

<tr>
<td valign="top">AF </td><td>Pan, Jinger
   <br>Yan, Ming
   <br>Laubrock, Jochen
   <br>Shu, Hua
   <br>Kliegl, Reinhold</td>
</tr>

<tr>
<td valign="top">TI </td><td>Eye-voice span during rapid automatized naming of digits and dice in
   Chinese normal and dyslexic children</td>
</tr>

<tr>
<td valign="top">SO </td><td>DEVELOPMENTAL SCIENCE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">ID </td><td>VISUAL-ATTENTION SPAN; DEVELOPMENTAL DYSLEXIA; DEFICIT HYPOTHESIS;
   READING FLUENCY; READERS; MOVEMENTS; SKILLS; PARAFOVEAL; AWARENESS;
   SPEED</td>
</tr>

<tr>
<td valign="top">AB </td><td>We measured Chinese dyslexic and control children's eye movements during rapid automatized naming (RAN) with alphanumeric (digits) and symbolic (dice surfaces) stimuli. Both types of stimuli required identical oral responses, controlling for effects associated with speech production. Results showed that naming dice was much slower than naming digits for both groups, but group differences in eye-movement measures and in the eye-voice span (i.e. the distance between the currently fixated item and the voiced item) were generally larger in digit-RAN than in dice-RAN. In addition, dyslexics were less efficient in parafoveal processing in these RAN tasks. Since the two RAN tasks required the same phonological output and on the assumption that naming dice is less practiced than naming digits in general, the results suggest that the translation of alphanumeric visual symbols into phonological codes is less efficient in dyslexic children. The dissociation of the print-to-sound conversion and phonological representation suggests that the degree of automaticity in translation from visual symbols to phonological codes in addition to phonological processing per se is also critical to understanding dyslexia.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Pan, Jinger; Shu, Hua] Beijing Normal Univ, State Key Lab Cognit
   Neurosci &amp; Learning, Beijing 100875, Peoples R China.
   <br>[Yan, Ming; Laubrock, Jochen; Kliegl, Reinhold] Univ Potsdam, Dept
   Psychol, D-14476 Potsdam, Germany.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kliegl, R (reprint author), Univ Potsdam, Dept Psychol, Karl Liebknecht Str 24-25, D-14476 Potsdam, Germany.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jingerpan@gmail.com; kliegl@uni-potsdam.de</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Laubrock, Jochen</display_name>&nbsp;</font></td><td><font size="3">B-8080-2008&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Kliegl, Reinhold</display_name>&nbsp;</font></td><td><font size="3">A-1276-2009&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Laubrock, Jochen</display_name>&nbsp;</font></td><td><font size="3">0000-0002-0798-8977&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Kliegl, Reinhold</display_name>&nbsp;</font></td><td><font size="3">0000-0002-0180-8488&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Pan, Jinger</display_name>&nbsp;</font></td><td><font size="3">0000-0002-8160-9867&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Yan, Ming</display_name>&nbsp;</font></td><td><font size="3">0000-0002-3772-5238&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>27</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>27</td>
</tr>

<tr>
<td valign="top">PD </td><td>NOV</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">VL </td><td>16</td>
</tr>

<tr>
<td valign="top">IS </td><td>6</td>
</tr>

<tr>
<td valign="top">BP </td><td>967</td>
</tr>

<tr>
<td valign="top">EP </td><td>979</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1111/desc.12075</td>
</tr>

<tr>
<td valign="top">SC </td><td>Psychology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000325549300016</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Takashima, R
   <br>Takiguchi, T
   <br>Ariki, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Takashima, Ryoichi
   <br>Takiguchi, Tetsuya
   <br>Ariki, Yasuo</td>
</tr>

<tr>
<td valign="top">TI </td><td>Exemplar-Based Voice Conversion Using Sparse Representation in Noisy
   Environments</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEICE TRANSACTIONS ON FUNDAMENTALS OF ELECTRONICS COMMUNICATIONS AND
   COMPUTER SCIENCES</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; exemplar-based; sparse coding; non-negative matrix
   factorization; noise robustness</td>
</tr>

<tr>
<td valign="top">ID </td><td>NONNEGATIVE MATRIX FACTORIZATION; ARTIFICIAL NEURAL-NETWORKS; SPEECH;
   TRANSFORMATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a voice conversion (VC) technique for noisy environments, where parallel exemplars are introduced to encode the source speech signal and synthesize the target speech signal. The parallel exemplars (dictionary) consist of the source exemplars and target exemplars, having the same texts uttered by the source and target speakers. The input source signal is decomposed into the source exemplars, noise exemplars and their weights (activities). Then, by using the weights of the source exemplars, the converted signal is constructed from the target exemplars. We carried out speaker conversion tasks using clean speech data and noise-added speech data. The effectiveness of this method was confirmed by comparing its effectiveness with that of a conventional Gaussian Mixture Model (GMM)-based method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Takashima, Ryoichi; Takiguchi, Tetsuya; Ariki, Yasuo] Kobe Univ, Grad
   Sch Syst Informat, Kobe, Hyogo 6578501, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Takashima, R (reprint author), Kobe Univ, Grad Sch Syst Informat, Kobe, Hyogo 6578501, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>takashima@me.cs.scitec.kobe-u.ac.jp; takigu@kobe-u.ac.jp;
   arild@kobe-u.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>16</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>16</td>
</tr>

<tr>
<td valign="top">PD </td><td>OCT</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">VL </td><td>E96A</td>
</tr>

<tr>
<td valign="top">IS </td><td>10</td>
</tr>

<tr>
<td valign="top">BP </td><td>1946</td>
</tr>

<tr>
<td valign="top">EP </td><td>1953</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1587/transfun.E96.A.1946</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000326667500005</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Bidelman, GM
   <br>Moreno, S
   <br>Alain, C</td>
</tr>

<tr>
<td valign="top">AF </td><td>Bidelman, Gavin M.
   <br>Moreno, Sylvain
   <br>Alain, Claude</td>
</tr>

<tr>
<td valign="top">TI </td><td>Tracing the emergence of categorical speech perception in the human
   auditory system</td>
</tr>

<tr>
<td valign="top">SO </td><td>NEUROIMAGE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Categorical perception; Speech perception; Brainstem response; Auditory
   event-related potentials (ERP); Neural computation</td>
</tr>

<tr>
<td valign="top">ID </td><td>FREQUENCY-FOLLOWING RESPONSES; VOICE-ONSET TIME; HUMAN BRAIN-STEM;
   EVENT-RELATED POTENTIALS; EVOKED-POTENTIALS; CORTICAL REPRESENTATION;
   PITCH SALIENCE; MUSICAL PITCH; NON-MUSICIANS; VOWEL SOUNDS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Speech perception requires the effortless mapping from smooth, seemingly continuous changes in sound features into discrete perceptual units, a conversion exemplified in the phenomenon of categorical perception. Explaining how/when the human brain performs this acoustic-phonetic transformation remains an elusive problem in current models and theories of speech perception. In previous attempts to decipher the neural basis of speech perception, it is often unclear whether the alleged brain correlates reflect an underlying percept or merely changes in neural activity that covary with parameters of the stimulus. Here, we recorded neuroelectric activity generated at both cortical and subcortical levels of the auditory pathway elicited by a speech vowel continuum whose percept varied categorically from /u/ to /a/. This integrative approach allows us to characterize how various auditory structures code, transform, and ultimately render the perception of speech material as well as dissociate brain responses reflecting changes in stimulus acoustics from those that index true internalized percepts. We find that activity from the brainstem mirrors properties of the speech waveform with remarkable fidelity, reflecting progressive changes in speech acoustics but not the discrete phonetic classes reported behaviorally. In comparison, patterns of late cortical evoked activity contain information reflecting distinct perceptual categories and predict the abstract phonetic speech boundaries heard by listeners. Our findings demonstrate a critical transformation in neural speech representations between brainstem and early auditory cortex analogous to an acoustic-phonetic mapping necessary to generate categorical speech percepts. Analytic modeling demonstrates that a simple nonlinearity accounts for the transformation between early (subcortical) brain activity and subsequent cortical/behavioral responses to speech (&gt;15-200 ms) thereby describing a plausible mechanism by which the brain achieves its acoustic-to-phonetic mapping. Results provide evidence that the neurophysiological underpinnings of categorical speech are present cortically by similar to 175 ms after sound enters the ear. (C) 2013 Elsevier Inc. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Bidelman, Gavin M.] Univ Memphis, Inst Intelligent Syst, Memphis, TN
   38105 USA.
   <br>[Bidelman, Gavin M.] Univ Memphis, Sch Commun Sci &amp; Disorders, Memphis,
   TN 38105 USA.
   <br>[Moreno, Sylvain; Alain, Claude] Baycrest Ctr Geriatr Care, Rotman Res
   Inst, Toronto, ON M6A 2E1, Canada.
   <br>[Alain, Claude] Univ Toronto, Dept Psychol, Toronto, ON M6A 2E1, Canada.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Bidelman, GM (reprint author), Univ Memphis, Sch Commun Sci &amp; Disorders, 807 Jefferson Ave, Memphis, TN 38105 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>g.bidelman@memphis.edu</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Bidelman, Gavin</display_name>&nbsp;</font></td><td><font size="3">D-9998-2016&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Bidelman, Gavin M</display_name>&nbsp;</font></td><td><font size="3">0000-0002-1821-3261&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>64</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>64</td>
</tr>

<tr>
<td valign="top">PD </td><td>OCT 1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">VL </td><td>79</td>
</tr>

<tr>
<td valign="top">BP </td><td>201</td>
</tr>

<tr>
<td valign="top">EP </td><td>212</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.neuroimage.2013.04.093</td>
</tr>

<tr>
<td valign="top">SC </td><td>Neurosciences &amp; Neurology; Radiology, Nuclear Medicine &amp; Medical Imaging</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000320412200021</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Segi, H
   <br>Takou, R
   <br>Seiyama, N
   <br>Takagi, T
   <br>Uematsu, Y
   <br>Saito, H
   <br>Ozawa, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Segi, Hiroyuki
   <br>Takou, Reiko
   <br>Seiyama, Nobumasa
   <br>Takagi, Tohru
   <br>Uematsu, Yuko
   <br>Saito, Hideo
   <br>Ozawa, Shinji</td>
</tr>

<tr>
<td valign="top">TI </td><td>An Automatic Broadcast System for a Weather Report Radio Program</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE TRANSACTIONS ON BROADCASTING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Recording-sentence set; speech-rate conversion; templates; voice
   synthesizer</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH SYNTHESIS SYSTEM</td>
</tr>

<tr>
<td valign="top">AB </td><td>Here we describe a speech-synthesis method using templates that can generate recording-sentence sets for speech databases and produce natural sounding synthesized speech. Applying this method to the Japan Broadcasting Corporation (NHK) weather report radio program reduced the size of the recording-sentence set required to just a fraction of that needed by a comparable method. After integrating the recording voice of the generated recording-sentence set into the speech database, speech was produced by a voice synthesizer using templates. In a paired-comparison test, 66 % of the speech samples synthesized by our system using templates were preferred to those produced by a conventional voice synthesizer. In an evaluation test using a five-point mean opinion score (MOS) scale, the speech samples synthesized by our system scored 4.97, whereas the maximum score for commercially available voice synthesizers was 3.09. In addition, we developed an automatic broadcast system for the weather report program using the speech-synthesis method and speech-rate converter. The system was evaluated using real weather data for more than 1 year, and exhibited sufficient stability and synthesized speech quality for broadcast purposes.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Segi, Hiroyuki; Takou, Reiko; Seiyama, Nobumasa] NHK Sci &amp; Technol Res
   Labs, Tokyo 1578510, Japan.
   <br>[Takagi, Tohru] NHK Engn Serv Inc, Tokyo 1578540, Japan.
   <br>[Uematsu, Yuko; Saito, Hideo] Keio Univ, Grad Sch Sci &amp; Technol, Sch Sci
   Open &amp; Environm Syst, Kanagawa 2238522, Japan.
   <br>[Ozawa, Shinji] Aichi Univ Technol, Grad Sch Technol, Sch Media &amp;
   Informat, Aichi 4430047, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Segi, H (reprint author), NHK Sci &amp; Technol Res Labs, Tokyo 1578510, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>segi.h-gs@nhk.or.jp; takou.r-go@nhk.or.jp; seiyama.n-ek@nhk.or.jp;
   takagi.t-fo@nhk.or.jp; yu-ko@hvrl.ics.keio.ac.jp;
   saito@hvrl.ics.keio.ac.jp; ozawa@aut.ac.jp</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Saito, Hideo</display_name>&nbsp;</font></td><td><font size="3">D-6223-2014&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PD </td><td>SEP</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">VL </td><td>59</td>
</tr>

<tr>
<td valign="top">IS </td><td>3</td>
</tr>

<tr>
<td valign="top">BP </td><td>548</td>
</tr>

<tr>
<td valign="top">EP </td><td>555</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TBC.2013.2272406</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000323717100015</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Rashid, AZMM
   <br>Craig, D
   <br>Mukul, SA
   <br>Khan, NA</td>
</tr>

<tr>
<td valign="top">AF </td><td>Rashid, A. Z. M. Manzoor
   <br>Craig, Donna
   <br>Mukul, Sharif Ahmed
   <br>Khan, Niaz Ahmed</td>
</tr>

<tr>
<td valign="top">TI </td><td>A journey towards shared governance: status and prospects for
   collaborative management in the protected areas of Bangladesh</td>
</tr>

<tr>
<td valign="top">SO </td><td>JOURNAL OF FORESTRY RESEARCH</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>protected area; co-management; Nishorgo; IPAC; governance</td>
</tr>

<tr>
<td valign="top">ID </td><td>NATIONAL-PARKS; NEEDS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Establishment of Protected Areas (PAs), in the face of rapid deforestation, forest degradation and climate change has been one of the key efforts in conservation of biodiversity worldwide in recent times. While Bangladesh has gained a degree of prominence in the world for its successful social forestry programs, the concept of collaborative protected area management is rather new in the country, initiated in 2004 by the Bangladesh Forest Department in five PAs with financial assistance from USAID. Based on empirical evidence from three of the pilot PAs, we examined the achievements and associated challenges and prospects for co-management. Our fieldwork revealed a number of challenges faced by co-management institutions: (1) institutions were dominated by the elite group, overshadowing the voice of the community people; (2) mutual trust and collective performance are key to good governance but had not taken root in the PAs; (3) encroachment onto forest land and subsequent conversion to agriculture remained a serious problem that discouraged forest-dependent people from participating actively in co-management initiatives; (4) legal provisions (including acts, rules and policies) were not clearly and adequately disseminated and understood at the community level; (5) there remained a degree of ambiguity regarding the roles and responsibilities of forest department (FD) and co-management committees (CMC) in field operations, and this was not enhancing transparency and accountability of the overall initiative; (6) the long-term sustainability of co-management institutions was another major concern, as the local intuitional structure was still in a nascent stage, and provisioning of resources (either internally or externally) remained somewhat uncertain. We offer recommendations for improvement.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Rashid, A. Z. M. Manzoor; Craig, Donna] Univ Western Sydney, Sch Law,
   Penrith, NSW 1797, Australia.
   <br>[Rashid, A. Z. M. Manzoor] Shahjalal Univ Sci &amp; Technol, Dept Forestry &amp;
   Environm Sci, Sylhet, Bangladesh.
   <br>[Mukul, Sharif Ahmed] Univ Queensland, Sch Agr &amp; Food Sci, Brisbane, Qld
   4072, Australia.
   <br>[Khan, Niaz Ahmed] Univ Dhaka, Dept Dev Studies, Dhaka, Bangladesh.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Rashid, AZMM (reprint author), Univ Western Sydney, Sch Law, Penrith, NSW 1797, Australia.</td>
</tr>

<tr>
<td valign="top">EM </td><td>pollen_forest@yahoo.com</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Mukul, Sharif A.</display_name>&nbsp;</font></td><td><font size="3">H-3859-2019&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Mukul, Sharif A.</display_name>&nbsp;</font></td><td><font size="3">H-5664-2012&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Mukul, Sharif A.</display_name>&nbsp;</font></td><td><font size="3">0000-0001-6955-2469&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Mukul, Sharif A.</display_name>&nbsp;</font></td><td><font size="3">0000-0001-6955-2469&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>14</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>15</td>
</tr>

<tr>
<td valign="top">PD </td><td>SEP</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">VL </td><td>24</td>
</tr>

<tr>
<td valign="top">IS </td><td>3</td>
</tr>

<tr>
<td valign="top">BP </td><td>599</td>
</tr>

<tr>
<td valign="top">EP </td><td>605</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/s11676-013-0391-4</td>
</tr>

<tr>
<td valign="top">SC </td><td>Forestry</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000322191800026</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Woodhouse, M
   <br>Goodrich, A
   <br>Margolis, R
   <br>James, T
   <br>Dhere, R
   <br>Gessert, T
   <br>Barnes, T
   <br>Eggert, R
   <br>Albin, D</td>
</tr>

<tr>
<td valign="top">AF </td><td>Woodhouse, Michael
   <br>Goodrich, Alan
   <br>Margolis, Robert
   <br>James, Ted
   <br>Dhere, Ramesh
   <br>Gessert, Tim
   <br>Barnes, Teresa
   <br>Eggert, Roderick
   <br>Albin, David</td>
</tr>

<tr>
<td valign="top">TI </td><td>Perspectives on the pathways for cadmium telluride photovoltaic module
   manufacturers to address expected increases in the price for tellurium</td>
</tr>

<tr>
<td valign="top">SO </td><td>SOLAR ENERGY MATERIALS AND SOLAR CELLS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Thin-film photovoltaics; Energy critical elements; Solar PV module
   manufacturing; Tellurium; PV economics</td>
</tr>

<tr>
<td valign="top">ID </td><td>FILM SOLAR-CELLS; TRANSPARENT CONDUCTING OXIDES; MATERIALS AVAILABILITY;
   EFFICIENCY; CDS/CDTE; LAYERS; CDS; PV; TE; DEPLOYMENT</td>
</tr>

<tr>
<td valign="top">AB </td><td>Since the days of the technology's conception, concerns have been voiced over potential supply constraints of Tellurium that could limit the large-scale deployment of the Cadmium Telluride (CdTe) solar photovoltaic technology. Because any potential supply-demand imbalance created by a Tellurium constraint would manifest itself in the form of a price increase (a trend that was already seen prior to the 2012 downturn in PV manufacturing), we have rigorously examined the sensitivity of total CdTe module manufacturing prices to the price of this minor metal. We found that module manufacturers could conceivably absorb a gradual increase in Te prices up to an order of magnitude higher than what was typical for 2011 without significantly compromising their near to mid-term competitive position within the PV industry (viewed here to be a $0.70/W module price)-if the pace of improvements in module power conversion efficiencies and reductions in the CdTe layer thickness is rapid enough. Realizing gains in module-area efficiencies while, at the same time, also reducing the CdTe thickness is certainly technically challenging and merits its own line of research. However, in order to accommodate up to an order of magnitude increase in Te prices while still keeping the cost of the active layer to a reasonable $0.15/W range, we find that the cost benefits gained by reducing the absorber layer thickness are expected to be as significant as those provided by efficiency gains alone. Realizing the optimistic target of 18% efficient modules with 1.0 mu m of CdTe could reduce the Te material intensity from today's requirement of around 74 metric tonnes (MT) per GW to 17 MT/GW; even so, we estimate that CdTe PV is likely to be material constrained to around 10 GW of annual production by 2020 unless new sources of Tellurium-beyond traditional copper byproduct sources at the current 55% recovery rate-come online. The economics of this mineral are such that a higher price offering is a necessary precondition in order to motivate enhanced recovery rates from copper mining, and for future direct mining projects. (C) 2012 Elsevier B.V. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Woodhouse, Michael; Goodrich, Alan; Margolis, Robert; James, Ted] Natl
   Renewable Energy Lab, Strateg Energy Anal Ctr, Golden, CO 80401 USA.
   <br>[Eggert, Roderick] Colorado Sch Mines, Golden, CO 80401 USA.
   <br>[Dhere, Ramesh; Gessert, Tim; Barnes, Teresa; Albin, David] Natl
   Renewable Energy Lab, Natl Ctr Photovolta, Golden, CO 80401 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Woodhouse, M (reprint author), Natl Renewable Energy Lab, Strateg Energy Anal Ctr, 1617 Cole Blvd, Golden, CO 80401 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>Michael.Woodhouse@nrel.gov; Alan.Goodrich@nrel.gov; David.Albin@nrel.gov</td>
</tr>

<tr>
<td valign="top">TC </td><td>49</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>49</td>
</tr>

<tr>
<td valign="top">PD </td><td>AUG</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">VL </td><td>115</td>
</tr>

<tr>
<td valign="top">BP </td><td>199</td>
</tr>

<tr>
<td valign="top">EP </td><td>212</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.solmat.2012.03.023</td>
</tr>

<tr>
<td valign="top">SC </td><td>Energy &amp; Fuels; Materials Science; Physics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000320681700028</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Uriz, AJ
   <br>Aguero, PD
   <br>Tulli, JC
   <br>Moreira, JC
   <br>Gonzalez, EL
   <br>Bonafonte, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Uriz, A. J.
   <br>Agueero, P. D.
   <br>Tulli, J. C.
   <br>Castineira Moreira, J.
   <br>Gonzalez, E. L.
   <br>Bonafonte, A.</td>
</tr>

<tr>
<td valign="top">TI </td><td>VOICE CONVERSION USING K-HISTOGRAMS AND RESIDUAL AVERAGING</td>
</tr>

<tr>
<td valign="top">SO </td><td>LATIN AMERICAN APPLIED RESEARCH</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice Conversion; K- Histograms; Residual Conversion; Voice Synthesis</td>
</tr>

<tr>
<td valign="top">AB </td><td>The main goal of a voice conversion system is to modify the voice of a source speaker, in order to be perceived as if it had been uttered by another specific speaker. Many approaches found in the literature convert only the features related to the vocal tract of the speaker. Our proposal is to convert those characteristics, and to process the signal passing through the vocal chords. Thus, the goal of this work is to obtain better scores in the voice conversion results.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Uriz, A. J.; Castineira Moreira, J.] Univ Nacl Mar Del Plata, Fac Ingn,
   CONICET, RA-7600 Mar Del Plata, Argentina.
   <br>[Agueero, P. D.; Tulli, J. C.; Gonzalez, E. L.] Univ Nacl Mar Del Plata,
   Fac Ingn, RA-7600 Mar Del Plata, Argentina.
   <br>[Bonafonte, A.] Univ Politecn Cataluna, Barcelona, Spain.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Uriz, AJ (reprint author), Univ Nacl Mar Del Plata, Fac Ingn, CONICET, RA-7600 Mar Del Plata, Argentina.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ajuriz@conicet.gov.ar; pdaguero@fi.mdp.edu.ar</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUL</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">VL </td><td>43</td>
</tr>

<tr>
<td valign="top">IS </td><td>3</td>
</tr>

<tr>
<td valign="top">BP </td><td>231</td>
</tr>

<tr>
<td valign="top">EP </td><td>236</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000335937200005</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hwang, I
   <br>Lee, H
   <br>Choi, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hwang, Inwook
   <br>Lee, Hyeseon
   <br>Choi, Seungmoon</td>
</tr>

<tr>
<td valign="top">TI </td><td>Real-Time Dual-Band Haptic Music Player for Mobile Devices</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE TRANSACTIONS ON HAPTICS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Haptic I/O; vibration; music; real time; dual band; dual-mode actuator</td>
</tr>

<tr>
<td valign="top">ID </td><td>MODEL; TRANSCRIPTION; FRAMEWORK; SYSTEM; AUDIO</td>
</tr>

<tr>
<td valign="top">AB </td><td>We introduce a novel dual-band haptic music player for real-time simultaneous vibrotactile playback with music in mobile devices. Our haptic music player features a new miniature dual-mode actuator that can produce vibrations consisting of two principal frequencies and a real-time vibration generation algorithm that can extract vibration commands from a music file for dual-band playback (bass and treble). The algorithm uses a "haptic equalizer" and provides plausible sound-to-touch modality conversion based on human perceptual data. In addition, we present a user study carried out to evaluate the subjective performance (precision, harmony, fun, and preference) of the haptic music player, in comparison with the current practice of bass-band-only vibrotactile playback via a single-frequency voice-coil actuator. The evaluation results indicated that the new dual-band playback outperforms the bass-only rendering, also providing several insights for further improvements. The developed system and experimental findings have implications for improving the multimedia experience with mobile devices.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Hwang, Inwook; Choi, Seungmoon] POSTECH, Dept Comp Sci &amp; Engn, Hapt &amp;
   Virtual Real Lab, Pohang 790784, Gyungsangbuk Do, South Korea.
   <br>[Lee, Hyeseon] POSTECH, Dept Ind &amp; Management Engn, Pohang 790784,
   Gyungsangbuk Do, South Korea.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hwang, I (reprint author), POSTECH, Dept Comp Sci &amp; Engn, Hapt &amp; Virtual Real Lab, Sci Bldg 4-115, Pohang 790784, Gyungsangbuk Do, South Korea.</td>
</tr>

<tr>
<td valign="top">EM </td><td>inux@postech.ac.kr; hyelee@postech.ac.kr; choism@postech.ac.kr</td>
</tr>

<tr xmlns:date="http://exslt.org/dates-and-times">
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Choi, Seungmoon</display_name>&nbsp;</font></td><td><font size="3">0000-0002-5889-1083&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>10</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>10</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUL-SEP</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">VL </td><td>6</td>
</tr>

<tr>
<td valign="top">IS </td><td>3</td>
</tr>

<tr>
<td valign="top">BP </td><td>340</td>
</tr>

<tr>
<td valign="top">EP </td><td>351</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TOH.2013.7</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000323896900008</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Pribil, J
   <br>Pribilova, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Pribil, Jiri
   <br>Pribilova, Anna</td>
</tr>

<tr>
<td valign="top">TI </td><td>Evaluation of influence of spectral and prosodic features on GMM
   classification of Czech and Slovak emotional speech</td>
</tr>

<tr>
<td valign="top">SO </td><td>EURASIP JOURNAL ON AUDIO SPEECH AND MUSIC PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>emotional speech recognition; GMM classifier; spectral and prosodic
   features of speech</td>
</tr>

<tr>
<td valign="top">ID </td><td>ROBUST ASR; RECOGNITION; MODELS; VERIFICATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This article analyzes and compares influence of different types of spectral and prosodic features for Czech and Slovak emotional speech classification based on Gaussian mixture models (GMM). Influence of initial setting of parameters (number of mixture components and used number of iterations) for GMM training process was analyzed, too. Subsequently, analysis was performed to find how correctness of emotion classification depends on the number and the order of the parameters in the input feature vector and on the computation complexity. Another test was carried out to verify the functionality of the proposed two-level architecture comprising the gender recognizer and of the emotional speech classifier. Next tests were realized to find dependence of some negative aspect (processing of the input speech signal with too short time duration, the gender of a speaker incorrectly determined, etc.) on the stability of the results generated during the GMM classification process. Evaluations and tests were realized with the speech material in the form of sentences of male and female speakers expressing four emotional states (joy, sadness, anger, and a neutral state) in Czech and Slovak languages. In addition, a comparative experiment using the speech data corpus in other language (German) was performed. The mean classification error rate of the whole classifier structure achieves about 21% for all four emotions and both genders, and the best obtained error rate was 3.5% for the sadness style of the female gender. These values are acceptable in this first stage of development of the GMM classifier. On the other hand, the test showed the principal importance of correct classification of the speaker gender in the first level, which has heavy influence on the resulting recognition score of the emotion classification. This GMM classifier should be used for evaluation of the synthetic speech quality after applied voice conversion and emotional speech style transformation.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Pribil, Jiri] Slovak Acad Sci, Inst Measurement Sci, SK-84104
   Bratislava, Slovakia.
   <br>[Pribilova, Anna] SUT, Fac Elect Engn &amp; Informat Technol, Inst Elect &amp;
   Photon, SK-81219 Bratislava, Slovakia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Pribil, J (reprint author), Slovak Acad Sci, Inst Measurement Sci, Dubravska Cesta 9, SK-84104 Bratislava, Slovakia.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jiri.pribil@savba.sk</td>
</tr>

<tr>
<td valign="top">TC </td><td>14</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>14</td>
</tr>

<tr>
<td valign="top">PD </td><td>APR 24</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">AR </td><td>8</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1186/1687-4722-2013-8</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000323350000001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Pribil, J
   <br>Pribilova, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Pribil, Jiri
   <br>Pribilova, Anna</td>
</tr>

<tr>
<td valign="top">TI </td><td>Determination of Formant Features in Czech and Slovak for GMM Emotional
   Speech Classifier</td>
</tr>

<tr>
<td valign="top">SO </td><td>RADIOENGINEERING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Formant features of speech; emotional speech; statistical analysis</td>
</tr>

<tr>
<td valign="top">ID </td><td>RECOGNITION; MODELS; IDENTIFICATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>The paper is aimed at determination of formant features (FF) which describe vocal tract characteristics. It comprises analysis of the first three formant positions together with their bandwidths and the formant tilts. Subsequently, the statistical evaluation and comparison of the FF was performed. This experiment was realized with the speech material in the form of sentences of male and female speakers expressing four emotional states (joy, sadness, anger, and a neutral state) in Czech and Slovak languages. The statistical distribution of the analyzed form ant frequencies and formant tilts shows good differentiation between neutral and emotional styles for both voices. Contrary to it, the values of the formant 3-dB bandwidths have no correlation with the type of the speaking style or the type of the voice. These spectral parameters together with the values of the other speech characteristics were used in the feature vector for Gaussian mixture models (GMM emotional speech style classifier that is currently developed The overall mean classification error rate achieves about 18 K and the best obtained error rate is 5 % for the sadness style of the female voice. These values are acceptable in this first stage of development of the GMM classifier that should be used for evaluation of the synthetic speech quality after applied voice conversion and emotional speech style transformation.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Pribil, Jiri] SAS, Inst Measurement Sci, SK-84104 Bratislava, Slovakia.
   <br>[Pribilova, Anna] SUT, Fac Elect Engn &amp; Informat Technol, Inst Elect &amp;
   Photon, SK-81219 Bratislava, Slovakia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Pribil, J (reprint author), SAS, Inst Measurement Sci, Dubravska Cesta 9, SK-84104 Bratislava, Slovakia.</td>
</tr>

<tr>
<td valign="top">EM </td><td>Jiri.Pribil@savba.sk; Anna.Pribilova@stuba.sk</td>
</tr>

<tr>
<td valign="top">TC </td><td>6</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>6</td>
</tr>

<tr>
<td valign="top">PD </td><td>APR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">VL </td><td>22</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">SI </td><td>SI</td>
</tr>

<tr>
<td valign="top">BP </td><td>52</td>
</tr>

<tr>
<td valign="top">EP </td><td>59</td>
</tr>

<tr>
<td valign="top">PN </td><td>1</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000318052400007</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wilkinson, EP
   <br>Abdel-Hamid, O
   <br>Galvin, JJ
   <br>Jiang, H
   <br>Fu, QJ</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wilkinson, Eric P.
   <br>Abdel-Hamid, Ossama
   <br>Galvin, John J., III
   <br>Jiang, Hui
   <br>Fu, Qian-Jie</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice conversion in cochlear implantation</td>
</tr>

<tr>
<td valign="top">SO </td><td>LARYNGOSCOPE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Otology; implants</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH RECOGNITION; TEMPORAL CUES; VOWEL RECOGNITION; USERS;
   DISCRIMINATION; PERCEPTION; TALKER; IDENTIFICATION; NORMALIZATION;
   VARIABILITY</td>
</tr>

<tr>
<td valign="top">AB </td><td>Objectives/Hypothesis: Voice conversion algorithms may benefit cochlear implant (CI) users who better understand speech produced by one talker than by another. It is unclear how the source or target talker's fundamental frequency (F0) information may contribute to perception of converted speech. This study evaluated voice conversion algorithms for CI users in which the source or target talker's F0 was included in the converted speech. Study Design: Development and evaluation of computerized voice conversion algorithms in CI patients. Methods: A series of cepstral analysis-based algorithms were developed and evaluated in six CI users. The algorithms converted talker voice gender (male-to-female, or female-to-male); either the source or target talker F0 was included in the converted speech. The voice conversion algorithms were evaluated in terms of recognition of IEEE sentences, speech quality, and voice gender discrimination. Results: Voice gender recognition performance showed that listeners strongly cued to the F0 that was included within the converted speech. For both IEEE sentence recognition and voice quality ratings, performance was poorer with the voice conversion algorithms than with original speech. Performance on female-to-male conversion was superior to male-to-female conversion. Conclusion: The strong cueing to F0 within the voice conversion algorithms suggests that CI users are able to utilize temporal periodicity information for some pitch-related tasks. Limitations on spectral channel information experienced by CI users may result in poorer performance with voice conversion algorithms due to distortion of speech formant information and degradation of the spectral envelope. Laryngoscope, 2013</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Wilkinson, Eric P.] House Ear Clin, Los Angeles, CA 90057 USA.
   <br>[Wilkinson, Eric P.; Galvin, John J., III; Fu, Qian-Jie] House Ear Res
   Inst, Los Angeles, CA 90034 USA.
   <br>[Abdel-Hamid, Ossama; Jiang, Hui] York Univ, Dept Comp Sci &amp; Engn,
   Toronto, ON M3J 2R7, Canada.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Wilkinson, EP (reprint author), House Ear Clin, 2100 W 3rd St,111, Los Angeles, CA 90057 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ewilkinson@hei.org</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PD </td><td>APR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">VL </td><td>123</td>
</tr>

<tr>
<td valign="top">SU </td><td>3</td>
</tr>

<tr>
<td valign="top">BP </td><td>S29</td>
</tr>

<tr>
<td valign="top">EP </td><td>S43</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1002/lary.23744</td>
</tr>

<tr>
<td valign="top">SC </td><td>Research &amp; Experimental Medicine; Otorhinolaryngology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000316688800001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Jenkins, LD
   <br>Garrison, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Jenkins, Lekelia D.
   <br>Garrison, Karen</td>
</tr>

<tr>
<td valign="top">TI </td><td>Fishing gear substitution to reduce bycatch and habitat impacts: An
   example of social-ecological research to inform policy</td>
</tr>

<tr>
<td valign="top">SO </td><td>MARINE POLICY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Fishing gear substitution; Fishing gear conversion; Fishing gear
   switching; Bycatch; Habitat impacts; Social-ecological systems</td>
</tr>

<tr>
<td valign="top">ID </td><td>SMALL-SCALE FISHERIES; SCENARIO ANALYSIS; BENTHIC COMMUNITIES;
   MANAGEMENT; SYSTEMS; BIODIVERSITY; SUSTAINABILITY; CONSERVATION;
   PERSPECTIVE; SELECTIVITY</td>
</tr>

<tr>
<td valign="top">AB </td><td>This study examined the feasibility of gear substitution as a means to reduce bycatch and habitat impacts of fisheries, using a social-ecological systems approach. The U.S. west coast sablefish fishery is an excellent subject for this study, because it permits three different gear types and has a problem with bycatch of overfished species. Bycatch rates were highest in trawls and lowest in pots. Combining interview data with findings from a previous study, affirmed that habitat impacts were highest with trawls and lowest with longlines. Interviews with 44 individuals analyzed using grounded theory yielded several common themes in the opinions of gear substitution. Positive opinion themes included that it would allow better management of the fish populations by reducing bycatch and would allow more business options, flexibility, and increased profit for some trawlers. The main negative opinion theme was that gear substitution could decrease landings needed to support shoreside infrastructure. Most stakeholder groups saw some benefit in gear substitution. Notably, the trawlers voiced a unanimous preference for converting to pots rather than longlines. A scenario analysis revealed that the preferable management option would be long-term gear conversion, but incentives are likely to be an important means of encouraging gear conversion. This ecological impacts rapid assessment provided a regional evaluation of bycatch and habitat impacts that had never been conducted before for these gear types. It also provided scientific support for a regulatory change that legally allows trawlers to practice gear substitution. (C) 2012 Elsevier Ltd. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Jenkins, Lekelia D.] Univ Washington, Sch Marine &amp; Environm Affairs,
   Seattle, WA 98105 USA.
   <br>[Garrison, Karen] Nat Resources Def Council, San Francisco, CA 94104 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Jenkins, LD (reprint author), Univ Washington, Sch Marine &amp; Environm Affairs, 3707 Brooklyn Ave NE, Seattle, WA 98105 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>kikij@uw.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>14</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>14</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">VL </td><td>38</td>
</tr>

<tr>
<td valign="top">BP </td><td>293</td>
</tr>

<tr>
<td valign="top">EP </td><td>303</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.marpol.2012.06.005</td>
</tr>

<tr>
<td valign="top">SC </td><td>Environmental Sciences &amp; Ecology; International Relations</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000313769600032</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Erro, D
   <br>Navas, E
   <br>Hernaez, I</td>
</tr>

<tr>
<td valign="top">AF </td><td>Erro, Daniel
   <br>Navas, Eva
   <br>Hernaez, Inma</td>
</tr>

<tr>
<td valign="top">TI </td><td>Parametric Voice Conversion Based on Bilinear Frequency Warping Plus
   Amplitude Scaling</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; Gaussian mixture model; bilinear function; frequency
   warping; amplitude scaling</td>
</tr>

<tr>
<td valign="top">ID </td><td>ARTIFICIAL NEURAL-NETWORKS; TO-SPEECH SYNTHESIS; SPEAKER ADAPTATION;
   TRANSFORMATION; ALGORITHM</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion methods based on frequency warping followed by amplitude scaling have been recently proposed. These methods modify the frequency axis of the source spectrum in such manner that some significant parts of it, usually the formants, are moved towards their image in the target speaker's spectrum. Amplitude scaling is then applied to compensate for the differences between warped source spectra and target spectra. This article presents a fully parametric formulation of a frequency warping plus amplitude scaling method in which bilinear frequency warping functions are used. Introducing this constraint allows for the conversion error to be described in the cepstral domain and to minimize it with respect to the parameters of the transformation through an iterative algorithm, even when multiple overlapping conversion classes are considered. The paper explores the advantages and limitations of this approach when applied to a cepstral representation of speech. We show that it achieves significant improvements in quality with respect to traditional methods based on Gaussian mixture models, with no loss in average conversion accuracy. Despite its relative simplicity, it achieves similar performance scores to state-of-the-art statistical methods involving dynamic features and global variance.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Erro, Daniel; Navas, Eva; Hernaez, Inma] Univ Basque Country UPV EHU,
   Aholab Signal Proc Lab, Bilbao 48013, Spain.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Erro, D (reprint author), Univ Basque Country UPV EHU, Aholab Signal Proc Lab, Bilbao 48013, Spain.</td>
</tr>

<tr>
<td valign="top">EM </td><td>derro@aholab.ehu.es</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Navas, Eva</display_name>&nbsp;</font></td><td><font size="3">H-4317-2013&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Erro, Daniel</display_name>&nbsp;</font></td><td><font size="3">H-7043-2015&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Hernaez Rioja, Inmaculada</display_name>&nbsp;</font></td><td><font size="3">K-8303-2012&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Navas, Eva</display_name>&nbsp;</font></td><td><font size="3">0000-0003-3804-4984&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Erro, Daniel</display_name>&nbsp;</font></td><td><font size="3">0000-0003-0954-6942&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Hernaez Rioja, Inmaculada</display_name>&nbsp;</font></td><td><font size="3">0000-0003-4447-7575&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>42</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>43</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">VL </td><td>21</td>
</tr>

<tr>
<td valign="top">IS </td><td>3</td>
</tr>

<tr>
<td valign="top">BP </td><td>556</td>
</tr>

<tr>
<td valign="top">EP </td><td>566</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASL.2012.2227735</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000313425100009</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Stamatakis, N
   <br>Vandeviver, C</td>
</tr>

<tr>
<td valign="top">AF </td><td>Stamatakis, Nikolaos
   <br>Vandeviver, Christophe</td>
</tr>

<tr>
<td valign="top">TI </td><td>Restorative justice in Belgian prisons: the results of an empirical
   research</td>
</tr>

<tr>
<td valign="top">SO </td><td>CRIME LAW AND SOCIAL CHANGE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">ID </td><td>RELIGION; CONVERSION; APOLOGY; MODEL</td>
</tr>

<tr>
<td valign="top">AB </td><td>Justice - when spelled with a capital 'J' - should be discursive [31] and based on equal respect ([40]: 206, 210) allowing a plurality of voices within the discourse. Particularly in the present research, this thread of pluralism is important. Prisoners' voices have rarely been heard. Yet, if we wish to be true to the principle that restorative justice is discursive, it follows that the discourse is not complete without also accommodating their voices. To date, little research attention has been paid to the inner motivations of imprisoned offenders for willing to participate in restorative justice initiatives, as well as to their perceptions about their relationships with the victim and the community and the impact of religion on them. Hence, the present empirical study, conducted in several prisons across Belgium, endeavours to shed light on these aspects that have been theoretically overlooked, providing valuable information at policy-level about the design of future restorative justice programmes.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Stamatakis, Nikolaos; Vandeviver, Christophe] Univ Ghent, B-9000 Ghent,
   Belgium.
   <br>[Stamatakis, Nikolaos] Univ Liege, Liege, Belgium.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Stamatakis, N (reprint author), Univ Ghent, B-9000 Ghent, Belgium.</td>
</tr>

<tr>
<td valign="top">EM </td><td>nikolaos.stamatakis@ugent.be</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Vandeviver, Christophe</display_name>&nbsp;</font></td><td><font size="3">F-6785-2017&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Vandeviver, Christophe</display_name>&nbsp;</font></td><td><font size="3">L-9726-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Vandeviver, Christophe</display_name>&nbsp;</font></td><td><font size="3">0000-0001-9714-7006&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Vandeviver, Christophe</display_name>&nbsp;</font></td><td><font size="3">0000-0001-9714-7006&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>4</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>4</td>
</tr>

<tr>
<td valign="top">PD </td><td>FEB</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">VL </td><td>59</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>79</td>
</tr>

<tr>
<td valign="top">EP </td><td>111</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/s10611-012-9408-8</td>
</tr>

<tr>
<td valign="top">SC </td><td>Criminology &amp; Penology; Social Sciences - Other Topics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000314029600005</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Trangkasombat, U</td>
</tr>

<tr>
<td valign="top">AF </td><td>Trangkasombat, Umaporn</td>
</tr>

<tr>
<td valign="top">TI </td><td>Cultural considerations in family therapy: boys with conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>ASIAN BIOMEDICINE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Boys; children; conversion disorder; culture; somatization</td>
</tr>

<tr>
<td valign="top">ID </td><td>CHILDREN; CHILDHOOD; PARENTS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Background: Conversion disorder is mostly reported in girls. One factor that makes girls vulnerable for emotional problems is the cultural practice of son preference, which is prevalent in many Asian countries including Thailand. However, other cultural factors make boys vulnerable to develop conversion disorders.
   <br>Objective: We reported clinical symptoms, family assessment, and important points in family therapy in two boys with conversion disorder.
   <br>Method: Two boys age 10 and 14 years old presented with seizures. Neurological tests were unremarkable and an organically-based seizure disorder was ruled out. The patients were diagnosed as having conversion disorder and were sent for psychiatric evaluation and treatment.
   <br>Result: Family assessment revealed some cultural practices that led to frustration and conversion. These factors included the culture of silence and the tendency to somatize; the cultural practice of mourning; and the parentification of children especially the first-born. In family therapy, therapeutic work included breaking the silence in the family and helping the family develop the ability to talk about feelings; helping the family deal with frustration Tore effectively, and dealing with somatic symptoms in an empathic manner. When the patients were able to reconnect with their families and to voice their needs, the body no longer needed to speak for them and the symptoms remitted.
   <br>Conclusion: The case vignettes illustrated the role culture plays in modifying the manifestations of both intrapsychic and interpersonal conflicts in boys. Understanding such a role will help therapists to treat conversion disorders in boys more effectively.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Chulalongkorn Univ, Fac Med, Dept Psychiat, Child Psychiat Unit, Bangkok
   10330, Thailand.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Trangkasombat, U (reprint author), Chulalongkorn Univ, Fac Med, Dept Psychiat, Child Psychiat Unit, Bangkok 10330, Thailand.</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PD </td><td>FEB</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">VL </td><td>7</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>131</td>
</tr>

<tr>
<td valign="top">EP </td><td>136</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.5372/1905-7415.0701.160</td>
</tr>

<tr>
<td valign="top">SC </td><td>Research &amp; Experimental Medicine</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000341273100017</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Dines, J
   <br>Liang, H
   <br>Saheer, L
   <br>Gibson, M
   <br>Byrne, W
   <br>Oura, K
   <br>Tokuda, K
   <br>Yamagishi, J
   <br>King, S
   <br>Wester, M
   <br>Hirsimaki, T
   <br>Karhila, R
   <br>Kurimo, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Dines, John
   <br>Liang, Hui
   <br>Saheer, Lakshmi
   <br>Gibson, Matthew
   <br>Byrne, William
   <br>Oura, Keiichiro
   <br>Tokuda, Keiichi
   <br>Yamagishi, Junichi
   <br>King, Simon
   <br>Wester, Mirjam
   <br>Hirsimaki, Teemu
   <br>Karhila, Reima
   <br>Kurimo, Mikko</td>
</tr>

<tr>
<td valign="top">TI </td><td>Personalising speech-to-speech translation: Unsupervised cross-lingual
   speaker adaptation for HMM-based speech synthesis</td>
</tr>

<tr>
<td valign="top">SO </td><td>COMPUTER SPEECH AND LANGUAGE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speech-to-speech translation; Cross-lingual speaker adaptation;
   HMM-based speech synthesis; Speaker adaptation; Voice conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>RECOGNITION; REPRESENTATIONS; ALGORITHMS</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper we present results of unsupervised cross-lingual speaker adaptation applied to text-to-speech synthesis. The application of our research is the personalisation of speech-to-speech translation in which we employ a HMM statistical framework for both speech recognition and synthesis. This framework provides a logical mechanism to adapt synthesised speech output to the voice of the user by way of speech recognition. In this work we present results of several different unsupervised and cross-lingual adaptation approaches as well as an end-to-end speaker adaptive speech-to-speech translation system. Our experiments show that we can successfully apply speaker adaptation in both unsupervised and cross-lingual scenarios and our proposed algorithms seem to generalise well for several language pairs. We also discuss important future directions including the need for better evaluation metrics. (C) 201 1 Elsevier Ltd. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Dines, John; Liang, Hui; Saheer, Lakshmi] Idiap Res Inst, Martigny,
   Switzerland.
   <br>[Gibson, Matthew; Byrne, William] Univ Cambridge, Dept Engn, Cambridge
   CB2 1TN, England.
   <br>[Oura, Keiichiro; Tokuda, Keiichi] Nagoya Inst Technol, Dept Comp Sci &amp;
   Engn, Nagoya, Aichi, Japan.
   <br>[Yamagishi, Junichi; King, Simon; Wester, Mirjam] Univ Edinburgh, Ctr
   Speech Technol CSTR, Edinburgh EH8 9YL, Midlothian, Scotland.
   <br>[Hirsimaki, Teemu; Karhila, Reima; Kurimo, Mikko] Aalto Univ, Adapt
   Informat Res Ctr, Helsinki, Finland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Dines, J (reprint author), Idiap Res Inst, Martigny, Switzerland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>john.dines@idiap.ch</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Kurimo, Mikko</display_name>&nbsp;</font></td><td><font size="3">F-6647-2012&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>King, Simon</display_name>&nbsp;</font></td><td><font size="3">0000-0002-2694-2843&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>8</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>8</td>
</tr>

<tr>
<td valign="top">PD </td><td>FEB</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">VL </td><td>27</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">SI </td><td>SI</td>
</tr>

<tr>
<td valign="top">BP </td><td>420</td>
</tr>

<tr>
<td valign="top">EP </td><td>437</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.csl.2011.08.003</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000312471500003</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ariwardhani, NW
   <br>Iribe, Y
   <br>Katsurada, K
   <br>Nitta, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ariwardhani, Narpendyah W.
   <br>Iribe, Yurie
   <br>Katsurada, Kouichi
   <br>Nitta, Tsuneo</td>
</tr>

<tr>
<td valign="top">BE </td><td>Sanei, S
   <br>Smaragdis, P
   <br>Nandi, A
   <br>Ho, ATS
   <br>Larsen, J</td>
</tr>

<tr>
<td valign="top">TI </td><td>VOICE CONVERSION FOR ARBITRARY SPEAKERS USING ARTICULATORY-MOVEMENT TO
   VOCAL-TRACT PARAMETER MAPPING</td>
</tr>

<tr>
<td valign="top">SO </td><td>2013 IEEE INTERNATIONAL WORKSHOP ON MACHINE LEARNING FOR SIGNAL
   PROCESSING (MLSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>IEEE International Workshop on Machine Learning for Signal Processing</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>23rd IEEE International Workshop on Machine Learning for Signal
   Processing (MLSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 22-25, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Southampton, ENGLAND</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice conversion; articulatory feature; neural network; arbitrary
   speaker; cross-lingual</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH RECOGNITION; FEATURE-EXTRACTION</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we propose voice conversion based on articulatory-movement (AM) to vocal tract parameter (VTP) mapping. An artificial neural network (ANN) is applied to map AM to VTP and to convert the source speaker's voice to the target speaker's voice. The proposed system is not only text independent voice conversion, but can also be used for an arbitrary source speaker. This means that our approach requires no source speaker data to build the voice conversion model and hence source speaker data is only required during testing phase. Preliminary cross-lingual voice conversion experiments are also conducted. The results of voice conversion were evaluated using subjective and objective measures to compare the performance of our proposed ANN-based voice conversion (VC) with the state-of-the-art Gaussian mixture model (GMM)-based VC. The experimental results show that the converted voice is intelligible and has speaker individuality of the target speaker.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ariwardhani, Narpendyah W.; Iribe, Yurie; Katsurada, Kouichi; Nitta,
   Tsuneo] Toyohashi Univ Technol, Grad Sch Engn, Toyohashi, Aichi, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ariwardhani, NW (reprint author), Toyohashi Univ Technol, Grad Sch Engn, Toyohashi, Aichi, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>narpen@vox.cs.tut.ac.jp; iribe@imc.tut.ac.jp; katsurada@cs.tut.ac.jp;
   nitta@cs.tut.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/MLSP.2013.6661965</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000345844100071</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Jokisch, O
   <br>Birhanu, Y
   <br>Hoffmann, R</td>
</tr>

<tr>
<td valign="top">AF </td><td>Jokisch, Oliver
   <br>Birhanu, Yitagessu
   <br>Hoffmann, Ruediger</td>
</tr>

<tr>
<td valign="top">BE </td><td>Kuzle, I
   <br>Capuder, T
   <br>Pandzic, H</td>
</tr>

<tr>
<td valign="top">TI </td><td>Runtime and Speech Quality Survey of a Voice Conversion Method</td>
</tr>

<tr>
<td valign="top">SO </td><td>2013 IEEE EUROCON</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE EUROCON Conference</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUL 01-04, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Zagreb, CROATIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; VTLN; runtime performance; speech quality; MOS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Several methods for voice conversion have been established. The research aims at the characteristics of a target speaker and a near-to-natural speech quality. This contribution summarizes the listening experiments with four conversion methods including the assessment of speech quality, listening effort and similarity to the target voice. The subjective evaluation of similarity is checked by an instrumental distance measure based on logarithmic spectral distortion. Practical applications of voice conversion require an appropriate runtime performance and memory use. We select a conversion method based on VTLN to demonstrate the runtime and quality trade-off. In the case example, we survey the quality assessment depending on different training constellations with a varied data amount and training time. Furthermore, we discuss the runtime performance of the selected conversion method under typical operating conditions. The experiments cover the influence of system resources, setting of conversion parameters (warping factors) and different training constellations. The observed real-time factors of a non-optimized laboratory VC version are inappropriate for typical application scenarios.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Jokisch, Oliver] Leipzig Univ Telecommun, Inst Commun Engn, Gustav
   Freytag St 43, D-04277 Leipzig, Germany.
   <br>[Birhanu, Yitagessu; Hoffmann, Ruediger] Tech Univ Dresden, Chair Syst
   Theory &amp; Speech Technol, D-01069 Dresden, Germany.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Jokisch, O (reprint author), Leipzig Univ Telecommun, Inst Commun Engn, Gustav Freytag St 43, D-04277 Leipzig, Germany.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jokisch@hft-leipzig.de; yitagesu.gebremedhin@ias.et.tu-dresden.de;
   ruediger.hoffmann@tu-dresden.de</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>1684</td>
</tr>

<tr>
<td valign="top">EP </td><td>1688</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000343135600247</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ma, RG
   <br>Liu, FZ</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ma, Ronggui
   <br>Liu, Fangzhou</td>
</tr>

<tr>
<td valign="top">BE </td><td>Kim, YH</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Design of a Speech Conversion System from PuTongHua to Cantonese Based
   on iFLY MSP 2.0</td>
</tr>

<tr>
<td valign="top">SO </td><td>MECHATRONICS, ROBOTICS AND AUTOMATION, PTS 1-3</td>
</tr>

<tr>
<td valign="top">SE </td><td>Applied Mechanics and Materials</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Mechatronics, Robotics and Automation (ICMRA
   2013)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUN 13-14, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Guangzhou, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speech Conversion System; speech recognition; Text To Speech; FLY MSP
   2.0</td>
</tr>

<tr>
<td valign="top">AB </td><td>The paper analyzes the working theory of a Speech Conversion System from PuTongHua to Cantonese based on iFLY MSP 2.0. In the system, QISR interface is chosen to complete speech recognition function which is the key technology to convert the voice information into the corresponding text information. Moreover, the QTTS interface is chosen to complete the text to speech function which is the key technology to transform the text which is the result of the speech recognition into the spoken information in Cantonese and then output. Finally, the computer assisted learning system is designed successfully in the environment of Visual C++ 6.0.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ma, Ronggui; Liu, Fangzhou] Changan Univ, Dept Informat Engn, Xian,
   Shaanxi, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ma, RG (reprint author), Changan Univ, Dept Informat Engn, Xian, Shaanxi, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>rgma@chd.edu.cn; lfz1988@sina.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">VL </td><td>373-375</td>
</tr>

<tr>
<td valign="top">BP </td><td>504</td>
</tr>

<tr>
<td valign="top">EP </td><td>508</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.4028/www.scientific.net/AMM.373-375.504</td>
</tr>

<tr>
<td valign="top">SC </td><td>Automation &amp; Control Systems; Engineering; Materials Science; Robotics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000339362300095</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Yun, YS
   <br>Ladner, RE</td>
</tr>

<tr>
<td valign="top">AF </td><td>Yun, Young-Sun
   <br>Ladner, Richard E.</td>
</tr>

<tr>
<td valign="top">BE </td><td>Habernal, I
   <br>Matousek, V</td>
</tr>

<tr>
<td valign="top">TI </td><td>Bilingual Voice Conversion by Weighted Frequency Warping Based on
   Formant Space</td>
</tr>

<tr>
<td valign="top">SO </td><td>TEXT, SPEECH, AND DIALOGUE, TSD 2013</td>
</tr>

<tr>
<td valign="top">SE </td><td>Lecture Notes in Computer Science</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>16th International Conference on Text, Speech, and Dialogue (TSD)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 01-05, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Pilsen, CZECH REPUBLIC</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; weighted frequency warping; formant space</td>
</tr>

<tr>
<td valign="top">ID </td><td>NORMALIZATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Voice conversion is a technique that transforms the source speaker's individuality to that of the target speaker. In this paper, we propose a simple and intuitive voice conversion algorithm that does not use training data between different languages, but uses text-to-speech generated speech rather than real recorded voices. The suggested method finds the transformed frequency by formant space warping. The formant space comprises four representative monophthongs for each language. The warping functions are represented by piecewise linear equations using pairs of four formants at matched monophthongs. Experimental results show the potential of the proposed method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Yun, Young-Sun] Hannam Univ, Dept Informat &amp; Commun Engn, Taejon
   306791, South Korea.
   <br>[Ladner, Richard E.] Univ Washington, Dept Comp Sci &amp; Engn, Seattle, WA
   98195 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Yun, YS (reprint author), Hannam Univ, Dept Informat &amp; Commun Engn, Taejon 306791, South Korea.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ysyun@hannam.kr; ladner@cs.washington.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">VL </td><td>8082</td>
</tr>

<tr>
<td valign="top">BP </td><td>137</td>
</tr>

<tr>
<td valign="top">EP </td><td>144</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000337294900018</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Yu, HZ
   <br>Zhang, JX
   <br>Shan, GR
   <br>Ma, N</td>
</tr>

<tr>
<td valign="top">AF </td><td>Yu, Hongzhi
   <br>Zhang, Jinxi
   <br>Shan, Guangrong
   <br>Ma, Ning</td>
</tr>

<tr>
<td valign="top">BE </td><td>Yarlagadda, P
   <br>Yang, SF
   <br>Lee, KM</td>
</tr>

<tr>
<td valign="top">TI </td><td>Research on Tibetan Language Synthesis System Front-end Text Processing
   Technology Based on HMM</td>
</tr>

<tr>
<td valign="top">SO </td><td>INFORMATION TECHNOLOGY APPLICATIONS IN INDUSTRY II, PTS 1-4</td>
</tr>

<tr>
<td valign="top">SE </td><td>Applied Mechanics and Materials</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>2nd International Conference on Information Technology and Management
   Innovation (ICITMI 2013)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUL 23-24, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Zhuhai, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speech synthesis; standardized text; word segmentation; prosodic
   information is divided; pronunciation conversion</td>
</tr>

<tr>
<td valign="top">AB </td><td>The standardization of the text, word segmentation, the basic stitching unit divided for rhythm analysis and pronunciation conversion is an important content of the speech synthesis system front-end text processing modules. Lhasa Tibetan language and voice characteristics proposed the implementation of a set of Tibetan speech synthesis text analysis module to analyze and describe the Lhasa Tibetan language layer information and maps voice layer. The completion of the study is to lay a solid foundation for further Tibetan speech synthesis system.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Yu, Hongzhi; Zhang, Jinxi; Ma, Ning] Northwest Univ Nationalities, Key
   Lab Chinas Natl Linguist Informat Technol, Lanzhou 730030, Peoples R
   China.
   <br>[Shan, Guangrong] Northwest Univ Nationalities, Math &amp; Comp Sci Inst,
   Lanzhou 730030, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Yu, HZ (reprint author), Northwest Univ Nationalities, Key Lab Chinas Natl Linguist Informat Technol, Lanzhou 730030, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>happy0happy@163.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">VL </td><td>411-414</td>
</tr>

<tr>
<td valign="top">BP </td><td>308</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.4028/www.scientific.net/AMM.411-414.308</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Materials Science; Mechanics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000336641400062</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nirmal, J
   <br>Patnaik, S
   <br>Zaveri, M
   <br>Kachare, P</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nirmal, Jagannath
   <br>Patnaik, Suprava
   <br>Zaveri, Mukesh
   <br>Kachare, Pramod</td>
</tr>

<tr>
<td valign="top">BE </td><td>Mandal, JK
   <br>Mukhopadhyay, A</td>
</tr>

<tr>
<td valign="top">TI </td><td>Multi-scale Speaker Transformation using Radial Basis Function</td>
</tr>

<tr>
<td valign="top">SO </td><td>FIRST INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE: MODELING
   TECHNIQUES AND APPLICATIONS (CIMTA) 2013</td>
</tr>

<tr>
<td valign="top">SE </td><td>Procedia Technology</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>1st International Conference on Computational Intelligence - Modeling
   Techniques and Applications (CIMTA)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 27-28, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Univ Kalyani, Kalyani, INDIA</td>
</tr>

<tr>
<td valign="top">HO </td><td>Univ Kalyani</td>
</tr>

<tr>
<td valign="top">DE </td><td>Dynamic time warping; Discrete wavelet transforms; Radial basis
   function; Speaker transformation</td>
</tr>

<tr>
<td valign="top">ID </td><td>ARTIFICIAL NEURAL-NETWORKS; VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>In speaker transformation, the speaker dependent spectral parameters are generally characterized by single scale features. These features approximate the vocal tract, but produce artifacts during speech signal reconstruction. In this paper, multi-resolution wavelet based feature set is proposed, which finely tunes the speaker specific characteristics of the speech signal. The Radial Basis Function is used to propose the mapping function for modifying these characteristics. The performance of the proposed system is evaluated using different objective and subjective measures. Evaluation results illustrate that the proposed algorithm maintains target voice individuality while maintaining the quality and naturalness of the speech signal. (C) 2013 The Authors. Published by Elsevier Ltd.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nirmal, Jagannath; Kachare, Pramod] KJSCOE Mumbai, Dept Elect Engn,
   Bombay, Maharashtra, India.
   <br>[Patnaik, Suprava] SVNIT, Dept Elect &amp; Telecommun Engn, Surat, India.
   <br>[Zaveri, Mukesh] SVNIT, Dept Comp Engn, Surat, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nirmal, J (reprint author), KJSCOE Mumbai, Dept Elect Engn, Bombay, Maharashtra, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jhnirmal@somaiya.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>4</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>4</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">VL </td><td>10</td>
</tr>

<tr>
<td valign="top">BP </td><td>311</td>
</tr>

<tr>
<td valign="top">EP </td><td>319</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.protcy.2013.12.366</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000335602500037</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ramani, B
   <br>Jeeva, MPA
   <br>Vijayalakshmi, P
   <br>Nagarajan, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ramani, B.
   <br>Jeeva, Actlin M. P.
   <br>Vijayalakshmi, P.
   <br>Nagarajan, T.</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Conversion-Based Multilingual to Polyglot Speech Synthesizer for
   Indian Languages</td>
</tr>

<tr>
<td valign="top">SO </td><td>2013 IEEE INTERNATIONAL CONFERENCE OF IEEE REGION 10 (TENCON)</td>
</tr>

<tr>
<td valign="top">SE </td><td>TENCON IEEE Region 10 Conference Proceedings</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference of Region 10 (TENCON)</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 22-25, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Xian, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">AB </td><td>A multilingual text-to-speech (TTS) system synthesizes speech signal in multiple languages for a given text, that is intelligible to human listener. However, given a mixed language text to the system, the synthesized output is observed to have speaker switching at the language switching points, which is annoying to the listeners. To overcome this switching effect, a polyglot speech synthesizer is developed, which generates synthesized speech in multiple languages with single voice identity. This can be achieved by inherent voice conversion during synthesis or by using voice conversion to convert the multilingual speech corpus to polyglot speech corpus and then perform synthesis. In this work, the polyglot speech corpus is obtained using Gaussian mixture model (GMM)-based cross-lingual voice conversion technique and a polyglot speech synthesizer for Indian languages is developed using hidden Markov model (HMM)- based synthesis technique. Here, the speech data collected from the native speakers for the Indian languages namely, Telugu, Malayalam, and Hindi are converted to have the voice identity of the native Tamil speaker. Building a HMM-based synthesizer using the obtained polyglot corpus enables the system to synthesize speech for any given text in any language or mixed language text. The performance of the polyglot speech synthesizer is evaluated for the similarity of the synthesized speech to the source or target speaker by performing ABX listening test. The scores obtained shows that the percentage of similarity to the target Tamil speaker varies from 73% to 86%. Further the performance of the system is analyzed for speaker switching.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ramani, B.; Jeeva, Actlin M. P.; Vijayalakshmi, P.; Nagarajan, T.] SSN
   Coll Engn, Madras 603110, Tamil Nadu, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ramani, B (reprint author), SSN Coll Engn, Madras 603110, Tamil Nadu, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ramanib@ssn.edu.in; vijayalakshmip@ssn.edu.in; nagarajant@ssn.edu.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000334921600309</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Huang, DY
   <br>Dong, MH
   <br>Li, HZ</td>
</tr>

<tr>
<td valign="top">AF </td><td>Huang, Dong-Yan
   <br>Dong, Minghui
   <br>Li, Haizhou</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>A DYNAMIC GAUSSIAN PROCESS FOR VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">SO </td><td>ELECTRONIC PROCEEDINGS OF THE 2013 IEEE INTERNATIONAL CONFERENCE ON
   MULTIMEDIA AND EXPO WORKSHOPS (ICMEW)</td>
</tr>

<tr>
<td valign="top">SE </td><td>IEEE International Conference on Multimedia and Expo Workshops</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Multimedia and Expo Workshops (ICMEW)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUL 15-19, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>San Jose, CA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Gaussian processes; covariance functions; voice conversion; sparse
   partial least squares regression; mapping function</td>
</tr>

<tr>
<td valign="top">ID </td><td>REGRESSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we explore Dynamic Gaussian Processes (DGP) based learning techniques for voice conversion. In particular, we propose to use dynamic squared exponential GP with sparse partial least squares (SPLS) technique to model non-linearities as well as to capture the dynamics in the source data. The concatenation of previous and next frames can well model dynamics Sparse partial least squares regression is used to find a mapping function in order to overcome the problem of overfitting. The proposed dynamic GP-based learning technique features simple, efficient and high accuracy without massive tuning. The experimental results show that the proposed approach for voice conversion is able to produce good similarity between the original and the converted target voices and achieves a great improvement in the sound quality compared to the state-of-the-art Gaussian mixture-based model.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Huang, Dong-Yan; Dong, Minghui; Li, Haizhou] ASTAR, Human Language
   Technol Dept, Inst Infocomm Res, Singapore 138632, Singapore.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Huang, DY (reprint author), ASTAR, Human Language Technol Dept, Inst Infocomm Res, 21-01 Connexis South Tower, Singapore 138632, Singapore.</td>
</tr>

<tr>
<td valign="top">EM </td><td>huang@i2r.a-star.edu.sg; mhdong@i2r.a-star.edu.sg; hli@i2r.a-star.edu.sg</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Li, Haizhou</display_name>&nbsp;</font></td><td><font size="3">Q-6438-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000335245800056</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nurminen, J
   <br>Silen, H
   <br>Helander, E
   <br>Gabbouj, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nurminen, Jani
   <br>Silen, Hanna
   <br>Helander, Elina
   <br>Gabbouj, Moncef</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Evaluation of Detailed Modeling of the LP Residual in Statistical Speech
   Synthesis</td>
</tr>

<tr>
<td valign="top">SO </td><td>2013 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS (ISCAS)</td>
</tr>

<tr>
<td valign="top">SE </td><td>IEEE International Symposium on Circuits and Systems</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Symposium on Circuits and Systems (ISCAS)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 19-23, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Beijing, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>statistical speech synthesis; linear prediction; residual modeling</td>
</tr>

<tr>
<td valign="top">AB </td><td>Speech parameterization remains an open question in statistical speech synthesis. In our earlier work we have shown that a framework developed originally for highly efficient speech storage can also be successfully applied for voice conversion and concatenative unit selection based speech synthesis. Recently, we have also used the same coding scheme in hybrid-form speech synthesis. In this paper, we further discuss the framework and apply it in statistical speech synthesis, concentrating specifically on the spectral modeling of the linear prediction (LP) residual. Perceptual evaluation demonstrates that the modeling of the spectral details remaining in the residual improves the quality of synthetic speech.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nurminen, Jani; Silen, Hanna; Helander, Elina; Gabbouj, Moncef] Tampere
   Univ Technol, Dept Signal Proc, FIN-33101 Tampere, Finland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nurminen, J (reprint author), Tampere Univ Technol, Dept Signal Proc, FIN-33101 Tampere, Finland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jani.nurminen@tut.fi; hanna.silen@tut.fi; elina.helander@tut.fi;
   moncef.gabbouj@tut.fi</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Gabbouj, Moncef</display_name>&nbsp;</font></td><td><font size="3">G-4293-2014&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Gabbouj, Moncef</display_name>&nbsp;</font></td><td><font size="3">0000-0002-9788-2323&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Nurminen, Jukka</display_name>&nbsp;</font></td><td><font size="3">0000-0001-5083-1927&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Helander, Elina</display_name>&nbsp;</font></td><td><font size="3">0000-0002-0076-0590&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>313</td>
</tr>

<tr>
<td valign="top">EP </td><td>316</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000332006800078</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hwang, HT
   <br>Tsao, Y
   <br>Wang, HM
   <br>Wang, YR
   <br>Chen, SH</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hwang, Hsin-Te
   <br>Tsao, Yu
   <br>Wang, Hsin-Min
   <br>Wang, Yih-Ru
   <br>Chen, Sin-Horng</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Incorporating Global Variance in the Training Phase of GMM-based Voice
   Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>2013 ASIA-PACIFIC SIGNAL AND INFORMATION PROCESSING ASSOCIATION ANNUAL
   SUMMIT AND CONFERENCE (APSIPA)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Asia-Pacific Signal and Information Processing Association Annual Summit
   and Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Annual Summit and Conference of the
   Asia-Pacific-Signal-and-Information-Processing-Association (APSIPA)</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 29-NOV 01, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Kaohsiung, TAIWAN</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH SYNTHESIS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Maximum likelihood-based trajectory mapping considering global variance (MLGV-based trajectory mapping) has been proposed for improving the quality of the converted speech of Gaussian mixture model-based voice conversion (GMM-based VC). Although the quality of the converted speech is significantly improved, the computational cost of the online conversion process is also increased because there is no closed form solution for parameter generation in MLGV-based trajectory mapping, and an iterative process is generally required. To reduce the online computational cost, we propose to incorporate GV in the training phase of GMM-based VC. Then, the conversion process can simply adopt ML-based trajectory mapping (without considering GV in the conversion phase), which has a closed form solution. In this way, it is expected that the quality of the converted speech can be improved without increasing the online computational cost. Our experimental results demonstrate that the proposed method yields a significant improvement in the quality of the converted speech comparing to the conventional GMM-based VC method. Meanwhile, comparing to MLGV-based trajectory mapping, the proposed method provides comparable converted speech quality with reduced computational cost in the conversion process.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Hwang, Hsin-Te; Wang, Yih-Ru; Chen, Sin-Horng] Natl Chiao Tung Univ,
   Dept Elect &amp; Comp Engn, Hsinchu, Taiwan.
   <br>[Tsao, Yu] Acad Sinica, Res Ctr Infomrat Technol Innovat, Taipei, Taiwan.
   <br>[Hwang, Hsin-Te; Wang, Hsin-Min] Acad Sinica, Inst Informat Sci, Taipei,
   Taiwan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hwang, HT (reprint author), Natl Chiao Tung Univ, Dept Elect &amp; Comp Engn, Hsinchu, Taiwan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>hwanght@iis.sinica.edu.tw; yu.tsao@citi.sinica.edu.tw;
   whm@iis.sinica.edu.tw; yrwang@cc.nctu.edu.tw; schen@mail.nctu.edu.tw</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000331094400076</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Tian, XH
   <br>Wu, ZZ
   <br>Chng, ES</td>
</tr>

<tr>
<td valign="top">AF </td><td>Tian, Xiaohai
   <br>Wu, Zhizheng
   <br>Chng, Eng Siong</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Local partial least square regression for spectral mapping in voice
   conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>2013 ASIA-PACIFIC SIGNAL AND INFORMATION PROCESSING ASSOCIATION ANNUAL
   SUMMIT AND CONFERENCE (APSIPA)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Asia-Pacific Signal and Information Processing Association Annual Summit
   and Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Annual Summit and Conference of the
   Asia-Pacific-Signal-and-Information-Processing-Association (APSIPA)</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 29-NOV 01, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Kaohsiung, TAIWAN</td>
</tr>

<tr>
<td valign="top">AB </td><td>Joint density Gaussian mixture model (JD-GMM) based method has been widely used in voice conversion task due to its flexible implementation. However, the statistical averaging effect during estimating the model parameters will result in over-smoothing the target spectral trajectories. Motivated by the local linear transformation method, which uses neighboring data rather than all the training data to estimate the transformation function for each feature vector, we proposed a local partial least square method to avoid the over-smoothing problem of JD-GMM and the over-fitting problem of local linear transformation when training data are limited. We conducted experiments using the VOICES database and measure both spectral distortion and correlation coefficient of the spectral parameter trajectory. The experimental results show that our proposed method obtain better performance as compared to baseline methods.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Tian, Xiaohai; Chng, Eng Siong] Nanyang Technol Univ, UBC Res Ctr
   Excellence Act Living El, Singapore 639798, Singapore.
   <br>[Wu, Zhizheng; Chng, Eng Siong] Nanyang Technol Univ, Sch Comp ENgn,
   Singapore, Singapore.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Tian, XH (reprint author), Nanyang Technol Univ, UBC Res Ctr Excellence Act Living El, Singapore 639798, Singapore.</td>
</tr>

<tr>
<td valign="top">EM </td><td>xhtian@ntu.edu.sg; wuzz@ntu.edu.sg; aseschng@ntu.edu.sg</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000331094400229</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wu, ZZ
   <br>Li, HZ</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wu, Zhizheng
   <br>Li, Haizhou</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice conversion and spoofing attack on speaker verification systems</td>
</tr>

<tr>
<td valign="top">SO </td><td>2013 ASIA-PACIFIC SIGNAL AND INFORMATION PROCESSING ASSOCIATION ANNUAL
   SUMMIT AND CONFERENCE (APSIPA)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Asia-Pacific Signal and Information Processing Association Annual Summit
   and Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Annual Summit and Conference of the
   Asia-Pacific-Signal-and-Information-Processing-Association (APSIPA)</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 29-NOV 01, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Kaohsiung, TAIWAN</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH SYNTHESIS; TRANSFORMATION; EXTRACTION; REGRESSION; ALGORITHM;
   SECURITY; MODEL; HMMS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Speaker verification system automatically accepts or rejects the claimed identity of a speaker. Recently, we have made major progress in speaker verification which leads to mass market adoption, such as in smartphone and in online commerce for user authentication. A major concern when deploying speaker verification technology is whether a system is robust against spoofing attacks. Speaker verification studies provided us a better insight into speaker characterization, which has contributed to the progress of voice conversion technology. Unfortunately, voice conversion has become one of the most easily accessible techniques to carry out spoofing attack, therefore, presents a threat to speaker verification systems. In this paper, we will briefly introduce the fundamentals of voice conversion and speaker verification technology. We then give an overview of recent spoofing attack studies under different conditions with a focus on voice conversion spoofing attack. We will also discuss anti-spoofing attack measures for speaker verification.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Wu, Zhizheng; Li, Haizhou] Nanyang Technol Univ, Singapore 639798,
   Singapore.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Wu, ZZ (reprint author), Nanyang Technol Univ, Singapore 639798, Singapore.</td>
</tr>

<tr>
<td valign="top">EM </td><td>wuzz@ntu.edu.sg; hli@i2r.a-star.edu.sg</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Li, Haizhou</display_name>&nbsp;</font></td><td><font size="3">Q-6438-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000331094400241</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Alegre, F
   <br>Amehraye, A
   <br>Evans, N</td>
</tr>

<tr>
<td valign="top">AF </td><td>Alegre, Federico
   <br>Amehraye, Asmaa
   <br>Evans, Nicholas</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>SPOOFING COUNTERMEASURES TO PROTECT AUTOMATIC SPEAKER VERIFICATION FROM
   VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2013 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 26-31, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Vancouver, CANADA</td>
</tr>

<tr>
<td valign="top">DE </td><td>automatic speaker verification; biometrics; spoofing; imposture;
   countermeasures</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a new countermeasure for the protection of automatic speaker verification systems from spoofed, converted voice signals. The new countermeasure exploits the common shift applied to the spectral slope of consecutive speech frames involved in the mapping of a spoofer's voice signal towards a statistical model of a given target. While the countermeasure exploits prior knowledge of the attack in an admittedly unrealistic sense, it is shown to detect almost all spoofed signals which otherwise provoke significant increases in false acceptance. The work also discusses the need for formal evaluations to develop new countermeasures which are less reliant on prior knowledge.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Alegre, Federico; Amehraye, Asmaa; Evans, Nicholas] EURECOM, Multimedia
   Commun Dept, Sophia Antipolis, France.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Alegre, F (reprint author), EURECOM, Multimedia Commun Dept, Sophia Antipolis, France.</td>
</tr>

<tr>
<td valign="top">EM </td><td>alegre@eurecom.fr; fillatre@eurecom.fr; evans@eurecom.fr</td>
</tr>

<tr>
<td valign="top">TC </td><td>18</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>18</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>3068</td>
</tr>

<tr>
<td valign="top">EP </td><td>3072</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000329611503047</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Obin, N
   <br>Lamare, F
   <br>Roebel, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Obin, Nicolas
   <br>Lamare, Francois
   <br>Roebel, Axel</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>SYLL-O-MATIC: AN ADAPTIVE TIME-FREQUENCY REPRESENTATION FOR THE
   AUTOMATIC SEGMENTATION OF SPEECH INTO SYLLABLES</td>
</tr>

<tr>
<td valign="top">SO </td><td>2013 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 26-31, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Vancouver, CANADA</td>
</tr>

<tr>
<td valign="top">DE </td><td>speech segmentation; syllable segmentation; time-frequency
   representation; information fusion</td>
</tr>

<tr>
<td valign="top">ID </td><td>RECOGNITION; CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper introduces novel paradigms for the segmentation of speech into syllables. The main idea of the proposed method is based on the use of a time-frequency representation of the speech signal, and the fusion of intensity and voicing measures through various frequency regions for the automatic selection of pertinent information for the segmentation. The time-frequency representation is used to exploit the speech characteristics depending on the frequency region. In this representation, intensity profiles are measured to provide information into various frequency regions, and voicing profiles are measured to determine the frequency regions that are pertinent for the segmentation. The proposed method outperforms conventional methods for the detection of syllable landmark and boundaries on the TIMIT database of American-English, and provides a promising paradigm for the segmentation of speech into syllables.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Obin, Nicolas; Lamare, Francois; Roebel, Axel] IRCAM, CNRS UMR 9912,
   STMS, Paris, France.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Obin, N (reprint author), IRCAM, CNRS UMR 9912, STMS, Paris, France.</td>
</tr>

<tr>
<td valign="top">EM </td><td>nobin@ircam.fr</td>
</tr>

<tr>
<td valign="top">TC </td><td>12</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>12</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>6699</td>
</tr>

<tr>
<td valign="top">EP </td><td>6703</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000329611506173</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<tr><td>EF</td><td></td></tr></table>