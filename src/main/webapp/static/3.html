<table border="0" cellpadding="2" cellspacing="0"><tr><td>FN</td><td>Clarivate Analytics Web of Science</td></tr><tr><td>VR</td><td>1.0</td></tr><style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sano, M
   <br>Zhu, CW
   <br>Kaye, J
   <br>Mundt, JC
   <br>Hayes, TL
   <br>Ferris, S
   <br>Thomas, RG
   <br>Sun, CK
   <br>Jiang, YX
   <br>Donohue, MC
   <br>Schneider, LS
   <br>Egelko, S
   <br>Aisen, PS
   <br>Feldman, HH</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sano, Mary
   <br>Zhu, Carolyn W.
   <br>Kaye, Jeffrey
   <br>Mundt, James C.
   <br>Hayes, Tamara L.
   <br>Ferris, Steven
   <br>Thomas, Ronald G.
   <br>Sun, Chung-Kai
   <br>Jiang, Yanxin
   <br>Donohue, Michael C.
   <br>Schneider, Lon S.
   <br>Egelko, Susan
   <br>Aisen, Paul S.
   <br>Feldman, Howard H.</td>
</tr>

<tr>
<td valign="top">CA </td><td>Alzheimer Dis Cooperative Study In</td>
</tr>

<tr>
<td valign="top">TI </td><td>A randomized clinical trial to evaluate home-based assessment of people
   over 75 years old</td>
</tr>

<tr>
<td valign="top">SO </td><td>ALZHEIMERS &amp; DEMENTIA</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Dementia prevention; Home-based assessment; Alzheimer's disease;
   Randomized clinical trial</td>
</tr>

<tr>
<td valign="top">ID </td><td>PREVENTION INSTRUMENT PROJECT; COGNITIVE FUNCTION; DEMENTIA;
   INDIVIDUALS; DECLINE</td>
</tr>

<tr>
<td valign="top">AB </td><td>Introduction: There is an unmet need for effective methods for conducting dementia prevention trials.
   <br>Methods: Home-based assessment study compared feasibility and efficiency, ability to capture change over time using in-home instruments, and ability to predict cognitive conversion using predefined triggers in a randomized clinical trial in (1) mail-in questionnaire/live telephone interviews, (2) automated telephone/interactive voice recognition, and (3) internet-based computer Kiosk technologies. Primary endpoint was defined as cognitive conversion.
   <br>Results: Analysis followed a modified intent-to-treat principle. Dropout rates were low and similar across technologies but participants in Kiosk were more likely to dropout earlier. Staff resources needed were higher in Kiosk. In-home instruments distinguished conversion and stable groups. Cognitively stable group showed improvement in cognitive measures. Triggering was associated with higher likelihood of conversion but statistically significant only in mail-in questionnaire/live telephone interviews.
   <br>Discussion: Relatively low efficiency of internet-based assessment compared with testing by liveassessors has implications for internet-based recruitment and assessment efforts currently proposed for diverse populations. Published by Elsevier Inc. on behalf of the Alzheimer's Association.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Sano, Mary; Zhu, Carolyn W.; Egelko, Susan] Icahn Sch Med Mt Sinai,
   Alzheimer Dis Res Ctr, Dept Psychiat, New York, NY 10029 USA.
   <br>[Sano, Mary; Zhu, Carolyn W.] James J Peters VAMC, Bronx, NY 10468 USA.
   <br>[Zhu, Carolyn W.] Icahn Sch Med Mt Sinai, Brookdale Dept Geriatr &amp;
   Palliat Med, New York, NY 10029 USA.
   <br>[Kaye, Jeffrey] Oregon Hlth &amp; Sci Univ, Dept Neurol &amp; Biomed Engn,
   Portland, OR 97201 USA.
   <br>[Mundt, James C.] Wisconsin Dept Hlth Serv, Div Care &amp; Treatment Serv,
   Mauston, WI USA.
   <br>[Hayes, Tamara L.] Oregon Hlth &amp; Sci Univ, Oregon Ctr Aging &amp; Technol,
   Dept Biomed Engn, Portland, OR 97201 USA.
   <br>[Ferris, Steven] NYU, Ctr Cognit Neurol, Langone Med Ctr, New York, NY
   USA.
   <br>[Thomas, Ronald G.] Dept Neurosci, San Diego, CA USA.
   <br>[Thomas, Ronald G.] Univ Calif San Diego, Dept Family &amp; Prevent Med, San
   Diego, CA 92103 USA.
   <br>[Sun, Chung-Kai; Jiang, Yanxin; Donohue, Michael C.; Aisen, Paul S.]
   Univ Southern Calif, Keck Sch Med, Alzheimers Therapeut Res Inst, San
   Diego, CA USA.
   <br>[Schneider, Lon S.] Univ Southern Calif, Keck Sch Med, Dept Psychiat &amp;
   Behav Sci, Los Angeles, CA USA.
   <br>[Feldman, Howard H.] Univ Calif San Diego, Dept Neurosci, San Diego, CA
   92103 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sano, M (reprint author), Icahn Sch Med Mt Sinai, Alzheimer Dis Res Ctr, Dept Psychiat, New York, NY 10029 USA.; Sano, M (reprint author), James J Peters VAMC, Bronx, NY 10468 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>mary.sano@mssm.edu</td>
</tr>

<tr xmlns:date="http://exslt.org/dates-and-times">
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Ferris, Steven</display_name>&nbsp;</font></td><td><font size="3">0000-0001-8641-6223&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAY</td>
</tr>

<tr>
<td valign="top">PY </td><td>2019</td>
</tr>

<tr>
<td valign="top">VL </td><td>15</td>
</tr>

<tr>
<td valign="top">IS </td><td>5</td>
</tr>

<tr>
<td valign="top">BP </td><td>615</td>
</tr>

<tr>
<td valign="top">EP </td><td>624</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.jalz.2019.01.007</td>
</tr>

<tr>
<td valign="top">SC </td><td>Neurosciences &amp; Neurology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000467220800002</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Alluri, KNRKR
   <br>Vuppala, AK</td>
</tr>

<tr>
<td valign="top">AF </td><td>Alluri, K. N. R. K. Raju
   <br>Vuppala, Anil Kumar</td>
</tr>

<tr>
<td valign="top">TI </td><td>Replay spoofing countermeasures using high spectro-temporal resolution
   features</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERNATIONAL JOURNAL OF SPEECH TECHNOLOGY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Automatic speaker recognition; Spoofing counter measures; Replay
   attacks; Single frequency filtering; Zero time windowing; Gaussian
   mixture models; Deep neural networks</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEAKER VERIFICATION; INSTANTANEOUS FREQUENCY; SPEECH</td>
</tr>

<tr>
<td valign="top">AB </td><td>The easy implementation of replay attacks by a fraudster poses a severe threat to automatic speaker verification (ASV) technology than the other spoofing attacks like speech synthesis and voice conversion. Replay attacks refer to an attack by a fraudster to get illegitimate access to an ASV system by playing back the speech sample collected from genuine target speaker. The significant cues that can differentiate between genuine and replay recordings are channel characteristics. To capture these characteristics, one need to extract features from the spectrum, which should have high spectral and temporal resolutions. Zero time windowing (ZTW) analysis of speech is one such time-frequency analysis technique, which results in high spectral and temporal resolution spectrum at each sampling instant. In this study, new features are proposed by applying cepstral analysis to ZTW spectrum. Experiments are performed on two publicly available replay attack databases namely BTAS 2016 and ASVspoof 2017. The first set of experiments are conducted using Gaussian mixture models to evaluate the potential of proposed features. Performance of the proposed system in terms of half total error rate is 0.75% and in terms of equal error rate is 14.75% on BTAS 2016 and ASVspoof 2017 evaluation sets respectively. A score level fusion is performed by using proposed features with previously proposed single frequency filtering cepstral coefficients. This fused result outperformed the previously reported best results on these two datasets.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Alluri, K. N. R. K. Raju; Vuppala, Anil Kumar] Int Inst Informat
   Technol, KCIS, Speech Proc Lab, Hyderabad, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Alluri, KNRKR (reprint author), Int Inst Informat Technol, KCIS, Speech Proc Lab, Hyderabad, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>raju.alluri@research.iiit.ac.in; anil.vuppala@iiit.ac.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2019</td>
</tr>

<tr>
<td valign="top">VL </td><td>22</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>271</td>
</tr>

<tr>
<td valign="top">EP </td><td>281</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/s10772-019-09602-z</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000460136300024</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Roy, N
   <br>Dietrich, M
   <br>Blomgren, M
   <br>Heller, A
   <br>Houtz, DR
   <br>Lee, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Roy, Nelson
   <br>Dietrich, Maria
   <br>Blomgren, Michael
   <br>Heller, Amanda
   <br>Houtz, Daniel R.
   <br>Lee, James</td>
</tr>

<tr>
<td valign="top">TI </td><td>Exploring the Neural Bases of Primary Muscle Tension Dysphonia: A Case
   Study Using Functional Magnetic Resonance Imaging</td>
</tr>

<tr>
<td valign="top">SO </td><td>JOURNAL OF VOICE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Muscle tension dysphonia; Conversion disorder; Functional voice
   disorder; Trait Theory; Behavioral inhibition system; fMRI</td>
</tr>

<tr>
<td valign="top">ID </td><td>LARYNGEAL MOTOR CORTEX; VOICE DISORDERS; STRESS REACTIVITY; LIFE EVENTS;
   PERSONALITY; VOCALIZATION; TRAIT; SUPPLEMENTARY; ACTIVATION; PHONATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Primary muscle tension dysphonia (pMTD) is a voice disorder that occurs in the absence of laryngeal pathology. Dysregulated activity of the paralaryngcal muscles is considered the proximal cause; however, the central origin of this aberrant laryngeal muscle activation is unclear. The Trait Theory (Roy and Bless, 2000a,b) proposed that specific personality traits can predispose one to laryngeal motor inhibition and pMTD, and this inhibition is mediated by a hyperactive "behavioral inhibition system (BIS)" composed of limbic system structures (and associated prefrontal connections). This case study used functional magnetic resonance imaging to detect brain activation changes associated with successful management of pMTD, thereby evaluating possible neural correlates of this poorly understood disorder.
   <br>Method. A 61-year-old woman with moderate-to-severe pMTD underwent functional magnetic resonance imaging scans before and immediately after successful treatment using manual circumlaryngeal techniques. Experimental stimuli were blocks of repeated vowel production and overt sentence reading.
   <br>Results. Significantly greater activation was observed pre- versus posttreatment in all regions of interest during sentence production, that is, periaqueductal gray, amygdala, hypothalamus, anterior cingulate cortex, hippocampus, dorsolateral prefrontal cortex, Brodmann area 10, and premotor and inferior sensorimotor cortex.
   <br>Conclusions. Our findings are compatible with overactivation of neural regions associated with the BIS (cingulate cortex, amygdala, hypothalamus, periaqueductal gray) and motor inhibition networks (eg, [pre-]supplementary motor area) along with the dorsolateral prefrontal cortex and medial prefrontal cortex. heightened input from limbic regions combined with dysfunctional prefrontal regulation may interfere with laryngeal motor preparation, initiation, and execution thereby contributing to disordered voice in pMTD.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Roy, Nelson; Blomgren, Michael; Heller, Amanda] Univ Utah, Dept Commun
   Sci &amp; Disorders, 390 South 1530 East, Salt Lake City, UT 84112 USA.
   <br>[Dietrich, Maria] Univ Missouri, Dept Commun Sci &amp; Disorders, Columbia,
   MO USA.
   <br>[Houtz, Daniel R.] Univ Utah Healthcare, Voice Disorders Ctr, Salt Lake
   City, UT USA.
   <br>[Lee, James] Univ Utah, Imaging &amp; Neurosci Ctr, Salt Lake City, UT USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Roy, N (reprint author), Univ Utah, Dept Commun Sci &amp; Disorders, 390 South 1530 East, Salt Lake City, UT 84112 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>nelson.roy@health.utah.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2019</td>
</tr>

<tr>
<td valign="top">VL </td><td>33</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>183</td>
</tr>

<tr>
<td valign="top">EP </td><td>194</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.jvoice.2017.11.009</td>
</tr>

<tr>
<td valign="top">SC </td><td>Audiology &amp; Speech-Language Pathology; Otorhinolaryngology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000459857200011</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Yan, DQ
   <br>Xiang, L
   <br>Wang, ZF
   <br>Wang, RD</td>
</tr>

<tr>
<td valign="top">AF </td><td>Yan, Diqun
   <br>Xiang, Li
   <br>Wang, Zhifeng
   <br>Wang, Rangding</td>
</tr>

<tr>
<td valign="top">TI </td><td>Detection of HMM Synthesized Speech by Wavelet Logarithmic Spectrum</td>
</tr>

<tr>
<td valign="top">SO </td><td>AUTOMATIC CONTROL AND COMPUTER SCIENCES</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>speech synthesis; spoofing attack; wavelet transform; classification</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEAKER VERIFICATION; COUNTERMEASURES</td>
</tr>

<tr>
<td valign="top">AB </td><td>Automatic speaker verification systems have achieved great performance and been widely adopted in many security applications. One of the important requirements for the verification system is its resilience to spoofing attacks, such as impersonation, replay, speech synthesis and voice conversion. Among these attacks, speech synthesis has a high risk to the verification systems. In this paper, a novel detection method for computer-generated speech, especially for HMM synthetic speech, is proposed. It is found that the wavelet coefficients in specified position show the obvious difference between the synthetic and natural speech. The logarithmic spectrum features are extracted from the wavelet coefficients and support vector machine is used as the classifier to evaluate the performance of our proposed algorithm. The experimental results over SAS corpus show that the proposed algorithm can achieve high detection accuracy and low equal error rate.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Yan, Diqun; Xiang, Li; Wang, Zhifeng; Wang, Rangding] Ningbo Univ, Coll
   Informat Sci &amp; Engn, Ningbo 315211, Zhejiang, Peoples R China.
   <br>[Yan, Diqun] Guangdong Key Lab Intelligent Informat Proc, Shenzhen
   518060, Peoples R China.
   <br>[Yan, Diqun] Shenzhen Key Lab Media Secur, Shenzhen 518060, Peoples R
   China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Yan, DQ (reprint author), Ningbo Univ, Coll Informat Sci &amp; Engn, Ningbo 315211, Zhejiang, Peoples R China.; Yan, DQ (reprint author), Guangdong Key Lab Intelligent Informat Proc, Shenzhen 518060, Peoples R China.; Yan, DQ (reprint author), Shenzhen Key Lab Media Secur, Shenzhen 518060, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>yandiqun@nbu.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>JAN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2019</td>
</tr>

<tr>
<td valign="top">VL </td><td>53</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>72</td>
</tr>

<tr>
<td valign="top">EP </td><td>79</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.3103/S014641161901005X</td>
</tr>

<tr>
<td valign="top">SC </td><td>Automation &amp; Control Systems</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000464869200009</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Biagetti, G
   <br>Crippa, P
   <br>Falaschetti, L
   <br>Turchetti, C</td>
</tr>

<tr>
<td valign="top">AF </td><td>Biagetti, Giorgio
   <br>Crippa, Paolo
   <br>Falaschetti, Laura
   <br>Turchetti, Claudio</td>
</tr>

<tr>
<td valign="top">TI </td><td>HMM speech synthesis based on MDCT representation</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERNATIONAL JOURNAL OF SPEECH TECHNOLOGY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speech synthesis; HMM; MDCT; Overlap-and-add; Mel-cepstral analysis</td>
</tr>

<tr>
<td valign="top">ID </td><td>PARAMETER GENERATION ALGORITHM; VOICE CONVERSION; SYNTHESIS SYSTEM;
   GLOBAL VARIANCE; NOISE</td>
</tr>

<tr>
<td valign="top">AB </td><td>Hidden Markov model (HMM) based text-to-speech (TTS) has become one of the most promising approaches, as it has proven to be a particularly flexible and robust framework to generate synthetic speech. However, several factors such as mel-cepstral vocoder and over-smoothing are responsible for causing quality degradation of synthetic speech. This paper presents an HMM speech synthesis technique based on the modified discrete cosine transform (MDCT) representation to cope with these two issues. To this end, we use an analysis/synthesis technique based on MDCT that guarantees a perfect reconstruction of the signal frame from feature vectors and allows for a 50% overlap between frames without increasing the data vector, in contrast to the conventional mel-cepstral spectral parameters that do not ensure direct speech waveform reconstruction. Experimental results show that a sound of good quality, conveniently evaluated using both objective and subjective tests, is obtained.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Biagetti, Giorgio; Crippa, Paolo; Falaschetti, Laura; Turchetti,
   Claudio] Univ Politecn Marche, DII, Via Brecce Bianche 12, I-60131
   Ancona, Italy.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Falaschetti, L (reprint author), Univ Politecn Marche, DII, Via Brecce Bianche 12, I-60131 Ancona, Italy.</td>
</tr>

<tr>
<td valign="top">EM </td><td>g.biagetti@univpm.it; p.crippa@univpm.it; l.falaschetti@univpm.it;
   c.turchetti@univpm.it</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Crippa, Paolo</display_name>&nbsp;</font></td><td><font size="3">N-3730-2016&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Falaschetti, Laura</display_name>&nbsp;</font></td><td><font size="3">0000-0003-3183-7682&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Crippa, Paolo</display_name>&nbsp;</font></td><td><font size="3">0000-0003-4504-7550&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>DEC</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">VL </td><td>21</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">BP </td><td>1045</td>
</tr>

<tr>
<td valign="top">EP </td><td>1055</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/s10772-018-09571-9</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000452913900026</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Cheyette, B</td>
</tr>

<tr>
<td valign="top">AF </td><td>Cheyette, Bryan</td>
</tr>

<tr>
<td valign="top">TI </td><td>Spark, trauma and the novel</td>
</tr>

<tr>
<td valign="top">SO </td><td>TEXTUAL PRACTICE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Muriel Spark; trauma; Africa; conversion; Aiding and Abetting; Robin
   Spark</td>
</tr>

<tr>
<td valign="top">AB </td><td>What is clear from even a cursory reading of Muriel Spark's dazzling and cunning fictions is that she engages with a bewildering range of literary modes but only in so far as they can be subsumed by her singular vision. Spark's quirky and playful voice refuses to be contained by any one doctrine or identity. First among the philosophies and identities which she finds absurd is that of the conventional realist novel with its humanist assumptions that the plot of a novel, with the individual at its heart, can be confused with life. This essay will juxtapose Spark's scepticism in relation to the conventional novel form with the fierce self-protection of her life-story (before she was a novelist) which she, paradoxically, refigures in many of her imaginative works. The focus is on her fictions set in Africa where she felt at her most vulnerable as the potential object of various 'shooting affairs'. It will show the ways in which she redeems such trauma in her late fiction. In the dismissal of the human-centred realist novel, and the fantasy that individuals can control the world, Spark, is equally anarchic and orthodox; playful and controlling.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Cheyette, Bryan] Univ Reading, Dept English Literature, Reading, Berks,
   England.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Cheyette, B (reprint author), Univ Reading, Dept English Literature, Reading, Berks, England.</td>
</tr>

<tr>
<td valign="top">EM </td><td>b.h.cheyette@reading.ac.uk</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>OCT 21</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">VL </td><td>32</td>
</tr>

<tr>
<td valign="top">IS </td><td>9</td>
</tr>

<tr>
<td valign="top">SI </td><td>SI</td>
</tr>

<tr>
<td valign="top">BP </td><td>1659</td>
</tr>

<tr>
<td valign="top">EP </td><td>1676</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1080/0950236X.2018.1533172</td>
</tr>

<tr>
<td valign="top">SC </td><td>Literature</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000450851000012</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Gallemore, C
   <br>Guisinger, A
   <br>Kruuse, M
   <br>Ruysschaert, D
   <br>Jespersen, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Gallemore, Caleb
   <br>Guisinger, Amy
   <br>Kruuse, Mikkel
   <br>Ruysschaert, Denis
   <br>Jespersen, Kristjan</td>
</tr>

<tr>
<td valign="top">TI </td><td>Escaping the "Teenage" Years: The Politics of Rigor and the Evolution of
   Private Environmental Standards</td>
</tr>

<tr>
<td valign="top">SO </td><td>ECOLOGICAL ECONOMICS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">ID </td><td>SUSTAINABLE PALM-OIL; GLOBAL VOLUNTARY STANDARDS; LATENT CLASS ANALYSIS;
   SUPPLY CHAIN; INSTITUTIONAL CHANGE; ROUND-TABLE; DEVELOPING-COUNTRIES;
   CORPORATE CONDUCT; CARBON EMISSIONS; LAND CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>While there is considerable literature on firms' motivations to form and join private environmental standards, less has been written on motivations to strengthen standards, once created. Using the Roundtable on Sustainable Palm Oil (RSPO) as a case, our study examines the politics of rigor, understood as value-chain traceability and on-the-ground requirements, within private environmental standards. We use polytomous variable latent class analysis to cluster RSPO members' reported challenges in sustainable palm oil and model the membership in clusters expressing concerns that the standard is either insufficiently rigorous or too difficult using random effects panel logistic regression. We find that more brand-exposed members are more likely to request greater rigor. Members in the middle of the value chain are more likely to voice concerns about the standard being excessively difficult, particularly noting costs and the challenge of securing supplies. We argue that avoiding reputational risk is a primary motivator for standards adherence, and, as a result, demands for increased rigor come primarily from firms with higher brand exposure. We conclude that the distribution of standard members across the supply chain can be a significant determinant of the way private environmental standards evolve and the level of rigor they achieve.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Gallemore, Caleb] Lafayette Coll, Int Affairs Program, Oeschle Ctr
   Global Educ 217, 730 High St, Easton, PA 18042 USA.
   <br>[Guisinger, Amy] Lafayette Coll, Dept Econ, Easton, PA 18042 USA.
   <br>[Kruuse, Mikkel; Jespersen, Kristjan] Copenhagen Business Sch, Dept
   Management Soc &amp; Commun, Copenhagen, Denmark.
   <br>[Ruysschaert, Denis] Univ Liege, Fac Gembloux, Dept Biose, Liege,
   Belgium.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Gallemore, C (reprint author), Lafayette Coll, Int Affairs Program, Oeschle Ctr Global Educ 217, 730 High St, Easton, PA 18042 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>gallemoc@lafayette.edu</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Gallemore, Caleb</display_name>&nbsp;</font></td><td><font size="3">0000-0003-1703-0241&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>OCT</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">VL </td><td>152</td>
</tr>

<tr>
<td valign="top">BP </td><td>76</td>
</tr>

<tr>
<td valign="top">EP </td><td>87</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.ecolecon.2018.05.023</td>
</tr>

<tr>
<td valign="top">SC </td><td>Environmental Sciences &amp; Ecology; Business &amp; Economics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000440118700008</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Lankina, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Lankina, Anna</td>
</tr>

<tr>
<td valign="top">TI </td><td>Leadership for the Christian Empire: Emperors and Bishops in the
   Ecclesiastical History of Philostorgius</td>
</tr>

<tr>
<td valign="top">SO </td><td>CHURCH HISTORY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">ID </td><td>CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>The fifth-century Ecclesiastical History of Philostorgius is an unusual example of a surviving minority source. Although scholars have mined his work for raw data on events between 320 and 425 c.e., in contrast to other contemporary ecclesiastical historians, Philostorgius has received little attention. His work has suffered derision, being seen as nothing more than Arian polemic and thus as more partisan than its pro-Nicene counterparts. This essay analyzes Philostorgius's role as one of many competitive voices participating in the composition of historical works for the elite readership of Constantinople in the fifth century. Philostorgius's Ecclesiastical History constituted an integral part of the historiography of late antiquity and early Christianity. His representation of the relationship between bishops and emperors reveals a distinctive theory of history which informs his entire work.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Lankina, Anna] Univ Florida, Hist &amp; Writing, Gainesville, FL 32611 USA.
   <br>[Lankina, Anna] Santa Fe Coll, Gainesville, FL 32606 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Lankina, A (reprint author), Univ Florida, Hist &amp; Writing, Gainesville, FL 32611 USA.; Lankina, A (reprint author), Santa Fe Coll, Gainesville, FL 32606 USA.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>SEP</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">VL </td><td>87</td>
</tr>

<tr>
<td valign="top">IS </td><td>3</td>
</tr>

<tr>
<td valign="top">BP </td><td>684</td>
</tr>

<tr>
<td valign="top">EP </td><td>717</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1017/S0009640718001579</td>
</tr>

<tr>
<td valign="top">SC </td><td>History; Religion</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000449629500002</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Yadav, J
   <br>Rao, KS</td>
</tr>

<tr>
<td valign="top">AF </td><td>Yadav, Jainath
   <br>Rao, K. Sreenivasa</td>
</tr>

<tr>
<td valign="top">TI </td><td>Neural network and GMM based feature mappings for consonant-vowel
   recognition in emotional environment</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERNATIONAL JOURNAL OF SPEECH TECHNOLOGY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Consonant-vowel recognition; Hidden Markov model; Neural network;
   Gaussian mixture model; Feature mapping; Vowel onset and Offset point</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION; VECTOR NORMALIZATION; SPEAKER VERIFICATION; SPEECH;
   TRANSFORMATION; TUTORIAL; MODELS</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this work, we propose a mapping function based feature transformation framework for developing consonant-vowel (CV) recognition system in the emotional environment. An effective way of conveying messages is by expressing emotions during human conversations. The characteristics of CV units differ from one emotion to other emotions. The performance of existing CV recognition systems is degraded in emotional environments. Therefore, we have proposed mapping functions based on artificial neural network and GMM models for increasing the accuracy of CV recognition in the emotional environment. The CV recognition system has been explored to transform emotional features to neutral features using proposed mapping functions at CV and phone levels to minimize mismatch between training and testing environments. Vowel onset and offset points have been used to identify vowel, consonant and transition segments. Transition segments are identified by considering initial 15% speech samples between vowel onset and offset points. The average performance of CV recognition system is increased significantly using feature mapping technique at phone level in three emotional environments (anger, happiness, and sadness).</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Yadav, Jainath; Rao, K. Sreenivasa] Indian Inst Technol, Comp Sci &amp;
   Engn, Kharagpur, W Bengal, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Yadav, J (reprint author), Indian Inst Technol, Comp Sci &amp; Engn, Kharagpur, W Bengal, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jaibhu38@gmail.com; ksrao@iitkgp.ac.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>SEP</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">VL </td><td>21</td>
</tr>

<tr>
<td valign="top">IS </td><td>3</td>
</tr>

<tr>
<td valign="top">BP </td><td>421</td>
</tr>

<tr>
<td valign="top">EP </td><td>433</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/s10772-017-9478-1</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000441953600004</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Titze, IR</td>
</tr>

<tr>
<td valign="top">AF </td><td>Titze, Ingo R.</td>
</tr>

<tr>
<td valign="top">TI </td><td>Where has all the power gone? Energy production and loss in vocalization</td>
</tr>

<tr>
<td valign="top">SO </td><td>SPEECH COMMUNICATION</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Vocal power; Vocal efficiency; Oral pressure; Vocal effort</td>
</tr>

<tr>
<td valign="top">ID </td><td>FINITE-ELEMENT MODEL; SUB-GLOTTAL PRESSURE; SUBGLOTTAL PRESSURE; VOWEL;
   FLOW; SOFT</td>
</tr>

<tr>
<td valign="top">AB </td><td>Human voice production for speech is an inefficient process in terms of energy expended to produce acoustic output. A traditional measure of vocal efficiency relates acoustic power radiated from the mouth to aerodynamic power produced in the trachea. This efficiency ranges between 0.001% and 1.0% in speech-like vocalization. Simplified Navier Stokes equations for non-steady compressible airflow from trachea to lips were used to calculate steady aerodynamic power, acoustic power, and combined total power at seven strategic locations along the airway. A portion of the airway was allowed to collapse to produce self-sustained oscillation for sound production. A conversion efficiency, defined as acoustic power generated in the glottis to aerodynamic power dissipated, was found to be on the order of 10%, but wall vibration, air viscosity, and kinetic pressure losses consumed almost all of that converted power. Thus, the acoustic power, reflected back and forth in the airway was dissipated at a level on the order of 99.9%, with a small fraction being radiated to the listener.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Titze, Ingo R.] Univ Utah, Natl Ctr Voice &amp; Speech, Salt Lake City, UT
   84101 USA.
   <br>[Titze, Ingo R.] Univ Iowa, Dept Commun Sci &amp; Disorders, Iowa City, IA
   USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Titze, IR (reprint author), Univ Utah, Natl Ctr Voice &amp; Speech, Salt Lake City, UT 84101 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ingo.titze@utah.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUL</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">VL </td><td>101</td>
</tr>

<tr>
<td valign="top">BP </td><td>26</td>
</tr>

<tr>
<td valign="top">EP </td><td>33</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.specom.2018.05.003</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000455419200003</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Solorzano, DYV
   <br>Santos, ZLZ
   <br>Espin, MVG</td>
</tr>

<tr>
<td valign="top">AF </td><td>Valdivieso Solorzano, Daniel Yoffre
   <br>Zambrano Santos, Zita Lucia
   <br>Garcia Espin, Maria Victoria</td>
</tr>

<tr>
<td valign="top">TI </td><td>THE DOCUMENTARY PRODUCTION AND ITS CONTRIBUTION IN THE AUDIOVISUAL
   REALIZATION IN PORTOVIEJO CANTON</td>
</tr>

<tr>
<td valign="top">SO </td><td>REVISTA SAN GREGORIO</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Documentaries; audiovisual production; history Papi Garcia; military
   discipline</td>
</tr>

<tr>
<td valign="top">AB </td><td>The documentary audiovisual genre, as a production tool, promotion of the conversion of stories, facts or contextual realities in a version of images and sounds that become visual application elements of the message, capturing the citizen's interest and testing social stories in the collection. summarized times. In this context, the present study aims to produce an audiovisual documentary about Papi Garcia, a story that, at the end of the 50s, moved the
   <br>society of the time, due to its extremely tragic connotation. A story that is recounted and refreshed in the voices of those who are nearby, are witnesses of historians who investigate the circumstantial socio-political and military nuances that enveloped the event at that time. Scientists have also consulted through the testimonial information regarding the facts, as well as the investigative report of historians that generate vitality to the twists that the story unleashes. From this perspective, the result achieved was the production of an audiovisual documentary that reflects the history of an emblematic figure from the city of Portoviejo-Manabi-Ecuador.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Valdivieso Solorzano, Daniel Yoffre; Zambrano Santos, Zita Lucia] Univ
   San Gregorio Portoviejo, Portoviejo, Ecuador.
   <br>[Garcia Espin, Maria Victoria] Al Cubo Comunicac Integral, Quito,
   Ecuador.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Solorzano, DYV (reprint author), Univ San Gregorio Portoviejo, Portoviejo, Ecuador.</td>
</tr>

<tr>
<td valign="top">EM </td><td>dyvaldivieso@sangregorio.edu.ec; zlzambrano@sangregorio.edu.ec;
   mavicky91@gmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUL-SEP</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">IS </td><td>24</td>
</tr>

<tr>
<td valign="top">BP </td><td>98</td>
</tr>

<tr>
<td valign="top">EP </td><td>109</td>
</tr>

<tr>
<td valign="top">SC </td><td>Social Sciences - Other Topics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000448938300012</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Scarano, L</td>
</tr>

<tr>
<td valign="top">AF </td><td>Scarano, Laura</td>
</tr>

<tr>
<td valign="top">TI </td><td>"Places with a bleeding wound": Fernando Valverde's internal geographies</td>
</tr>

<tr>
<td valign="top">SO </td><td>KAMCHATKA-REVISTA DE ANALISIS CULTURAL</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Fernando Valverde; spatial turn; internal geography; poetry of
   uncertainty</td>
</tr>

<tr>
<td valign="top">AB </td><td>In 2006 Fernando Valverde (Granada, 1980) published the volume Poesia (1997-2017), where he collected his complete poetry until that date: Viento favorable (1997-2002), Razones para huir de una ciudad con frio (2004), Los ojos del pelicano (2010) and La insistencia del dano (2014). We analyse here the establishment of a peculiar poetic cartography, which plays a crucial role: it connects territories and cultures through the same existential matrix, shaping an internal geography based on the subjective appropriation of city and nature. This conversion of the "place" into a "practiced space" (Micheal De Certeau) is realized here though two devices: the map and the travel. Valverde initiates his poetry with the metaphor of "walking": the present appropriation of the natural space, but above all the urban space, is achieved by an authorial voice who selects and builds his itinerary in the confluence of the historic experience, both public and private. Space and subject are, thus, the two coordinates we will deal with, considering them as two sides of the same coin, conceiving them as categories with a cultural and epistemological thickness, which consolidates the ancestral concept of homo viator, within an urban globalized imaginary, closest to something that has already become a phenomenon of our age: "the spatial turn".</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Scarano, Laura] Univ Nacl Mar del Plata, Mar Del Plata, Buenos Aires,
   Argentina.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Scarano, L (reprint author), Univ Nacl Mar del Plata, Mar Del Plata, Buenos Aires, Argentina.</td>
</tr>

<tr>
<td valign="top">EM </td><td>laurarosanascarano@gmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUL</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">IS </td><td>11</td>
</tr>

<tr>
<td valign="top">BP </td><td>289</td>
</tr>

<tr>
<td valign="top">EP </td><td>310</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.7203/KAM.11.11218</td>
</tr>

<tr>
<td valign="top">SC </td><td>Arts &amp; Humanities - Other Topics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000445532200014</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Rosales-Huamani, JA
   <br>Castillo-Sequera, JL
   <br>Montalvan-Figueroa, JC
   <br>Andrade-Choque, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Aurelio Rosales-Huamani, Jimmy
   <br>Luis Castillo-Sequera, Jose
   <br>Carlos Montalvan-Figueroa, Juan
   <br>Andrade-Choque, Joseps</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Prototype of Speech Interface Based on the Google Cloud Platform to
   Access a Semantic Website</td>
</tr>

<tr>
<td valign="top">SO </td><td>SYMMETRY-BASEL</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>artificial intelligence; semantic web; natural language; Google Cloud
   Speech; SPARQL</td>
</tr>

<tr>
<td valign="top">ID </td><td>ONTOLOGIES; WEB; TRANSLATION; SYSTEM</td>
</tr>

<tr>
<td valign="top">AB </td><td>The main restriction of the Semantic Web is the difficulty of the SPARQL language, which is necessary for extracting information from the Knowledge Representation also known as ontology. Making the Semantic Web accessible for people who do not know SPARQL is essential for the use of friendlier interfaces, and a good alternative is Natural Language. This paper shows the implementation of a friendly prototype interface activated by voice to query and retrieving information from websites built with Semantic Web tools. In that way, the end users avoid the complicated SPARQL language. To achieve this, the interface recognizes a speech query and converts it into text, it processes the text through a Java program and identifies keywords, generates a SPARQL query, extracts the information from the website and reads it in a voice for the user. In our work, Google Cloud Speech API makes Speech-to-Text conversions and Text-to Speech conversions are made with SVOX Pico. As a result, we have measured three variables: the success rate in queries, the response time of query and a usability survey. The values of the variables allow the evaluation of our prototype. Finally, the interface proposed provides us with a new approach in the problem, using the Cloud like a Service, reducing barriers of access to the Semantic Web for people without technical knowledge of Semantic Web technologies.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Aurelio Rosales-Huamani, Jimmy; Carlos Montalvan-Figueroa, Juan;
   Andrade-Choque, Joseps] Natl Univ Engn, Dept Syst Engn, Lima 15333, Peru.
   <br>[Luis Castillo-Sequera, Jose] Univ Alcala De Henares, Higher Polytech
   Sch, Dept Comp Sci, Alcala De Henares 28871, Spain.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Rosales-Huamani, JA (reprint author), Natl Univ Engn, Dept Syst Engn, Lima 15333, Peru.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jrosales@uni.edu.pe; jluis.castillo@uah.es; jmontalvan@uni.pe;
   jandradec@uni.pe</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Sequera, Jose Luis Castillo</display_name>&nbsp;</font></td><td><font size="3">M-7165-2017&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Sequera, Jose Luis Castillo</display_name>&nbsp;</font></td><td><font size="3">0000-0002-9131-1618&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Rosales, Jimmy</display_name>&nbsp;</font></td><td><font size="3">0000-0002-3737-8694&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>andrade choque, joseps</display_name>&nbsp;</font></td><td><font size="3">0000-0001-5645-7286&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUL</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">VL </td><td>10</td>
</tr>

<tr>
<td valign="top">IS </td><td>7</td>
</tr>

<tr>
<td valign="top">AR </td><td>268</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.3390/sym10070268</td>
</tr>

<tr>
<td valign="top">SC </td><td>Science &amp; Technology - Other Topics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000440215400038</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hanilci, C</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hanilci, Cemal</td>
</tr>

<tr>
<td valign="top">TI </td><td>Linear prediction residual features for automatic speaker verification
   anti-spoofing</td>
</tr>

<tr>
<td valign="top">SO </td><td>MULTIMEDIA TOOLS AND APPLICATIONS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speaker verification; Anti-spoofing; Countermeasure; Linear prediction
   residual</td>
</tr>

<tr>
<td valign="top">ID </td><td>COUNTERMEASURES; BIOMETRICS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Automatic speaker verification (ASV) systems are highly vulnerable against spoofing attacks. Anti-spoofing, determining whether a speech signal is natural/genuine or spoofed, is very important for improving the reliability of the ASV systems. Spoofing attacks using the speech signals generated using speech synthesis and voice conversion have recently received great interest due to the 2015 edition of Automatic Speaker Verification Spoofing and Countermeasures Challenge (ASVspoof 2015). In this paper, we propose to use linear prediction (LP) residual based features for anti-spoofing. Three different features extracted from LP residual signal were compared using the ASVspoof 2015 database. Experimental results indicate that LP residual phase cepstral coefficients (LPRPC) and LP residual Hilbert envelope cepstral coefficients (LPRHEC) obtained from the analytic signal of the LP residual yield promising results for anti-spoofing. The proposed features are found to outperform standard Mel-frequency cepstral coefficients (MFCC) and Cosine Phase (CosPhase) features. LPRPC and LPRHEC features give the smallest equal error rates (EER) for eight spoofing methods out of ten spoofing attacks in comparison to MFCC and CosPhase features.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Hanilci, Cemal] Bursa Tech Univ, Dept Elect &amp; Elect Engn, Bursa, Turkey.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hanilci, C (reprint author), Bursa Tech Univ, Dept Elect &amp; Elect Engn, Bursa, Turkey.</td>
</tr>

<tr>
<td valign="top">EM </td><td>cemal.hanilci@btu.edu.tr</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Hanilci, Cemal</display_name>&nbsp;</font></td><td><font size="3">S-4967-2016&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUL</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">VL </td><td>77</td>
</tr>

<tr>
<td valign="top">IS </td><td>13</td>
</tr>

<tr>
<td valign="top">BP </td><td>16099</td>
</tr>

<tr>
<td valign="top">EP </td><td>16111</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/s11042-017-5181-0</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000439750300005</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wasim, M
   <br>Shaikh, A
   <br>Siddiqui, AA
   <br>Ahmed, L
   <br>Ali, SF
   <br>Saeed, F</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wasim, Muhammad
   <br>Shaikh, Abdulbasit
   <br>Siddiqui, Adnan Ahmed
   <br>Ahmed, Lubaid
   <br>Ali, Syed Faisal
   <br>Saeed, Fauzan</td>
</tr>

<tr>
<td valign="top">TI </td><td>Communicator for Hearing-Impaired Persons using Pakistan Sign Language
   (PSL)</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Communicator; hearing-impaired; Pakistan Sign Language (PSL); hand
   gesture; special person; token</td>
</tr>

<tr>
<td valign="top">AB </td><td>Communication with a hearing-impaired individual is a big challenge for a normal person. Hearingimpaired people uses hand gesture language (sign language) to communicate with each other, which is not easy to understand by a normal person because he/she is not trained to understand sign language. This communication gap between a hearing- impaired and a normal person created big problem for hearing-impaired individuals during their shopping, hospitalization, at their schools and homes. Especially in case of emergency, it is very difficult to understand the statement of a hearing- impaired one's who uses sign language. In the last few years researchers and developers from all over the world presented different ideas and works to solve this problem but no such solution is available to resolve this issue and can create two-way communication between hearing-impaired and normal persons. This paper presented a detail description about a two-way communication system based on Pakistan Sign Language (PSL). This duplex system is developed through conversion from the text in simple English into hand gestures and vice versa. However, conversion from hand gestures is available not only in text but also with voice providing more convenience to normal person. Main objective is to facilitate a large population and making hearingimpaired persons, the vital part of our civilization. A normal person can enter the text (sentence) in application, after the checking of spelling and grammar, the text is divided into tokens and sub-tokens. A token is a gesture against each word of the text while sub-tokens are the gestures of each character of the words. The combination of tokens created the gestures of text. On the other hand when gestures were input in to the application, using image processing technique, the nature of hand gesture were recognized and converted into corresponding text or voice.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Wasim, Muhammad; Ahmed, Lubaid; Ali, Syed Faisal; Saeed, Fauzan] Usman
   Inst Technol, Dept Comp Sci, Karachi, Pakistan.
   <br>[Siddiqui, Adnan Ahmed] Hamdard Univ, Dept Comp Sci, Karachi, Pakistan.
   <br>[Shaikh, Abdulbasit] IBA, Dept Comp Sci, Karachi, Pakistan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Wasim, M (reprint author), Usman Inst Technol, Dept Comp Sci, Karachi, Pakistan.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAY</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">VL </td><td>9</td>
</tr>

<tr>
<td valign="top">IS </td><td>5</td>
</tr>

<tr>
<td valign="top">BP </td><td>197</td>
</tr>

<tr>
<td valign="top">EP </td><td>202</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000435403400025</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ling, ZH
   <br>Ai, Y
   <br>Gu, Y
   <br>Dai, LR</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ling, Zhen-Hua
   <br>Ai, Yang
   <br>Gu, Yu
   <br>Dai, Li-Rong</td>
</tr>

<tr>
<td valign="top">TI </td><td>Waveform Modeling and Generation Using Hierarchical Recurrent Neural
   Networks for Speech Bandwidth Extension</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speech bandwidth extension; recurrent neural networks; dilated
   convolutional neural networks; bottleneck features</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION; ENHANCEMENT</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a waveform modeling and generation method using hierarchical recurrent neural networks (HRNN) for speech bandwidth extension (BWE). Different from conventional BWE methods that predict spectral parameters for reconstructing wideband speech waveforms, this BWE method models and predicts waveform samples directly without using vocoders. Inspired by SampleRNN, which is an unconditional neural audio generator, the HRNN model represents the distribution of each wideband or high-frequency waveform sample conditioned on the input narrowband waveform samples using a neural network composed of long short-term memory (LSTM) layers and feed-forward layers. The LSTM layers forma hierarchical structure and each layer operates at a specific temporal resolution to efficiently capture long-span dependencies between temporal sequences. Furthermore, additional conditions, such as the bottleneck features derived from narrowband speech using a deep neural network based state classifier, are employed as auxiliary input to further improve the quality of generated wideband speech. The experimental results of comparing several waveform modeling methods show that the HRNN-based method can achieve better speech quality and run-time efficiency than the dilated convolutional neural network based method and the plain sample-level recurrent neural network based method. Our proposed method also outperforms the conventional vocoder-based BWE method using LSTM-RNNs in terms of the subjective quality of the reconstructed wideband speech.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ling, Zhen-Hua; Ai, Yang; Gu, Yu; Dai, Li-Rong] Univ Sci &amp; Technol
   China, Natl Engn Lab Speech &amp; Language Informat Proc, Hefei 230027,
   Anhui, Peoples R China.
   <br>[Gu, Yu] Baidu Speech Dept, Baidu Technol Pk, Beijing 100193, Peoples R
   China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ling, ZH (reprint author), Univ Sci &amp; Technol China, Natl Engn Lab Speech &amp; Language Informat Proc, Hefei 230027, Anhui, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>zhling@ustc.edu.cn; ay8067@mail.ustc.edu.cn; guyu04@baidu.com;
   lrdai@ustc.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAY</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">VL </td><td>26</td>
</tr>

<tr>
<td valign="top">IS </td><td>5</td>
</tr>

<tr>
<td valign="top">BP </td><td>883</td>
</tr>

<tr>
<td valign="top">EP </td><td>894</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TASLP.2018.2798811</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000427867300003</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ibrahim, AAA
   <br>Jimat, A
   <br>Andrias, RM
   <br>Yusof, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ibrahim, Ag Asri Ag
   <br>Jimat, Alter
   <br>Andrias, Ryan Macdonell
   <br>Yusof, Soffri</td>
</tr>

<tr>
<td valign="top">TI </td><td>The Sound Framework of Parameter Mapping in Sonifying 3D Hands Movements</td>
</tr>

<tr>
<td valign="top">SO </td><td>ADVANCED SCIENCE LETTERS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>2nd International Conference on Recent Advances in Nanosciences and
   Nanotechnology (ICRANN)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 19-20, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>New Delhi, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Sonification; Parameter Mapping; Body Movements</td>
</tr>

<tr>
<td valign="top">AB </td><td>In physiotherapy, most of instructions by trainers or physiotherapists in body movements are normally done through voice instructions or touches. To follow these instructions without seeing it is not easy especially for blind people. It is hoped by listening to non-speech sounds as an alternative of medium of instructions; a person should be able to follow the exact body movements. Thus, this paper explains the potential of using non-speech sound to represent 3D hand movements. In this research, the conversion of data-to-sound technique was using Parameter Mapping, where the movement coordinates were mapped with sound properties. Series of experimental design have been carried out and the results will be discussed.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ibrahim, Ag Asri Ag; Jimat, Alter; Andrias, Ryan Macdonell; Yusof,
   Soffri] Univ Malaysia Sabah, Kota Kinabalu, Malaysia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ibrahim, AAA (reprint author), Univ Malaysia Sabah, Kota Kinabalu, Malaysia.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>FEB</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">VL </td><td>24</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>1325</td>
</tr>

<tr>
<td valign="top">EP </td><td>1329</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1166/asl.2018.10742</td>
</tr>

<tr>
<td valign="top">SC </td><td>Science &amp; Technology - Other Topics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000432368000116</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Zhao, YJ
   <br>Togneri, R
   <br>Sreeram, V</td>
</tr>

<tr>
<td valign="top">AF </td><td>Zhao, Yuanjun
   <br>Togneri, Roberto
   <br>Sreeram, Victor</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Spoofing Detection Using Adaptive Weighting Framework and Clustering
   Analysis</td>
</tr>

<tr>
<td valign="top">SO </td><td>19TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2018), VOLS 1-6: SPEECH RESEARCH FOR EMERGING
   MARKETS IN MULTILINGUAL SOCIETIES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>19th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2018)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 02-SEP 06, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hyderabad, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>automatic speaker verification; anti-spoofing countermeasures; CQCC;
   SCC; adaptive weighting; clustering</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEAKER VERIFICATION; COUNTERMEASURES</td>
</tr>

<tr>
<td valign="top">AB </td><td>Security of Automatic Speaker Verification (ASV) systems against imposters are now focusing on anti-spoofing countermeasures. Under the severe threat of various speech spoofing techniques, ASV systems can easily be 'fooled' by spoofed speech which sounds as real as human-beings. As two effective solutions, the Constant Q Cepstral Coefficients (CQCC) and the Scattering Cepstral Coefficients (SCC) perform well on the detection of artificial speech signals, especially for attacks from speech synthesis (SS) and voice conversion (VC). However, for spoofing subsets generated by different approaches, a low Equal Error Rate (EER) cannot be maintained. In this paper, an adaptive weighting based standalone detector is proposed to address the selective detection degradation. The clustering property of the genuine and the spoofed subsets are analysed for the selection of suitable weighting factors. With a Gaussian Mixture Model (GMM) classifier as the back-end, the proposed detector is evaluated on the ASVspoof 2015 database. The EERs of 0.01% and 0.20% are obtained on the known and the unknown attacks, respectively. This presents an essential complementation between the CQCC and the SCC and also promotes the future research on generalized countermeasures.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Zhao, Yuanjun; Togneri, Roberto; Sreeram, Victor] UWS, Sch Elect &amp; Comp
   Engn, Nedlands, WA, Australia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Zhao, YJ (reprint author), UWS, Sch Elect &amp; Comp Engn, Nedlands, WA, Australia.</td>
</tr>

<tr>
<td valign="top">EM </td><td>yuanjun.zhao@research.uwa.edu.au; roberto.togneri@uwa.edu.au;
   victor.sreeram@uwa.edu.au</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>626</td>
</tr>

<tr>
<td valign="top">EP </td><td>630</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2018-1042</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000465363900132</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wickramasinghe, B
   <br>Irtza, S
   <br>Ambikairajah, E
   <br>Epps, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wickramasinghe, Buddhi
   <br>Irtza, Saad
   <br>Ambikairajah, Eliathamby
   <br>Epps, Julien</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Frequency Domain Linear Prediction Features for Replay Spoofing Attack
   Detection</td>
</tr>

<tr>
<td valign="top">SO </td><td>19TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2018), VOLS 1-6: SPEECH RESEARCH FOR EMERGING
   MARKETS IN MULTILINGUAL SOCIETIES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>19th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2018)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 02-SEP 06, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hyderabad, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>ASVspoof 2017; frequency domain linear prediction; convolutional neural
   network; replay attack</td>
</tr>

<tr>
<td valign="top">ID </td><td>ENVELOPE</td>
</tr>

<tr>
<td valign="top">AB </td><td>Automatic speaker verification (ASV) systems are vulnerable to various types of spoofing attacks such as speech synthesis, voice conversion and replay attacks. Recent research has highlighted the need for more effective countermeasures for replay attacks, which can be very challenging to detect, however replayed speech has previously shown frequency band-specific differences when compared with genuine speech. In this paper, we propose the use of long-term temporal envelopes of subband signals using a frequency domain linear prediction (FDLP) framework. This flexible framework makes use of temporal envelope information, which has not previously been investigated for replay spoofing detection. Evaluations of the proposed system and its fusion with other subsystems were carried out on the ASVspoof 2017 database. Interestingly, smoother temporal envelopes, based on very long windows of up to 1 second, seem to be most successful and show good prospects for performance improvements via fusion.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Wickramasinghe, Buddhi; Irtza, Saad; Ambikairajah, Eliathamby; Epps,
   Julien] UNSW Australia, Sch Elect Engn &amp; Telecommun, Sydney, NSW,
   Australia.
   <br>[Wickramasinghe, Buddhi; Ambikairajah, Eliathamby; Epps, Julien] CSIRO,
   Data61, Sydney, NSW, Australia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Wickramasinghe, B (reprint author), UNSW Australia, Sch Elect Engn &amp; Telecommun, Sydney, NSW, Australia.; Wickramasinghe, B (reprint author), CSIRO, Data61, Sydney, NSW, Australia.</td>
</tr>

<tr>
<td valign="top">EM </td><td>b.wickramasinghe@student.unsw.edu.au</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>661</td>
</tr>

<tr>
<td valign="top">EP </td><td>665</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2018-1574</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000465363900139</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Gomez-Alanis, A
   <br>Peinado, AM
   <br>Gonzalez, JA
   <br>Gomez, AM</td>
</tr>

<tr>
<td valign="top">AF </td><td>Gomez-Alanis, Alejandro
   <br>Peinado, Antonio M.
   <br>Gonzalez, Jose A.
   <br>Gomez, Angel M.</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Deep Identity Representation for Noise Robust Spoofing Detection</td>
</tr>

<tr>
<td valign="top">SO </td><td>19TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2018), VOLS 1-6: SPEECH RESEARCH FOR EMERGING
   MARKETS IN MULTILINGUAL SOCIETIES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>19th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2018)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 02-SEP 06, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hyderabad, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Spoofing detection; noise robustness; speaker verification; deep
   learning; missing-data masks</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH DETECTION</td>
</tr>

<tr>
<td valign="top">AB </td><td>The issue of the spoofing attacks which may affect automatic speaker verification systems (ASVs) has recently received an increased attention, so that a number of countermeasures have been developed for detecting high technology attacks such as speech synthesis and voice conversion. However, the performance of anti-spoofing systems degrades significantly in noisy conditions. To address this issue, we propose a deep learning framework to extract spoofing identity vectors, as well as the use of soft missing-data masks. The proposed feature extraction employs a convolutional neural network (CNN) plus a recurrent neural network (RNN) in order to provide a single deep feature vector per utterance. Thus, the CNN is treated as a convolutional feature extractor that operates at the frame level. On top of the CNN outputs, the RNN is employed to obtain a single spoofing identity representation of the whole utterance. Experimental evaluation is carried out on both a clean and a noisy version of the ASVSpoof2015 corpus. The experimental results show that our proposals clearly outperforms other methods recently proposed such as the popular CQCC+GMM system or other similar deep feature systems for both seen and unseen noisy conditions.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Gomez-Alanis, Alejandro; Peinado, Antonio M.; Gomez, Angel M.] Univ
   Granada, Granada, Spain.
   <br>[Gonzalez, Jose A.] Univ Malaga, Malaga, Spain.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Gomez-Alanis, A (reprint author), Univ Granada, Granada, Spain.</td>
</tr>

<tr>
<td valign="top">EM </td><td>agomezalanis@ugr.es; amp@ugr.es; jgonzalez@lcc.uma.es; amgg@ugr.es</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>676</td>
</tr>

<tr>
<td valign="top">EP </td><td>680</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2018-1909</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000465363900142</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hsu, WN
   <br>Glass, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hsu, Wei-Ning
   <br>Glass, James</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Scalable Factorized Hierarchical Variational Autoencoder Training</td>
</tr>

<tr>
<td valign="top">SO </td><td>19TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2018), VOLS 1-6: SPEECH RESEARCH FOR EMERGING
   MARKETS IN MULTILINGUAL SOCIETIES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>19th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2018)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 02-SEP 06, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hyderabad, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>unsupervised learning; speech representation learning; factorized
   hierarchical variational autoencoder</td>
</tr>

<tr>
<td valign="top">AB </td><td>Deep generative models have achieved great success in unsupervised learning with the ability to capture complex nonlinear relationships between latent generating factors and observations. Among them, a factorized hierarchical variational autoencoder (FHVAE) is a variational inference-based model that formulates a hierarchical generative process for sequential data. Specifically, an FHVAE model can learn disentangled and interpretable representations, which have been proven useful for numerous speech applications. such as speaker verification, robust speech recognition, and voice conversion. However, as we will elaborate in this paper, the training algorithm proposed in the original paper is not scalable to datasets of thousands of hours, which makes this model less applicable on a larger scale. After identifying limitations in terms of runtime, memory, and hyperparameter optimization, we propose a hierarchical sampling training algorithm to address all three issues. Our proposed method is evaluated comprehensively on a wide variety of datasets, ranging from 3 to 1,000 hours and involving different types of generating factors. such as recording conditions and noise types. In addition, we also present a new visualization method for qualitatively evaluating the performance with respect to the interpretability and disentanglement. Models trained with our proposed algorithm demonstrate the desired characteristics on all the datasets.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Hsu, Wei-Ning; Glass, James] MIT, Comp Sci &amp; Artificial Intelligence
   Lab, 77 Massachusetts Ave, Cambridge, MA 02139 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hsu, WN (reprint author), MIT, Comp Sci &amp; Artificial Intelligence Lab, 77 Massachusetts Ave, Cambridge, MA 02139 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>wnhsu@mit.edu; glass@mit.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>1462</td>
</tr>

<tr>
<td valign="top">EP </td><td>1466</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2018-1034</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000465363900305</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Reddy, NA
   <br>Rao, MVA
   <br>Meenakshi, GN
   <br>Ghosh, PK</td>
</tr>

<tr>
<td valign="top">AF </td><td>Reddy, Abinay N.
   <br>Rao, Achuth M., V
   <br>Meenakshi, G. Nisha
   <br>Ghosh, Prasanta Kumar</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Reconstructing neutral speech from tracheoesophageal speech</td>
</tr>

<tr>
<td valign="top">SO </td><td>19TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2018), VOLS 1-6: SPEECH RESEARCH FOR EMERGING
   MARKETS IN MULTILINGUAL SOCIETIES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>19th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2018)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 02-SEP 06, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hyderabad, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice Prosthesis; Laryngectomy; Whispered speech; Tracheoesophageal</td>
</tr>

<tr>
<td valign="top">ID </td><td>MULTIDIMENSIONAL ASSESSMENT; FEMALE ESOPHAGEAL; LARYNGECTOMY; VOICE</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this work, we propose a tracheoesophageal (TE) speech to neutral speech conversion system using data collected from a laryngectomee. In laryngectomees, in the absence of vocal folds, it is the vibration of the esophagus that gives rise to a low frequency pitch during speech production. This pitch is manifested as impulse-like noise in the recorded speech. We propose a method to first 'whisperize' the TE speech prior to the linear predictive coding (LPC) based synthesis which uses pitch derived from the energy contour. In order to perform 'whisperization', we model the LPC residual signal as the sum of white noise and impulses introduced by the esophageal vibrations. We model these impulses and white noise using Bemoulli-Gaussian distribution and Gaussian distribution, respectively. The strength and location of the impulses are estimated using Gibbs sampling in order to remove the impulse-like noise from speech to obtain whispered speech. Subjective evaluation via listening test reveals that the 'whisperization' step in the proposed method aids in synthesizing a more natural sounding neutral speech. A different listening test shows that the listeners prefer the synthesized speech from the proposed method similar to 93% (absolute) times more than the best baseline scheme.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Reddy, Abinay N.; Rao, Achuth M., V; Meenakshi, G. Nisha; Ghosh,
   Prasanta Kumar] Indian Inst Sci IISc, Elect Engn, Bangalore 560012,
   Karnataka, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Reddy, NA (reprint author), Indian Inst Sci IISc, Elect Engn, Bangalore 560012, Karnataka, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>nainireddy@iisc.ac.in; achuthr@iisc.ac.in; nishag@iisc.ac.in;
   prasantg@iisc.ac.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>1541</td>
</tr>

<tr>
<td valign="top">EP </td><td>1545</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2018-1907</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000465363900324</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Dasgupta, H
   <br>Pandey, PC
   <br>Nataraj, KS</td>
</tr>

<tr>
<td valign="top">AF </td><td>Dasgupta, Hirak
   <br>Pandey, Prem C.
   <br>Nataraj, K. S.</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Detection of Glottal Excitation Epochs in Speech Signal Using Hilbert
   Envelope</td>
</tr>

<tr>
<td valign="top">SO </td><td>19TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2018), VOLS 1-6: SPEECH RESEARCH FOR EMERGING
   MARKETS IN MULTILINGUAL SOCIETIES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>19th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2018)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 02-SEP 06, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hyderabad, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>fundamental frequency; glottal excitation epoch; Hilbert envelope; pitch
   period</td>
</tr>

<tr>
<td valign="top">AB </td><td>A technique, suitable for real-time processing, is presented for detection of glottal excitation epochs in voiced speech. It uses Hilbert envelope to enhance saliency of the glottal excitation epochs and to reduce the ripples due to the vocal tract filter. The processing comprises the steps of dynamic range compression, calculation of the Hilbert envelope, and epoch marking. The first step reduces amplitude variation by applying A-law on the signal envelope. The second step calculates the Hilbert envelope using the output of an FIR filter-based Hilbert transformer and the delay-compensated signal. The third step uses a dynamic peak detector with fast rise and slow fall and nonlinear smoothing using a two-step median-mean filter to further enhance the saliency of the epochs, followed by a differentiator to mark them. The technique is tested using the CMU-ARCTIC database with simultaneously recorded speech and EGG signals. The results showed a good match in the performance of the proposed technique with those of the state-of-the-art techniques and its robustness against highpass filtering. It may be useful for diagnosis of voice disorders and high-quality voice conversion.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Dasgupta, Hirak; Pandey, Prem C.; Nataraj, K. S.] Indian Inst Technol,
   Dept Elect Engn, Mumbai, Maharashtra, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Dasgupta, H (reprint author), Indian Inst Technol, Dept Elect Engn, Mumbai, Maharashtra, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>hirakdgpt@ee.iitb.ac.in; pcpandey@ee.iitb.ac.in; natarajks@ee.iitb.ac.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>2132</td>
</tr>

<tr>
<td valign="top">EP </td><td>2136</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2018-2014</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000465363900447</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Taguchi, F
   <br>Kaburagi, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Taguchi, Fumiaki
   <br>Kaburagi, Tokihiko</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Articulatory-to-speech conversion using bi-directional long short-term
   memory</td>
</tr>

<tr>
<td valign="top">SO </td><td>19TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2018), VOLS 1-6: SPEECH RESEARCH FOR EMERGING
   MARKETS IN MULTILINGUAL SOCIETIES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>19th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2018)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 02-SEP 06, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hyderabad, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Articulatory movement; EMA; Vocal-tract spectrum; Deep learning;
   Articulatory-to-acoustic mapping</td>
</tr>

<tr>
<td valign="top">ID </td><td>MOVEMENTS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Methods for synthesizing speech sounds from the motion of articulatory organs can be used to produce substitute speech for people who have undergone laryngectomy. To achieve this goal, feature parameters representing the spectral envelope of speech, directly related to the acoustic characteristics of the vocal tract, has been estimated from articulatory movements. Within this framework, speech can be synthesized by driving the filter obtained from a spectral envelope with noise signals. In the current study, we examined an alternative method that generates speech sounds directly from the motion pattern of articulatory organs based on the implicit relationships between articulatory movements and the source signal of speech. These implicit relationships were estimated by considering that articulatory movements are involved in phonological representations of speech that are also related to sound source information such as the temporal pattern of pitch and voiced/unvoiced flag. We developed a method for simultaneously estimating the spectral envelope and sound source parameters from articulatory data obtained with an electromagnetic articulography (EMA) sensor. Furthermore, objective evaluation of estimated speech parameters and subjective evaluation of the word error rate were performed to examine the effectiveness of our method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Taguchi, Fumiaki] Kyushu Univ, Grad Sch Design, Minami Ku, Shiobaru
   4-9-1, Fukuoka, Fukuoka 8158540, Japan.
   <br>[Kaburagi, Tokihiko] Kyushu Univ, Fac Design, Minami Ku, Shiobaru 4-9-1,
   Fukuoka, Fukuoka 8158540, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Taguchi, F (reprint author), Kyushu Univ, Grad Sch Design, Minami Ku, Shiobaru 4-9-1, Fukuoka, Fukuoka 8158540, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>taguchi.f.664@s.kyushu-u.ac.jp; kabu@design.kyushu-u.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>2499</td>
</tr>

<tr>
<td valign="top">EP </td><td>2503</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2018-999</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000465363900525</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Shah, N
   <br>Shah, NJ
   <br>Patil, HA</td>
</tr>

<tr>
<td valign="top">AF </td><td>Shah, Neil
   <br>Shah, Nirmesh J.
   <br>Patil, Hemant A.</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Effectiveness of Generative Adversarial Network for Non-Audible
   Murmur-to-Whisper Speech Conversion</td>
</tr>

<tr>
<td valign="top">SO </td><td>19TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2018), VOLS 1-6: SPEECH RESEARCH FOR EMERGING
   MARKETS IN MULTILINGUAL SOCIETIES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>19th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2018)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 02-SEP 06, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hyderabad, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Non-Audible Murmur (NAM); generative adversarial network (GAN);
   whispered speech</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>The murmur produced by the speaker and captured by the Non-Audible Murmur (NAM)-one of the Silent Speech Interface (SSI) technique, suffers from the speech quality degradation. This is due to the lack of radiation effect at the lips and lowpass nature of the soft tissue, which attenuates the high frequency related information. In this work, a novel method for NAM-to Whisper (NAM2WHSP) speech conversion incorporating Generative Adversarial Network (GAN) is proposed. The GAN minimizes the distributional divergence between the whispered speech and the generated speech parameters (through adversarial optimization). The objective and subjective evaluation performed on the proposed system, justifies the ability of adversarial optimization over Maximum Likelihood (ML)-based optimization networks, such as a Deep Neural Network (DNN), in preserving and improving the speech quality and intelligibility. The adversarial optimization learns the mapping function with 54.2 % relative improvement in MOS and 29.83 % absolute reduction in % WER w.r.t. the state-of-the-art mapping techniques. Furthermore, we evaluated the proposed framework by analyzing the level of contextual information and the number of training utterances required for optimizing the network parameters, for the given task and database.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Shah, Neil; Shah, Nirmesh J.; Patil, Hemant A.] Dhirubhai Ambani Inst
   Informat &amp; Commun Technol, Speech Res Lab, Gandhinagar, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Shah, N (reprint author), Dhirubhai Ambani Inst Informat &amp; Commun Technol, Speech Res Lab, Gandhinagar, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>neil_shah@daiict.ac.in; nirmesh88_shah@daiict.ac.in;
   hemant_patil@daiict.ac.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>3157</td>
</tr>

<tr>
<td valign="top">EP </td><td>3161</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2018-1565</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000465363900658</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Pandey, PSK
   <br>Kulkarni, R</td>
</tr>

<tr>
<td valign="top">AF </td><td>Pandey, Pranjali Susheel Kumar
   <br>Kulkarni, Ramesh</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Traffic Sign Detection for Advanced Driver Assistance System</td>
</tr>

<tr>
<td valign="top">SO </td><td>2018 INTERNATIONAL CONFERENCE ON ADVANCES IN COMMUNICATION AND COMPUTING
   TECHNOLOGY (ICACCT)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>1st International Conference on Advances in Communication and Computing
   Technology (ICACCT)</td>
</tr>

<tr>
<td valign="top">CY </td><td>FEB 08-09, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Sangamner, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>advanced driver assistance; driver safety; traffic sign detection;
   contour detection; computer vision</td>
</tr>

<tr>
<td valign="top">AB </td><td>Development of safety features so as to prevent ignorance of traffic sign boards mounted on road is one of the major technical challenges in the automobile industry. Ignoring traffic signs can lead to major road accidents. Therefore, using driver assistance system to timely assist with warning and information road signs can be of significant help in prevention of accidents. This paper aims towards the detection of road signs using contour analysis approach. In this paper Blue, Green, Red (BGR) to Hue, Saturation, Value (HSV) conversion model and morphological filter for noise filtering are used to make the result more robust. The system has a voice trigger to alert the driver of road signs via audio message and thus helping in prevention of accidents.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Pandey, Pranjali Susheel Kumar; Kulkarni, Ramesh] VES Inst Technol,
   Mumbai, Maharashtra, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Pandey, PSK (reprint author), VES Inst Technol, Mumbai, Maharashtra, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>pranjali.pandey24@gmail.com; ramesh.kulkarni@ves.ac.in</td>
</tr>

<tr xmlns:date="http://exslt.org/dates-and-times">
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>KULKARNI, RAMESH</display_name>&nbsp;</font></td><td><font size="3">E-7019-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>KULKARNI, RAMESH</display_name>&nbsp;</font></td><td><font size="3">0000-0002-1909-8226&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>182</td>
</tr>

<tr>
<td valign="top">EP </td><td>186</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000465215400033</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Bakar, B
   <br>Hanilci, C</td>
</tr>

<tr>
<td valign="top">AF </td><td>Bakar, Bekir
   <br>Hanilci, Cemal</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>AN EXPERIMENTAL STUDY ON AUDIO REPLAY ATTACK DETECTION USING DEEP NEURAL
   NETWORKS</td>
</tr>

<tr>
<td valign="top">SO </td><td>2018 IEEE WORKSHOP ON SPOKEN LANGUAGE TECHNOLOGY (SLT 2018)</td>
</tr>

<tr>
<td valign="top">SE </td><td>IEEE Workshop on Spoken Language Technology</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE Workshop on Spoken Language Technology (SLT)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 18-21, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Athens, GREECE</td>
</tr>

<tr>
<td valign="top">DE </td><td>speaker verification; replay attack detection; deep neural networks;
   countermeasures</td>
</tr>

<tr>
<td valign="top">AB </td><td>Automatic speaker verification (ASV) systems can be easily spoofed by previously recorded speech, synthesized speech and speech signal that artificially generated by voice conversion techniques. In order to increase the reliability of the ASV systems, detecting spoofing attacks whether a given speech signal is genuine or spoofed plays an important role. In this paper, we consider the detection of replay attacks which is the most accessible attack type against ASV systems. To this end, we utilize a deep neural network (DNN) based classifier using features extracted from the long-term average spectrum. The experiments are conducted on the latest edition of Automatic Speaker Verification Spoofing and Countermeasures Challenge (ASVspoof 2017) database. The results are compared with the ASVspoof 2017 baseline system which consists of Gaussian mixture model (GMM) classifier with constant-Q transform cepstral coefficients (CQCC) front-end as well as the GMM with standard mel-frequency cepstrum coefficients (MFCC) features. Experimental results reveal that DNN considerably outperforms the well-known and successful GMM classifier. It is found that long term average spectrum (LTAS) based features are superior to CQCC and MFCC in terms of equal error rate (EER). Finally, we find that high-frequency components convey much more discriminative information for replay attack detection independent of features and classifiers.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Bakar, Bekir; Hanilci, Cemal] Bursa Tech Univ, Dept Elect &amp; Elect Engn,
   Bursa, Turkey.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Bakar, B (reprint author), Bursa Tech Univ, Dept Elect &amp; Elect Engn, Bursa, Turkey.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>132</td>
</tr>

<tr>
<td valign="top">EP </td><td>138</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000463141800020</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Tanaka, K
   <br>Kaneko, T
   <br>Hojo, N
   <br>Kameoka, H</td>
</tr>

<tr>
<td valign="top">AF </td><td>Tanaka, Kou
   <br>Kaneko, Takuhiro
   <br>Hojo, Nobukatsu
   <br>Kameoka, Hirokazu</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>SYNTHETIC-TO-NATURAL SPEECH WAVEFORM CONVERSION USING CYCLE-CONSISTENT
   ADVERSARIAL NETWORKS</td>
</tr>

<tr>
<td valign="top">SO </td><td>2018 IEEE WORKSHOP ON SPOKEN LANGUAGE TECHNOLOGY (SLT 2018)</td>
</tr>

<tr>
<td valign="top">SE </td><td>IEEE Workshop on Spoken Language Technology</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE Workshop on Spoken Language Technology (SLT)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 18-21, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Athens, GREECE</td>
</tr>

<tr>
<td valign="top">DE </td><td>Statistical parametric speech synthesis; postfilter; deep neural
   network; generative adversarial network; cycle-consistent adversarial
   network</td>
</tr>

<tr>
<td valign="top">AB </td><td>We propose a learning-based filter that allows us to directly modify a synthetic speech waveform into a natural speech waveform. Speech-processing systems using a vocoder framework such as statistical parametric speech synthesis and voice conversion are convenient especially for a limited number of data because it is possible to represent and process interpretable acoustic features over a compact space, such as the fundamental frequency (F-0) and mel-cepstrum. However, a well-known problem that leads to the quality degradation of generated speech is an over-smoothing effect that eliminates some detailed structure of generated/converted acoustic features. To address this issue, we propose a synthetic-to-natural speech waveform conversion technique that uses cycle-consistent adversarial networks and which does not require any explicit assumption about speech waveform in adversarial learning. In contrast to current techniques, since our modification is performed at the waveform level, we expect that the proposed method will also make it possible to generate "vocoder-less" sounding speech even if the input speech is synthesized using a vocoder framework. The experimental results demonstrate that our proposed method can 1) alleviate the over-smoothing effect of the acoustic features despite the direct modification method used for the waveform and 2) greatly improve the naturalness of the generated speech sounds.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Tanaka, Kou; Kaneko, Takuhiro; Hojo, Nobukatsu; Kameoka, Hirokazu] NTT
   Corp, NTT Commun Sci Labs, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Tanaka, K (reprint author), NTT Corp, NTT Commun Sci Labs, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ftanaka.ko@lab.ntt.co.jp; kaneko.takuhiro@lab.ntt.co.jp;
   hojo.nobukatsu@lab.ntt.co.jp; kameoka.hirokazu@lab.ntt.co.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>632</td>
</tr>

<tr>
<td valign="top">EP </td><td>639</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000463141800088</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Gupta, A
   <br>Joshi, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Gupta, Arpita
   <br>Joshi, Akshay</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Speech Recognition using Artificial Neural Network</td>
</tr>

<tr>
<td valign="top">SO </td><td>PROCEEDINGS OF THE 2018 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATION
   AND SIGNAL PROCESSING (ICCSP)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>7th IEEE International Conference on Communication and Signal Processing
   (IEEE ICCSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 03-05, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Adhiparasakthi Engn Coll, Melmaruvathur, INDIA</td>
</tr>

<tr>
<td valign="top">HO </td><td>Adhiparasakthi Engn Coll</td>
</tr>

<tr>
<td valign="top">DE </td><td>LSTM; MFCC features; Recurrent Neural Network; Restricted Boltzmann
   Machine</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper propose two approaches for speech recognition via supervised and unsupervised learning. Speech signals are non-stationary signals. Treating speech in computing domain falls under sequential learning task i.e. if we want to make a sense of current statement, we may need to go through the context in which it was spoken. Recurrent Neural Networks (RNN) is been used in speech recognition problems because of its powerful sequence modeling capacity. In this paper we have proposed Bi-directional Recurrent Neural Network with Long Short Term Memory model (LSTM), so that speech signal reconstruction can be done in a proper way without performance loss. For unsupervised learning, model is designed on the basis of Restricted Boltzmann Machine (RBM) which generates a reconstruction based output and helps in conversion of voice into text, each letter by letter.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Gupta, Arpita; Joshi, Akshay] Birla Inst Technol &amp; Sci, Dept Elect &amp;
   Elect Engn, Pilani 333031, Rajasthan, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Gupta, A (reprint author), Birla Inst Technol &amp; Sci, Dept Elect &amp; Elect Engn, Pilani 333031, Rajasthan, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>h2016102@pilani.bits-pilani.ac.in; akshu.jai@gmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>68</td>
</tr>

<tr>
<td valign="top">EP </td><td>71</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000462057000015</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ahmed, T
   <br>Wahid, MF
   <br>Habib, MA</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ahmed, Tasnim
   <br>Wahid, Md. Ferdous
   <br>Habib, Md. Ahsan</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Implementation of Bangla Speech Recognition in Voice Input Speech Output
   (VISO) Calculator</td>
</tr>

<tr>
<td valign="top">SO </td><td>2018 INTERNATIONAL CONFERENCE ON BANGLA SPEECH AND LANGUAGE PROCESSING
   (ICBSLP)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Bangla Speech and Language Processing
   (ICBSLP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 21-22, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Sylhet, BANGLADESH</td>
</tr>

<tr>
<td valign="top">DE </td><td>VISO Calculator; Bangla Speech-recognition; Android TTS API; Hidden
   Markov Models; CMU Sphinx</td>
</tr>

<tr>
<td valign="top">AB </td><td>Automatic Speech-recognition (ASR) is an engaging way for human-computer interaction. It is a process of deriving linguistic information from acoustic signals while removing noise signal associated with it. The speaker-independent Voice Input Speech Output (VISO) calculator application is a promising implementation of speech-recognition technology, which is designed to recognize numbers and symbols to perform mathematical operations. But none of such applications provide support for Bangla language yet. Thus, in this paper, we have designed a VISO calculator application that can recognize both isolated and continuous speech in Bangla language and derive mathematical expression. The expression is then calculated and the auditory output is pronounced in Bangla. The task of speech-to-text (STT) conversion is performed by Hidden Markov models (HMM) in CMU Sphinx, while the result is converted to speech using Android Text-to-Speech (TTS) API. The application is able to recognize Bangla real numbers such as (1), (2), (3) etc. and several mathematical symbols such as (+), (-), (*), (/), (.) etc. This voice-based application is helpful for people having physical disabilities such as congenital disorder or visual impairment and people with less mathematical knowledge, who use Bangla as their native language.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ahmed, Tasnim; Wahid, Md. Ferdous] Hajee Mohammad Danesh Sci &amp; Technol
   Univ, Dept Elect &amp; Elect Engn, Dinajpur, Bangladesh.
   <br>[Habib, Md. Ahsan] Hajee Mohammad Danesh Sci &amp; Technol Univ, Dept Comp
   Sci &amp; Engn, Dinajpur, Bangladesh.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ahmed, T (reprint author), Hajee Mohammad Danesh Sci &amp; Technol Univ, Dept Elect &amp; Elect Engn, Dinajpur, Bangladesh.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ahmed1302185@gmail.com; mfwahid26@gmail.com; ahsan.sust@gmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000460564300034</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Lian, HL
   <br>Hu, YT
   <br>Zhou, J
   <br>Wang, HB
   <br>Tao, L</td>
</tr>

<tr>
<td valign="top">AF </td><td>Lian, Hailun
   <br>Hu, Yuting
   <br>Zhou, Jian
   <br>Wang, Huabin
   <br>Tao, Liang</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Whisper to Normal Speech Based on Deep Neural Networks with MCC and F0
   Features</td>
</tr>

<tr>
<td valign="top">SO </td><td>2018 IEEE 23RD INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING
   (DSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Digital Signal Processing</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>23rd IEEE International Conference on Digital Signal Processing (DSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>NOV 19-21, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Shanghai, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>whisper to normal speech conversion; DNN (Deep neural networks); MCC
   (Mel Cepstral Coefficients)</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we propose a method of converting whisper to normal speech, using low dimensional Mel Cepstral Coefficients (MCC) combined with Deep Neural Networks (DNN). The whisper to normal speech conversion is divided Into two modules, that is, spectrum conversion and fundamental frequency (F0) estimation. The MCC features are used to characterize the spectrum envelope. We use DNN to model relationship of low dimensional MCC between whispered speech and Its normal counterpart. DNN can not only fit the data well, but can also tackle the issue of smoothness. In the module of F0 estimation, the F0 Is estimated using MCC features both of normal speech and whisper. Specifically, the fundamental frequency of the voice and unvoiced speech frame are estimated simultaneously in order to reduce the modeling complexity. Experimental results show that the converted speech gains better performance both in the aspect of speech quality and intelligibility.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Lian, Hailun; Hu, Yuting; Zhou, Jian; Wang, Huabin; Tao, Liang] Anhui
   Univ, MOE Key Lab Intelligent Comp &amp; Signal Proc, Hefei, Anhui, Peoples
   R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Zhou, J (reprint author), Anhui Univ, MOE Key Lab Intelligent Comp &amp; Signal Proc, Hefei, Anhui, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jzhou@ahu.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000458909600269</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Luo, ZJ
   <br>Chen, JH
   <br>Cai, XJ
   <br>Tanaka, K
   <br>Takiguchi, T
   <br>Kinkyo, T
   <br>Hamori, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Luo, Zhaojie
   <br>Chen, Jinhui
   <br>Cai, Xiao Jing
   <br>Tanaka, Katsuyuki
   <br>Takiguchi, Tetsuya
   <br>Kinkyo, Takuji
   <br>Hamori, Shigeyuki</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Oil Price Forecasting Using Supervised GANs with Continuous Wavelet
   Transform Features</td>
</tr>

<tr>
<td valign="top">SO </td><td>2018 24TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION (ICPR)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Pattern Recognition</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>24th International Conference on Pattern Recognition (ICPR)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 20-24, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Chinese Acad Sci, Inst Automat, Beijing, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">HO </td><td>Chinese Acad Sci, Inst Automat</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper proposes a novel approach based on a supervised Generative Adversarial Networks (GANs) model that forecasts the crude oil prices with Adaptive Scales Continuous Wavelet Transform (AS-CWT). In our study, we first confirmed that the possibility of using Continuous Wavelet Transform (CWT) to decompose an oil price series into various components, such as the sequence of days, weeks, months and years, so that the decomposed new time series can be used as inputs for a deep-learning (DL) training model. Second, we find that applying the proposed adaptive scales in the CWT method can strengthen the dependence of inputs and provide more useful information, which can improve the forecasting performance. Finally, we use the supervised GANs model as a training model, which can provide more accurate forecasts than those of the naive forecast (NF) model and other nonlinear models, such as Neural Networks (NNs), and Deep Belief Networks (DBNs) when dealing with a limited amount of oil prices data.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Luo, Zhaojie; Chen, Jinhui; Takiguchi, Tetsuya] Kobe Univ, Grad Sch
   Syst Informat, Kobe, Hyogo, Japan.
   <br>[Cai, Xiao Jing; Tanaka, Katsuyuki; Kinkyo, Takuji; Hamori, Shigeyuki]
   Kobe Univ, Grad Sch Econ, Kobe, Hyogo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Luo, ZJ (reprint author), Kobe Univ, Grad Sch Syst Informat, Kobe, Hyogo, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>luozhaojie@me.cs.scitec.kobe-u.ac.jp; ianchen@me.cs.scitec.kobe-u.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>830</td>
</tr>

<tr>
<td valign="top">EP </td><td>835</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000455146800139</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Grosz, T
   <br>Gosztolya, G
   <br>Toth, L
   <br>Csapo, TG
   <br>Marko, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Grosz, Tamas
   <br>Gosztolya, Gabor
   <br>Toth, Laszlo
   <br>Csapo, Tamas Gabor
   <br>Marko, Alexandra</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>F0 ESTIMATION FOR DNN-BASED ULTRASOUND SILENT SPEECH INTERFACES</td>
</tr>

<tr>
<td valign="top">SO </td><td>2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech and Signal Processing
   (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 15-20, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Calgary, CANADA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Silent speech interface; articulatory-to-acoustic mapping; DNN;
   fundamental frequency</td>
</tr>

<tr>
<td valign="top">AB </td><td>State-of-the-art silent speech interface systems apply vocoders to generate the speech signal directly from articulatory data. Most of these approaches concentrate on estimating just the spectral features of the vocoder, and use the original F0, a constant F0 or white noise as excitation. This solution is based on the assumption that the F0 curve is unpredictable from articulatory data that does not contain direct measurements of the vocal fold vibration. Here, we experimented with deep neural networks to perform articulatory-to-acoustic conversion from ultrasound images, with an emphasis on estimating the voicing feature and the F0 curve from the ultrasound input. Contrary to the common belief that F0 is unpredictable, we attained a correlation rate of 0.74 between the original and the predicted F0 curve. What is more, the listening tests revealed that our subjects could not distinguish the sentences synthesized using the DNN-estimated and the original F0 curve, and ranked them as having the same quality.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Grosz, Tamas; Gosztolya, Gabor] MTA SZTE Res Grp Artificial
   Intelligence, Szeged, Hungary.
   <br>[Toth, Laszlo] Univ Szeged, Inst Informat, Szeged, Hungary.
   <br>[Csapo, Tamas Gabor] Budapest Univ Technol &amp; Econ, Dept Telecommun &amp;
   Media Informat, Budapest, Hungary.
   <br>[Marko, Alexandra] Eotvos Lorand Univ, Dept Phonet, Budapest, Hungary.
   <br>[Csapo, Tamas Gabor; Marko, Alexandra] MTA ELTE Lendulet Lingual
   Articulat Res Grp, Budapest, Hungary.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Grosz, T (reprint author), MTA SZTE Res Grp Artificial Intelligence, Szeged, Hungary.</td>
</tr>

<tr>
<td valign="top">EM </td><td>groszt@inf.u-szeged.hu; ggabor@inf.u-szeged.hu; tothl@inf.u-szeged.hu;
   csapot@tmit.bme.hu; marko.alexandra@btk.elte.hu</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>291</td>
</tr>

<tr>
<td valign="top">EP </td><td>295</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000446384600059</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Jiao, YS
   <br>Tu, M
   <br>Berisha, V
   <br>Liss, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Jiao, Yishan
   <br>Tu, Ming
   <br>Berisha, Visar
   <br>Liss, Julie</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>SIMULATING DYSARTHRIC SPEECH FOR TRAINING DATA AUGMENTATION IN CLINICAL
   SPEECH APPLICATIONS</td>
</tr>

<tr>
<td valign="top">SO </td><td>2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech and Signal Processing
   (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 15-20, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Calgary, CANADA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Dysarthric speech; voice conversion; adversarial training; data
   augmentation</td>
</tr>

<tr>
<td valign="top">ID </td><td>INTELLIGIBILITY; CLASSIFICATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Training machine learning algorithms for speech applications requires large, labeled training data sets. This is problematic for clinical applications where obtaining such data is prohibitively expensive because of privacy concerns or lack of access. As a result, clinical speech applications typically rely on small data sets with only tens of speakers. In this paper, we propose a method for simulating training data for clinical applications by transforming healthy speech to dysarthric speech using adversarial training We evaluate the efficacy of our approach using both objective and subjective criteria. We present the transformed samples to five experienced speech-language pathologists (SLPs) and ask them to identify the samples as healthy or dysarthric. The results reveal that the SLPs identify the transformed speech as dysarthric 65% of the time. In a pilot classification experiment, we show that by using the simulated speech samples to balance an existing dataset, the classification accuracy improves by similar to 10% after data augmentation.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Jiao, Yishan; Tu, Ming; Berisha, Visar; Liss, Julie] Arizona State
   Univ, Dept Speech &amp; Hearing Sci, Tempe, AZ 85287 USA.
   <br>[Berisha, Visar] Arizona State Univ, Sch Elect Comp &amp; Energy Engn,
   Tempe, AZ 85287 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Jiao, YS (reprint author), Arizona State Univ, Dept Speech &amp; Hearing Sci, Tempe, AZ 85287 USA.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">BP </td><td>6009</td>
</tr>

<tr>
<td valign="top">EP </td><td>6013</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000446384606034</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Li, JY
   <br>Shi, RZ
   <br>Cheng, CM
   <br>Li, SC</td>
</tr>

<tr>
<td valign="top">AF </td><td>Li, Jiayao
   <br>Shi, Ruizhi
   <br>Cheng, Chunming
   <br>Li, Sicong</td>
</tr>

<tr>
<td valign="top">TI </td><td>Roaming Path Generation Algorithm and Optimization Based On Bezier Curve</td>
</tr>

<tr>
<td valign="top">SO </td><td>IFAC PAPERSONLINE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>6th International-Federation-of-Automatic-Control (IFAC) Conference on
   Bio-Robotics (BIOROBOTICS)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUL 13-15, 2018</td>
</tr>

<tr>
<td valign="top">CL </td><td>Beijing, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Bezier curve; roaming algorithm; camera control; scene roaming</td>
</tr>

<tr>
<td valign="top">AB </td><td>Camera roaming plays an extremely important role in constructing various roaming scenarios for virtual systems. However, the existing path (camera) roaming algorithm has the disadvantages of excessive human participation in the roaming path generation process, unpredictable roaming effect, unsmart camera conversion, and mismatched roaming camera and commentary. Therefore, a convenient and automatic path roaming algorithm is proposed. At the same time, the intelligent conversion of the camera is implemented. Based on this, optimization is performed to enable smart matching between the camera and the commentary audio and narration text. First, the key model in the roaming scene is marked to obtain its spatial position; Then the Bessel curve generation principle is studied, and Bessel splines are fitted to the key points according to the mathematical model of the Bezier curve; And then the camera movement is invoked, and the function implements roaming. Finally, the algorithm is optimized to achieve intelligent matching of voice subtitles and pictures. In this way, the automation and intelligence of the roaming path are realized. (C) 2018, IFAC (International Federation of Automatic Control) Hosting by Elsevier Ltd. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Li, Jiayao; Shi, Ruizhi; Cheng, Chunming; Li, Sicong] China Agr Univ,
   Coll Informat &amp; Elect Engn, Beijing 100083, Peoples R China.
   <br>[Shi, Ruizhi] Minist Agr, Key Lab Agr Informat Acquisit Technol, Beijing
   100083, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Shi, RZ (reprint author), China Agr Univ, Coll Informat &amp; Elect Engn, Beijing 100083, Peoples R China.; Shi, RZ (reprint author), Minist Agr, Key Lab Agr Informat Acquisit Technol, Beijing 100083, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>Jiayao_Lee@cau.edu.cn; sunruizhi@cau.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">VL </td><td>51</td>
</tr>

<tr>
<td valign="top">IS </td><td>17</td>
</tr>

<tr>
<td valign="top">BP </td><td>339</td>
</tr>

<tr>
<td valign="top">EP </td><td>345</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.ifacol.2018.08.201</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000444516000062</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Lorensen, MR
   <br>Buch-Hansen, G</td>
</tr>

<tr>
<td valign="top">AF </td><td>Lorensen, Marlene Ringgaard
   <br>Buch-Hansen, Gitte</td>
</tr>

<tr>
<td valign="top">TI </td><td>Listening to the voices: refugees as co-authors of practical theology</td>
</tr>

<tr>
<td valign="top">SO </td><td>PRACTICAL THEOLOGY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Ecclesiology; conversion; refugees; Eucharist; liminality; social
   capital</td>
</tr>

<tr>
<td valign="top">AB </td><td>Based on participant observation and interviews with Middle-Eastern asylum seekers, we describe the complexity of motives involved in conversions from Islam to Christianity. As a primary case study, we have selected a young Iranian woman because she manages to describe the liminal situation of living 'underground' which tends to leave most asylum seekers speechless. Through a revision of Bourdieu's theory of social capital, we illustrate how conversion can become a means of existential survival in a situation of social marginalisation and psychological liminality. We regard the Iranian woman as a co-interpreter of practical theology because in her testimony we hear echoes of Pauline participation theology and the radical sacramental realism found in Augustine's interpretation of the Eucharist. Finally, we demonstrate how the presence of refugees in the congregation has nudged the ethnically Danish 'hosts' to move away from a hierarchy of generosity to a community based on reciprocity and mutual vulnerability.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Lorensen, Marlene Ringgaard; Buch-Hansen, Gitte] Univ Copenhagen, Fac
   Theol, Karen Blixens Plads 16, DK-2300 Copenhagen S, Denmark.
   <br>[Buch-Hansen, Gitte] Univ Copenhagen, Fac Theol, Bibl Exegesis Sect,
   Copenhagen S, Denmark.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Lorensen, MR (reprint author), Univ Copenhagen, Fac Theol, Karen Blixens Plads 16, DK-2300 Copenhagen S, Denmark.</td>
</tr>

<tr>
<td valign="top">EM </td><td>Mrl@teol.ku.dk</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">VL </td><td>11</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>29</td>
</tr>

<tr>
<td valign="top">EP </td><td>41</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1080/1756073X.2017.1415577</td>
</tr>

<tr>
<td valign="top">SC </td><td>Religion</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000435467400004</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Martinez, C</td>
</tr>

<tr>
<td valign="top">AF </td><td>Martinez, Chloe</td>
</tr>

<tr>
<td valign="top">TI </td><td>The Autobiographical Pose: Life Narrative and Religious Transformation
   in the Mirabai Tradition</td>
</tr>

<tr>
<td valign="top">SO </td><td>SOUTH ASIA-JOURNAL OF SOUTH ASIAN STUDIES</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Autobiography; bhakti; conversion; India; life narrative; Mirabai;
   poetry; religion; religious transformation; South Asia</td>
</tr>

<tr>
<td valign="top">AB </td><td>The literary corpus of the sixteenth-century North Indian bhakti poet-saint Mirabai has grown over time as devotees have used (and continue to use) her name, life story and first-person voice in poems. Drawing on hagiographies, written and oral poems, printed collections and performative engagements with Mira, I argue that these moments of autobiographical posing' reveal autobiography as powerful for speaking about religious transformation, in particular the issues of authority, experience and critique. Furthermore, the centrality of autobiographical speech in the tradition is linked to an increasing emphasis on Mira as a figure of religious transformation, and bhakti itself as a transformative path.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Martinez, Chloe] Claremont Mckenna Coll, Dept Religious Studies,
   Claremont, CA 91711 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Martinez, C (reprint author), Claremont Mckenna Coll, Dept Religious Studies, Claremont, CA 91711 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>chloeamartinez@gmail.com</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Martinez, Chloe</display_name>&nbsp;</font></td><td><font size="3">0000-0003-3104-0710&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">VL </td><td>41</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>418</td>
</tr>

<tr>
<td valign="top">EP </td><td>434</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1080/00856401.2018.1443240</td>
</tr>

<tr>
<td valign="top">SC </td><td>Area Studies; History; Asian Studies</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000432639400011</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Bouton-Touboulic, AI</td>
</tr>

<tr>
<td valign="top">AF </td><td>Bouton-Touboulic, Anne-Isabelle</td>
</tr>

<tr>
<td valign="top">TI </td><td>Body Language in Augustine's Confessiones and De doctrina christiana</td>
</tr>

<tr>
<td valign="top">SO </td><td>AUGUSTINIAN STUDIES</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">AB </td><td>This article examines the role of bodily expressions within Augustine's theory of signs and language. Philosophical reflection, rhetorical practice, and his own homiletical experience all led Augustine to consider the role played by the body in communicative acts. The invesitgation is sharpened via careful analysis of the rhetorical category of actio and close readings of particular passages that are relevant for Augustine's understanding of the process of learning language in general and of learning the catechism in particular. The centrality of bodiy signs for the dramatization of the famous scene of Augustine's conversion in the Milanese garden is also discussed: here, voice and physiognomy express the tragedy of the will, even as bodily signs (taken as natural signs) prove crucial to Augustine's particular retelling of the story.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Bouton-Touboulic, Anne-Isabelle] Univ Lille, Lille, France.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Bouton-Touboulic, AI (reprint author), Univ Lille, Lille, France.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2018</td>
</tr>

<tr>
<td valign="top">VL </td><td>49</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>1</td>
</tr>

<tr>
<td valign="top">EP </td><td>23</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.5840/augstudies20175329</td>
</tr>

<tr>
<td valign="top">SC </td><td>History; Philosophy; Religion</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000426247100001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sakellariou, E
   <br>Karantinou, K
   <br>Goffin, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sakellariou, Evy
   <br>Karantinou, Kalipso
   <br>Goffin, Keith</td>
</tr>

<tr>
<td valign="top">TI </td><td>"Telling tales": Stories, metaphors and tacit knowledge at the fuzzy
   front-end of NPD</td>
</tr>

<tr>
<td valign="top">SO </td><td>CREATIVITY AND INNOVATION MANAGEMENT</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">ID </td><td>PRODUCT DEVELOPMENT; CUSTOMER; INNOVATION; CREATION; INFORMATION;
   INTEGRATION; CONSUMERS; SUCCESS; VOICE</td>
</tr>

<tr>
<td valign="top">AB </td><td>Breakthrough ideas depend upon the generation of new knowledge, which emerge from the conversion of tacit knowledge at the fuzzy front-end (FFE) of new product development (NPD). The occurrence of stories and metaphors has been strongly linked to tacit knowledge, however, empirical studies that examine how stories and metaphors harness tacit knowledge in the FFE are lacking. In addition, how managers can use stories and metaphors to develop breakthrough product ideas is unclear. To address these gaps, an in-situ' empirical case study was conducted in the European subsidiary of a B2C multinational. This study investigated the role, characteristics and interplay of stories and metaphors emerging in discussions between managers and customers in a collaborative design workshop (CDW). Taking a knowledge management theoretical perspective, the data were analyzed using the well-known SECI framework. The results clearly show that significant new knowledge was created based on the discussions in a CDW and stories and metaphors were important mechanisms for this. Importantly, it was stories related to product usage that triggered breakthrough ideas. The study extends the understanding of how breakthrough ideas emerge; it proposes a tentative conceptual framework; and it provides managers with recommendations of how to use stories and metaphors effectively during the FFE.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Sakellariou, Evy] Amer Coll Greece, 6 Gravias Str Agia Paraskevi,
   Athens 15342, Greece.
   <br>[Karantinou, Kalipso] Athens Univ Econ &amp; Business, Athens, Greece.
   <br>[Goffin, Keith] Cranfield Sch Management, Cranfield, Beds, England.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sakellariou, E (reprint author), Amer Coll Greece, 6 Gravias Str Agia Paraskevi, Athens 15342, Greece.</td>
</tr>

<tr>
<td valign="top">EM </td><td>evy.sakellariou@gmail.com; k.goffin@cranfield.ac.uk</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>DEC</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>26</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">BP </td><td>353</td>
</tr>

<tr>
<td valign="top">EP </td><td>369</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1111/caim.12237</td>
</tr>

<tr>
<td valign="top">SC </td><td>Business &amp; Economics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000416146800004</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Herzig, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Herzig, Tamar</td>
</tr>

<tr>
<td valign="top">TI </td><td>"For the Salvation of This Girl's Soul": Nuns as Converters of Jews in
   Early Modern Italy</td>
</tr>

<tr>
<td valign="top">SO </td><td>RELIGIONS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>conversion; nuns; monastic enclosure; Council of Trent; Tridentine
   reforms; Jewish-Christian relations; Houses of Catechumens; forced
   baptism; Early Modern Italy; convent education</td>
</tr>

<tr>
<td valign="top">AB </td><td>This article argues that converting Jewish girls and women constituted an important expression of Italian nuns' religiosity throughout the age of Catholic Reform. Unlike their male counterparts, however, converting nuns rarely left behind accounts of their conversionary efforts. Moreover, since these endeavors were directed exclusively at female Jews they are often obscured in the historical record and in modern historiography. The article tackles the difficulties of recovering the voices of converting nuns and presents examples that suggest how they could be circumvented. Exploring the potential of drawing on previously understudied texts, such as nuns' supplications, the article calls for the integration of this specific manifestation of female devotion into the scholarship and teaching on women's religious life in the early modern era.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Herzig, Tamar] Tel Aviv Univ, Dept Hist, IL-6997801 Tel Aviv, Israel.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Herzig, T (reprint author), Tel Aviv Univ, Dept Hist, IL-6997801 Tel Aviv, Israel.</td>
</tr>

<tr>
<td valign="top">EM </td><td>therzig@post.tau.ac.il</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>NOV</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>8</td>
</tr>

<tr>
<td valign="top">IS </td><td>11</td>
</tr>

<tr>
<td valign="top">AR </td><td>252</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.3390/rel8110252</td>
</tr>

<tr>
<td valign="top">SC </td><td>Religion</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000416586200019</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Tanaka, K
   <br>Toda, T
   <br>Nakamura, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Tanaka, Kou
   <br>Toda, Tomoki
   <br>Nakamura, Satoshi</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Vibration Control Method of an Electrolarynx Based on Statistical F-0
   Pattern Prediction</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEICE TRANSACTIONS ON INFORMATION AND SYSTEMS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>laryngectomee; electrolarynx; voice restoration; speech enhancement;
   statistical F-0 pattern prediction</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION; SPEECH ENHANCEMENT; GENERATION; FREQUENCY</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a novel speaking aid system to help laryngectomees produce more naturally sounding electrolaryngeal (EL) speech. An electrolarynx is an external device to generate excitation signals, instead of vibration of the vocal folds. Although the conventional EL speech is quite intelligible, its naturalness suffers from the unnatural fundamental frequency (F-0) patterns of the mechanically generated excitation signals. To improve the naturalness of EL speech, we have proposed EL speech enhancement methods using statistical F-0 pattern prediction. In these methods, the original EL speech recorded by a microphone is presented from a loudspeaker after performing the speech enhancement. These methods are effective for some situation, such as telecommunication, but it is not suitable for face-to-face conversation because not only the enhanced EL speech but also the original EL speech is presented to listeners. In this paper, to develop an EL speech enhancement also effective for face-to-face conversation, we propose a method for directly controlling F-0 patterns of the excitation signals to be generated from the electrolarynx using the statistical F-0 prediction. To get an "actual feel" of the proposed system, we also implement a prototype system. By using the prototype system, we find latency issues caused by a real-time processing. To address these latency issues, we furthermore propose segmental continuous F-0 pattern modeling and forthcoming F-0 pattern modeling. With evaluations through simulation, we demonstrate that our proposed system is capable of effectively addressing the issues of latency and those of electrolarynx in term of the naturalness.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Tanaka, Kou; Nakamura, Satoshi] Nara Inst Sci &amp; Technol NAIST, Ikoma
   6300192, Japan.
   <br>[Toda, Tomoki] Nagoya Univ, Informat Technol Ctr, Nagoya, Aichi 4640814,
   Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Tanaka, K (reprint author), Nara Inst Sci &amp; Technol NAIST, Ikoma 6300192, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ko-t@is.naist.jp; tomoki@icts.nagoya-u.ac.jp; s-nakamura@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>SEP</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>E100D</td>
</tr>

<tr>
<td valign="top">IS </td><td>9</td>
</tr>

<tr>
<td valign="top">BP </td><td>2165</td>
</tr>

<tr>
<td valign="top">EP </td><td>2173</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1587/transinf.2016EDP7485</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000410765400026</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Joshi, S
   <br>Sensarma, P</td>
</tr>

<tr>
<td valign="top">AF </td><td>Joshi, Sridhar
   <br>Sensarma, Parthasarathi</td>
</tr>

<tr>
<td valign="top">TI </td><td>Hybrid controller for mid-power audio application</td>
</tr>

<tr>
<td valign="top">SO </td><td>IET POWER ELECTRONICS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>loudspeakers; audio-frequency amplifiers; audio signal processing;
   transfer functions; continuous systems; discrete systems; variable
   structure systems; nonlinear control systems; least squares
   approximations; linear systems; control system synthesis; voltage-mode
   algorithms; power 20 W; resistance 4 ohm; electric signal conversion;
   acoustic signal conversion; frequency response data; least-square
   approximation; transfer function; speaker current; linear controller;
   filter size constraints; switching frequency; nonlinear control law;
   SMCA; sliding-mode controlled amplifier; zero-lag properties; ideal
   constant gain; transfer characteristics; output filter; phase lag;
   speaker voice coil electrical impedance; reference audio signal;
   nonideal transfer characteristic; switched voltage-mode audio amplifier;
   moving coil loudspeaker; mid-power audio application; hybrid controller</td>
</tr>

<tr>
<td valign="top">ID </td><td>AMPLIFIER; FEEDBACK; VOLTAGE</td>
</tr>

<tr>
<td valign="top">AB </td><td>Driving current of a moving coil loudspeaker, fed from a switched voltage-mode audio amplifier, display non-ideal transfer characteristic relative to the reference audio signal. This is chiefly due to the effective speaker voice coil electrical impedance and phase lag provided by the output filter. This study investigates the deviation in the transfer characteristics from the ideal constant gain and zero-lag properties and proposes a hybrid controller to mitigate the drawback. The hybrid controller comprises a sliding-mode controlled amplifier (SMCA) which has a non-linear control law and ensures maximal command following among voltage-mode algorithms, realisable under switching frequency and filter size constraints. Additionally, a linear controller fine-tunes the speaker current and its design is based on the transfer function of the SMCA and loudspeaker, which are least-square approximated from experimental frequency response data. A 20W amplifier, driving a 4 speaker load, is developed and experimental results obtained there from establishing the improved electric to acoustic signal conversion using the hybrid controller.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Joshi, Sridhar; Sensarma, Parthasarathi] Indian Inst Technol, Dept
   Elect Engn, ACES 103, Kanpur 208016, Uttar Pradesh, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Joshi, S (reprint author), Indian Inst Technol, Dept Elect Engn, ACES 103, Kanpur 208016, Uttar Pradesh, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>sjoshi@iitk.ac.in</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Joshi, Sridhar</display_name>&nbsp;</font></td><td><font size="3">I-8576-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Joshi, Sridhar</display_name>&nbsp;</font></td><td><font size="3">0000-0002-5210-1773&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>AUG</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>10</td>
</tr>

<tr>
<td valign="top">IS </td><td>10</td>
</tr>

<tr>
<td valign="top">BP </td><td>1200</td>
</tr>

<tr>
<td valign="top">EP </td><td>1207</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1049/iet-pel.2017.0015</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000407018700011</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wen, D
   <br>Zhang, XT
   <br>Wan, J
   <br>Fu, J
   <br>Lei, JB</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wen, Dong
   <br>Zhang, Xingting
   <br>Wan, Jie
   <br>Fu, Jing
   <br>Lei, Jianbo</td>
</tr>

<tr>
<td valign="top">TI </td><td>The challenges of emerging HISs in bridging the communication gaps among
   physicians and nurses in China: an interview study</td>
</tr>

<tr>
<td valign="top">SO </td><td>BMC MEDICAL INFORMATICS AND DECISION MAKING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Physician-nurse communication; Information needs; Communication models;
   Hospital communication systems</td>
</tr>

<tr>
<td valign="top">ID </td><td>INFORMATION NEEDS; PRIMARY-CARE; SEEKING; BURNOUT</td>
</tr>

<tr>
<td valign="top">AB </td><td>Background: To explore the current situation, existing problems and possible causes of said problems with regards to physician-nurse communication under an environment of increasingly widespread usage of Hospital Information Systems and to seek out new potential strategies in information technology to improve physician-nurse communication.
   <br>Methods: Semi-structured interviews were conducted with 20 physicians and nurses in five leading tertiary grade A hospitals in Beijing, China (two physicians and two nurses in each hospital). The interviews primarily included three aspects comprising the current situation and problems of clinical physician-nurse communication, the application and problems of Hospital Information Systems, and assessments on the improvement of physician-nurse communication through the usage of information technology. The inductive conventional content analysis approach was employed.
   <br>Results: (1) Physicians and nurses are generally quite satisfied with the current situation of communication. However, the information needs of nurses are prone to being overlooked, and the communication methods are primarily synchronous communication such as face-to-face and phone communication. (2) Hospital Information Systems are gradually being used for physician-nurse communication; in the meantime, physicians and nurses face challenges with regards to the improvement of physician-nurse communication through the usage of information technology. Challenges differ based on the different stages of using the system and the different levels of understanding of physicians and nurses towards information technology. Their dissatisfaction mainly deals with system errors and the level of convenience in using the system. (3) In-depth interviews found that in general, physicians and nurses have a strong interest and trust in improving physician-nurse communication through appropriate information technology, e.g., communication methods such as information reminders for physicians and nurses through mobile devices and instant voice-to-text conversion methods.
   <br>Conclusions: There are objective risks in physician-nurse communication in Chinese hospitals, and clinical information systems lack solutions to the relevant problems. Developing a dedicated, mobile, quick and convenient module for physician-nurse communication within existing hospital information system with automatic reminders for important information that segregates between synchronous and asynchronous communication according to the different types of information could help improve physician-nurse communication.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Wen, Dong; Zhang, Xingting] Peking Univ, Hosp 3, 49 North Garden Rd,
   Beijing 100191, Peoples R China.
   <br>[Wen, Dong; Zhang, Xingting; Lei, Jianbo] Peking Univ, Ctr Med Informat,
   38 Xueyuan Rd, Beijing 100191, Peoples R China.
   <br>[Wan, Jie; Fu, Jing] Southwest Med Univ, Sch Nursing, 319 Zhongshan Rd,
   Luzhou City, Sichuan, Peoples R China.
   <br>[Lei, Jianbo] Southwest Med Univ, Sch Med Informat &amp; Engn, 319 Zhongshan
   Rd, Luzhou City 646000, Sichuan Provinc, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Lei, JB (reprint author), Peking Univ, Ctr Med Informat, 38 Xueyuan Rd, Beijing 100191, Peoples R China.; Lei, JB (reprint author), Southwest Med Univ, Sch Med Informat &amp; Engn, 319 Zhongshan Rd, Luzhou City 646000, Sichuan Provinc, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jblei@hsc.pku.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUN 12</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>17</td>
</tr>

<tr>
<td valign="top">AR </td><td>85</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1186/s12911-017-0473-x</td>
</tr>

<tr>
<td valign="top">SC </td><td>Medical Informatics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000403875600002</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Quiroa, NI</td>
</tr>

<tr>
<td valign="top">AF </td><td>Quiroa, Nestor I.</td>
</tr>

<tr>
<td valign="top">TI </td><td>Friar Francisco Ximenez and the Popol Vuh: From Religious Treatise to a
   Digital Sacred Book</td>
</tr>

<tr>
<td valign="top">SO </td><td>ETHNOHISTORY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Popol Wuj; Digitization; Francisco Ximenez; Maya-K'iche'</td>
</tr>

<tr>
<td valign="top">ID </td><td>MAYA</td>
</tr>

<tr>
<td valign="top">AB </td><td>The Popol Vuh represents the most studied indigenous text of Mesoamerica, with its breadth of detail concerning Maya worldview, religion, cosmology, and society. Regarded as an ethnohistorical treasure, this narrative has been read exclusively as a freestanding, self-contained text to inquire into a history far removed from when it was actually created. Consequently, the colonial context of the text itself has been omitted, including the central role of Dominican friar Francisco Ximenez as transcriber and translator of the only available copy. This article reframes the Popol Vuh within its historical and physical ecclesiastic context, recovering Friar Ximenez's voice within his manuscript. It is argued that his work was first and foremost intended to be a religious treatise to carry out the conversion of the K'iche' to Christianity. This study offers an alternative, holistic understanding of Friar Ximenez's manuscript by tracing two historical moments that contributed to the transformation of the Popol Vuh narrative from a religious treatise into what it is now in the prevailing view, an ancient text or "sacred book."</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Quiroa, Nestor I.] Wheaton Coll, Wheaton, IL 60187 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Quiroa, NI (reprint author), Wheaton Coll, Wheaton, IL 60187 USA.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>APR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>64</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>241</td>
</tr>

<tr>
<td valign="top">EP </td><td>270</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1215/00141801-3789145</td>
</tr>

<tr>
<td valign="top">SC </td><td>Anthropology; History</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000399256300004</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Clifton-Soderstrom, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Clifton-Soderstrom, Michelle</td>
</tr>

<tr>
<td valign="top">TI </td><td>Common Sense, Plain Sense, and Faithful Dissent: Evangelical
   Interpretation and the Ethics of Marriage Equality</td>
</tr>

<tr>
<td valign="top">SO </td><td>JOURNAL OF THE SOCIETY OF CHRISTIAN ETHICS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article; Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Annual Meeting of the Society-of-Christian-Ethics</td>
</tr>

<tr>
<td valign="top">CY </td><td>JAN, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Toronto, CANADA</td>
</tr>

<tr>
<td valign="top">AB </td><td>This essay constructs a moral theology of reading scripture by retrieving habits and virtues of early evangelical readers that have potential to help evangelicals reengage one another on divisive topics such as marriage equality. I use this moral theology of reading scripture to diagnose power dynamics operative in commonsense assumptions around gender and plain-sense interpretive frameworks that privilege literal interpretations. I interact with the historical trajectory of evangelicalism that values conversion as the telos of reading scripture. Continental Pietists also read in a spirit of faithful dissent, allowing them to cast the interpretive net widely, to welcome new readers, and to challenge social barriers that excluded marginalized voices. Faithful dissent is a necessary habit that generates courage to be open to the Spirit's work of blessing and renewal in the world.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Clifton-Soderstrom, Michelle] North Pk Theol Seminary, Theol &amp; Eth,
   3225 W Foster Ave, Chicago, IL 60625 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Clifton-Soderstrom, M (reprint author), North Pk Theol Seminary, Theol &amp; Eth, 3225 W Foster Ave, Chicago, IL 60625 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>mclifton-soderstrom@northpark.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>SPR-SUM</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>37</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>101</td>
</tr>

<tr>
<td valign="top">EP </td><td>117</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1353/sce.2017.0001</td>
</tr>

<tr>
<td valign="top">SC </td><td>Religion</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000415880300007</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Pannofino, N
   <br>Cardano, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Pannofino, Nicola
   <br>Cardano, Mario</td>
</tr>

<tr>
<td valign="top">TI </td><td>Exes Speak Out, Narratives of Apostasy: Jehovah's Witnesses, Scientology
   and Soka Gakkai</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERNATIONAL JOURNAL FOR THE STUDY OF NEW RELIGIONS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>apostasy; narrative; Jehovah's Witnesses; Soka Gakkai; Scientology</td>
</tr>

<tr>
<td valign="top">ID </td><td>RELIGIOUS CONVERSION; EXIT NARRATIVES; IDENTITY; ORGANIZATIONS;
   SOCIOLOGY</td>
</tr>

<tr>
<td valign="top">AB </td><td>The paper presents a study of the trajectories of apostasy from three religious movements, Jehovah's Witnesses, the Soka Gakkai Buddhist institute and the Church of Scientology, through the analysis of a body of autobiographical narratives posted online by Italian apostates. Even more than being the account of a past religious experience, these narratives are the last stage in the gradual articulation of a voice with which the disaffected believers publicly express a critical view of the organizations they have left, charging them with using practices of interdiction to prevent dissent by their members. The common theme that emerges from these stories is not the loss of faith, but the discovery of a hidden deception, the breach of the implicit pact of trust that bound the narrator to the religious group.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Pannofino, Nicola; Cardano, Mario] Univ Turin, Turin, Italy.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Pannofino, N (reprint author), Univ Turin, Turin, Italy.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>8</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>1</td>
</tr>

<tr>
<td valign="top">EP </td><td>26</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1558/ijsnr.34152</td>
</tr>

<tr>
<td valign="top">SC </td><td>Religion</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000442162700001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Fu, YL
   <br>Ding, ZZ
   <br>Wang, DL</td>
</tr>

<tr>
<td valign="top">AF </td><td>Fu, Yinling
   <br>Ding, Zhizhong
   <br>Wang, Dingliang</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>A New Type of Portable MANET Terminal with Two Modes of CSMA and SOTDMA</td>
</tr>

<tr>
<td valign="top">SO </td><td>PROCEEDINGS OF 2017 3RD IEEE INTERNATIONAL CONFERENCE ON COMPUTER AND
   COMMUNICATIONS (ICCC)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>3rd IEEE International Conference on Computer and Communications (ICCC)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 13-16, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Chengdu, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Ad-hoc network; MANET; CSMA; TDMA; SOTDMA; IEEE 802.11; emergency
   communication; DES; MD5</td>
</tr>

<tr>
<td valign="top">ID </td><td>AD HOC NETWORKS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Mobile Ad-hoc Network (MANET) is a new multi hop, temporary, self-organizing network composed of wireless mobile nodes. As a flexible and infrastructure-less network, it has great potential applications in mobile communication. However, contention-based MAC protocols in MANET such as Carrier Sense Multiple Access with Collision Avoidance (CSMA/CA) can't ensure a reliable channel transmission because of the access collisions. Although slot-based MAC protocols such as Time Division Multiple Access (TDMA) can solve the collisions' problem, the network throughput rate of TDMA is less than CSMA/CA with small nodes. In this paper, the design and implementation of a portable MANET communication terminal are presented, which can support both CSMA and Self-Organized Time Division Multiple Access (SOTDMA) in order to take the advantages of CSMA and Time Division Multiple Address (TDMA) in different scenarios. In addition to the common functionality of text, voice and video transmission, it also provides emergency and military application oriented functions, such as network switching, situation awareness, data encryption, time synchronization, text-to-speech conversion, and network authentication and so on.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Fu, Yinling; Ding, Zhizhong] Hefei Univ Technol, Inst Commun &amp; Informat
   Syst, Hefei, Anhui, Peoples R China.
   <br>[Wang, Dingliang] Huawei Technol Co Ltd, Shanghai, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Fu, YL (reprint author), Hefei Univ Technol, Inst Commun &amp; Informat Syst, Hefei, Anhui, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>fyl824824@163.com; zzding@hfut.edu.cn; wangding1894@qq.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>443</td>
</tr>

<tr>
<td valign="top">EP </td><td>452</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000440623600084</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Reyes, CG</td>
</tr>

<tr>
<td valign="top">AF </td><td>Gutierrez Reyes, Cintia</td>
</tr>

<tr>
<td valign="top">BE </td><td>Perez, EM
   <br>Martinez, E
   <br>DosSantos, FCS</td>
</tr>

<tr>
<td valign="top">TI </td><td>The aesthetics of silence: Juan Munoz and the schizophonia</td>
</tr>

<tr>
<td valign="top">SO </td><td>GLOCAL: CODIFICAR, MEDIAR, TRANSFORMAR, VIVIR</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>3rd International Congress of Visual Arts Research (ANIAV)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUL 06-07, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Valencia, SPAIN</td>
</tr>

<tr>
<td valign="top">DE </td><td>Silence; postmodernism; schizophonia; panophonia; Juan Munoz</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper is intended to analyze the silence as an intrinsic aesthetic category postmodernist artistic approaches. Basis for this to poetic of a "rhizome" speech, where the work, material or immaterial no longer has a single reading, there is not a clear relationship between the signifier and the signified therefore it remains open polysemy that invites the recipient to set up your own way of approaching artistic experience. An experience that appears as a system that will facilitate its recognition in gossiping.
   <br>Faced with these premises an empty room extends as meditation area for a Faustian experience, or as Calvin says, "as ordeal culminating in the conquest of the right to speak" [1]. However, the dialogue established body is orphaned, and a voice in which we distinguish the source appears. Therefore, we will establish what types of silence are emerging between the approaches of contemporary art: schizophonic, panophonic or conversion.
   <br>We will approach specifically to the work of Juan Munoz, as an example guarantor of a kind of silence where the estrangement stands as a bulwark in our own thoughts are held. To conclude as Cage said, that silence comes with its own devastating irony, "the silence is to have an empty space time and let it work with its magnetic property" [2].</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Gutierrez Reyes, Cintia] Univ Malaga, Dept Hist Arte, Grp I D I Pract
   Subjetividad Artes Contemporneas, Malaga, Spain.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Reyes, CG (reprint author), Univ Malaga, Dept Hist Arte, Grp I D I Pract Subjetividad Artes Contemporneas, Malaga, Spain.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>326</td>
</tr>

<tr>
<td valign="top">EP </td><td>331</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.4995/ANIAV.2017.5755</td>
</tr>

<tr>
<td valign="top">SC </td><td>Art</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000430606200064</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kumar, MSP
   <br>Lathasree, V
   <br>Karishma, SN</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kumar, Malladi Sai Phani
   <br>Lathasree, Veerapalli
   <br>Karishma, S. N.</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Novel Contour Based Detection and GrabCut Segmentation for Sign Language
   Recognition</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 2ND IEEE INTERNATIONAL CONFERENCE ON WIRELESS COMMUNICATIONS,
   SIGNAL PROCESSING AND NETWORKING (WISPNET)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>2nd IEEE International Conference on Wireless Communications, Signal
   Processing and Networking (WiSPNET)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 22-24, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Chennai, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>complex background; contour based detection; GrabCut algorithm; gesture
   segmentation; American Sign Language</td>
</tr>

<tr>
<td valign="top">ID </td><td>MODEL</td>
</tr>

<tr>
<td valign="top">AB </td><td>The main intention of this paper is to build an automatic computer aided hand gesture to voice conversion system for people suffering from Aphonia, a medical term for speech impairment. We have assumed the input gesture images given to the system as simple and complex depending on the background of the image. A novel contour based image segmentation algorithm is proposed in this paper to detect the boundary of the foreground from images with simple dark background. The traditional GrabCut algorithm is employed for segmentation of foreground from images with complex background. This algorithm iteratively segments the image to extract the foreground accurately. The American Sign Language (ASL) 26 finger-spelled alphabet images are taken as the dataset for the two above mentioned algorithms. For the dataset that we have generated, it is observed from the results that contour based segmentation algorithm provides absolutely perfect results. The number of iterations required for GrabCut algorithm to segment the foreground may vary from image to image depending on the background. Out of all the 26 gesture of alphabets, Q, R and S need 6 number of iterations at maximum. A minimum of 1 iteration is required for alphabets E, J and O. On an average, 3 iterations of GrabCut algorithm is required to completely segment the foreground from images with complex background.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kumar, Malladi Sai Phani] IIT Kharagpur, ATDC, Kharagpur 721302, W
   Bengal, India.
   <br>[Lathasree, Veerapalli] Infosys Ltd, Hyderabad 500045, Andhra Prades,
   India.
   <br>[Karishma, S. N.] IIT Kharagpur, Dept E&amp;ECE, Kharagpur 721302, W Bengal,
   India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kumar, MSP (reprint author), IIT Kharagpur, ATDC, Kharagpur 721302, W Bengal, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>saiphani.malladi@gmail.com; lathasreeveerapalli@gmail.com;
   karishma.rgukt@gmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>738</td>
</tr>

<tr>
<td valign="top">EP </td><td>742</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000428513600131</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Gharat, M
   <br>Patanwala, R
   <br>Ganaparthi, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Gharat, Madhura
   <br>Patanwala, Rizwan
   <br>Ganaparthi, Adithi</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Audio guidance system for blind</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 INTERNATIONAL CONFERENCE OF ELECTRONICS, COMMUNICATION AND
   AEROSPACE TECHNOLOGY (ICECA), VOL 1</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International conference of Electronics, Communication and Aerospace
   Technology (ICECA)</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 20-22, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Coimbatore, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Radio Frequency Identification (RFID); Ultrasonic sensor; Raspberry Pi;
   speech recognition</td>
</tr>

<tr>
<td valign="top">AB </td><td>With over 280 million visually impaired people worldwide, there is a crucial need for an assistive device that allows the blind people to navigate freely. Location information for indoor environment is limited. Radio Frequency Identification (RFID) tags is an effective way of giving location information to the users. We propose a RFID based system for independent navigation in a building for blind or visually impaired people. The conversion of speech to text is carried out using speech recognition software modules. This system is initiated by providing a voice command, and specifying the destination to be reached by the blind person. This navigation system will guide the blind person along the path by providing audio navigation assistance to reach the desired destination. To avoid collision ultrasonic sensor will be interfaced with the Raspberry Pi. By implementing the above technique, blind people can navigate independently and they can acquire information about their current location within the intended building.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Gharat, Madhura; Patanwala, Rizwan; Ganaparthi, Adithi] Vidyalankar
   Inst Technol, Dept Elect &amp; Telecommun, Antop Hill,Vidyalankar Coll Marg,
   Bombay 400037, Maharashtra, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Gharat, M (reprint author), Vidyalankar Inst Technol, Dept Elect &amp; Telecommun, Antop Hill,Vidyalankar Coll Marg, Bombay 400037, Maharashtra, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>madhuragharat007@gmail.com; rizwanpatanwala7700@gmail.com;
   adithi2006@rediffmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>381</td>
</tr>

<tr>
<td valign="top">EP </td><td>384</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000427581200072</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Alam, J
   <br>Kenny, P</td>
</tr>

<tr>
<td valign="top">AF </td><td>Alam, Jahangir
   <br>Kenny, Patrick</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Spoofing Detection Employing Infinite Impulse Response - Constant Q
   Transform-based Feature Representations</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 25TH EUROPEAN SIGNAL PROCESSING CONFERENCE (EUSIPCO)</td>
</tr>

<tr>
<td valign="top">SE </td><td>European Signal Processing Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>25th European Signal Processing Conference (EUSIPCO)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 28-SEP 02, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>GREECE</td>
</tr>

<tr>
<td valign="top">DE </td><td>spoofing detection; ASVspoof2015; GMM; bottleneck features; ICQC</td>
</tr>

<tr>
<td valign="top">AB </td><td>Speaker recognition researchers acknowledge that systems which aim to verify speakers automatically based on their pronunciation of an utterance are vulnerable to spoofing attacks using voice conversion and speech synthesis technologies. The first automatic speaker verification spoofing and countermeasures challenge (ASVspoof2015) was designed to stimulate interest in this problem among the speaker recognition communities. In the course of the challenge and subsequently, it became clear that the most effective countermeasures against spoofing attacks are low-level acoustic features (typically extracted at 10 ms intervals) designed to detect artifacts in synthetic or voice converted speech. In this work, we demonstrate the effectiveness of the infinite impulse response constant Q transform (IIR-CQT) spectrum-based cepstral coefficients (ICQC) as anti-spoofing front-end. The IIR-CQT spectrum is estimated by filtering the multi-resolution fast Fourier transform with an infinite impulse response filter. These features can be used on their own with a standard Gaussian mixture model backend to detect spoofing attacks or they can be used in tandem with bottleneck features which are extracted from a bottleneck layer in a deep neural network designed to discriminate between synthetic and natural speech. We show that the ICQC features are capable of producing very low equal error rates on the individual spoofing attacks in the ASVspoof2015 data set (0.02% on the known attacks, 0.23% on the unknown attacks, and 0.13% on average). Moreover, with a single decision threshold (common to all of the attacks), the ICQC front end yielded an equal error rate of 0.20%.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Alam, Jahangir; Kenny, Patrick] Comp Res Inst Montreal, Montreal, PQ,
   Canada.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Alam, J (reprint author), Comp Res Inst Montreal, Montreal, PQ, Canada.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jahangir.alam@crim.ca; Patrick.kenny@crim.ca</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>101</td>
</tr>

<tr>
<td valign="top">EP </td><td>105</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000426986000021</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Putra, DA
   <br>Nasution, SM
   <br>Azmi, F</td>
</tr>

<tr>
<td valign="top">AF </td><td>Putra, Dhipo Arsyandana
   <br>Nasution, Surya Michrandi
   <br>Azmi, Fairuz</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Analysis Of Flight Data Recorder Compression Reliability For Airplane On
   Demand Blackbox Data Transmission</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 INTERNATIONAL CONFERENCE ON CONTROL, ELECTRONICS, RENEWABLE ENERGY
   AND COMMUNICATIONS (ICCREC)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Control, Electronics, Renewable Energy and
   Communications (ICCREC)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 26-28, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Yogyakarta, INDONESIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>blackbox; Flight Data Recorder (FDR); Data Compression)</td>
</tr>

<tr>
<td valign="top">AB </td><td>Currently, Blackbox records all important information that happens during the accident that is Flight Data Recorder (FDR) and Cockpit Voice Recorder (CVR). The data is for by applying a compression method to make both of them get into transmission with on demand. This way is probably to present the data without has to find the black box. It will make the investigator easier on the evacuation process. In this study, a program created is the conversion of data into binary, split and merge data and data compression. The compression results are based on the type and size of the FDR data. The algorithm is to make the data size smaller and the decompression process takes a long time compared to the compression process. The results of the analysis get the result that when the compression process is faster to 1295.33 second than FDR data recording process. and the time required decompression time longer to 3805.10 second. On the simulation FDR, from the process parsing, compression, decompression until merge all data needed time to 5008,1316 second.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Putra, Dhipo Arsyandana; Nasution, Surya Michrandi; Azmi, Fairuz]
   Telkom Univ, Elect Engn Fac, Bandung, Indonesia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Putra, DA (reprint author), Telkom Univ, Elect Engn Fac, Bandung, Indonesia.</td>
</tr>

<tr>
<td valign="top">EM </td><td>dhipoaputra@student.telkomuniversity.ac.id;
   michrandi@telkomuniversity.ac.id; worldliner@telkomuniversity.ac.id</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>143</td>
</tr>

<tr>
<td valign="top">EP </td><td>147</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000426991700028</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nugroho, S
   <br>Nasution, SM
   <br>Azmi, F</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nugroho, Setianto
   <br>Nasution, Surya Michrandi
   <br>Azmi, Fairuz</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Analysis Of Cockpit Voice Recorder Compression Reliability For Airplane
   On Demand Blackbox Data Transmission</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 INTERNATIONAL CONFERENCE ON CONTROL, ELECTRONICS, RENEWABLE ENERGY
   AND COMMUNICATIONS (ICCREC)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Control, Electronics, Renewable Energy and
   Communications (ICCREC)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 26-28, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Yogyakarta, INDONESIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Black box; FDR; CVR; Compression; Simulation</td>
</tr>

<tr>
<td valign="top">AB </td><td>The black box records all the important information that takes place on a flying plane. The black box is a tool used on airplanes to store all activities during flight. The black box has a Flight Data Recorder (FDR) and a Cockpit Voice Recorder. FDR and CVR function to record and record the existing in the aircraft. The black box has recordings of information that occurs during the flight. Data generated FDR and CVR have a very large size is very difficult to transmit in real time. For that in this research make system that can divide data and compress data. The sound data in the conversion becomes binary by analog digital converter. The results of the data in binary form will be parsed into multiple partitions. Then the data partition into the compression process of data directly processed to obtain an efficient data size. The results of analysts seen from the size of data and time. In the conversion process becomes binary and parsing data takes about 223.4587466 seconds for 50 partitions. For time / partition takes about 4.46053 seconds. For compression takes 39614.72399 seconds for 50 partitions while for time / partition takes 792.2944798 seconds</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nugroho, Setianto; Nasution, Surya Michrandi; Azmi, Fairuz] Telkom
   Univ, Elect Engn Fac, Bandung, Indonesia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nugroho, S (reprint author), Telkom Univ, Elect Engn Fac, Bandung, Indonesia.</td>
</tr>

<tr>
<td valign="top">EM </td><td>setiantonugroho76@gmail.com; michrandi@telkomuniversity.ac.id;
   worldliner@telkomuniversity.ac.id</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>148</td>
</tr>

<tr>
<td valign="top">EP </td><td>152</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000426991700029</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Zhao, YJ
   <br>Togneri, R
   <br>Sreeram, V</td>
</tr>

<tr>
<td valign="top">AF </td><td>Zhao, Yuanjun
   <br>Togneri, Roberto
   <br>Sreeram, Victor</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Compressed High Dimensional Features for Speaker Spoofing Detection</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 ASIA-PACIFIC SIGNAL AND INFORMATION PROCESSING ASSOCIATION ANNUAL
   SUMMIT AND CONFERENCE (APSIPA ASC 2017)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Asia-Pacific Signal and Information Processing Association Annual Summit
   and Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th Annual Summit and Conference of the
   Asia-Pacific-Signal-and-Information-Processing-Association (APSIPA ASC)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 12-15, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Kuala Lumpur, MALAYSIA</td>
</tr>

<tr>
<td valign="top">AB </td><td>The vulnerability in Automatic Speaker Verification (ASV) systems to spoofing attacks such as speech synthesis (SS) and voice conversion (VC) has been recently proved. High-dimensional magnitude and phase based features possess outstanding spoofing detection performance but are not compatible with the Gaussian Mixture Model (GMM) classifiers which are commonly deployed in speaker recognition systems. In this paper, a Compressed Sensing (CS) framework is initially combined with high-dimensional (HD) features and a derived CS-HD based feature is proposed. A standalone spoofing detector assembled with the GMM classifier is evaluated on the ASVspoof 2015 database. Two ASV systems integrated with the spoofing detector are also tested. For the separate detector, an equal error rate (EER) of 0.01% and 535% are reached on the evaluation set for known attack and unknown attack, respectively. While for the ASV systems, the best EERs of 0.02% and 5.26% are achieved. The proposed CS-HD feature can obtain similar results with lower dimension than other systems. This suggests that the verification system can be made more computationally efficient.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Zhao, Yuanjun; Togneri, Roberto; Sreeram, Victor] Univ Western
   Australia, Sch Elect Elect &amp; Comp Engn, Perth, WA, Australia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Zhao, YJ (reprint author), Univ Western Australia, Sch Elect Elect &amp; Comp Engn, Perth, WA, Australia.</td>
</tr>

<tr>
<td valign="top">EM </td><td>yuanjun.zhao@research.uwa.edu.au; roberto.togneri@uwa.edu.au;
   victor.sreeram@uwa.edu.au</td>
</tr>

<tr xmlns:date="http://exslt.org/dates-and-times">
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Togneri, Roberto</display_name>&nbsp;</font></td><td><font size="3">0000-0002-3778-4633&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>569</td>
</tr>

<tr>
<td valign="top">EP </td><td>572</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000425879400115</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Tian, YZ
   <br>Gao, X
   <br>Luan, MX
   <br>Li, L</td>
</tr>

<tr>
<td valign="top">AF </td><td>Tian, Yingzhong
   <br>Gao, Xu
   <br>Luan, Mingxuan
   <br>Li, Long</td>
</tr>

<tr>
<td valign="top">BE </td><td>ElFergany, A
   <br>Rojas, AL
   <br>Szeto, WY</td>
</tr>

<tr>
<td valign="top">TI </td><td>Research on Multi-Sensor Fusion of Layered Intelligent System for Indoor
   Mobile Robot</td>
</tr>

<tr>
<td valign="top">SO </td><td>PROCEEDINGS OF THE 2017 2ND INTERNATIONAL CONFERENCE ON CONTROL,
   AUTOMATION AND ARTIFICIAL INTELLIGENCE (CAAI 2017)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Advances in Intelligent Systems Research</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>2nd International Conference on Control, Automation and Artificial
   Intelligence (CAAI)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUN 25-26, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Sanya, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>multi-sensor fusion; layered intelligent system; indoor mobile robot;
   several modules</td>
</tr>

<tr>
<td valign="top">AB </td><td>as the demand for smart home grows, robot manufacturers develop new indoor mobile robots, which allow for intelligent and high accuracy of the robot system. However, there is not such a standard system. In order to solve the above problems, this work focuses on the main problems of multilayer intelligent system on the indoor mobile robot: the recognition of voice information with semantic understanding and coordinate matching, which guiding the robot through the internal positioning and navigation algorithm to reach the matching coordinates. The proposed Multi-sensor fusion of layered intelligent system integrates different modules where each module encapsulates a number of related algorithms responsible for robot control in human-robot collaboration, such as the AMCL locating algorithm. The goal is to extract high-level voice information from human-robot conversions and control the robot to the target point. The system consists of several modules as sensor fusion, speech recognition, semantic understanding, coordinate matching, map construction, robot positioning, path planning and feedback processing. The general architecture and main approaches are presented as well as the future developments planned.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Tian, Yingzhong; Gao, Xu; Luan, Mingxuan; Li, Long] Shanghai Univ, Sch
   Mech Engn &amp; Automat, Shanghai 200072, Peoples R China.
   <br>[Tian, Yingzhong; Li, Long] Shanghai Key Lab Intelligent Mfg &amp; Robot,
   Shanghai 200072, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Li, L (reprint author), Shanghai Univ, Sch Mech Engn &amp; Automat, Shanghai 200072, Peoples R China.; Li, L (reprint author), Shanghai Key Lab Intelligent Mfg &amp; Robot, Shanghai 200072, Peoples R China.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>134</td>
</tr>

<tr>
<td valign="top">BP </td><td>81</td>
</tr>

<tr>
<td valign="top">EP </td><td>84</td>
</tr>

<tr>
<td valign="top">SC </td><td>Automation &amp; Control Systems; Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000426676200016</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Boza-Quispe, G
   <br>Montalvan-Figueroa, J
   <br>Puente-Mansilla, F
   <br>Rosales-Huamani, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Boza-Quispe, Gustavo
   <br>Montalvan-Figueroa, Juan
   <br>Puente-Mansilla, Fabricio
   <br>Rosales-Huamani, Jimmy</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Friendly Speech User Interface based on Google Cloud Platform to
   Access a Tourism Semantic Website</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 CHILEAN CONFERENCE ON ELECTRICAL, ELECTRONICS ENGINEERING,
   INFORMATION AND COMMUNICATION TECHNOLOGIES (CHILECON)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>CHILEAN Conference on Electrical, Electronics Engineering, Information
   and Communication Technologies (CHILECON)</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 18-20, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Pucon, CHILE</td>
</tr>

<tr>
<td valign="top">DE </td><td>semantic web; SPARQL; text-to-speech; speech-to-text; tourism; raspberry
   pi; google cloud platform; natural language</td>
</tr>

<tr>
<td valign="top">ID </td><td>WEB</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a speech interface to extract domain-specific information from a tourism semantic website as a way to avoid complicated and unfriendly SPARQL queries. First, we present a user-oriented website with a semantic knowledge model of tourism. This model is also known as ontology, represents a common understanding of domain in which semantics of data is machine understandable. Second, we make in Raspberry Pi an interface which has the capability to recognize speech queries and give an oral response. Our interface analyzes each speech query, convert speech to text and extract keywords from the text. Later, these keywords are compared with a list of pattern templates to match an SPARQL query. Finally, this SPARQL query extracts information, in text, from the ontology of the semantic website; and the interface will read it, in voice, for the user. Google Cloud Speech API makes speech-to-text conversions and Text-to-Speech conversions with SVOX Pico. As results, we will present the response time of different speech queries, where we show three stages; time of our website server, the response time of Google Cloud Platform and a constant latency time. With this interface, we reduce barriers of access to the semantic web for people without technical knowledge of Semantic Web technologies and visually impaired people.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Boza-Quispe, Gustavo; Montalvan-Figueroa, Juan; Puente-Mansilla,
   Fabricio; Rosales-Huamani, Jimmy] Natl Univ Engn, Lima, Peru.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Boza-Quispe, G (reprint author), Natl Univ Engn, Lima, Peru.</td>
</tr>

<tr>
<td valign="top">EM </td><td>gbozaq@uni.pe; jmontalvanf@uni.pe; fpuentem@uni.pe; jrosales@uni.edu.pe</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000425925000067</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Eller-Boyko, D
   <br>Grace, F</td>
</tr>

<tr>
<td valign="top">AF </td><td>Eller-Boyko, Diane
   <br>Grace, Fran</td>
</tr>

<tr>
<td valign="top">TI </td><td>Longing for the Feminine: Reflections on Love, Sexual Orientation,
   Individuation, and the Soul</td>
</tr>

<tr>
<td valign="top">SO </td><td>PSYCHOLOGICAL PERSPECTIVES-A QUARTERLY JOURNAL OF JUNGIAN THOUGHT</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">AB </td><td>This article is a collaboration that represents several years of dialogue about our topic, alongside the individual depth work done by each of us to overcome the negativities that often poison collaborations and feeling connections: envy, enmeshment, and passivity. It is the unique expression of two women working together in a co-creative process pulsed by feminine principles. Diane is a Jungian analyst and has spent her life close to the unconscious. Fran is a writer, educator, and lifelong student of mystical paths. The article gives an account of Diane's longing for the divine feminine and the particular meaning it has for her as a lesbian in this time of global upheaval. In particular, her story highlights the psychic suffering that individuals go through when their same-sex attractions and love orientation are judged as psychologically "immature" or religiously "sinful." Neither Christian-based "conversion therapy" nor Jungian analysis turned Diane into a heterosexual woman. Her story reveals the benefit of Jung's depth psychology even as it underscores the singularity of every person's individuation process. The article's unusual format is a tandem of Diane's first-person sharing of her soul's journey and Fran's witnessing of the journey's profound significance. We found that both voices were needed. Truth requires not only the one who lives a journey with courage but also the one who witnesses it with a loving heart. Ultimately, we see the personal journeys of gays and lesbians as significant to the larger context of the evolution of human consciousness.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Grace, Fran] Univ Redlands, Religious Studies, Redlands, CA 92373 USA.
   <br>[Grace, Fran] Univ Redlands, Meditat Room Program, Redlands, CA 92373
   USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Grace, F (reprint author), Univ Redlands, Religious Studies, Redlands, CA 92373 USA.; Grace, F (reprint author), Univ Redlands, Meditat Room Program, Redlands, CA 92373 USA.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>60</td>
</tr>

<tr>
<td valign="top">IS </td><td>3</td>
</tr>

<tr>
<td valign="top">BP </td><td>289</td>
</tr>

<tr>
<td valign="top">EP </td><td>316</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1080/00332925.2017.1350800</td>
</tr>

<tr>
<td valign="top">SC </td><td>Psychology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000424513900004</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Su, SY
   <br>Chiu, CK
   <br>Su, L
   <br>Yang, YH</td>
</tr>

<tr>
<td valign="top">AF </td><td>Su, Shih-Yang
   <br>Chiu, Cheng-Kai
   <br>Su, Li
   <br>Yang, Yi-Hsuan</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>AUTOMATIC CONVERSION OF POP MUSIC INTO CHIPTUNES FOR 8-BIT PIXEL ART</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 05-09, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>New Orleans, LA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Audio mosaicing; chiptune; synthesis</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we propose an audio mosaicing method that converts Pop songs into a specific music style called "chiptune," or "8-bit music." The goal is to reproduce Pop songs by using the sound of the chips on the old game consoles in 1980s/1990s. The proposed method goes through a procedure that first analyzes the pitches of an incoming Pop song in the frequency domain, and then synthesizes the song with template waveforms in the time domain to make it sound like 8-bit music. Because a Pop song is usually composed of the vocal melody and the instrumental accompaniment, in the analysis stage we use a singing voice separation algorithm to separate the vocals from the instruments, and then apply different pitch detection algorithms to transcribe the two separated sources. We validate through a subjective listening test that the proposed method creates much better 8-bit music than existing nonnegative matrix factorization based methods can do. Moreover, we find that synthesis in the time domain is important for this task.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Su, Shih-Yang; Chiu, Cheng-Kai; Su, Li; Yang, Yi-Hsuan] Acad Sinica,
   Res Ctr Informat Technol Innovat, Taipei, Taiwan.
   <br>[Su, Shih-Yang; Chiu, Cheng-Kai] Natl Tsing Hua Univ, Dept Comp Sci,
   Hsinchu, Taiwan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Su, SY (reprint author), Acad Sinica, Res Ctr Informat Technol Innovat, Taipei, Taiwan.; Su, SY (reprint author), Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu, Taiwan.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>411</td>
</tr>

<tr>
<td valign="top">EP </td><td>415</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000414286200083</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Jacobs, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Jacobs, Martin</td>
</tr>

<tr>
<td valign="top">TI </td><td>Sephardic Migration and Cultural Transfer: The Ottoman and Spanish
   Expansion through a Cinquecento Jewish Lens</td>
</tr>

<tr>
<td valign="top">SO </td><td>JOURNAL OF EARLY MODERN HISTORY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Ottoman Empire; Spanish Empire; New World explorations; cultural
   transfer; Sephardic Jews; Genoa; Lopez de Gomara</td>
</tr>

<tr>
<td valign="top">AB </td><td>This study explores the reading and writing practices of Joseph Ha-Kohen, a sixteenth-century Jewish chronicler from Genoa, against the background of his Italian and Spanish sources: in what ways and why did he adapt, change or subvert their narratives? It focuses on two of Ha-Kohen's major works: his Franco-Turkish Chronicle, and his Hebrew adaptation of Lopez de Gomara's account of the Spanish conquests in the Americas. Based on these writings, the essay asks how the author's Sephardic identity and migration experience inform his ideas about the Habsburg and Ottoman Empires. Notably, the Jewish chronicler applauds the Ottomans' conversion of churches into mosques, while he condemns the forced Christianization of the Amerindians. At the same time, Ha-Kohen shares cultural attitudes with Gomara and voices qualified support for a Spanish civilizing mission. These ambiguities in Ha-Kohen's writings-oscillating between praise and repudiation of imperial ideology-prove to be emblematic of post-expulsion Sephardic Jewry.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Jacobs, Martin] Washington Univ St Louis, St Louis, MO 63130 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Jacobs, M (reprint author), Washington Univ St Louis, St Louis, MO 63130 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>mjacobs@wustl.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>21</td>
</tr>

<tr>
<td valign="top">IS </td><td>6</td>
</tr>

<tr>
<td valign="top">BP </td><td>516</td>
</tr>

<tr>
<td valign="top">EP </td><td>542</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1163/15700658-12342556</td>
</tr>

<tr>
<td valign="top">SC </td><td>History</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000418491900002</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>McQuitty, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>McQuitty, Jane</td>
</tr>

<tr>
<td valign="top">TI </td><td>Feralness A Sibling of Wilderness</td>
</tr>

<tr>
<td valign="top">SO </td><td>JOURNAL OF ARCHITECTURAL EDUCATION</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">ID </td><td>ECOSYSTEMS</td>
</tr>

<tr>
<td valign="top">AB </td><td>In North America, the classic voice of colonial peoples' connectedness to nature and a wellspring of distinct new identity has been the romantic individualist writing of affinity for wilderness. The truth is, however, that wilderness account makes North Americans, Australians, and New Zealanders culturally blind to an emerging split between wilderness as a land management concept and the state of the wild characteristic of the lands near the cities where 80 percent of us now live. To say land is wilderness, one has to imagine a static systemic context creating conditions that, if it were not for the colonizing project of land conversion, population implantation, mineral exploitation, the land would forever reflect. What about land that runs away from past colonial domestications? What about land that has hybridized with colonial escapee species? Thoroughly worked over lands are fallow on the edges of cities, and new kinds of wilds are emerging upon them. Let me propose that the time is ripe for the sibling of wilderness and for cultural forms exploring and reflecting its stories, for how can you preserve what you cannot name or the culture has never helped you to categorize?</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[McQuitty, Jane] Univ Calgary, Fac Environm Design, Calgary, AB, Canada.</td>
</tr>

<tr>
<td valign="top">RP </td><td>McQuitty, J (reprint author), Univ Calgary, Fac Environm Design, Calgary, AB, Canada.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>71</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>238</td>
</tr>

<tr>
<td valign="top">EP </td><td>240</td>
</tr>

<tr>
<td valign="top">SC </td><td>Architecture</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000416730800013</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hanilci, C</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hanilci, Canal</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Speaker Recognition Anti-spoofing Using Linear Prediction Residual</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 25TH SIGNAL PROCESSING AND COMMUNICATIONS APPLICATIONS CONFERENCE
   (SIU)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Signal Processing and Communications Applications Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>25th Signal Processing and Communications Applications Conference (SIU)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 15-18, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Antalya, TURKEY</td>
</tr>

<tr>
<td valign="top">DE </td><td>speaker recognition; spoofing attacks; anti-spoofing</td>
</tr>

<tr>
<td valign="top">ID </td><td>VERIFICATION; BIOMETRICS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Speaker recognition systems have recently proven to be highly vulnerable against spoofing attacks. Therefore, it is important to detect spoofing attacks performed by speech synthesis and voice conversion in order to improve the reliability of speaker recgnition systems. To this end, in this study, we propose to use of features extracted from linear prediction (LP) resdiual signal for the detection of SS and VC based spoofing attacks against speaker recognition systems. Experiments are conducted on recently released ASVspoof 2015 database which consists of spoofed speech signals generated by ten different SS and VC algorithms. Experimental results show that, SS and VC attacks can effectively be detected using the features extracted from LP residual signal and mel frequency cepstral coefficients (MFCC) using Gaussian mixture model (GMM) classifier.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Hanilci, Canal] Bursa Tekn Univ, Elekt Elekt Muhendisligi, Bursa,
   Turkey.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hanilci, C (reprint author), Bursa Tekn Univ, Elekt Elekt Muhendisligi, Bursa, Turkey.</td>
</tr>

<tr>
<td valign="top">EM </td><td>cemal.hanilci@btu.edu.tr</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Hanilci, Cemal</display_name>&nbsp;</font></td><td><font size="3">S-4967-2016&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science; Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000413813100011</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Bril, I</td>
</tr>

<tr>
<td valign="top">AF </td><td>Bril, Isabelle</td>
</tr>

<tr>
<td valign="top">TI </td><td>Roots and stems in Amis and Nelemwa (Austronesian) Lexical categories
   and functional flexibility</td>
</tr>

<tr>
<td valign="top">SO </td><td>STUDIES IN LANGUAGE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Austronesian; Formosan; Oceanic; alignment system; applicative
   constructions; categorial flexibility; primary and secondary derivation;
   conversion; asymmetrical derivation; category-changing derivation;
   categorially neutral roots; functional flexibility; stems; voice
   systems; verb classes</td>
</tr>

<tr>
<td valign="top">AB </td><td>In constrast with Nelemwa (Oceanic, New Caledonia) whose lexemes are most generally subcategorised as nouns or verbs and undergo category-changing derivations, in Amis (Formosan), roots are pervasively categorially neutral, yet they contain semantic features and instructions that allow or disallow combination with primary derivational affixes which specify their class and category. Lexical categories are expressed after roots are derived into morphosyntactic words projected in a syntactic frame; they are then quite rigidly subcategorised as verbal, nominal or adjectival-modifying heads. Still, word forms display some functional flexibility; for instance, nouns and derived nouns, pronouns, numerals may be predicative in equative, ascriptive and focus constructions, simply by being in the syntactic position of the verb. Such functional flexibility is asymmetrical and does not apply to derived verb stems which must be nominalised to achieve argument function.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Bril, Isabelle] CNRS, LACITO, 7 Rue Guy Moquet,Bat D, F-94800
   Villejuif, France.
   <br>[Bril, Isabelle] LABEX EFL, Paris, France.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Bril, I (reprint author), CNRS, LACITO, 7 Rue Guy Moquet,Bat D, F-94800 Villejuif, France.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ibril@vjf.cnrs.fr</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>41</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>358</td>
</tr>

<tr>
<td valign="top">EP </td><td>407</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1075/sl.41.2.04bri</td>
</tr>

<tr>
<td valign="top">SC </td><td>Linguistics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000412944000004</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Konev, A
   <br>Kostyuchenko, E
   <br>Yakimuk, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Konev, Anton
   <br>Kostyuchenko, Evgeny
   <br>Yakimuk, Alexey</td>
</tr>

<tr>
<td valign="top">BE </td><td>Martyushev, N
   <br>Avramchuk, V
   <br>Faerman, V</td>
</tr>

<tr>
<td valign="top">TI </td><td>The program complex for vocal recognition</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGIES IN BUSINESS AND
   INDUSTRY 2016</td>
</tr>

<tr>
<td valign="top">SE </td><td>Journal of Physics Conference Series</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Information Technologies in Business and
   Industry (ITBI)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 21-23, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Tomsk Polytechn Univ, Tomsk, RUSSIA</td>
</tr>

<tr>
<td valign="top">HO </td><td>Tomsk Polytechn Univ</td>
</tr>

<tr>
<td valign="top">AB </td><td>This article discusses the possibility of applying the algorithm of determining the pitch frequency for the note recognition problems. Preliminary study of programs-analogues were carried out for programs with function "recognition of the music". The software package based on the algorithm for pitch frequency calculation was implemented and tested. It was shown that the algorithm allows recognizing the notes in the vocal performance of the user. A single musical instrument, a set of musical instruments, and a human voice humming a tune can be the sound source. The input file is initially presented in the. wav format or is recorded in this format from a microphone. Processing is performed by sequentially determining the pitch frequency and conversion of its values to the note. According to test results, modification of algorithms used in the complex was planned.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Konev, Anton; Kostyuchenko, Evgeny; Yakimuk, Alexey] Tomsk State Univ
   Control Syst &amp; Radioelect, 40 Lenina Ave, Tomsk 634012, Russia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Konev, A (reprint author), Tomsk State Univ Control Syst &amp; Radioelect, 40 Lenina Ave, Tomsk 634012, Russia.</td>
</tr>

<tr>
<td valign="top">EM </td><td>kaa1@keva.tusur.ru; key@keva.tusur.ru; yay@keva.tusur.ru</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Konev, Anton</display_name>&nbsp;</font></td><td><font size="3">C-6776-2019&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Yakimuk, Alexey</display_name>&nbsp;</font></td><td><font size="3">Q-4937-2019&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Kostyuchenko, Evgeny</display_name>&nbsp;</font></td><td><font size="3">M-6310-2014&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Konev, Anton</display_name>&nbsp;</font></td><td><font size="3">0000-0002-3222-9956&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Yakimuk, Alexey</display_name>&nbsp;</font></td><td><font size="3">0000-0001-9736-7658&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Kostyuchenko, Evgeny</display_name>&nbsp;</font></td><td><font size="3">0000-0001-8000-2716&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>803</td>
</tr>

<tr>
<td valign="top">AR </td><td>UNSP 012077</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1088/1742-6596/803/1/012077</td>
</tr>

<tr>
<td valign="top">SC </td><td>Physics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000406133000077</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Bisi, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Bisi, Monica</td>
</tr>

<tr>
<td valign="top">TI </td><td>"Pellegrini form": the conversion of writing in The butterflies by
   Gozzano</td>
</tr>

<tr>
<td valign="top">SO </td><td>CRITICA LETTERARIA</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">AB </td><td>The particular rhetorical choices through which Gozzano gives form to the metamorphoses of caterpillars and then to the life of butterflies in Epistole entomologiche, in their close correspondence with the dynamic material they represent, suggest that the author may indeed have changed perspective, at least formally, compared to his first two collections. Against the backdrop of borrowings from Dante and a philosophical outlook inherited from the first half of the Twentieth century, this study aims to demonstrate the peculiarities of that "other voice" with which Gozzano promised to return after the Colloqui.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Bisi, Monica] Univ Cattolica Sacro Cuore, Milan, Italy.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Bisi, M (reprint author), Univ Cattolica Sacro Cuore, Milan, Italy.</td>
</tr>

<tr>
<td valign="top">EM </td><td>monica.bisi@unicatt.it</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">VL </td><td>45</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>357</td>
</tr>

<tr>
<td valign="top">EP </td><td>380</td>
</tr>

<tr>
<td valign="top">SC </td><td>Literature</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000405252500009</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Chadha, A
   <br>Nirmal, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Chadha, Ankita
   <br>Nirmal, Jagannath</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Full Band adaptive Harmonic Model Based Speaker Identity
   Transformation using Radial Basis Function</td>
</tr>

<tr>
<td valign="top">SO </td><td>PROCEEDINGS OF 2017 11TH INTERNATIONAL CONFERENCE ON INTELLIGENT SYSTEMS
   AND CONTROL (ISCO 2017)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>11th International Conference on Intelligent Systems and Control (ISCO)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JAN 05-06, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Karpagam Coll Engn, Coimbatore, INDIA</td>
</tr>

<tr>
<td valign="top">HO </td><td>Karpagam Coll Engn</td>
</tr>

<tr>
<td valign="top">DE </td><td>full-band adaptive Harmonic Model (a-HM); Line Spectral Frequency;
   LP-Residual; Radial Basis Function; Speaker transformation</td>
</tr>

<tr>
<td valign="top">ID </td><td>ARTIFICIAL NEURAL-NETWORKS; VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Speaker Transformation adapts the speaker dependent characteristics of the source speaker according to that of a target speaker, so that it is perceived like the target speaker. Speaker Transformation is generally carried out using speech analysis-synthesis system. The full-band adaptive Harmonic Model (a-HM) based analysis-synthesis has ability to produce a high quality resynthesized speech. Thus inn this paper, a full band a-HM is proposed to represent the speaker dependent parameters of the source and target speech signal. The Radial Basis Function (RBF) neural network is developed to capture non-linear relationship between source and target a-HM based features. In the state of art method, Line Spectral Frequency (LSF) is used to represent the vocal tract and LP-residual for the glottal excitation of the speech signal. The RBF is used to map the LSF of source speaker to that of the target speakers and state of art residual selection method is used for modification of source residual to that of target residual. The performance of the proposed a-HM based speaker transformation is compared with the state of the art features using various objective and subjective measures. The results reveal that the a-HM feature based speaker transformation performs profoundly well in contrast to the state of the art technique.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Chadha, Ankita] Sardar Patel Natl Inst Technol, Dept Elect Engn, Surat,
   India.
   <br>[Nirmal, Jagannath] KJ Somaiya Coll Engn, Dept Elect Engn, Bombay,
   Maharashtra, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Chadha, A (reprint author), Sardar Patel Natl Inst Technol, Dept Elect Engn, Surat, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ankitaism@gmail.com; jhnirmal@somaiya.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>217</td>
</tr>

<tr>
<td valign="top">EP </td><td>223</td>
</tr>

<tr>
<td valign="top">SC </td><td>Automation &amp; Control Systems; Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000403387400040</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Tanaka, K
   <br>Kameoka, H
   <br>Toda, T
   <br>Nakamura, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Tanaka, Kou
   <br>Kameoka, Hirokazu
   <br>Toda, Tomoki
   <br>Nakamura, Satoshi</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Physically Constrained Statistical F-0 Prediction for Electrolaryngeal
   Speech Enhancement</td>
</tr>

<tr>
<td valign="top">SO </td><td>18TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2017), VOLS 1-6: SITUATED INTERACTION</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>18th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2017)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 20-24, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Stockholm, SWEDEN</td>
</tr>

<tr>
<td valign="top">DE </td><td>electrolaryngeal speech; statistical F-0 prediction; generative model;
   speech enhancement</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Electrolaryngeal (EL) speech produced by a laryngectomee using an electrolarynx to mechanically generate artificial excitation sounds severely suffers from unnatural fundamental frequency (F-0) patterns caused by monotonic excitation sounds. To address this issue, we have previously proposed EL speech enhancement systems using statistical F-0 pattern prediction methods based on a Gaussian Mixture Model (GMM), making it possible to predict the underlying F-0 pattern of EL speech from its spectral feature sequence. Our previous work revealed that the naturalness of the predicted F-0 pattern can be improved by incorporating a physically based generative model of F-0 patterns into the GMM-based statistical F-0 prediction system within a Product-of-Expert framework. However. one drawback of this method is that it requires an iterative procedure to obtain a predicted F-0 pattern. making it difficult to realize a real-time system. In this paper, we propose yet another approach to physically based statistical F-0 pattern prediction by using a HMM-GMM framework. This approach is noteworthy in that it allows to generate an F-0 pattern that is both statistically likely and physically natural without iterative procedures. Experimental results demonstrated that the proposed method was capable of generating F-0 patterns more similar to those in normal speech than the conventional GMM-based method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Tanaka, Kou; Nakamura, Satoshi] Nara Inst Sci &amp; Technol, Grad Sch
   Informat Sci, Ikoma, Japan.
   <br>[Kameoka, Hirokazu] NTT Corp, NTT Commun Sci Labs, Tokyo, Japan.
   <br>[Toda, Tomoki] Nagoya Univ, Informat Technol Ctr, Nagoya, Aichi, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Tanaka, K (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Ikoma, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ko-t@is.naist.jp; kameoka.hirokazu@lab.ntt.co.jp;
   tomoki@icts.nagoya-u.ac.jp; s-nakamura@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>1069</td>
</tr>

<tr>
<td valign="top">EP </td><td>1073</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2017-688</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000457505000224</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Lenarczyk, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Lenarczyk, Michat</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Real time pitch shifting with formant structure preservation using the
   phase vocoder</td>
</tr>

<tr>
<td valign="top">SO </td><td>18TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2017), VOLS 1-6: SITUATED INTERACTION</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>18th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2017)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 20-24, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Stockholm, SWEDEN</td>
</tr>

<tr>
<td valign="top">DE </td><td>voice conversion; nonparametric transformation; phase vocoder</td>
</tr>

<tr>
<td valign="top">AB </td><td>Pitch shifting in speech is presented based on the use of the phase vocoder in combination with spectral whitening and envelope reconstruction, applied respectively before and after the transformation. A band preservation technique is introduced to contain quality degradation when downscaling the pitch. The transposition ratio is fixed in advance by selecting analysis and synthesis window sizes. Real time performance is demonstrated for window sizes having adequate factorization required by fast Fourier transformation.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Lenarczyk, Michat] Polish Acad Sci, IPI PAN Inst Comp Sci, Warsaw,
   Poland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Lenarczyk, M (reprint author), Polish Acad Sci, IPI PAN Inst Comp Sci, Warsaw, Poland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>m.lenarczyk@phd.ipipan.waw.pl</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>2032</td>
</tr>

<tr>
<td valign="top">EP </td><td>2033</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000457505000419</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sarkar, AK
   <br>Sahidullah, M
   <br>Tan, ZH
   <br>Kinnunen, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sarkar, Achintya Kr.
   <br>Sahidullah, Md.
   <br>Tan, Zheng-Hua
   <br>Kinnunen, Tomi</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Improving Speaker Verification Performance in Presence of Spoofing
   Attacks Using Out-of-Domain Spoofed Data</td>
</tr>

<tr>
<td valign="top">SO </td><td>18TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2017), VOLS 1-6: SITUATED INTERACTION</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>18th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2017)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 20-24, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Stockholm, SWEDEN</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speaker verification; Spoofing; UBM; Cross-corpora</td>
</tr>

<tr>
<td valign="top">ID </td><td>COUNTERMEASURES</td>
</tr>

<tr>
<td valign="top">AB </td><td>Automatic speaker verification (ASV) systems are vulnerable to spoofing attacks using speech generated by voice conversion and speech synthesis techniques. Commonly, a countermeasure (CM) system is integrated with an ASV system for improved protection against spoofing attacks. But integration of the two systems is challenging and often leads to increased false rejection rates. Furthermore, the performance of CM severely degrades if in-domain development data are unavailable. In this study, therefore. we propose a solution that uses two separate background models - one from human speech and another from spoofed data. During test, the ASV score for an input utterance is computed as the difference of the log-likelihood against the target model and the combination of the log-likelihoods against two background models. Evaluation experiments are conducted using the joint ASV and CM protocol of ASV spoof 2015 corpus consisting of text-independent ASV tasks with short utterances. Our proposed system reduces error rates in the presence of spoofing attacks by using out-of-domain spoofed data for system development, while maintaining the performance for zero-effort imposter attacks compared to the baseline system.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Sarkar, Achintya Kr.; Tan, Zheng-Hua] Aalborg Univ, Dept Elect Syst,
   Aalborg, Denmark.
   <br>[Sahidullah, Md.; Kinnunen, Tomi] Univ Eastern Finland, Sch Comp, Speech
   &amp; Image Proc Unit, Kuopio, Finland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sarkar, AK (reprint author), Aalborg Univ, Dept Elect Syst, Aalborg, Denmark.</td>
</tr>

<tr>
<td valign="top">EM </td><td>akc@es.aau.dk; sahid@cs.uef.fi; zt@es.aau.dk; tkinnu@cs.uef.fi</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>2611</td>
</tr>

<tr>
<td valign="top">EP </td><td>2615</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2017-1758</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000457505000542</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kim, T
   <br>Park, C
   <br>Park, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kim, Taeil
   <br>Park, Chulsun
   <br>Park, Sungkwon</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>The Application of Compression Methods for RoIP Data Transmission
   Efficiency in the HFC Network</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 INTERNATIONAL CONFERENCE ON SIGNALS AND SYSTEMS (ICSIGSYS)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>1st International Conference on Signals and Systems (ICSigSys)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 16-18, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Bali, INDONESIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>HFC; RoIP; Huffman coding; up/dowm sampling; Data compression</td>
</tr>

<tr>
<td valign="top">AB </td><td>Recently, with the construction of the All IP network infrastructure, Hybrid Fiber Coaxial (HFC) cable network is undergoing a digital transition based on optical IP network. HFC is a communication technology in which fiber optic cables and coaxial cables are used in different parts of the network to transport broadband content such as video, data and voice. Accordingly, there is applying Radio Over IP (RoIP) technology that is a generic term that describes the application of Voice over IP (VoIP) on two-way radio networks. RoIP is a technology for transmitting radio frequency signals using a digital IP network. Transmission of RF signals using the IP network requires conversion to digital data. However, a large amount of data is a generated during the digital conversion process. This makes efficient data transmission impossible. In this paper, we reduce the amount of data by using up/down sampling and Huffman compression methods. By using this method, it can transmit RF signal efficiently. Then, we measured compression ratio and Error Vector Magnitude (EVM), which is a performance degradation index due to compression, for performance evaluation based on modulation.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kim, Taeil; Park, Chulsun; Park, Sungkwon] Hanyang Univ, Dept Elect &amp;
   Comp Engn, Seoul, South Korea.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Park, S (reprint author), Hanyang Univ, Dept Elect &amp; Comp Engn, Seoul, South Korea.</td>
</tr>

<tr>
<td valign="top">EM </td><td>tikim0704@hanyang.ac.kr; imkingha@hanyang.ac.kr; sp2996@hanyang.ac.kr</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>134</td>
</tr>

<tr>
<td valign="top">EP </td><td>137</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000463803000025</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kamble, MR
   <br>Patil, HA</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kamble, Madhu R.
   <br>Patil, Hemant A.</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Novel Energy Separation Based Frequency Modulation Features For Spoofed
   Speech Classification</td>
</tr>

<tr>
<td valign="top">SO </td><td>2017 NINTH INTERNATIONAL CONFERENCE ON ADVANCES IN PATTERN RECOGNITION
   (ICAPR)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th International Conference on Advances in Pattern Recognition (ICAPR)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 27-30, 2017</td>
</tr>

<tr>
<td valign="top">CL </td><td>Indian Stat Inst Bangalore, Bangalore, INDIA</td>
</tr>

<tr>
<td valign="top">HO </td><td>Indian Stat Inst Bangalore</td>
</tr>

<tr>
<td valign="top">DE </td><td>Automatic Speaker Verification; Spoofing Attacks; Teager Energy
   Operator; Amplitude Envelope; Instantaneous Frequency</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEAKER VERIFICATION; BANDWIDTH</td>
</tr>

<tr>
<td valign="top">AB </td><td>Speech Synthesis (SS) and Voice Conversion (VC) methods provides a great risk for Automatic Speaker Verification (ASV) system. In this paper, we tried to find the difference between natural and spoofed speech signals using Teager Energy Operator-based Energy Separation Algorithm (TEO-ESA). Here, we exploit the contribution of Amplitude Envelope (AE) and Instantaneous Frequency (IF) in each narrowband filtered signals energy via ESA to capture possible changes in a temporal and spectral envelope of the synthetic speech signal generated by the machines as opposed to natural signals. Furthermore, IF was used for classification of natural vs. spoof speech with Gaussian Mixture Model (GMM) as a classifier. These findings may assist to distinguish these two speeches and provide an aid to alleviate possible impostor attacks in voice biometrics. The experiments are done on ASV Spoof 2015 Challenge database. We have compared proposed Energy Separation Algorithm-Instantaneous Frequency Cosine Coefficients (ESA-IFCC) with Mel Frequency Cepstral Coefficients (MFCC) features. On the development set, MFCC alone gave an Equal Error Rate (EER) of (6.98 %) and ESA-IFCC gave (5.43 %) with 13-D static features. With score level fusion of MFCC and ESA-IFCC EER reduced to 3.45 % on static feature vector. The EER decreases further to 2.01 % and 1.89 % for Delta and Delta Delta features. On evaluation set, the overall average error rate for known and unknown attacks was 6.79 % for ESA-IFCC and was significantly better than the MFCC (9.15 %) and their score-level fused EER (7.16 %).</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kamble, Madhu R.; Patil, Hemant A.] DA IICT, Speech Res Lab,
   Gandhinagar, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kamble, MR (reprint author), DA IICT, Speech Res Lab, Gandhinagar, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>madhu_kamble@daiict.ac.in; hemant_patil@daiict.ac.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2017</td>
</tr>

<tr>
<td valign="top">BP </td><td>326</td>
</tr>

<tr>
<td valign="top">EP </td><td>331</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000458728700054</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Oshima, Y
   <br>Takamichi, S
   <br>Toda, T
   <br>Neubig, G
   <br>Sakti, S
   <br>Nakamura, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Oshima, Yuji
   <br>Takamichi, Shinnosuke
   <br>Toda, Tomoki
   <br>Neubig, Graham
   <br>Sakti, Sakriani
   <br>Nakamura, Satoshi</td>
</tr>

<tr>
<td valign="top">TI </td><td>Non-Native Text-to-Speech Preserving Speaker Individuality Based on
   Partial Correction of Prosodic and Phonetic Characteristics</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEICE TRANSACTIONS ON INFORMATION AND SYSTEMS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>cross-lingual speech synthesis; English-Read-by-Japanese; speaker
   individuality; HMM-based speech synthesis; prosody correction; phonetic
   correction</td>
</tr>

<tr>
<td valign="top">ID </td><td>ADAPTATION; ALGORITHMS</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a novel non-native speech synthesis technique that preserves the individuality of a non-native speaker. Cross-lingual speech synthesis based on voice conversion or Hidden Markov Model (HMM)-based speech synthesis is a technique to synthesize foreign language speech using a target speaker's natural speech uttered in his/her mother tongue. Although the technique holds promise to improve a wide variety of applications, it tends to cause degradation of target speaker's individuality in synthetic speech compared to intra-lingual speech synthesis. This paper proposes a new approach to speech synthesis that preserves speaker individuality by using non-native speech spoken by the target speaker. Although the use of non-native speech makes it possible to preserve the speaker individuality in the synthesized target speech, naturalness is significantly degraded as the synthesized speech waveform is directly affected by unnatural prosody and pronunciation often caused by differences in the linguistic systems of the source and target languages. To improve naturalness while preserving speaker individuality, we propose (1) a prosody correction method based on model adaptation, and (2) a phonetic correction method based on spectrum replacement for unvoiced consonants. The experimental results using English speech uttered by native Japanese speakers demonstrate that (1) the proposed methods are capable of significantly improving naturalness while preserving the speaker individuality in synthetic speech, and (2) the proposed methods also improve intelligibility as confirmed by a dictation test.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Oshima, Yuji; Toda, Tomoki; Neubig, Graham; Sakti, Sakriani; Nakamura,
   Satoshi] Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Ikoma 6300192,
   Japan.
   <br>[Takamichi, Shinnosuke] Univ Tokyo, Grad Sch Informat Sci &amp; Technol,
   Dept Informat Phys &amp; Comp, Tokyo 1138656, Japan.
   <br>[Toda, Tomoki] Nagoya Univ, Ctr Informat Technol, Nagoya, Aichi 4648601,
   Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Takamichi, S (reprint author), Univ Tokyo, Grad Sch Informat Sci &amp; Technol, Dept Informat Phys &amp; Comp, Tokyo 1138656, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>shinnosuke_takamichi@ipc.i.u-tokyo.ac.jp; tomoki@icts.nagoya-u.ac.jp;
   neubig@is.naist.jp; ssakti@is.naist.jp; s-nakamura@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>DEC</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>E99D</td>
</tr>

<tr>
<td valign="top">IS </td><td>12</td>
</tr>

<tr>
<td valign="top">BP </td><td>3132</td>
</tr>

<tr>
<td valign="top">EP </td><td>3139</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1587/transinf.2016EDP7231</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000393063600029</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Cornellier, B</td>
</tr>

<tr>
<td valign="top">AF </td><td>Cornellier, Bruno</td>
</tr>

<tr>
<td valign="top">TI </td><td>Jackie Chan's Indian play: immigration, Asianness, and the contracting
   self in the American settler colony</td>
</tr>

<tr>
<td valign="top">SO </td><td>SETTLER COLONIAL STUDIES</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper is concerned with what I define as a cultural politics of being from' in the American settler colony. I use Tom Dey's Western/cop-buddy' hybrid Shanghai Noon (2000), and most specifically what is presented in the film as actor Jackie Chan's failed and comedic partaking in the white and homosocial tradition of Indian play. I use the film as a textual anchor for the critical analysis of the types of contracting subjectivities that are implicitly privileged within the foundational delineations separating settlers, Natives, and arrivants in what President Obama described as a nation of immigrants that is carried on the back of the non-immigrant status of Native Americans. I explain that in his conversion from Chinese national to Asian American - a transition that is prefaced by his disqualification from the settler's exclusive struggle over nativeness, followed by his peculiar transition into an (Asian) American cowboy - Chan's character becomes representative of this dialectical (and, in his case, always already incomplete) movement from genealogy to autology, or from pre-modern forms of marriage, ethnic affiliations, and social constraints, to a settler-liberal ideal of intimacy, reasoned consent, and self-sovereignty. In other words, once Chan's character fulfils this trajectory of self-discovery in the refashioning of place and self, his new-found voice and sexuality bespeak his idiosyncratic conversion into an Asian American. I argue that his conversion illuminates some of the complex modalities of raced and gendered intimacy in settler articulations of the privileged, contracting subjectivity of liberal colonialism. What such representation of an idealized, autological migrant subject allows us to suggest, is that non-Native people of colour, as they re-enact themselves settler colonial modes of Native dispossession and/or co-optation, often remain peculiarly moored to the hegemonic structure of feeling of American settlerism in ways that often put upon them assimilationist demands that are parallel to those imposed upon Indigenous bodies as always incomplete white liberal subjects in the making.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Cornellier, Bruno] Univ Winnipeg, Dept English, 515 Portage Ave,
   Winnipeg, MB R3B 2E9, Canada.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Cornellier, B (reprint author), Univ Winnipeg, Dept English, 515 Portage Ave, Winnipeg, MB R3B 2E9, Canada.</td>
</tr>

<tr>
<td valign="top">EM </td><td>b.cornellier@uwinnipeg.ca</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>NOV</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>6</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">SI </td><td>SI</td>
</tr>

<tr>
<td valign="top">BP </td><td>403</td>
</tr>

<tr>
<td valign="top">EP </td><td>422</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1080/2201473X.2015.1090565</td>
</tr>

<tr>
<td valign="top">SC </td><td>Social Sciences - Other Topics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000382589400007</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ohtani, Y
   <br>Tamura, M
   <br>Morita, M
   <br>Akamine, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ohtani, Yamato
   <br>Tamura, Masatsune
   <br>Morita, Masahiro
   <br>Akamine, Masami</td>
</tr>

<tr>
<td valign="top">TI </td><td>Statistical Bandwidth Extension for Speech Synthesis Based on Gaussian
   Mixture Model with Sub-Band Basis Spectrum Model</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEICE TRANSACTIONS ON INFORMATION AND SYSTEMS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>speech enhancement; voice conversion; bandwidth extension; sub-band
   basis spectrum model; Gaussian mixture model</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION; TELEPHONE SPEECH; EIGENVOICES; EXPANSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper describes a novel statistical bandwidth extension (BWE) technique based on a Gaussian mixture model (GMM) and a sub-band basis spectrum model (SBM), in which each dimensional component represents a specific acoustic space in the frequency domain. The proposed method can achieve the BWE from speech data with an arbitrary frequency bandwidth whereas the conventional methods perform the conversion from fixed narrow-band data. In the proposed method, we train a GMM with SBM parameters extracted from full-band spectra in advance. According to the bandwidth of input signal, the trained GMM is reconstructed to the GMM of the joint probability density between low-band SBM and high-band SBM components. Then high-band SBM components are estimated from low-band SBM components of the input signal based on the reconstructed GMM. Finally, BWE is achieved by adding the spectra decoded from estimated high-band SBM components to the ones of the input signal. To construct the full-band signal from the narrow-band one, we apply this method to log-amplitude spectra and aperiodic components. Objective and subjective evaluation results show that the proposed method extends the bandwidth of speech data robustly for the log-amplitude spectra. Experimental results also indicate that the aperiodic component extracted from the upsampled narrow-band signal realizes the same performance as the restored and the full-band aperiodic components in the proposed method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ohtani, Yamato; Tamura, Masatsune; Morita, Masahiro; Akamine, Masami]
   Toshiba Co Ltd, Corp Res &amp; Dev Ctr, Knowledge Media Lab, Kawasaki,
   Kanagawa 2128582, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ohtani, Y (reprint author), Toshiba Co Ltd, Corp Res &amp; Dev Ctr, Knowledge Media Lab, Kawasaki, Kanagawa 2128582, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>yamato.ohtani@toshiba.co.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>OCT</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>E99D</td>
</tr>

<tr>
<td valign="top">IS </td><td>10</td>
</tr>

<tr>
<td valign="top">BP </td><td>2481</td>
</tr>

<tr>
<td valign="top">EP </td><td>2489</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1587/transinf.2016SLP0006</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000388743500008</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ferreira, JA</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ferreira, Joel Antonio</td>
</tr>

<tr>
<td valign="top">TI </td><td>Onesimus: a character in silent in the Letter do Philemon?</td>
</tr>

<tr>
<td valign="top">SO </td><td>HORIZONTE-REVISTA DE ESTUDOS DE TEOLOGIA E CIENCIAS DA RELIGIAO</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Slave; Protagonist; Margin; Prisoner; Freedom</td>
</tr>

<tr>
<td valign="top">AB </td><td>By traditional biblical methods (Historical-Critical and Literary Critic) already studied the Letter to Philemon, deepening the protagonist Paul within the modes of production Roman slave. With this current article enjoyed by the Sociological Method by Model Conflictual/Contradiction, the proposal is to look the same Letter to Philemon, from someone who was silenced in the original text. As this type of Reading aims to detect asymmetries, looking at the text with "suspicion" and give voice and space to those in the "margin", the protagonist becomes the one who was under the Roman pyramid: Onesimus the slave. The hypothesis of this article is that, in prison, the slave Onesimus became the state of passivity to become a questioning of Paul's practice. The Apostle then the rawness of jail, he felt touched. His practice now materialized in the fight for freedom of the slave. Onesimus would have made Paul's conversion. With this reading, we find Jesus Christ in the flesh of the slave.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ferreira, Joel Antonio] CEBI, Indaiatuba, SP, Brazil.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ferreira, JA (reprint author), CEBI, Indaiatuba, SP, Brazil.</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Antonio Ferreira, Joel</display_name>&nbsp;</font></td><td><font size="3">0000-0002-2934-3416&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>APR-JUN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>14</td>
</tr>

<tr>
<td valign="top">IS </td><td>42</td>
</tr>

<tr>
<td valign="top">BP </td><td>377</td>
</tr>

<tr>
<td valign="top">EP </td><td>401</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.5752/P.2175-5841.2016v14n42p377</td>
</tr>

<tr>
<td valign="top">SC </td><td>Religion</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000379868500008</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Gallegos-Hernandez, JF
   <br>Cruz-Esquivel, I
   <br>Ortiz-Maldonado, AL
   <br>Minauro-Munoz, GG
   <br>Arias-Ceballos, H
   <br>Pichardo-Romero, P</td>
</tr>

<tr>
<td valign="top">AF </td><td>Francisco Gallegos-Hernandez, Jose
   <br>Cruz-Esquivel, Ivan
   <br>Lilia Ortiz-Maldonado, Alma
   <br>Gabriel Minauro-Munoz, Gerardo
   <br>Arias-Ceballos, Hector
   <br>Pichardo-Romero, Pablo</td>
</tr>

<tr>
<td valign="top">TI </td><td>Laringeal conservative surgery in patients candidates for combined
   treatment with chemo-radiotherapy</td>
</tr>

<tr>
<td valign="top">SO </td><td>CIRUGIA Y CIRUJANOS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Laryngeal cancer; Conservative surge; Conservative laryngectony; Partial
   laryngectom</td>
</tr>

<tr>
<td valign="top">ID </td><td>LARYNX PRESERVATION; CANCER; RADIOTHERAPY; LARYNGECTOMY</td>
</tr>

<tr>
<td valign="top">AB </td><td>Background: The standard of care for advanced-stage laryngeal cancer is combined treatment (chemo-radiotherapy). However, the complications with this treatment are not few, mainly in swallowing. Conservative laryngeal surgery remains an effective alternative for cancer control without the complications of chemo-radiotherapy.
   <br>Material and methods: Retrospective study was conducted on patients with laryngeal cancer cT3, cNO with paraglottic infiltration, fixation of the vocal cord, minimal invasion of the hyothyroepiglottic space, but with normal arytenoid mobility and no sub-glottic extension, were treated with subtotal supracricoid laryngectomy. Complications, sequels of treatment, and local recurrence were evaluated. Bronchial aspiration was studied with radioactive swallow.
   <br>Results: There were 25 patients, 22 with negative surgical margins, one had tumour contact with the surgical margins, and 2 were positive. Two patients received postoperative radiotherapy. The mean decannulation was 15 days and removal of nasogastric tube 25 days. During the mean follow-up of 26 months, none of the patients had tumour recurrence or required conversion to total laryngectomy. In all patients swallowing has been normal and none required permanent or temporary tracheotomy or definitive gastrostomy. The voice is considered intelligible in all patients. Radioactive swallow showed aspiration in 15/25 patients, with none being clinically relevant. There were postoperative complications in 5 patients, and 4 patients required re-intervention but no conversion to total laryngectomy.
   <br>Conclusion: Conservative surgery is an effective surgical-alternative to chemo-radiotherapy in patients with locally advanced laryngeal cancer, providing oncological control, acceptable complications and minimal sequels. Although most patients have aspiration, this does not affect functional status. (C) 2015 Published by Masson Doyma Mexico S.A.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Francisco Gallegos-Hernandez, Jose; Cruz-Esquivel, Ivan; Lilia
   Ortiz-Maldonado, Alma; Gabriel Minauro-Munoz, Gerardo; Arias-Ceballos,
   Hector] Inst Mexicano Seguro Social, Ctr Med Nacl Siglo 21, Hosp Oncol,
   Dept Tumores Cabeza &amp; Cuello, Mexico City, DF, Mexico.
   <br>[Pichardo-Romero, Pablo] Inst Mexicano Seguro Social, Ctr Med Nacl Siglo
   21, Hosp Oncol, Dept Med Nucl, Mexico City, DF, Mexico.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Gallegos-Hernandez, JF (reprint author), Inst Mexicano Seguro Social, Ctr Med Nacl Siglo 21, Hosp Oncol, Dept Tumores Cabeza &amp; Cuello, Mexico City, DF, Mexico.; Gallegos-Hernandez, JF (reprint author), Ave Cuauhtemoc 330, Mexico City 6725, DF, Mexico.</td>
</tr>

<tr>
<td valign="top">EM </td><td>gal61@prodigy.net.mx</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAR-APR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>84</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>96</td>
</tr>

<tr>
<td valign="top">EP </td><td>101</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.circir.2015.10.009</td>
</tr>

<tr>
<td valign="top">SC </td><td>Surgery</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000372807500002</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Aliwi, HSH
   <br>Sumari, P</td>
</tr>

<tr>
<td valign="top">AF </td><td>Aliwi, Hadeel Saleh Haj
   <br>Sumari, Putra</td>
</tr>

<tr>
<td valign="top">TI </td><td>A PROPOSED MAPPING ARCHITECTURE BETWEEN IAX AND JINGLE PROTOCOLS</td>
</tr>

<tr>
<td valign="top">SO </td><td>ADVANCES IN SCIENCE AND TECHNOLOGY-RESEARCH JOURNAL</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>signaling protocol; mapping architecture; IAX; Jingle; translation
   gateway</td>
</tr>

<tr>
<td valign="top">AB </td><td>Nowadays, multimedia communication has improved rapidly to allow people to communicate via the Internet. However, Internet users cannot communicate with each other unless they use the same chatting applications since each chatting application uses a certain signaling protocol to make the media call. The mapping architecture is a very critical issue since it solves the communication problems between any two protocols, as well as it enables people around the world to make a voice/video call even if they use different chatting applications. Providing the interoperability between different signaling protocols and multimedia applications takes the advantages of more than one protocol. Many mapping architectures have been proposed to ease exchanging the media between at least two users without facing any difficulties such as SIP-Jingle, IAX-RSW, H. 323-MGCP, etc. However, the design of any of the existing mapping architectures has some weaknesses related to larger delay, time consuming, and security matters. The only way to overcome these problems is to propose an efficient mapping architecture. This paper proposed a new mapping architecture between Inter-Asterisk eXchange Protocol and Jingle Protocol. The proposed mapping architecture consists of IAX domain (IAX client, IAX server, IAX-to-Jingle gateway), and Jingle domain (Jingle client, Jingle server, Jingle-to-IAX gateway). The tasks of the translation gateways are represented by the URI conversion, media capability exchange, translator of call setup and teardown signals, and real time media transmission.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Aliwi, Hadeel Saleh Haj; Sumari, Putra] Univ Sains Malaysia, Sch Comp
   Sci, Pulau Penang 11800, Malaysia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Aliwi, HSH; Sumari, P (reprint author), Univ Sains Malaysia, Sch Comp Sci, Pulau Penang 11800, Malaysia.</td>
</tr>

<tr>
<td valign="top">EM </td><td>hadeelsaleh12@yahoo.com; putras@usm.my</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>10</td>
</tr>

<tr>
<td valign="top">IS </td><td>29</td>
</tr>

<tr>
<td valign="top">BP </td><td>141</td>
</tr>

<tr>
<td valign="top">EP </td><td>146</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000371475500015</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Zhu, YC
   <br>Yang, XL
   <br>Fu, TT</td>
</tr>

<tr>
<td valign="top">AF </td><td>Zhu, Yuchuan
   <br>Yang, Xulei
   <br>Fu, Tiantian</td>
</tr>

<tr>
<td valign="top">TI </td><td>Dynamic modeling and experimental investigations of a magnetostrictive
   nozzle-flapper servovalve pilot stage</td>
</tr>

<tr>
<td valign="top">SO </td><td>PROCEEDINGS OF THE INSTITUTION OF MECHANICAL ENGINEERS PART I-JOURNAL OF
   SYSTEMS AND CONTROL ENGINEERING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>System modeling; system simulation; servohydraulic systems; dynamic
   modeling; physical modelling</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE COIL MOTOR; VALVE; HYSTERESIS; SIMULATION; DESIGN; ACTUATORS</td>
</tr>

<tr>
<td valign="top">AB </td><td>A magnetostrictive nozzle-flapper servovalve pilot stage is presented in this article, which is directly driven by a giant magnetostrictive actuator and features three nozzles for the development of large flow rate servovalve. According to the energy conversion sequence in this servovalve, a giant magnetostrictive actuator magnetization model, a giant magnetostrictive material rod eddy loss model and a servovalve dynamic pressure model are all established to enable quantitative depiction and modelling of the dynamic pressure response process of magnetostrictive nozzle-flapper servovalve pilot stage. Consequently, the matched simulation model of the magnetostrictive nozzle-flapper servovalve pilot stage with the mathematic model is followed to be established, and two unknown parameters of complex permeability are determined using the test data from the giant magnetostrictive actuator. By running this simulation model, flapper displacement and output pressure under different structural parameters and variational excited frequencies are determined, certain parameters that are sensitive to the dynamic characteristics of magnetostrictive nozzle-flapper servovalve pilot stage driven by giant magnetostrictive actuator are found and the accompanying rules are revealed. Finally, the experimental system of a magnetostrictive nozzle-flapper servovalve pilot stage driven by giant magnetostrictive actuator was built; both the step-input voltage response curve and the sine-input voltage response curve were captured; and these curves show that the amplitude bandwidth (-3dB) and the phase bandwidth (-90 degrees) of a magnetostrictive nozzle-flapper servovalve pilot stage can approach 150 and 110Hz, respectively, which exhibit good agreement with the simulation results. Therefore, the magnetostrictive nozzle-flapper servovalve pilot stage offers a very promising prospect of the novel servovalves with the high-frequency response and the large flow rate.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Zhu, Yuchuan; Yang, Xulei; Fu, Tiantian] Nanjing Univ Aeronaut &amp;
   Astronaut, Coll Mech &amp; Elect Engn, Nanjing 210016, Peoples R China.
   <br>[Zhu, Yuchuan] Univ Maryland, Dept Aerosp Engn, College Pk, MD 20742 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Zhu, YC (reprint author), Nanjing Univ Aeronaut &amp; Astronaut, Coll Mech &amp; Elect Engn, Nanjing 210016, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>meeyczhu@nuaa.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>230</td>
</tr>

<tr>
<td valign="top">IS </td><td>3</td>
</tr>

<tr>
<td valign="top">BP </td><td>244</td>
</tr>

<tr>
<td valign="top">EP </td><td>254</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1177/0959651815621668</td>
</tr>

<tr>
<td valign="top">SC </td><td>Automation &amp; Control Systems</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000371324600004</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Lee, KS</td>
</tr>

<tr>
<td valign="top">AF </td><td>Lee, Ki-Seung</td>
</tr>

<tr>
<td valign="top">TI </td><td>Speech synthesis using acoustic Doppler signal</td>
</tr>

<tr>
<td valign="top">SO </td><td>JOURNAL OF THE ACOUSTICAL SOCIETY OF KOREA</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speech synthesis; Ultrasonic Doppler signals; Silence speech interface;
   Voice conversion</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, a method synthesizing speech signal using the 40 kHz ultrasonic signals reflected from the articulatory muscles was introduced and performance was evaluated. When the ultrasound signals are radiated to articulating face, the Doppler effects caused by movements of lips, jaw, and chin observed. The signals that have different frequencies from that of the transmitted signals are found in the received signals. These ADS (Acoustic-Doppler Signals) were used for estimating of the speech parameters in this study. Prior to synthesizing speech signal, a quantitative correlation analysis between ADS and speech signals was carried out on each frequency bin. According to the results, the feasibility of the ADS-based speech synthesis was validated. ADS-to-speech transformation was achieved by the joint Gaussian mixture model-based conversion rules. The experimental results from the 5 subjects showed that filter bank energy and LPC (Linear Predictive Coefficient) cepstrum coefficients are the optimal features for ADS, and speech, respectively. In the subjective evaluation where synthesized speech signals were obtained using the excitation sources extracted from original speech signals, it was confirmed that the ADS-to-speech conversion method yielded 72.2 % average recognition rates.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Lee, Ki-Seung] Konkuk Univ, Dept Elect Engn, 120 Neungdong Ro, Seoul
   05029, South Korea.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Lee, KS (reprint author), Konkuk Univ, Dept Elect Engn, 120 Neungdong Ro, Seoul 05029, South Korea.</td>
</tr>

<tr>
<td valign="top">EM </td><td>kseung@konkuk.ac.kr</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>35</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>134</td>
</tr>

<tr>
<td valign="top">EP </td><td>142</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.7776/ASK.2016.35.2.134</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000409141600007</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Li, YH
   <br>Tian, B
   <br>Yi, KC
   <br>Yu, Q</td>
</tr>

<tr>
<td valign="top">AF </td><td>Li, Yunhua
   <br>Tian, Bin
   <br>Yi, Ke-Chu
   <br>Yu, Quan</td>
</tr>

<tr>
<td valign="top">TI </td><td>An Effective Carrier Frequency and Phase Offset Tracking Scheme in the
   Case of Symbol Rate Sampling</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEICE TRANSACTIONS ON COMMUNICATIONS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>carrier synchronization; FLL-assisted-PLL; low SNR; symbol rate
   sampling; LDPC; voice coder</td>
</tr>

<tr>
<td valign="top">ID </td><td>PARITY-CHECK CODES; RECOVERY LOOP; SYNCHRONIZATION; ALGORITHMS;
   RECEIVERS; DESIGN; NOISE</td>
</tr>

<tr>
<td valign="top">AB </td><td>In modern communication systems, it is a critical and challenging issue for existing carrier tracking techniques to achieve near-ideal carrier synchronization without the help of pilot signals in the case of symbol rate sampling and low signal-to-noise ratio (SNR). To overcome this issue, this paper proposes an effective carrier frequency and phase offset tracking scheme which has a robust confluent synchronization architecture whose main components are a digital frequency-locked loop (FLL), a digital phase-locked loop (PLL), a modified symbol hard decision block and some sampling rate conversion blocks. As received signals are sampled at symbol baud rate, this carrier tracking scheme is still able to obtain precise estimated values of carrier synchronization parameters under the condition of very low SNRs. The performance of the proposed carrier synchronization scheme is also evaluated by using MonteCarlo method. Simulation results confirm the feasibility of this carrier tracking scheme and demonstrate that it ensures that both the rate-3/4 irregular low-density parity-code (LDPC) coded system and the military voice transmission system utilizing the direct sequence spread spectrum (DSSS) technique achieve satisfactory bit-error rate (BER) performance at correspondingly low SNRs.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Li, Yunhua; Tian, Bin; Yi, Ke-Chu; Yu, Quan] Xidian Univ, State Key Lab
   Integrated Serv Networks, Xian, Peoples R China.
   <br>[Tian, Bin; Yi, Ke-Chu; Yu, Quan] Xidian Univ, Sch Telecommun Engn,
   Xian, Peoples R China.
   <br>[Yu, Quan] Inst Chinese Elect Equipment Syst Engn Corp, Beijing, Peoples
   R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Li, YH (reprint author), Xidian Univ, State Key Lab Integrated Serv Networks, Xian, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>yunhuali0816@126.com; btian@xidian.edu.cn; kchyi@mail.xidian.edu.cn;
   yuquan@public3.bta.net.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>FEB</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>E99B</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>337</td>
</tr>

<tr>
<td valign="top">EP </td><td>346</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1587/transcom.2015EBP3186</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000374778900006</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Pouget, M
   <br>Nahorna, O
   <br>Hueber, T
   <br>Bailly, G</td>
</tr>

<tr>
<td valign="top">AF </td><td>Pouget, Mael
   <br>Nahorna, Olha
   <br>Hueber, Thomas
   <br>Bailly, Gerard</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Adaptive Latency for Part-of-Speech Tagging in Incremental
   Text-to-Speech Synthesis</td>
</tr>

<tr>
<td valign="top">SO </td><td>17TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2016), VOLS 1-5: UNDERSTANDING SPEECH
   PROCESSING IN HUMANS AND MACHINES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>17th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2016)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 08-12, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>San Francisco, CA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Incremental speech synthesis; natural language processing;
   classification; TTS; part-of-speech</td>
</tr>

<tr>
<td valign="top">AB </td><td>Incremental text-to-speech systems aim at synthesizing a text 'on-the-fly', while the user is typing a sentence. In this context, this article addresses the problem of the part-of-speech tagging (POS, i.e. lexical category) which is a critical step for accurate grapheme-to-phoneme conversion and prosody estimation. Here, the main challenge is to estimate the POS of a given word without knowing its 'right context' (i.e. the following words which are not available yet). To address this issue, we propose a method based on a set of decision trees estimating online whether a given POS tag is likely to be modified when more right-contextual information becomes available. In such a case, the synthesis is delayed until POS stability is guaranteed. This results in delivering the synthetic voice in word chunks of variable length. Objective evaluation on French shows that the proposed method is able to estimate POS tags with more than a 92% accuracy (compared to a non-incremental system) while minimizing the synthesis latency (between 1 and 4 words). Perceptual evaluation (ranking test) is then carried in the context of HMM-based speech synthesis. Experimental results show that the word grouping resulting from the proposed method is rated more acceptable than word-by-word incremental synthesis.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Pouget, Mael; Nahorna, Olha; Hueber, Thomas; Bailly, Gerard] CNRS,
   GIPSA Lab, Grenoble, France.
   <br>[Pouget, Mael; Nahorna, Olha; Hueber, Thomas; Bailly, Gerard] Univ
   Grenoble Alpes, GIPSA Lab, Grenoble, France.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Pouget, M (reprint author), CNRS, GIPSA Lab, Grenoble, France.; Pouget, M (reprint author), Univ Grenoble Alpes, GIPSA Lab, Grenoble, France.</td>
</tr>

<tr>
<td valign="top">EM </td><td>mael.pouget@gipsalab.fr; olha.nahorna@gipsalab.fr;
   thomas.hueber@gipsalab.fr; gerard.bailly@gipsalab.fr</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>2846</td>
</tr>

<tr>
<td valign="top">EP </td><td>2850</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2016-165</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science; Engineering; Linguistics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000409394401274</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Chen, YY
   <br>Wu, CH
   <br>Huang, YF</td>
</tr>

<tr>
<td valign="top">AF </td><td>Chen, Yan-You
   <br>Wu, Chung-Hsien
   <br>Huang, Yu -Fong</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Generation of Emotion Control Vector using MDS-based Space
   Transformation for Expressive Speech Synthesis</td>
</tr>

<tr>
<td valign="top">SO </td><td>17TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2016), VOLS 1-5: UNDERSTANDING SPEECH
   PROCESSING IN HUMANS AND MACHINES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>17th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2016)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 08-12, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>San Francisco, CA</td>
</tr>

<tr>
<td valign="top">DE </td><td>speech synthesis; emotion control; control vector generation</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION; MARKOV MODEL</td>
</tr>

<tr>
<td valign="top">AB </td><td>In control vector-based expressive speech synthesis, the emotion/style control vector defined in the categorical (CAT) emotion space is uneasy to be precisely defined by the user to synthesize the speech with the desired emotion/style. This paper applies the arousal-valence (AV) space to the multiple regression hidden semi-Markov model (MRHSMM)-based synthesis framework for expressive speech synthesis. In this study, the user can designate a specific emotion by defining the AV values in the AV space. The multidimensional scaling (MDS) method is adopted to project the AV emotion space and the categorical (CAT) emotion space onto their corresponding orthogonal coordinate systems. A transformation approach is thus proposed to transform the AV values to the emotion control vector in CAT emotion space for MRHSMM-based expressive speech synthesis. In the synthesis phase given the input text and desired emotion, with the transformed emotion control vector, the speech with the desired emotion is generated from the MRHSMMs. Experimental result shows the proposed method is helpful for the user to easily and precisely determine the desired emotion for expressive speech synthesis.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Chen, Yan-You] Natl Cheng Kung Univ, Dept Elect Engn, Tainan, Taiwan.
   <br>[Wu, Chung-Hsien; Huang, Yu -Fong] Natl Cheng Kung Univ, Dept Comp Sci &amp;
   Informat Engn, Tainan, Taiwan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Chen, YY (reprint author), Natl Cheng Kung Univ, Dept Elect Engn, Tainan, Taiwan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>n2896136@mail.ncku.edu.tw; chunghsienwu@gmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>3176</td>
</tr>

<tr>
<td valign="top">EP </td><td>3180</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.21437/Interspeech.2016-815</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science; Engineering; Linguistics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000409394402016</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ghosh, P
   <br>Chingtham, TS
   <br>Ghose, MK</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ghosh, Papri
   <br>Chingtham, Tejbanta Singh
   <br>Ghose, Mrinal Kanti</td>
</tr>

<tr>
<td valign="top">BE </td><td>Bhattacharyya, S
   <br>Bhaumik, AK
   <br>Dutta, HS
   <br>Maulik, U
   <br>Bhattacharjee, D
   <br>Majumdar, D
   <br>Gao, XZ
   <br>Pan, I
   <br>Bhaumik, H
   <br>Mondal, A</td>
</tr>

<tr>
<td valign="top">TI </td><td>SLHAR: A Supervised Learning approach for Homophone Ambiguity Reduction
   from Speech Recognition System</td>
</tr>

<tr>
<td valign="top">SO </td><td>2016 SECOND IEEE INTERNATIONAL CONFERENCE ON RESEARCH IN COMPUTATIONAL
   INTELLIGENCE AND COMMUNICATION NETWORKS (ICRCICN)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>2nd IEEE International Conference on Research in Computational
   Intelligence and Communication Networks (ICRCICN)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 23-25, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Kolkata, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Human Computer Interaction (HCI); Homophones; Speech Recognition System
   (SRS); Speech-to-Text (STT); Ambiguity; Supervised Learning; Machine
   Learning; Supervised Learning based Homophone Ambiguity Reduction
   (SLHAR)</td>
</tr>

<tr>
<td valign="top">AB </td><td>An automatic Speech to Text (STT) conversion technology has been developed for making a visual text layout of the Speech Input for advancement of Science and Technology. This technology enables people an alternative way to understand voice communication, and pursue instruction using their visual ability. The visual ability becomes more powerful than the listening ability some time more than even in remote communication, and STT conversion plays a role as an important tool in such cases. The system faces many kind of ambiguity during STT. The research focuses on the Homophone Ambiguity and with the help of the Classified Supervised learning it tries to improve it partially. In the proposed Supervised learning based Homophone Ambiguity Reduction (SLHAR), a large dataset are taken as homophones and Homophone Sets are assembled by Hierarchical Clustering Method. The proposed system communicates with the user in case of Homophones and converts them to the text format.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ghosh, Papri] MAKAUT, SIEM, Comp Sci &amp; Engn, Kolkata, W Bengal, India.
   <br>[Chingtham, Tejbanta Singh; Ghose, Mrinal Kanti] SMIT, Comp Sci &amp; Engn,
   Sikkim, Smu, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ghosh, P (reprint author), MAKAUT, SIEM, Comp Sci &amp; Engn, Kolkata, W Bengal, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>papri.mss@gmail.com; chingtham@gmail.com; mrinal.ghose@smu.edu.in</td>
</tr>

<tr xmlns:date="http://exslt.org/dates-and-times">
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>kanti ghose, mrinal</display_name>&nbsp;</font></td><td><font size="3">0000-0002-9240-0444&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>12</td>
</tr>

<tr>
<td valign="top">EP </td><td>16</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000404433200003</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Xue, YW
   <br>Hamada, Y
   <br>Akagi, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Xue, Yawen
   <br>Hamada, Yasuhiro
   <br>Akagi, Masato</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Conversion to Emotional Speech based on Three-layered Model in
   Dimensional Approach and Parameterization of Dynamic Features in Prosody</td>
</tr>

<tr>
<td valign="top">SO </td><td>2016 ASIA-PACIFIC SIGNAL AND INFORMATION PROCESSING ASSOCIATION ANNUAL
   SUMMIT AND CONFERENCE (APSIPA)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Asia-Pacific Signal and Information Processing Association Annual Summit
   and Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Asia-Pacific Signal and Information Processing Association Annual Summit
   and Conference (APSIPA)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 13-16, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Jeju, SOUTH KOREA</td>
</tr>

<tr>
<td valign="top">ID </td><td>PERCEPTION; EXTRACTION; INFERENCE; SYSTEM</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper proposes a system to convert neutral speech to emotional with controlled intensity of emotions. Most of previous researches considering synthesis of emotional voices used statistical or concatenative methods that can synthesize emotions in categorical emotional states such as joy, angry, sad, etc. While humans sometimes enhance or relieve emotional states and intensity during daily life, synthesized emotional speech in categories is not enough to describe these phenomena precisely. A dimensional approach which can represent emotion as a point in a dimensional space can express emotions with continuous intensity. Employing the dimensional approach to describe emotion, we conduct a three-layered model to estimate displacement of the acoustic features of the target emotional speech from that of source (neutral) speech and propose a rule-based conversion method to modify acoustic features of source (neutral) speech to synthesize the target emotional speech. To convert the source speech freely and easily, we introduce two methods to parameterize dynamic features in prosody, that is, Fujisaki model for f0 contour and target prediction model for power envelope. Evaluation results show that subjects can perceive intended emotion with satisfactory order of emotional intensity and naturalness. This fact means that this system not only has the ability to synthesize emotional speech in category but also can control the order of emotional intensity in dimensional space even in the same emotion category.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Xue, Yawen; Akagi, Masato] Japan Adv Inst Sci &amp; Technol, Nomi,
   Ishikawa, Japan.
   <br>[Hamada, Yasuhiro] Meiji Univ, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Xue, YW (reprint author), Japan Adv Inst Sci &amp; Technol, Nomi, Ishikawa, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>xue_yawen@jaist.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000393591800018</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Perfetto, S
   <br>Infante, F
   <br>Mayer, D
   <br>Herold, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Perfetto, S.
   <br>Infante, F.
   <br>Mayer, D.
   <br>Herold, S.</td>
</tr>

<tr>
<td valign="top">BE </td><td>Sas, P
   <br>Moens, D
   <br>VanDeWalle, A</td>
</tr>

<tr>
<td valign="top">TI </td><td>Impact of damping factor on the effectiveness of an energy harvesting
   vibration absorber</td>
</tr>

<tr>
<td valign="top">SO </td><td>PROCEEDINGS OF ISMA2016 INTERNATIONAL CONFERENCE ON NOISE AND VIBRATION
   ENGINEERING AND USD2016 INTERNATIONAL CONFERENCE ON UNCERTAINTY IN
   STRUCTURAL DYNAMICS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Noise and Vibration Engineering (ISMA) /
   International Conference on Uncertainty in Structural Dynamics (USD)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 19-21, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Leuven, BELGIUM</td>
</tr>

<tr>
<td valign="top">ID </td><td>EDDY-CURRENT DAMPER; SUPPRESSION; BEAM</td>
</tr>

<tr>
<td valign="top">AB </td><td>Vibration absorbers are usually used to reduce the vibration of mechanical systems. On the other hand, the vibration can also excite piezoelectric transducers for mechanical-to-electrical energy conversion in order to have self-powered sensors. The transducers can be integrated in the structure of the absorber obtaining an energy harvesting vibration absorber (EHVA). Nevertheless, despite the structures of absorber and of energy harvester can be very similar, the parameters necessary to design optimal devices are usually different. In particular, the requirements about the damping are in contrast. In this work the impact of the EHVA damping on the effectiveness of vibration reduction and power generation has been investigated in simulation and experimentally. A test rig with adjustable damping via an active control has been set up for experimental validation. A voltage driven voice coil actuator has been used for this purpose with a relative velocity feedback control.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Perfetto, S.; Mayer, D.] Fraunhofer Inst LBF, Dept Reliabil &amp; Syst
   Integrat, Darmstadt, Germany.
   <br>[Infante, F.; Herold, S.] Fraunhofer Inst LBF, Dept Vibrat Optimizat
   Technol, Bartningstr 47, D-64289 Darmstadt, Germany.</td>
</tr>

<tr>
<td valign="top">EM </td><td>sara.perfetto@lbf.fraunhofer.de</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Mayer, Dirk</display_name>&nbsp;</font></td><td><font size="3">0000-0002-4972-6529&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>1511</td>
</tr>

<tr>
<td valign="top">EP </td><td>1525</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000392486302040</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Korshunov, P
   <br>Marcel, S
   <br>Muckenhirn, H
   <br>Goncalves, AR
   <br>Mello, AGS
   <br>Violato, RPV
   <br>Simoes, FO
   <br>Neto, MU
   <br>Angeloni, MDA
   <br>Stuchi, JA
   <br>Dinkel, H
   <br>Chen, N
   <br>Qian, Y
   <br>Paul, D
   <br>Saha, G
   <br>Sahidullah, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Korshunov, P.
   <br>Marcel, S.
   <br>Muckenhirn, H.
   <br>Goncalves, A. R.
   <br>Mello, A. G. Souza
   <br>Violato, R. P. Velloso
   <br>Simoes, F. O.
   <br>Neto, M. U.
   <br>Angeloni, M. de Assis
   <br>Stuchi, J. A.
   <br>Dinkel, H.
   <br>Chen, N.
   <br>Qian, Y.
   <br>Paul, D.
   <br>Saha, G.
   <br>Sahidullah, Md</td>
</tr>

<tr>
<td valign="top">CA </td><td>Idiap Team
   <br>CPqD Team
   <br>SJTUSpeech Team
   <br>IITKGP ABSP team</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Overview of BTAS 2016 Speaker Anti-spoofing Competition</td>
</tr>

<tr>
<td valign="top">SO </td><td>2016 IEEE 8TH INTERNATIONAL CONFERENCE ON BIOMETRICS THEORY,
   APPLICATIONS AND SYSTEMS (BTAS)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Biometrics Theory Applications and Systems</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>8th IEEE International Conference on Biometrics - Theory, Applications
   and Systems (BTAS)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 06-09, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Niagara Falls, NY</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper provides an overview of the Speaker Anti spoofing Competition organized by Biometric group at Idiap Research Institute for the IEEE International Conference on Biometrics: Theory, Applications, and Systems (BTAS 2016). The competition used AVspoof database, which contains a comprehensive set of presentation attacks, including, (i) direct replay attacks when a genuine data is played back using a laptop and two phones (Samsung Galaxy S4 and iPhone 3G), (ii) synthesized speech replayed with a laptop, and (iii) speech created with a voice conversion algorithm, also replayed with a laptop.
   <br>The paper states competition goals, describes the database and the evaluation protocol, discusses solutions for spoofing or presentation attack detection submitted by the participants, and presents the results of the evaluation.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Korshunov, P.; Marcel, S.; Muckenhirn, H.; Idiap Team] Idiap Res Inst,
   Martigny, Switzerland.
   <br>[Goncalves, A. R.; Mello, A. G. Souza; Violato, R. P. Velloso; Simoes,
   F. O.; Neto, M. U.; Angeloni, M. de Assis; Stuchi, J. A.; CPqD Team] Ctr
   Res &amp; Dev Telecommun, Sao Paulo, Brazil.
   <br>[Dinkel, H.; Chen, N.; Qian, Y.; SJTUSpeech Team] Shanghai Jiao Tong
   Univ, Shanghai, Peoples R China.
   <br>[Paul, D.; Saha, G.] Indian Inst Technol, Kharagpur, W Bengal, India.
   <br>[Sahidullah, Md; IITKGP ABSP team] Univ Eastern Finland, Joensuu,
   Finland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Korshunov, P (reprint author), Idiap Res Inst, Martigny, Switzerland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>pavel.korshunov@idiap.ch; sebastien.marcelpavel.korshunov@idiap.ch;
   hannah.muckenhirnpavel.korshunov@idiap.ch; andrerg@cpqd.com.br;
   amello@cpqd.com.br; rviolato@cpqd.com.br; simoes@cpqd.com.br;
   uliani@cpqd.com.br; massis@cpqd.com.br; jastuchi@cpqd.com.br;
   heinrich.dinkel@sjtu.edu.cn; bobchennan@jhu.edu; yanminqian@sjtu.edu.cn;
   dipjyotipaul@ece.iitkgp.ernet.in; gsaha@ece.iitkgp.ernet.in;
   sahid@cs.uef.fi</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">SC </td><td>Mathematical &amp; Computational Biology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000392217100046</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Tanaka, K
   <br>Toda, T
   <br>Neubig, G
   <br>Nakamura, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Tanaka, Kou
   <br>Toda, Tomoki
   <br>Neubig, Graham
   <br>Nakamura, Satoshi</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Real-Time Vibration Control of An Electrolarynx based on Statistical F-0
   Contour Prediction</td>
</tr>

<tr>
<td valign="top">SO </td><td>2016 24TH EUROPEAN SIGNAL PROCESSING CONFERENCE (EUSIPCO)</td>
</tr>

<tr>
<td valign="top">SE </td><td>European Signal Processing Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>24th European Signal Processing Conference (EUSIPCO)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 28-SEP 02, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Budapest, HUNGARY</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION; SPEECH ENHANCEMENT</td>
</tr>

<tr>
<td valign="top">AB </td><td>An electrolarynx is a speaking aid device to artificially generate excitation sounds to help laryngectomees produce electrolaryngeal (EL) speech. Although EL speech is quite intelligible, its naturalness significantly suffers from the unnatural fundamental frequency (F-0) patterns of the mechanical excitation sounds. To make it possible to produce more naturally sounding EL speech, we have proposed a method to automatically control F-0 patterns of the excitation sounds generated from the electrolarynx based on the statistical F-0 prediction, which predicts F-0 patterns from the produced EL speech in real-time. In our previous work, we have developed a prototype system by implementing the proposed real-time prediction method in an actual, physical electrolarynx, and through the use of the prototype system, we have found that improvements of the naturalness of EL speech yielded by the prototype system tend to be lower than that yielded by the batch-type prediction. In this paper, we examine negative impacts caused by latency of the real-time prediction on the F-0 prediction accuracy, and to alleviate them, we also propose two methods, 1) modeling of segmented continuous F-0 (CF0) patterns and 2) prediction of forthcoming F-0 values. The experimental results demonstrate that 1) the conventional real-time prediction method needs a large delay to predict CF0 patterns and 2) the proposed methods have positive impacts on the real-time prediction.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Tanaka, Kou; Neubig, Graham; Nakamura, Satoshi] Nara Inst Sci &amp;
   Technol, Grad Sch Informat Sci, 8916-5 Takayama Cho, Ikoma, Nara, Japan.
   <br>[Toda, Tomoki] Nagoya Univ, Informat Technol Ctr, Chikusa Ku, Furo Cho,
   Nagoya, Aichi, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Tanaka, K (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, 8916-5 Takayama Cho, Ikoma, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ko-t@is.naist.jp; tomoki@icts.nagoya-u.ac.jp; neubig@is.naist.jp;
   s-nakamura@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>1333</td>
</tr>

<tr>
<td valign="top">EP </td><td>1337</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000391891900255</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Muckenhirn, H
   <br>Magimai-Doss, M
   <br>Marcel, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Muckenhirn, Hannah
   <br>Magimai-Doss, Mathew
   <br>Marcel, Sebastien</td>
</tr>

<tr>
<td valign="top">BE </td><td>Bromme, A
   <br>Busch, C
   <br>Rathgeb, C
   <br>Uhl, A</td>
</tr>

<tr>
<td valign="top">TI </td><td>Presentation Attack Detection Using Long-Term Spectral Statistics for
   Trustworthy Speaker Verification</td>
</tr>

<tr>
<td valign="top">SO </td><td>PROCEEDINGS OF THE 15TH INTERNATIONAL CONFERENCE OF THE BIOMETRICS
   SPECIAL INTEREST GROUP (BIOSIG 2016)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Lecture Notes in Informatics-Proceedings</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>15th International Conference of the Biometrics Special Interest Group
   (BIOSIG)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 21-23, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Darmstadt, GERMANY</td>
</tr>

<tr>
<td valign="top">ID </td><td>AVERAGE SPECTRUM; FREQUENCY</td>
</tr>

<tr>
<td valign="top">AB </td><td>In recent years, there has been a growing interest in developing countermeasures against non zero-effort attacks for speaker verification systems. Until now, the focus has been on logical access attacks, where the spoofed samples are injected into the system through a software-based process. This paper investigates a more realistic type of attack, referred to as physical access or presentation attacks, where the spoofed samples are presented as input to the microphone. To detect such attacks, we propose a binary classifier based approach that uses long-term spectral statistics as feature input. Experimental studies on the AVspoof database, which contains presentation attacks based on replay, speech synthesis and voice conversion, shows that the proposed approach can yield significantly low detection error rate with a linear classifier (half total error rate of 0.038%). Furthermore, an investigation on Interspeech 2015 ASVspoof challenge dataset shows that it is equally capable of detecting logical access attacks.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Muckenhirn, Hannah; Magimai-Doss, Mathew; Marcel, Sebastien] Idiap Res
   Inst, Martigny, Switzerland.
   <br>[Muckenhirn, Hannah] Ecole Polytech Fed Lausanne, Lausanne, Switzerland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Muckenhirn, H (reprint author), Idiap Res Inst, Martigny, Switzerland.; Muckenhirn, H (reprint author), Ecole Polytech Fed Lausanne, Lausanne, Switzerland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>hannah.muckenhirn@idiap.ch; mathew@idiap.ch; sebastien.marcel@idiap.ch</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>P-260</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000391421200034</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Karotiya, NA
   <br>Wyawahare, NP
   <br>Haridas, SL</td>
</tr>

<tr>
<td valign="top">AF </td><td>Karotiya, Nikhil A.
   <br>Wyawahare, Nikhil P.
   <br>Haridas, S. L.</td>
</tr>

<tr>
<td valign="top">BE </td><td>Devamalar, PMB
   <br>Bai, VT
   <br>Moorthi, M</td>
</tr>

<tr>
<td valign="top">TI </td><td>Review paper on Point to Point Communication with the use of Power over
   Ethernet based on VOIP system on Asterisk</td>
</tr>

<tr>
<td valign="top">SO </td><td>PROCEEDINGS OF THE 2016 IEEE 2ND INTERNATIONAL CONFERENCE ON ADVANCES IN
   ELECTRICAL &amp; ELECTRONICS, INFORMATION, COMMUNICATION &amp; BIO INFORMATICS
   (IEEE AEEICB-2016)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>2nd IEEE International Conference On Advances in Electrical and
   Electronics, Information, Communication and Bio Informatics (IEEE
   AEEICB)</td>
</tr>

<tr>
<td valign="top">CY </td><td>FEB 27-28, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Lush Green Prathyusha Campus, Chennai, INDIA</td>
</tr>

<tr>
<td valign="top">HO </td><td>Lush Green Prathyusha Campus</td>
</tr>

<tr>
<td valign="top">DE </td><td>Voice over internet protocol (VOIP); ISM; Wi-Fi</td>
</tr>

<tr>
<td valign="top">AB </td><td>Nowadays point to point communication have been drawing a lot of interest, because in this it communicate at specific place as per our choice at a time so that all signal will go in one direction which avoid wastage of signal which spread in all direction. ISM (Industrial, Scientific and Medical) band is a part of the radio spectrum that can be used for many purposes without a license. It is also called unlicensed band. Its range is 2.4-2.483Ghz. For good bandwidth and long transmission ranges the unused portion of ISM provides potential for wireless network. White space is a vacant frequency and its device operates only in unoccupied frequency. Indexing of antenna means to give a unique code or ID to each requirement. Conversion of antenna means to send a RF signal from Omni-directional pattern to directional way. VOIP is a voice over internet protocol useful for transmission of voice data over internet. Asterisk is a software package use to do calling, video conference and many features.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Karotiya, Nikhil A.; Wyawahare, Nikhil P.] GH Raisoni Coll Engn, Dept
   Elect Engn, Nagpur, Maharashtra, India.
   <br>[Haridas, S. L.] GH Raisoni Coll Engn, Dept Elect &amp; Telecommun Engn,
   Nagpur, Maharashtra, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Karotiya, NA (reprint author), GH Raisoni Coll Engn, Dept Elect Engn, Nagpur, Maharashtra, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>nikskrty@gmail.com; nikhil.wyawahare@raisoni.net;
   sanjay.haridas@raisoni.net</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>729</td>
</tr>

<tr>
<td valign="top">EP </td><td>732</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000391073300149</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Subashini, P
   <br>Krishnaveni, M
   <br>Manjutha, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Subashini, P.
   <br>Krishnaveni, M.
   <br>Manjutha, M.</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Optimized Boundary Detection Algorithm For Postal Signs Recognition
   System Using Variant Based Particle Swarm Intelligence</td>
</tr>

<tr>
<td valign="top">SO </td><td>2016 INTERNATIONAL CONFERENCE ON COMPUTATION SYSTEM AND INFORMATION
   TECHNOLOGY FOR SUSTAINABLE SOLUTIONS (CSITSS)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>1st IEEE International Conference on Computational Systems and
   Information Technology for Sustainable Solutions (CSITSS)</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 06-08, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>R V Coll Engn, Bengaluru, INDIA</td>
</tr>

<tr>
<td valign="top">HO </td><td>R V Coll Engn</td>
</tr>

<tr>
<td valign="top">DE </td><td>Postal Video Sign; Histogram; Particle Swarm Optimization; Boundary
   Detection; Object Tracking; Kalman Filter</td>
</tr>

<tr>
<td valign="top">AB </td><td>Sign Language is the only mode of communication for deaf and dumb people to convey their messages. Many difficulties are faced by the hearing impaired people when they come across certain areas like Banking, Hospital and Post Office. Especially, there is no proper communication aid available in post offices to support disabled people. From available literature, it is understood that computational methods have been existing in the area of sign language recognition for hearing impaired people. These recognition system acts as an interpreter to accomplish the conversion of sign language into text or voice. This paper proposes an efficient object tracking method, that improves the performance of the video recognition system, by introducing Variant based Particle Swarm Optimization (VPSO) technique in Kalman Filter (KF) through postal video signs. The experimental results prove that VPSO based Efficient Kalman Filter (EKF) provides results better than a traditional KF.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Subashini, P.; Krishnaveni, M.; Manjutha, M.] Avinashilingam Inst Home
   Sci &amp; Higher Educ Women, Dept Comp Sci, Coimbatore, Tamil Nadu, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Subashini, P (reprint author), Avinashilingam Inst Home Sci &amp; Higher Educ Women, Dept Comp Sci, Coimbatore, Tamil Nadu, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>mail.p.subashini@gmail.com; krishnaveni.rd@gmail.com;
   manjutham@gmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>163</td>
</tr>

<tr>
<td valign="top">EP </td><td>167</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000390719100031</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Aleksy, M
   <br>Fantana, N</td>
</tr>

<tr>
<td valign="top">AF </td><td>Aleksy, Markus
   <br>Fantana, Nicolaie</td>
</tr>

<tr>
<td valign="top">BE </td><td>Younas, M
   <br>Awan, I
   <br>Kryvinska, N
   <br>Strauss, C
   <br>VanThanh, D</td>
</tr>

<tr>
<td valign="top">TI </td><td>Utilizing Multiple Interaction Styles to Collect Installed Base
   Information Using Wearable and Mobile Devices</td>
</tr>

<tr>
<td valign="top">SO </td><td>MOBILE WEB AND INTELLIGENT INFORMATION SYSTEMS, (MOBIWIS 2016)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Lecture Notes in Computer Science</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>13th International Conference on Mobile Web and Intelligent Information
   Systems (MobiWis)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 22-24, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Vienna, AUSTRIA</td>
</tr>

<tr>
<td valign="top">AB </td><td>Excellent service is a key component of making industrial plants work without unexpected shutdowns and safety hazards to workers. Especially, up-to-date information regarding installed base is crucial to support the entire life cycle of systems and products as well as to provide tailored service offerings. However, the myriad and variety of industrial equipment and systems manufactured throughout various periods increase the effort related to the collection of corresponding installed base data. Moreover, organizational changes, such as corporate mergers or company take-overs can introduce additional complexities, such as intersecting serial numbers or the existence of heterogeneous identification plates. In addition, the time related to collecting installed base data is critical since it is often done during customer visits by well trained service engineers that have to focus on solving time-critical problems. Thus, the corresponding data is often processed slowly because of time consuming media conversion and paper work before and after the actual service work.
   <br>In this paper, we present an approach to collect installed base data utilizing wearable and mobile devices. Here, we use different interaction styles at the same time. The proposed approach falls back on using existing hardware components, voice commands, and QR codes in parallel.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Aleksy, Markus; Fantana, Nicolaie] ABB Corp Res, Wallstadter Str 59,
   D-68526 Ladenburg, Germany.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Aleksy, M (reprint author), ABB Corp Res, Wallstadter Str 59, D-68526 Ladenburg, Germany.</td>
</tr>

<tr>
<td valign="top">EM </td><td>markus.aleksy@de.abb.com; nicolaie.fantana@de.abb.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>9847</td>
</tr>

<tr>
<td valign="top">BP </td><td>313</td>
</tr>

<tr>
<td valign="top">EP </td><td>320</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/978-3-319-44215-0_26</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000388793100026</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hong, D
   <br>Lee, MG
   <br>Choi, YM
   <br>Jeong, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hong, Daewoong
   <br>Lee, Moon Gu
   <br>Choi, Young-Man
   <br>Jeong, Jaehwa</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Experimental Verification for a Theoretical Model of Contact-Mode
   Triboelectric Nano-Generators</td>
</tr>

<tr>
<td valign="top">SO </td><td>2016 IEEE INTERNATIONAL CONFERENCE ON ADVANCED INTELLIGENT MECHATRONICS
   (AIM)</td>
</tr>

<tr>
<td valign="top">SE </td><td>IEEE ASME International Conference on Advanced Intelligent Mechatronics</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Advanced Intelligent Mechatronics (AIM)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUL 12-15, 2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Banff, CANADA</td>
</tr>

<tr>
<td valign="top">ID </td><td>POWER GENERATOR</td>
</tr>

<tr>
<td valign="top">AB </td><td>As a promising technology of an energy conversion, a triboelectric nano-generator was proposed and many studies have been presented. However, there have been few studies for experimental verification of the theoretical model of triboelectric nano-generators. In this paper, we experimentally verified the theoretical model of the contact-mode triboelectric nano-generators (CM-TENG). Especially, we focused on the model at the short circuit condition and load circuit condition. For an accurately quantitative analysis, we designed and instrumented a precise experimental system which consists of a 2-axis planar stage, a 2-axis gonio stage, a 3-axis force-torque sensor, a capacitive displacement sensor, and a voice coil actuator. The proposed system can minimize misalignment and parallelism errors and can measure the high frequency and low level current of the CM-TENG in real time.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Hong, Daewoong] Korea Univ, Dept Control &amp; Instrumentat Engn, Seoul,
   South Korea.
   <br>[Lee, Moon Gu; Choi, Young-Man; Jeong, Jaehwa] Ajou Univ, Dept Mech
   Engn, Suwon, South Korea.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Choi, YM (reprint author), Ajou Univ, Dept Mech Engn, Suwon, South Korea.</td>
</tr>

<tr>
<td valign="top">EM </td><td>smiletong@korea.ac.kr; moongulee@ajou.ac.kr; ymanchoi@ajou.ac.kr;
   jaehwa@korea.ac.kr</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">BP </td><td>1329</td>
</tr>

<tr>
<td valign="top">EP </td><td>1332</td>
</tr>

<tr>
<td valign="top">SC </td><td>Automation &amp; Control Systems; Computer Science; Engineering; Robotics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000387100300216</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Chetty, R
   <br>Curr, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Chetty, Rajendra
   <br>Curr, Matthew</td>
</tr>

<tr>
<td valign="top">TI </td><td>Deaf to women: Rhodes's refusal to hear women or his own feminine voice
   within - a reading of Schreiner's Trooper Peter Halket of Mashonaland</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERNATIONAL JOURNAL OF AFRICAN RENAISSANCE STUDIES</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>feminist perspective; Olive Schreiner; Trooper Peter Halket of
   Mashonaland</td>
</tr>

<tr>
<td valign="top">AB </td><td>E.D. Morel's chapter, The story of Southern Rhodesia', in his signal text The black man's burden (1920), provides intertextual reference to Olive Schreiner's work Trooper Peter Halket of Mashonaland (1897) in discussing libertarian thought and distinguishing aspects of male/female authorship. Schreiner's feminist perspective affords her a wider purview of colonialist prerogatives than those exhibited by several contemporary male observers or commentators. The figure of Jesus, as pictured in her neglected political/moral parable, far from being ironic, sentimental or evangelical in purpose, embodies her ideal balance of female and male qualities. Schreiner relies on this redemptive icon both in an ethical and gendered sense to project new understanding and enlightenment onto the strife of the day, which allows her, in turn, to expose and critique Rhodes's male deafness both to women and his own feminine nature. By contrast, Halket's conversion, his feminisation, holds up the alternative of hope versus Rhodes's predatory male soul and final moral damnation.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Chetty, Rajendra; Curr, Matthew] Cape Peninsula Univ Technol, Fac Educ,
   Cape Town, South Africa.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Chetty, R (reprint author), Cape Peninsula Univ Technol, Fac Educ, Cape Town, South Africa.</td>
</tr>

<tr>
<td valign="top">EM </td><td>Chettyr@cput.ac.za; CurrM@cput.ac.za</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>chetty, rajendra</display_name>&nbsp;</font></td><td><font size="3">0000-0002-4219-6932&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>11</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>5</td>
</tr>

<tr>
<td valign="top">EP </td><td>21</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1080/18186874.2016.1212460</td>
</tr>

<tr>
<td valign="top">SC </td><td>Area Studies</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000383175700002</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Esfandiari, H
   <br>Korula, N
   <br>Mirrokni, V</td>
</tr>

<tr>
<td valign="top">AF </td><td>Esfandiari, Hossein
   <br>Korula, Nitish
   <br>Mirrokni, Vahab</td>
</tr>

<tr>
<td valign="top">BE </td><td>Lee, DD
   <br>Sugiyama, M
   <br>Luxburg, UV
   <br>Guyon, I
   <br>Garnett, R</td>
</tr>

<tr>
<td valign="top">TI </td><td>Bi-Objective Online Matching and Submodular Allocations</td>
</tr>

<tr>
<td valign="top">SO </td><td>ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Advances in Neural Information Processing Systems</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>30th Conference on Neural Information Processing Systems (NIPS)</td>
</tr>

<tr>
<td valign="top">CY </td><td>2016</td>
</tr>

<tr>
<td valign="top">CL </td><td>Barcelona, SPAIN</td>
</tr>

<tr>
<td valign="top">ID </td><td>APPROXIMATIONS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Online allocation problems have been widely studied due to their numerous practical applications (particularly to Internet advertising), as well as considerable theoretical interest. The main challenge in such problems is making assignment decisions in the face of uncertainty about future input; effective algorithms need to predict which constraints are most likely to bind, and learn the balance between short-term gain and the value of long-term resource availability.
   <br>In many important applications, the algorithm designer is faced with multiple objectives to optimize. In particular, in online advertising it is fairly common to optimize multiple metrics, such as clicks, conversions, and impressions, as well as other metrics which may be largely uncorrelated such as 'share of voice', and 'buyer surplus'. While there has been considerable work on multi-objective offline optimization (when the entire input is known in advance), very little is known about the online case, particularly in the case of adversarial input. In this paper, we give the first results for bi-objective online submodular optimization, providing almost matching upper and lower bounds for allocating items to agents with two submodular value functions. We also study practically relevant special cases of this problem related to Internet advertising, and obtain improved results. All our algorithms are nearly best possible, as well as being efficient and easy to implement in practice.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Esfandiari, Hossein] Univ Maryland, College Pk, MD 20740 USA.
   <br>[Korula, Nitish; Mirrokni, Vahab] Google Res, New York, NY 10011 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Esfandiari, H (reprint author), Univ Maryland, College Pk, MD 20740 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>hossein@cs.umd.edu; nitish@google.com; mirrokni@google.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>29</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000458973700049</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Mulligan, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Mulligan, Maureen</td>
</tr>

<tr>
<td valign="top">TI </td><td>Ideological Conversions: Three Women Activists in the 1930s</td>
</tr>

<tr>
<td valign="top">SO </td><td>RUPKATHA JOURNAL ON INTERDISCIPLINARY STUDIES IN HUMANITIES</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Women; politics; 193os; activism; class; Spain; Balkans; feminism</td>
</tr>

<tr>
<td valign="top">AB </td><td>What would lead three upper middle class women to question the ideology of their peers and challenge the accepted world view to the extent that they cross the line between acceptance of the status quo and political activism? This paper looks at the life and work of three women - two British and one American who, in the 1930s, experienced a dramatic change in the way they interpreted the world, which led them to a conversion to a different political viewpoint that had an almost evangelical quality to the way it would affect their subsequent life. Margot Heinemann, Rebecca West and Martha Gellhorn: three impressive writers and activists whom we remember now for their fervent defence of political causes - the struggle of the working class for autonomy, the alternative philosophy and quality of life that existed in the divided Balkans, and La Causa, the doomed Republican fight for democracy, respectively. Apart from their intrinsically interesting individual conversions to the faith of a new cause which we can trace in each of these women, their experiences reflect a wider movement in the twentieth century. Heinemann and Gellhorn represent a tendency which has dominated the century - the struggle between the rich and the poor, the powerful and the oppressed; fascism and dictatorship versus socialism and democracy. West represents a new respect for a culture that is not a dominant, first world power; she is one of a few writers of her time who looked around the world and discovered values that were not merely material in another way of life. This is a global shift in the appreciation of another culture which has led in the direction of recent political movements based around "thinking globally and acting locally". Finally, all three writers implicitly echo what has been possibly the biggest social "crossing" of the twentieth century: the struggle for women to find their voice and exercise power: to cross over from second class citizen to equal member of society.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Mulligan, Maureen] Univ Las Palmas Gran Canaria, Dept Modern Philol,
   Las Palmas Gran Canaria, Spain.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Mulligan, M (reprint author), Univ Las Palmas Gran Canaria, Dept Modern Philol, Las Palmas Gran Canaria, Spain.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2016</td>
</tr>

<tr>
<td valign="top">VL </td><td>8</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>97</td>
</tr>

<tr>
<td valign="top">EP </td><td>105</td>
</tr>

<tr>
<td valign="top">SC </td><td>Arts &amp; Humanities - Other Topics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000437034100010</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Yamada, M
   <br>Sigal, L
   <br>Raptis, M
   <br>Toyoda, M
   <br>Chang, Y
   <br>Sugiyama, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Yamada, Makoto
   <br>Sigal, Leonid
   <br>Raptis, Michalis
   <br>Toyoda, Machiko
   <br>Chang, Yi
   <br>Sugiyama, Masashi</td>
</tr>

<tr>
<td valign="top">TI </td><td>Cross-Domain Matching with Squared-Loss Mutual Information</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Cross-domain object matching; cross-domain temporal alignment;
   squared-loss mutual information</td>
</tr>

<tr>
<td valign="top">ID </td><td>REDUCTION; ALGORITHM; ALIGNMENT</td>
</tr>

<tr>
<td valign="top">AB </td><td>The goal of cross-domain matching (CDM) is to find correspondences between two sets of objects in different domains in an unsupervised way. CDM has various interesting applications, including photo album summarization where photos are automatically aligned into a designed frame expressed in the Cartesian coordinate system, and temporal alignment which aligns sequences such as videos that are potentially expressed using different features. In this paper, we propose an information-theoretic CDM framework based on squared-loss mutual information (SMI). The proposed approach can directly handle non-linearly related objects/sequences with different dimensions, with the ability that hyper-parameters can be objectively optimized by cross-validation. We apply the proposed method to several real-world problems including image matching, unpaired voice conversion, photo album summarization, cross-feature video and cross-domain video-to-mocap alignment, and Kinect-based action recognition, and experimentally demonstrate that the proposed method is a promising alternative to state-of-the-art CDM methods.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Yamada, Makoto; Chang, Yi] Yahoo Labs, Sunnyvale, CA 94089 USA.
   <br>[Sigal, Leonid] Disney Res, Pittsburgh, PA 15213 USA.
   <br>[Raptis, Michalis] Comcast Labs, Washington, DC USA.
   <br>[Toyoda, Machiko] NTT CS Labs, Kyoto, Japan.
   <br>[Sugiyama, Masashi] Univ Tokyo, Grad Sch Frontier Sci, Dept Complex Sci
   &amp; Engn, Bunkyo Ku, Tokyo 1130033, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Yamada, M (reprint author), Yahoo Labs, Sunnyvale, CA 94089 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>makotoy@yahoo-inc.com; lsigal@disneyresearch.com; mraptis@cs.ucla.edu;
   toyoda.machiko@lab.ntt.co.jp; yichang@yahoo-inc.com;
   sugi@k.u-tokyo.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>SEP</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">VL </td><td>37</td>
</tr>

<tr>
<td valign="top">IS </td><td>9</td>
</tr>

<tr>
<td valign="top">BP </td><td>1764</td>
</tr>

<tr>
<td valign="top">EP </td><td>1776</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/TPAMI.2014.2388235</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000359216600003</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Pereira, JC
   <br>Teixeira, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Pereira, Jose Casimiro
   <br>Teixeira, Antonio</td>
</tr>

<tr>
<td valign="top">TI </td><td>Trainable NLG for Data to Portuguese - With application to a Medication
   Assistant</td>
</tr>

<tr>
<td valign="top">SO </td><td>LINGUAMATICA</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Natural Language Generation; NLG; data2text; machine translation;
   medication assistant</td>
</tr>

<tr>
<td valign="top">AB </td><td>New equipments, such as smartphones and tablets, are changing human computer interaction. These devices present several challenges, especially due to their small screen and keyboard. In order to use text and voice in multimodal interaction, it is essential to deploy modules to translate the internal information of the applications into sentences or texts, in order to display it on screen or synthesize it. Also, these modules must generate phrases and texts in the user's native language; the development should not require considerable resources; and the outcome of the generation should achieve a good degree of variability.
   <br>Our main objective is to propose, implement and evaluate a method of data conversion to Portuguese which can be developed with a minimum of time and knowledge, but without compromising the necessary variability and quality of what is generated. The developed system, for a Medication Assistant, is intended to create descriptions, in natural language, of medication to be taken. Motivated by recent results, we opted for an approach based on machine translation, with models trained on a small parallel corpus.
   <br>For that, a new corpus was created. With it, two variants of the system were trained: phrase-based translation and syntax-based translation. The two variants were evaluated by automatic measurements - BLEU and Meteor - and by humans. The results showed that a phrase-based approach produced better results than a syntax-based one: human evaluators evaluated 60% of phrase-based responses as good, or very good, compared to only 46% of syntax-based responses. Considering the corpus size, we judge this value (60%) as good.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Pereira, Jose Casimiro] Inst Politecn Tomar, Tomar, Portugal.
   <br>[Teixeira, Antonio] Univ Aveiro, Dept Elect Telec &amp; Informat IEETA,
   Aveiro, Portugal.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Pereira, JC (reprint author), Inst Politecn Tomar, Tomar, Portugal.</td>
</tr>

<tr>
<td valign="top">EM </td><td>casimiro@ipt.pt; ajst@ua.pt</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Teixeira, Antonio</display_name>&nbsp;</font></td><td><font size="3">0000-0002-7675-1236&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUL</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">VL </td><td>7</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>3</td>
</tr>

<tr>
<td valign="top">EP </td><td>21</td>
</tr>

<tr>
<td valign="top">SC </td><td>Linguistics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000371641300002</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kirshner, B
   <br>Jefferson, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kirshner, Ben
   <br>Jefferson, Antwan</td>
</tr>

<tr>
<td valign="top">TI </td><td>Participatory Democracy and Struggling Schools: Making Space for Youth
   in School Turnarounds</td>
</tr>

<tr>
<td valign="top">SO </td><td>TEACHERS COLLEGE RECORD</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">ID </td><td>STUDENT VOICE; SOCIAL-CHANGE; INTERSECTION; ENGAGEMENT; EDUCATION;
   ACTIVISM; REFORM</td>
</tr>

<tr>
<td valign="top">AB </td><td>Background/Context: Federal policy, as codified in Race to the Top (RTT) funding guidelines, outlines four types of intervention: turnaround, restart, closure, and transformation. RTT has embraced a technocratic paradigm for school reform that frames choice less as the opportunity for the public to deliberate about what it wants from its schools and more in terms of the freedom of individual families to choose, as customers, from a diverse array of school options. This market-based system has eroded substantive opportunities for parents and students to participate in decisions about their schools. Although scholars have developed compelling arguments about the need to involve parents and teachers in a more deliberative and democratic approach to intervening in low-performing schools, there is little scholarship focused on the role of young people in school intervention processes.
   <br>Purpose: There is widespread agreement among progressive critics that RTT interventions are not sufficiently democratic. More work is needed to develop participatory approaches. In some cases this may require departing from a strict "evidence-based" framework and imagining new alternatives consistent with values of social justice and educational equity. It also requires expanding existing treatments of deliberative democracy theory to include young people.
   <br>Research Design &amp; Findings: This article makes a conceptual argument rooted in theory, empirical literature, and practical experience in schools. After explaining theories of participatory democracy, youth-adult partnerships, and thirdspace, we propose five practices that should guide a deliberative, participatory approach to public decision-making about schools. These are: border-crossing facilitation, participatory research, multilingual and multicultural discourse practices, authentic decision-making, and joint work and distributed expertise.
   <br>Conclusions/Recommendations: The current school turnaround paradigm, embodied by closures, conversion to charters, and teacher reassignments, has left a great deal of collateral damage in its wake. Teachers work under threat of firing. We propose an alternative approach to improve struggling public neighborhood schools-not just another option in a menu of turnaround strategies, but an alternative frame and set of practices that expands the conversation about intervention. This approach encourages deliberation and communication among diverse networks of students, teachers, and families.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kirshner, Ben] Univ Colorado Boulder, Boulder, CO 80309 USA.
   <br>[Jefferson, Antwan] Univ Colorado Denver, Sch Educ &amp; Huma Dev, Denver,
   CO USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kirshner, B (reprint author), Univ Colorado Boulder, Boulder, CO 80309 USA.</td>
</tr>

<tr>
<td valign="top">TC </td><td>2</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>2</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">VL </td><td>117</td>
</tr>

<tr>
<td valign="top">IS </td><td>6</td>
</tr>

<tr>
<td valign="top">AR </td><td>060303</td>
</tr>

<tr>
<td valign="top">SC </td><td>Education &amp; Educational Research</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000356235500003</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hernandez, JFG
   <br>Abrego, JA
   <br>Maldonado, ALO
   <br>Munoz, GGM
   <br>Ceballos, HA
   <br>Romero, PP
   <br>Morales, AM</td>
</tr>

<tr>
<td valign="top">AF </td><td>Gallegos Hernandez, Jose Francisco
   <br>Alberto Abrego, Jose
   <br>Ortiz Maldonado, Alma Lilia
   <br>Minauro Munoz, Gerardo Gabriel
   <br>Arias Ceballos, Hector
   <br>Pichardo Romero, Pablo
   <br>Mantilla Morales, Alejandra</td>
</tr>

<tr>
<td valign="top">TI </td><td>Conservative laryngeal surgery in larynx cancer patients who are
   candidates for combined treatment with chemo-radiotherapy</td>
</tr>

<tr>
<td valign="top">SO </td><td>GACETA MEXICANA DE ONCOLOGIA</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Laryngeal cancer; Conservative surgery; Conservative laryngectorny;
   Partial laryngectomy</td>
</tr>

<tr>
<td valign="top">AB </td><td>Background: The non-surgical organ-preserving standard of care for advanced-stage laryngeal cancer is combined treatment (chemo-radiotherapy). However, complications occurring with this treatment are not few, and mainly with regards to swallowing. Conservative laryngeal surgery remains an effective alternative for cancer control without the complications associated with chemo-radiotherapy.
   <br>Material and methods: Retrospective study that included patients with cT3, cNO laryngeal cancer with paraglottic infiltration, vocal cord fixation, but with normal arytenoid mobility, and without subglottic infiltration, who were treated with supracricoid subtotal laryngectomy. Complications, treatment sequels, and recurrence were assessed. Bronchial aspiration was studied with swallowing scintigraphy.
   <br>Results: Twenty-five patients underwent the intervention. Surgical margins were negative in 22, and in one, they were in contact with the tumour, and in 2 they were positive. Two patients received post-operative radiotherapy. Mean time to de-cannulation was 15 days, and 25 days to nasogastric tube removal. Mean follow-up was 26 months. None of the patients has had tumour recurrence or has required conversion to total laryngectomy. In all patients, swallowing has been normal and no one has required permanent tracheotomy. The voice is considered to be intelligible in all patients. Swallowing scintigraphy showed aspiration in 15/25 patients, which was not clinically relevant. Five patients had post-operative complications, with 4 patients requiring re-intervention, but no one required conversion to total laryngectomy.
   <br>Conclusion: Conservative surgery is an effective alternative to chemo-radiotherapy that offers cancer control with acceptable complications and minimal sequels. Although most patients experience aspiration, this does not affect the functional status. (C) 2015 Sociedad Mexicana de Oncologia. Published by Masson Doyma Mexico S.A.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Gallegos Hernandez, Jose Francisco; Alberto Abrego, Jose; Ortiz
   Maldonado, Alma Lilia; Minauro Munoz, Gerardo Gabriel; Arias Ceballos,
   Hector] Inst Mexicano Seguro Social, Ctr Medico Nacl Siglo Xxi, Hosp
   Oncol, Dept Tumores Cabeza &amp; Cuello, Mexico City, DF, Mexico.
   <br>[Pichardo Romero, Pablo] Inst Mexicano Seguro Social, Ctr Medico Nacl
   Siglo Xxi, Hosp Oncol, Dept Med Nucl, Mexico City, DF, Mexico.
   <br>[Mantilla Morales, Alejandra] Inst Mexicano Seguro Social, Ctr Medico
   Nacl Siglo Xxi, Hosp Oncol, Dept Patol, Mexico City, DF, Mexico.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hernandez, JFG (reprint author), Inst Mexicano Seguro Social, Ctr Medico Nacl Siglo Xxi, Hosp Oncol, Dept Tumores Cabeza &amp; Cuello, Mexico City, DF, Mexico.</td>
</tr>

<tr>
<td valign="top">EM </td><td>gal61@prodigy.net.mx</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAR-APR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">VL </td><td>14</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>92</td>
</tr>

<tr>
<td valign="top">EP </td><td>96</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.gamo.2015.06.019</td>
</tr>

<tr>
<td valign="top">SC </td><td>Oncology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000421598300005</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ishida, K
   <br>Shabanpour, R
   <br>Meister, T
   <br>Boroujeni, BK
   <br>Carta, C
   <br>Ellinger, F
   <br>Petti, L
   <br>Munzenrieder, N
   <br>Salvatore, GA
   <br>Troster, G</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ishida, K.
   <br>Shabanpour, R.
   <br>Meister, T.
   <br>Boroujeni, B. K.
   <br>Carta, C.
   <br>Ellinger, F.
   <br>Petti, L.
   <br>Munzenrieder, N.
   <br>Salvatore, G. A.
   <br>Troster, G.</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>20 MHz Carrier Frequency AM Receiver in Flexible a-IGZO TFT Technology
   with Textile Antennas</td>
</tr>

<tr>
<td valign="top">SO </td><td>2015 IEEE INTERNATIONAL SYMPOSIUM ON RADIO-FREQUENCY INTEGRATION
   TECHNOLOGY (RFIT)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Symposium on Radio-Frequency Integration Technology
   (RFIT)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 26-28, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Sendai, JAPAN</td>
</tr>

<tr>
<td valign="top">DE </td><td>flexible electronics; flexible receivers; AM receiver; a-IGZO; thin-film
   transistor; textile antennas</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents an AM receiver implemented in a flexible a-IGZO TFT technology. The circuit consists of a four-stage cascode amplifier at the RF input, a detector based on a source follower, and a common-source circuit for the baseband amplification. The measured conversion gain is very flat against frequency and exceeds 15 dB for carrier frequencies ranging from 2 to 20 MHz, which covers a relevant portion of the shortwave radio band. The 3 dB-bandwidth of the baseband signal ranges from 400 Hz to 10 kHz: this is comparable to the so-called voice band and is suitable to low-rate data communications. Additionally, the AM receiver is tested in combination with two textile antennas. The flexible a-IGZO receiver successfully detected the baseband signal through the textile antennas, demonstrating for the first time wireless transmission for this class of technologies.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ishida, K.; Shabanpour, R.; Meister, T.; Boroujeni, B. K.; Carta, C.;
   Ellinger, F.] Tech Univ Dresden, Dresden, Germany.
   <br>[Petti, L.; Munzenrieder, N.; Salvatore, G. A.; Troster, G.] Swiss Fed
   Inst Technol Zurich, Zurich, Switzerland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ishida, K (reprint author), Tech Univ Dresden, Dresden, Germany.</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Carta, Corrado</display_name>&nbsp;</font></td><td><font size="3">I-9096-2019&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Salvatore, Giovanni Antonio</display_name>&nbsp;</font></td><td><font size="3">G-6966-2016&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Carta, Corrado</display_name>&nbsp;</font></td><td><font size="3">0000-0001-9147-0160&nbsp;&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Salvatore, Giovanni Antonio</display_name>&nbsp;</font></td><td><font size="3">0000-0002-8983-3257&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>142</td>
</tr>

<tr>
<td valign="top">EP </td><td>144</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380575600048</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Karthikeyan, S
   <br>Devi, KV
   <br>Valarmathi, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Karthikeyan, S.
   <br>Devi, K. Vimala
   <br>Valarmathi, K.</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Internet of Things: Hospice Appliances Monitoring and Control System</td>
</tr>

<tr>
<td valign="top">SO </td><td>PROCEEDINGS OF 2015 ONLINE INTERNATIONAL CONFERENCE ON GREEN ENGINEERING
   AND TECHNOLOGIES (IC-GET)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Proceedings of Online International Conference on Green Engineering and
   Technologies (IC-GET)</td>
</tr>

<tr>
<td valign="top">CY </td><td>NOV 27-27, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Coimbatore, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Hospice automation System (HAS); Internet of Things (IoT); Cloud
   networking; Wi-Fi network; Intel Galileo Microcontroller</td>
</tr>

<tr>
<td valign="top">AB </td><td>With the growing interest on wireless network architectures for hospital management services is a renewed demand for efficient and reliable data transfer between the doctor and the patient. Voice and data traffic carried over a variety of access technologies is collected via technology-specific access networks like, Digital Subscriber Line [DSL], Passive Optical Network [PON], and Wireless Fidelity [Wi-Fi]. Metro and core networks need to aggregate the various user flows from different access network nodes and provide scalable and cost-effective distribution of various flow types (e.g., Internet access, voice, video on demand, and broadcast TV services) to the relevant service access points. Among the most promising applications provided by IoT is the latest and emerging internet technology. Internet of things is a growing network of everyday object-from industrial machine to consumer goods that can share information and complete tasks while you are handling many works in a place. This paper proposes a support to developed Wireless Hospital Digital Interface (WHDI) technology using the gateway of hardware and software, Wi-Fi protocol data and the ZigBee protocol data conversion method and integration of cloud networking to provide the doctors to do complete monitoring and control functionalities of the hospice environment using wireless sensors and actuators modules to interface wirelessly to all sources within their hospital and storing the data in the cloud.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Karthikeyan, S.] M Kumarasamy Coll Engn, Karur, India.
   <br>[Devi, K. Vimala; Valarmathi, K.] PSR Engn Coll, Sivakasi, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Karthikeyan, S (reprint author), M Kumarasamy Coll Engn, Karur, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>skarthik.me@gmail.com; k.vimaladevi@gmail.com; krvalarmathi@yahoo.co.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">SC </td><td>Science &amp; Technology - Other Topics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380573800005</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Fine, R</td>
</tr>

<tr>
<td valign="top">AF </td><td>Fine, Ruth</td>
</tr>

<tr>
<td valign="top">TI </td><td>TURNS WITH THE RICOTE'S DISCOURSE (QUIXOTE II, 54): CONVERSION AND OTHER
   PARADOXES</td>
</tr>

<tr>
<td valign="top">SO </td><td>MONTEAGUDO</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Don Quijote; narrative fiction of the Golden Age; moriscos; conversos;
   the literature of the conversos; polyphony; paradoxical narration</td>
</tr>

<tr>
<td valign="top">AB </td><td>This article aims to contribute to the analysis of Ricote's discourse (Quixote II, 54) from a specific perspective, that of the converso's condition (converted both from the Islam -moriscos-and from Judaism) during the sixteenth and seventeenth centuries in the Iberian Peninsula and its exiles. According to the standpoint assumed in this study, Ricote's discourse offers a revealing approach to the converso's condition, to its contradictions and dilemmas, novelizing in a suggestive manner some of the paradoxes inherent to that condition. In this regard, the analysis of the passage will exemplify the interaction, overlapping and conflict of voices and perspectives that characterized both the self-perception and the representation of the conversos.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Fine, Ruth] Hebrew Univ Jerusalem, IL-91905 Jerusalem, Israel.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Fine, R (reprint author), Hebrew Univ Jerusalem, IL-91905 Jerusalem, Israel.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">IS </td><td>20</td>
</tr>

<tr>
<td valign="top">BP </td><td>29</td>
</tr>

<tr>
<td valign="top">EP </td><td>40</td>
</tr>

<tr>
<td valign="top">SC </td><td>Literature</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000384113200003</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Oshima, Y
   <br>Takamichi, S
   <br>Toda, T
   <br>Neubig, G
   <br>Sakti, S
   <br>Nakamura, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Oshima, Yuji
   <br>Takamichi, Shinnosuke
   <br>Toda, Tomoki
   <br>Neubig, Graham
   <br>Sakti, Sakriani
   <br>Nakamura, Satoshi</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INT SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Non-native Speech Synthesis Preserving Speaker Individuality based on
   Partial Correction of Prosodic and Phonetic Characteristics</td>
</tr>

<tr>
<td valign="top">SO </td><td>16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2015), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>16th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2015)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 06-10, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Dresden, GERMANY</td>
</tr>

<tr>
<td valign="top">DE </td><td>cross-lingual speech synthesis; English-Read-by-Japanese (ERJ); speaker
   individuality; HMM-based speech synthesis; prosodic correction; phonetic
   correction</td>
</tr>

<tr>
<td valign="top">ID </td><td>ADAPTATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a novel non-native speech synthesis technique that preserves the individuality of a non-native speaker. Cross-lingual speech synthesis based on voice conversion or HMM-based speech synthesis, which synthesizes foreign language speech of a specific non-native speaker reflecting the speaker-dependent acoustic characteristics extracted from the speaker's natural speech in his/her mother tongue, tends to cause a degradation of speaker individuality in synthetic speech compared to intra-lingual speech synthesis. This paper proposes a new approach to cross-lingual speech synthesis that preserves speaker individuality by explicitly using non-native speech spoken by the target speaker. Although the use of non-native speech makes it possible to preserve the speaker individuality in the synthesized target speech, naturalness is significantly degraded as the speech is directly affected by unnatural prosody and pronunciation often caused by differences in the linguistic systems of the source and target languages. To improve naturalness while preserving speaker individuality, we propose (1) a prosodic correction method based on model adaptation, and (2) a phonetic correction method based on spectrum replacement for unvoiced consonants. The experimental results demonstrate that these proposed methods are capable of significantly improving naturalness while preserving the speaker individuality in synthetic speech.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Oshima, Yuji; Takamichi, Shinnosuke; Toda, Tomoki; Neubig, Graham;
   Sakti, Sakriani; Nakamura, Satoshi] Nara Inst Sci &amp; Technol, Grad Sch
   Informat Sci, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Oshima, Y (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>shinnosuket@is.naist.jp; tomoki@is.naist.jp; neubig@is.naist.jp;
   ssakti@is.naist.jp; s-nakamura@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>299</td>
</tr>

<tr>
<td valign="top">EP </td><td>303</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380581600061</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sitaram, S
   <br>Parlikar, A
   <br>Anumanchipalli, GK
   <br>Black, AW</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sitaram, Sunayana
   <br>Parlikar, Alok
   <br>Anumanchipalli, Gopala Krishna
   <br>Black, Alan W.</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INT SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Universal Grapheme-based Speech Synthesis</td>
</tr>

<tr>
<td valign="top">SO </td><td>16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2015), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>16th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2015)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 06-10, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Dresden, GERMANY</td>
</tr>

<tr>
<td valign="top">DE </td><td>speech synthesis; lexicons; pronunciation; low resources</td>
</tr>

<tr>
<td valign="top">AB </td><td>Grapheme-to-phoneme conversion follows the text processing step in speech synthesis. Typically, lexicons or Letter-to-Sound rules are used to map graphemes to phonemes. However, in some languages, such resources may not be readily available. In this paper, we describe a universal front end that supports using grapheme information alone to build usable speech synthesis systems. This work takes advantage of an explicit mapping of Unicode characters from a wide range of scripts to a single phoneset to create support for building speech synthesizers for most languages in the world. We compare the efficacy of this front end to the baseline approach of treating every single grapheme as a separate phoneme for synthesis by building voices for twelve languages across several language families and to front ends with linguistic knowledge in languages with higher resources. In addition, we improve our models by using Random Forests as opposed to using single Classification and Regression Trees. We find that the common universal front end performs better than the raw graphemes in general. We also find that using Random Forests lead to a significant improvement in synthesis quality, which is better than the quality of the knowledge based front end in many cases.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Sitaram, Sunayana; Parlikar, Alok; Black, Alan W.] Carnegie Mellon
   Univ, Pittsburgh, PA 15213 USA.
   <br>[Anumanchipalli, Gopala Krishna] Univ Calif San Francisco, San
   Francisco, CA 94143 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sitaram, S (reprint author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ssitaram@cs.cmu.edu; aup@cs.cmu.edu; AnumanchipalliG@neurosurg.ucsf.edu;
   awb@cs.cmu.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>3360</td>
</tr>

<tr>
<td valign="top">EP </td><td>3364</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380581601238</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Anil, MC
   <br>Shirbahadurkar, SD</td>
</tr>

<tr>
<td valign="top">AF </td><td>Anil, Manjare Chandraprabha
   <br>Shirbahadurkar, S. D.</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Expressive Speech Synthesis using Prosodic Modification for Marathi
   Language</td>
</tr>

<tr>
<td valign="top">SO </td><td>2ND INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING AND INTEGRATED
   NETWORKS (SPIN) 2015</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>2nd International Conference on Signal Processing and Integrated
   Networks (SPIN)</td>
</tr>

<tr>
<td valign="top">CY </td><td>FEB 19-20, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Amity Sch Engn &amp; Technol, Noida, INDIA</td>
</tr>

<tr>
<td valign="top">HO </td><td>Amity Sch Engn &amp; Technol</td>
</tr>

<tr>
<td valign="top">DE </td><td>Fundamental frequency; Prosody; Pitch Contour; TD-PSOLA; speech
   synthesis</td>
</tr>

<tr>
<td valign="top">ID </td><td>CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper describes Prosodic modification in Neutral speech for generating expressive synthetic speech in Marathi Text-to-Speech (TTS) system. This paper focuses on voice modification technique maintaining acceptable quality and naturalness with reduced database. In this paper, a method to modify fundamental frequency contour is proposed for Marathi (language spoken in Maharashtra, India) TTS system. The emotion content in the speech is highly correlated to phonetic description and prosodic features such as fundamental frequency and time duration of that phone. Speech modification is done in time domain. Speech modification is done by a process of compressing the speech signal in time domain without changing its spectral features. For prosody generation, we have obtained a primary pitch curve for the word, based on the location followed by punctuation marks. Question mark and exclamation mark in the text are studied to modify prosody. TD-PSOLA (Time Domain Pitch Synchronous Overlap and Add) algorithm can improve the prosody of the synthesized speech. Prosodic changes can be possible by modifying different parameters like pitch level, pitch contour, pitch variation. The experimental results showed that the proposed prosody modification, based on pitch and duration modification can improve speech quality.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Anil, Manjare Chandraprabha] JSPMs Rajarshi Shahu Coll Engn, Dept Elect
   &amp; Telecommun Engn, Pune 411033, Maharashtra, India.
   <br>[Shirbahadurkar, S. D.] Dr DY Patil Coll Engn, Pune 410506, Maharashtra,
   India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Anil, MC (reprint author), JSPMs Rajarshi Shahu Coll Engn, Dept Elect &amp; Telecommun Engn, Pune 411033, Maharashtra, India.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>126</td>
</tr>

<tr>
<td valign="top">EP </td><td>130</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000382970300026</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Santamarina, B
   <br>Vaccaro, I
   <br>Beltran, O</td>
</tr>

<tr>
<td valign="top">AF </td><td>Santamarina, Beatriz
   <br>Vaccaro, Ismael
   <br>Beltran, Oriol</td>
</tr>

<tr>
<td valign="top">TI </td><td>THE STERILIZATION OF ECO-CRITICISM: FROM SUSTAINABLE DEVELOPMENT TO
   GREEN CAPITALISM.</td>
</tr>

<tr>
<td valign="top">SO </td><td>ANDULI</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>environmentalism; environmental crisis; eco-criticism; development;
   growth; sustainable development; sustainability</td>
</tr>

<tr>
<td valign="top">ID </td><td>CARBON; CONSERVATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Development has been a dominant trope of global political and economic life since the 1950s. As such it has been inevitably linked to some of the most important social processes of this era: colonialism, globalization, post-colonialism, global ecological crisis, the rise of environmentalism, and more. Consolidation of the contemporary consumer society came, hand in hand, with the certainty that it sustained a way of life that, like collateral damage, included a global ecological crisis. From many parts of the world new voices raised concerns about the costs of globalization and proposed alternatives and solutions; thus, modern eco-criticism was born. This article analyzes the historical process of emergence of eco-critical concepts as well as appropriation, redefinition, and use of these concepts by politicians and economists. Specifically, we reflect on how "development" and "growth" under heavy criticism during the 70s were gradually transformed into "sustainable development" first, and, as this conversion was still raising significant disapproval, to "sustainability" later. Adoption of these new ideological frameworks aimed at legitimizing development allowed Western societies to ignore more critical approaches such us "zero growth" or "degrowth".</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Santamarina, Beatriz] Univ Valencia, E-46003 Valencia, Spain.
   <br>[Vaccaro, Ismael] McGill Univ, Montreal, PQ H3A 2T5, Canada.
   <br>[Beltran, Oriol] Univ Barcelona, E-08007 Barcelona, Spain.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Santamarina, B (reprint author), Univ Valencia, E-46003 Valencia, Spain.</td>
</tr>

<tr>
<td valign="top">EM </td><td>Beatriz.Santamarina@uv.es</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">IS </td><td>14</td>
</tr>

<tr>
<td valign="top">BP </td><td>13</td>
</tr>

<tr>
<td valign="top">EP </td><td>28</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.12795/anduli.2015.i14.01</td>
</tr>

<tr>
<td valign="top">SC </td><td>Sociology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000381392300001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Langarani, MSE
   <br>van Santen, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Langarani, Mahsa Sadat Elyasi
   <br>van Santen, Jan</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>SPEAKER INTONATION ADAPTATION FOR TRANSFORMING TEXT-TO-SPEECH SYNTHESIS
   SPEAKER IDENTITY</td>
</tr>

<tr>
<td valign="top">SO </td><td>2015 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING
   (ASRU)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 13-17, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Scottsdale, AR</td>
</tr>

<tr>
<td valign="top">DE </td><td>Prosody; Intonation modeling; Text-to-Speech synthesis; Adaptation</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION; PROSODY CONVERSION; EMOTIONAL SPEECH; PITCH</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this study, we propose a new intonation adaptation method to transform the perceived identity of a Text-To-Speech system to that of a target speaker with a small amount of training data. In the proposed method, during training we fit parametrized accent and phrase curves to parallel recordings of the target speaker F-0 curves, and estimate the parameters of a mapping between the corresponding parameter spaces. During test, we fit the accent and phrase curves to the source utterances, apply the mapping, and create an F-0 contour from the mapped accent and phrase curves. We compare the proposed method with a baseline adaptation method in which the source F-0 contour is transformed linearly such that the per-utterance mean and variance of the target F-0 contour is left unaltered. Perceptual tests showed that the proposed method was better than the baseline method in two subjective tests that assess similarity to the target speaker and speech quality, respectively.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Langarani, Mahsa Sadat Elyasi; van Santen, Jan] Oregon Hlth &amp; Sci Univ,
   Ctr Spoken Language Understanding, Portland, OR 97201 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Langarani, MSE (reprint author), Oregon Hlth &amp; Sci Univ, Ctr Spoken Language Understanding, Portland, OR 97201 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>elyasila@ohsu.edu; vansantj@ohsu.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>116</td>
</tr>

<tr>
<td valign="top">EP </td><td>123</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380604800018</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Labhane, MB
   <br>PrachiPalsodkar</td>
</tr>

<tr>
<td valign="top">AF </td><td>Labhane, Mrunalini. B.
   <br>PrachiPalsodkar</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Various Architectures of Analog to Digital Converter</td>
</tr>

<tr>
<td valign="top">SO </td><td>2015 INTERNATIONAL CONFERENCE ON COMMUNICATIONS AND SIGNAL PROCESSING
   (ICCSP)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>2015 International Conference on Communications and Signal Processing
   (ICCSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 02-04, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Adhiparasakthi Engn Coll,Dept Elect &amp; Commun Engn, Melmaruvathur, INDIA</td>
</tr>

<tr>
<td valign="top">HO </td><td>Adhiparasakthi Engn Coll,Dept Elect &amp; Commun Engn</td>
</tr>

<tr>
<td valign="top">DE </td><td>Analog to digital converter; TIQ; CMOS-LTE; Open loop comparator; DT
   Comparator; ramp ADC; Flash; successive approximation ADC; integrating
   ADC; Pipeline ADC</td>
</tr>

<tr>
<td valign="top">AB </td><td>With the change in advanced technology and the need for electronic equipments, the development of electronic instruments stimulate more efforts for more innovative ideas and better performance of design. In real world physical values, such as pressure, humidity, temperature and voice can be measured as analog (continuous) signals. However, to process these analog signals by electronic digital equipments, we need to convert these analog signals into digital or discreet signals or binary. In this paper, there are many types of Analog-to-Digital Converters (ADCs) which can be classified according to the applications and concept on which they were designed. ADCs including, Direct conversion ADC (Flash), successive approximation ADC, integrating ADC, Pipeline ADC. In some cases, many comparators are used to reduce the complexity, power and improve the linearity which are also explained in this paper.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Labhane, Mrunalini. B.; PrachiPalsodkar] RashtraSantTukodojiMaharaj
   Nagpur Univ, YeshwantraoChavan Coll Engn, Nagpur 441110, Maharashtra,
   India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Labhane, MB (reprint author), RashtraSantTukodojiMaharaj Nagpur Univ, YeshwantraoChavan Coll Engn, Nagpur 441110, Maharashtra, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>mrunal.labhane@gmail.com; prachi.palsodkar@gmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>1199</td>
</tr>

<tr>
<td valign="top">EP </td><td>1203</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380448200173</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sathe-Pathak, B
   <br>Patil, S
   <br>Panat, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sathe-Pathak, Bageshree
   <br>Patil, Shalaka
   <br>Panat, Ashish</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Conversion of one Emotional state to other of a Speech Signal using
   Artificial Neural Network</td>
</tr>

<tr>
<td valign="top">SO </td><td>2015 1st International Conference on Next Generation Computing
   Technologies (NGCT)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>1st International Conference on Next Generation Computing Technologies
   (NGCT)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 04-05, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Dehradun, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Emotion transformation; Spectral transformation; Packet Decomposition;
   Artificial Neural Network</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a novel emotion transformation scheme of speech signal which is text independent and speaker independent. Speech signals as many other signals are inherently multi-scale in nature, owing to contributions from events occurring with different localizations in time and frequency. Therefore, emotion dependent spectral parameters those characterized by single scale features, approximate the vocal tract, but produce artefacts during speech signal reconstruction. In this paper, multi-resolution spectral transformation technique of Discrete Wavelet Packet Decomposition has been used along with the use of Artificial Neural Network for generation of transform function. This paper specifically carries out transformation of Neutral emotion to Angry, Happy and Sad emotions. The transform function is generated in three different techniques, using three types of Artificial Neural Networks (ANNs), namely, Feed Forward Neural Network (FFNN), Generalized Regression Neural Network (GRNN) and Radial Basis Network (RBN). Results of all the three ANNs are compared using both objective as well as subjective analysis.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Sathe-Pathak, Bageshree; Panat, Ashish] Priyadarshini Coll Engn, Dept
   Elect, Nagpur, Maharashtra, India.
   <br>[Patil, Shalaka] Cummins Coll Engn Women Pune, Elect &amp; Telecommun Dept,
   Pune, Maharashtra, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sathe-Pathak, B (reprint author), Priyadarshini Coll Engn, Dept Elect, Nagpur, Maharashtra, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>bvpathak100@yahoo.com; shalakapatils@gmail.com; ashishpanat@gmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>831</td>
</tr>

<tr>
<td valign="top">EP </td><td>835</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380503400166</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Paul, D
   <br>Pal, M
   <br>Saha, G</td>
</tr>

<tr>
<td valign="top">AF </td><td>Paul, Dipjyoti
   <br>Pal, Monisankha
   <br>Saha, Goutam</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Novel Speech Features for Improved Detection of Spoofing Attacks</td>
</tr>

<tr>
<td valign="top">SO </td><td>2015 ANNUAL IEEE INDIA CONFERENCE (INDICON)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Annual IEEE India Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>12 IEEE Int C Elect Energy Env Communications Computer Control</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 17-20, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>New Delhi, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>anti-spoofing; ASVspoof 2015; countermeasures; mel-frequency cepstral
   coefficient (MFCC); speech-signal-based frequency cepstral coefficient
   (SFCC); speaker recognition</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION; SPEAKER VERIFICATION; RECOGNITION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Now-a-days, speech-based biometric systems such as automatic speaker verification (ASV) are highly prone to spoofing attacks by an imposture. With recent development in various voice conversion (VC) and speech synthesis (SS) algorithms, these spoofing attacks can pose a serious potential threat to the current state-of-the-art ASV systems. To impede such attacks and enhance the security of the ASV systems, the development of efficient anti-spoofing algorithms is essential that can differentiate synthetic or converted speech from natural or human speech. In this paper, we propose a set of novel speech features for detecting spoofing attacks. The proposed features are computed using alternative frequency-warping technique and formant-specific block transformation of filter bank log energies. We have evaluated existing and proposed features against several kinds of synthetic speech data from ASVspoof 2015 corpora. The results show that the proposed techniques outperform existing approaches for various spoofing attack detection task. The techniques investigated in this paper can also accurately classify natural and synthetic speech as equal error rates (EERs) of 0% have been achieved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Paul, Dipjyoti; Pal, Monisankha; Saha, Goutam] Indian Inst Technol,
   Dept Elect &amp; Elect Commun Engn, Kharagpur, W Bengal, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Paul, D (reprint author), Indian Inst Technol, Dept Elect &amp; Elect Commun Engn, Kharagpur, W Bengal, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>dipjyotipaul@ece.iitkgp.ernet.in; monisankhapal@iitkgp.ac.in;
   gsaha@ece.iitkgp.ernet.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380475300700</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Yadav, J
   <br>Rao, KS</td>
</tr>

<tr>
<td valign="top">AF </td><td>Yadav, Jainath
   <br>Rao, K. Sreenivasa</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">BE </td><td>Nagabhusan, TN
   <br>Sundararajan, N
   <br>Suresh, S</td>
</tr>

<tr>
<td valign="top">TI </td><td>Generation of emotional speech by prosody imposition on Sentence, Word
   and Syllable level fragments of neutral speech</td>
</tr>

<tr>
<td valign="top">SO </td><td>2015 INTERNATIONAL CONFERENCE ON COGNITIVE COMPUTING AND INFORMATION
   PROCESSING (CCIP)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>2015 International Conference on Cognitive Computing and Information
   Processing (CCIP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 03-04, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Noida, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Neutral text-to-speech system; prosody; duration pattern; intonation
   pattern; Praat; Praat Script; PSOLA Algorithm</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION; SYSTEM</td>
</tr>

<tr>
<td valign="top">AB </td><td>In emotional-speech, it is observed that some words and phrases are spoken prominently, compared to neutral-speech. The prominence of these specific words and phrases are reflected in the form of prosodic features such as duration, intonation and intensity patterns of the words or phrases. The neutral speech and emotional speech have basic difference due to prosody aspects of speech. Three acoustic aspects of prosodic features were examined: the pitch contour, durations, and the intensity contour. These prosodic features from Hindi emotional-speech are imposed on Hindi neutral-speech at three different levels; sentence, word and syllable levels. The pitch contour, durations, and the intensity contour were imposed on neutral-speech using Praat tool with the help of Praat script. Subjective result indicates that syllable level fragments are good choice than word or sentence level fragments for generating emotional speech from neutral speech.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Yadav, Jainath; Rao, K. Sreenivasa] Indian Inst Technol Kharagpur, Sch
   Informat Technol, Kharagpur 721302, W Bengal, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Yadav, J (reprint author), Indian Inst Technol Kharagpur, Sch Informat Technol, Kharagpur 721302, W Bengal, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jaibhu38@gmail.com; ksrao@iitkgp.ac.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380430600016</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Mukherjee, K
   <br>Chatterjee, D</td>
</tr>

<tr>
<td valign="top">AF </td><td>Mukherjee, Kingshuk
   <br>Chatterjee, Debdatta</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Augmentative and Alternative Communication Device Based on Eye-Blink
   Detection and Conversion to Morse-Code to Aid Paralyzed Individuals</td>
</tr>

<tr>
<td valign="top">SO </td><td>2015 International Conference on Communication, Information &amp; Computing
   Technology (ICCICT)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Communication, Information &amp; Computing
   Technology (ICCICT)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JAN 15-17, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Inst Technol Mumbai, Mumbai, INDIA</td>
</tr>

<tr>
<td valign="top">HO </td><td>Inst Technol Mumbai</td>
</tr>

<tr>
<td valign="top">DE </td><td>Augmentative and Alternative Communication (AAC); Speech disorder; IR
   sensor; Eye-blink detector; Morse code converter</td>
</tr>

<tr>
<td valign="top">AB </td><td>There are several medical disorders that can lead to an individual becoming paralyzed or having motor speech disorders that inhibits speech or voice production. Conditions such as Locked-In Syndrome (LIS) or motor neuron diseases such as Amyotrophic Lateral Sclerosis (ALS) and Cerebral Palsy are among the common diseases that affect speech. In all or most such cases, the patient loses the ability to communicate with the external world in an effective manner even though his intelligence is mostly unaffected. Not only does that cause extreme distress to that individual, but also to his family and friends. Some customized Augmentative and Alternative Communication(AAC) devices have been developed that uses signals from the patient and converts them into some form of data that can be communicated but such devices are very expensive and are practically out of reach for most people. In this document, we have designed an extremely low priced device that reads and converts eye-blinks from the patient to a universally accepted communication code-The Morse code.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Mukherjee, Kingshuk; Chatterjee, Debdatta] Natl Inst Technol, Dept
   Elect Engn, Durgapur 713209, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Mukherjee, K (reprint author), Natl Inst Technol, Dept Elect Engn, Durgapur 713209, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>kingdgp08@gmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380538100096</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Pallavi, GS
   <br>Mohana
   <br>Aradhya, HVR</td>
</tr>

<tr>
<td valign="top">AF </td><td>Pallavi, G. S.
   <br>Mohana
   <br>Aradhya, H. V. Ravish</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>CAP to DIAMETER Protocol Converter</td>
</tr>

<tr>
<td valign="top">SO </td><td>2015 International Conference on Green Computing and Internet of Things
   (ICGCIoT)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Green Computing and Internet of Things
   (ICGCIoT)</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 08-10, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>GALGOTIAS Educ Inst, Greater Noida, INDIA</td>
</tr>

<tr>
<td valign="top">HO </td><td>GALGOTIAS Educ Inst</td>
</tr>

<tr>
<td valign="top">DE </td><td>Request For Comments (RFC-6733); IWF; CAMEL; DIAMETER; CAP; LTE; CDMA;
   IMS; 3G; 4G; MSC; EAP; SS7; 3GPP</td>
</tr>

<tr>
<td valign="top">AB </td><td>Customized applications for mobile network enhanced logic (CAMEL) application part (CAP) is a protocol used in 3G networks in order to communicate between the different components in the network. Where as in 4G networks DIAMETER protocol is used which provides higher data rates and increased throughput in the network, when compared to the CAMEL protocol. This paper describes the conversion of CAMEL protocol to DIAMETER protocol which helps in increased throughput and bandwidth utilization. The obtained result shows the call processing capability under various traffic conditions such as voice calls and short message service (SMS) calls depending on the SCP Action and CAP control modes. The CAP - Ro translator is a part of the interworking function (IWF).</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Pallavi, G. S.; Mohana] RV Coll Engn, Dept Telecommun, Bangalore,
   Karnataka, India.
   <br>[Aradhya, H. V. Ravish] RV Coll Engn, Dept ECE, Bangalore, Karnataka,
   India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Pallavi, GS (reprint author), RV Coll Engn, Dept Telecommun, Bangalore, Karnataka, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>gspallu6@gmail.com; mohana.rvce@gmail.com; ravisharadhya@rvce.edu.in</td>
</tr>

<tr xmlns:date="http://exslt.org/dates-and-times">
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>V, Aradhya H.</display_name>&nbsp;</font></td><td><font size="3">O-6994-2016&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>V, Aradhya H.</display_name>&nbsp;</font></td><td><font size="3">0000-0003-3076-9120&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>598</td>
</tr>

<tr>
<td valign="top">EP </td><td>602</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Science &amp; Technology - Other Topics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380517100119</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Anil, MC
   <br>Shirbahadurkar, SD</td>
</tr>

<tr>
<td valign="top">AF </td><td>Anil, Manjare Chandraprabha
   <br>Shirbahadurkar, S. D.</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Pitch and Duration Modification for Expressive Speech Synthesis in
   Marathi TTS system</td>
</tr>

<tr>
<td valign="top">SO </td><td>2015 INTERNATIONAL CONFERENCE ON PERVASIVE COMPUTING (ICPC)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Pervasive Computing (ICPC)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JAN 08-10, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Pune, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Fundamental frequency; Pitch Contour; Phase-Vocoder; Prosody; Speech
   synthesis</td>
</tr>

<tr>
<td valign="top">ID </td><td>PHASE-VOCODER</td>
</tr>

<tr>
<td valign="top">AB </td><td>Generating expressive synthetic speech is very important in high quality Marathi Text-to-Speech (TTS) system. This paper focuses on voice conversion and modification technique maintaining acceptable quality and naturalness with reduced database. In this paper, a method to modify fundamental frequency contour is proposed for Marathi TTS system. The naturalness of the speech is highly correlated to phonetic description and prosodic features such as Fundamental frequency and duration of that phone. For prosody generation, we have obtained a primary pitch curve for the word, based on the location followed by punctuation marks. Question mark and exclamation mark in the text are studied to modify prosody. Phase-Vocoder technique can be used to improve the prosody of the synthesized speech. The experimental results showed that the proposed prosody modification, based on pitch and duration modification can improve speech quality.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Anil, Manjare Chandraprabha] JSPMs Rajarshi Shahu Coll Engn, Dept Elect
   &amp; Telecommun Engn, Pune 411033, Maharashtra, India.
   <br>[Shirbahadurkar, S. D.] Dr DY Patil Coll Engn, Pune 410506, Maharashtra,
   India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Anil, MC (reprint author), JSPMs Rajarshi Shahu Coll Engn, Dept Elect &amp; Telecommun Engn, Pune 411033, Maharashtra, India.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380407300016</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>El Kadhi, A
   <br>Gherri, F
   <br>Amiri, H</td>
</tr>

<tr>
<td valign="top">AF </td><td>El Kadhi, Aymen
   <br>Gherri, Fadhila
   <br>Amiri, Hamid</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Building diphone database for Arabic text to speech synthesis system</td>
</tr>

<tr>
<td valign="top">SO </td><td>3RD INTERNATIONAL CONFERENCE ON CONTROL, ENGINEERING &amp; INFORMATION
   TECHNOLOGY (CEIT 2015)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>INTERNATIONAL CONFERENCE ON CONTROL ENGINEERING &amp; INFORMATION TECHNOLOGY
   (CEIT)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 25-27, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Tlemcen, ALGERIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Text To Speech synthesis; Arabic language; diphones;
   grapheme-to-phonemes conversion; database preparation; corpus</td>
</tr>

<tr>
<td valign="top">AB </td><td>The need to integrate automatic communication systems in daily applications and technology progress has encouraged speech synthesis sector development. Many variety of voice systems synthesis exist but diphones concatenation method appears to be efficient to contribute to speech synthesis of many languages. The first task in any development of speech synthesis system is to prepare the data base used for this aim. This paper describes the process of building an Arabic speech corpus based on diphones. We developed an acoustic data base of arabic diphones by studying the different possible combinations of phonemes to list all the possible diphones.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[El Kadhi, Aymen; Gherri, Fadhila; Amiri, Hamid] Univ Tunis El Manar,
   Natl Engn Sch Tunis, Tunis, Tunisia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>El Kadhi, A (reprint author), Univ Tunis El Manar, Natl Engn Sch Tunis, Tunis, Tunisia.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ay.kadhi@yahoo.fr; gherri.fadhila@gmail.com; hamidlamiri@gmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">SC </td><td>Automation &amp; Control Systems; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380433000172</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sharma, B
   <br>Adiga, N
   <br>Prasanna, SRM</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sharma, Bidisha
   <br>Adiga, Nagaraj
   <br>Prasanna, S. R. Mahadeva</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Development of Assamese Text-to-Speech Synthesis System</td>
</tr>

<tr>
<td valign="top">SO </td><td>TENCON 2015 - 2015 IEEE REGION 10 CONFERENCE</td>
</tr>

<tr>
<td valign="top">SE </td><td>TENCON IEEE Region 10 Conference Proceedings</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE Region 10 Conference</td>
</tr>

<tr>
<td valign="top">CY </td><td>NOV 01-04, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Macao, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Assamese TTS; letter to Sound rules for Assamese; Database design;
   Semi-automatic segmentation; bilingual TTS</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents the design and development of Assamese Text to speech (TTS) synthesis system. In particular, work focused on designing language specific rules, developing quality database, data segmentation, and to handle bilingual sound units. In Assamese language, till now no study is done to construct the grapheme to phoneme conversion rules. In this work, grapheme to phoneme conversion rules are proposed for Assamese language. The database is recorded by checking the speaking rate, variation in amplitude level, dc wandering, and clipping during data collection. A significant improvement in the synthesized voice is observed by ensuring uniform speaking rate, controlling variation in the signal amplitude level, and avoiding dc wandering and clipping during data collection. A semi-automatic segmentation approach is developed for data segmentation. Initially, segmentation is done by automatic process and later manual correction of segmentation boundaries is done to improve quality and intelligibility. It also reduce time required for the segmentation process. The developed TTS can work in bilingual mode. It can switch between Assamese and English language smoothly and maintains the sentence level intonation even for mixed texts.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Sharma, Bidisha; Adiga, Nagaraj; Prasanna, S. R. Mahadeva] Indian Inst
   Technol Guwahati, Dept Elect &amp; Elect Engn, Gauhati, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sharma, B (reprint author), Indian Inst Technol Guwahati, Dept Elect &amp; Elect Engn, Gauhati, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>s.bidisha@iitg.ernet.in; nagaraj@iitg.ernet.in; prasanna@iitg.ernet.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000380489200076</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Saito, Y
   <br>Nose, T
   <br>Shinozaki, T
   <br>Ito, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Saito, Yuki
   <br>Nose, Takashi
   <br>Shinozaki, Takahiro
   <br>Ito, Akinori</td>
</tr>

<tr>
<td valign="top">BE </td><td>Pan, JS
   <br>Lee, I
   <br>Huang, HC
   <br>Yang, CY</td>
</tr>

<tr>
<td valign="top">TI </td><td>Conversion of Speaker's Face Image Using PCA and Animation Unit for
   Video Chatting</td>
</tr>

<tr>
<td valign="top">SO </td><td>2015 INTERNATIONAL CONFERENCE ON INTELLIGENT INFORMATION HIDING AND
   MULTIMEDIA SIGNAL PROCESSING (IIH-MSP)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>11th International Conference on Intelligent Information Hiding and
   Multimedia Signal Processing (IIH-MSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 23-25, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Adelaide, AUSTRALIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speaker conversion; Face conversion; Neural network; Principal component
   analysis; Kinect v2</td>
</tr>

<tr>
<td valign="top">AB </td><td>Video chat is a good way of personal communication; however, there is a privacy issue in the video chat because we need to disclose one's identity such as face or voice when chatting. In this paper, we propose two methods by which face image of a speaker is converted into that of different person to conceal the speaker's identity. In the first method, we first prepare the speech and video data of the original and target speakers for training the conversion model. The face image features are calculated using the PCA to the whole pixels of the image. In the second method, the animation units extracted by Kinect are used as an intermediate feature, and we train a model that converts the animation unit to the target speaker's face image. In both methods, we used a neural network as the conversion model. We conducted experiments, and the first method could convert the whole shape of the speakers, while small movements such as mouth movement cannot be converted. The second method could convert both the whole shape of the face and mouth movement: however, the quality of face image was deteriorated.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Saito, Yuki; Nose, Takashi; Ito, Akinori] Tohoku Univ, Grad Sch Engn,
   Aoba Ku, 6-6-05 Aramaki Aza Aoba, Sendai, Miyagi 9808579, Japan.
   <br>[Shinozaki, Takahiro] Tokyo Inst Technol, Interdisciplinary Grad Sch Sci
   &amp; Engn, Midori Ku, Yokohama, Kanagawa 2268502, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Saito, Y (reprint author), Tohoku Univ, Grad Sch Engn, Aoba Ku, 6-6-05 Aramaki Aza Aoba, Sendai, Miyagi 9808579, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>yuki.saito.t6@dc.tohoku.ac.jp; tnose@m.tohoku.ac.jp;
   aito@spcom.ecei.tohoku.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>433</td>
</tr>

<tr>
<td valign="top">EP </td><td>436</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/IIH-MSP.2015.85</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000375671900107</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wei, M
   <br>Sun, KX
   <br>Wang, CX
   <br>Cheng, XF</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wei Min
   <br>Sun Kexue
   <br>Wang Chenxi
   <br>Cheng Xiefeng</td>
</tr>

<tr>
<td valign="top">BE </td><td>Du, W
   <br>Zhou, X</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Design and Implementation for Heart Sound Detection Instrument based
   on FPGA</td>
</tr>

<tr>
<td valign="top">SO </td><td>PROCEEDINGS OF THE 2015 3RD INTERNATIONAL CONFERENCE ON MACHINERY,
   MATERIALS AND INFORMATION TECHNOLOGY APPLICATIONS</td>
</tr>

<tr>
<td valign="top">SE </td><td>ACSR-Advances in Comptuer Science Research</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>3rd International Conference on Machinery, Materials and Information
   Technology Applications (ICMMITA)</td>
</tr>

<tr>
<td valign="top">CY </td><td>NOV 28-29, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Qingdao, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Heart Sound Acquisition; FPGA; A/D Conversion; Voice Broadcast</td>
</tr>

<tr>
<td valign="top">AB </td><td>According to the problem that traditional heart detection equipment is bulky and expensive, approach to designing and implementing portable heart sound detection instrument is proposed in this paper. After the heart sound signal is filtered and amplified by hardware circuits and FPGA, the SCM receives the pre-denoised signal and calculates the heart rate, broadcasting the result in time. Experiments show that the instrument is convenient for patients to diagnose by themselves and make patients receive treatment without delay, which improves the timeliness and accuracy of cardiac auscultation.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Wei Min; Sun Kexue; Wang Chenxi; Cheng Xiefeng] Nanjing Univ Posts &amp;
   Telecommun, Sch Elect Sci &amp; Engn, Nanjing 210023, Jiangsu, Peoples R
   China.
   <br>[Sun Kexue; Cheng Xiefeng] Jiangsu Prov Engn Lab RF Integrat &amp;
   Micropackage, Nanjing 210023, Jiangsu, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Wei, M (reprint author), Nanjing Univ Posts &amp; Telecommun, Sch Elect Sci &amp; Engn, Nanjing 210023, Jiangsu, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>sunkx@njupt.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">VL </td><td>35</td>
</tr>

<tr>
<td valign="top">BP </td><td>278</td>
</tr>

<tr>
<td valign="top">EP </td><td>282</td>
</tr>

<tr>
<td valign="top">SC </td><td>Automation &amp; Control Systems; Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000371522300055</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ishida, K
   <br>Shabanpour, R
   <br>Meister, T
   <br>Boroujeni, BK
   <br>Carta, C
   <br>Petti, L
   <br>Munzenrieder, N
   <br>Salvatore, GA
   <br>Troster, G
   <br>Ellinger, F</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ishida, K.
   <br>Shabanpour, R.
   <br>Meister, T.
   <br>Boroujeni, B. K.
   <br>Carta, C.
   <br>Petti, L.
   <br>Muenzenrieder, N.
   <br>Salvatore, G. A.
   <br>Troester, G.
   <br>Ellinger, F.</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>15 dB Conversion Gain, 20 MHz Carrier Frequency AM Receiver in Flexible
   a-IGZO TFT Technology with Textile Antennas</td>
</tr>

<tr>
<td valign="top">SO </td><td>2015 SYMPOSIUM ON VLSI TECHNOLOGY (VLSI TECHNOLOGY)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Symposium on VLSI Technology-Digest of Technical Papers</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>35th Anniversary of the Symposium on VLSI Technology (VLSI Technology)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUN 16-18, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Kyoto, JAPAN</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents an AM receiver implemented in a flexible a-IGZO TFT technology. The circuit consists of a four-stage cascode amplifier at the RF input, a detector based on a source follower, and a common source circuit for the baseband amplification. The measured conversion gain is very flat and exceeds 15 dB from 2 to 20 MHz carrier frequency range, which covers a relevant portion of the shortwave radio band. The 3 dB-bandwidth of the audio signal is 400 Hz to 10 kHz, which is comparable to the so-called voice band, and it is also suitable to low-rate data communication. In addition, an integrated demonstration of the AM receiver and textile antennas is carried out. The flexible a-IGZO receiver successfully detected the baseband signal through the textile antennas, demonstrating for the first time wireless transmission for this class of technologies.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ishida, K.; Shabanpour, R.; Meister, T.; Boroujeni, B. K.; Carta, C.;
   Ellinger, F.] Tech Univ Dresden, D-01062 Dresden, Germany.
   <br>[Petti, L.; Muenzenrieder, N.; Salvatore, G. A.; Troester, G.] Swiss Fed
   Inst Technol Zurich, Zurich, Switzerland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ishida, K (reprint author), Tech Univ Dresden, D-01062 Dresden, Germany.</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Salvatore, Giovanni Antonio</display_name>&nbsp;</font></td><td><font size="3">G-6966-2016&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Salvatore, Giovanni Antonio</display_name>&nbsp;</font></td><td><font size="3">0000-0002-8983-3257&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000370559200019</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Rajaganapathy, S
   <br>Aravind, B
   <br>Keerthana, B
   <br>Sivagami, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Rajaganapathy, S.
   <br>Aravind, B.
   <br>Keerthana, B.
   <br>Sivagami, M.</td>
</tr>

<tr>
<td valign="top">BE </td><td>Vijayakumar, V
   <br>Neelanarayanan, V</td>
</tr>

<tr>
<td valign="top">TI </td><td>Conversation of Sign Language to Speech with Human Gestures</td>
</tr>

<tr>
<td valign="top">SO </td><td>BIG DATA, CLOUD AND COMPUTING CHALLENGES</td>
</tr>

<tr>
<td valign="top">SE </td><td>Procedia Computer Science</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>2nd International Symposium on Big Data and Cloud Computing Challenges
   (ISBCC)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 12-13, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>VIT Univ, Chennai, INDIA</td>
</tr>

<tr>
<td valign="top">HO </td><td>VIT Univ</td>
</tr>

<tr>
<td valign="top">DE </td><td>Sign Language; Natural User Interface; Gesture Recognition; Motion
   Sensing; Skeleton Tracking; Human Machine Interaction; Microsoft Kinect;
   Xbox 360 KINECT; IR sensor</td>
</tr>

<tr>
<td valign="top">AB </td><td>Inability to speak is considered to be true disability. People with this disability use different modes to communicate with others, there are n number of methods available for their communication one such common method of communication is sign language. Sign language allows people to communicate with human body language; each word has a set of human actions representing a particular expression. The motive of the paper is to convert the human sign language to Voice with human gesture understanding and motion capture. This is achieved with the help of Microsoft Kinect a motion capture device from Microsoft. There are a few systems available for sign language to speech conversion but none of them provide natural user interface. For consideration if a person who has a disability to speak can stand perform the system and the system converts the human gestures as speech and plays it loud so that the person could actually communicate to a mass crowd gathering. Also the system is planned in bringing high efficiency for the users for improved communication. (C) 2015 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Rajaganapathy, S.; Aravind, B.; Keerthana, B.; Sivagami, M.] VIT Univ,
   Madras, Tamil Nadu, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Rajaganapathy, S (reprint author), VIT Univ, Madras, Tamil Nadu, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>rajaganapathy.s2010@vit.ac.in; aravind.b2010@vit.ac.in;
   keerthana.b2010@vit.ac.in; msivagami@vit.ac.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">VL </td><td>50</td>
</tr>

<tr>
<td valign="top">BP </td><td>10</td>
</tr>

<tr>
<td valign="top">EP </td><td>15</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.procs.2015.04.004</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000358675300002</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Basavaraju, R
   <br>Hegde, C</td>
</tr>

<tr>
<td valign="top">AF </td><td>Basavaraju, R.
   <br>Hegde, Chetana</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Traffic Signal Time Analysis and Voice - Based App for Visually Impaired
   Pedestrians</td>
</tr>

<tr>
<td valign="top">SO </td><td>2015 INTERNATIONAL CONFERENCE ON EMERGING RESEARCH IN ELECTRONICS,
   COMPUTER SCIENCE AND TECHNOLOGY (ICERECT)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Emerging Research in Electronics, Computer
   Science and Technology (ICERECT)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 17-19, 2015</td>
</tr>

<tr>
<td valign="top">CL </td><td>Mandya, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Android SDK; connected components; correlation coefficient; histogram;
   preprocessing; segmentation; text to speech conversion</td>
</tr>

<tr>
<td valign="top">AB </td><td>Analysis of traffic signal and ensure the safety of visually impaired people is a major challenge in helping disabled people. In this paper, we propose development of an application software which can be easily installed on a mobile device equipped with a camera. This application opens a camera and captures the traffic timer display upon tapping the app-icon. The timer image is then processed to segment the digits in them to identify the actual time in the numeric form. The detected time is then converted as a voice message and played using the app. Thus, the visually impaired person can hear the message about the time left to turn on the pedestrian signal. Getting this alert message, he/she can safely cross the road. The accuracy of the proposed algorithm is found to be 100% as it detected all the digits in every instance of the timer display image.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Basavaraju, R.] Visvesvraya Technol Univ, VTU RRC, Bangalore,
   Karnataka, India.
   <br>[Hegde, Chetana] RNS Inst Technol, Dept MCA, Bangalore, Karnataka, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Basavaraju, R (reprint author), Visvesvraya Technol Univ, VTU RRC, Bangalore, Karnataka, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>basavaraju.revanna@gmail.com; chetanahegde@ieee.org</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">BP </td><td>472</td>
</tr>

<tr>
<td valign="top">EP </td><td>475</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000411710100086</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Grandjeat, YC</td>
</tr>

<tr>
<td valign="top">AF </td><td>Grandjeat, Yves-Charles</td>
</tr>

<tr>
<td valign="top">TI </td><td>Poetic Co-operation in the Works of "Nature Writers of Our Own Time"</td>
</tr>

<tr>
<td valign="top">SO </td><td>MIRANDA</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>nature writers; U.S.A.; land ethic; ecocriticism; land poetics; ritual;
   reconnection; animals; death; energy; rhythm</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper invites several U.S. nature writers-notably Peter Matthiessen, Terry Tempest Williams and Gary Snyder-to outline the contours of a "land poetics" in keeping with Aldo Leopold's "land ethic". How can the writer-poet reach for a "middle voice" which would sound both, and with equal significance, the nature writer and nature itself, turning the latter into a subject rather than an object of discourse? While sketching out the intellectual context, in the field of ecocriticism, which has made it possible to imagine such a common "middle voice," this paper argues that the voice cannot materialize without some form of ritual conversion. The writers it looks at are committed to such a ritual-and not just thematic or formal-practice of poetic discourse, which first calls for a sacrifice, usually performed in the presence of an animal figure: that of the individual, sovereign figure of the Western self. This undoing releases the rhythmic materiality of language, discourse and narrative from the strictures of the self to open them up to life at large. In this context, the poetic statement acts like an offering, a momentary concretion, or "fruiting" (Snyder), in the energetic flow by means of which the ecosystem keeps realigning itself.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Grandjeat, Yves-Charles] Univ Bordeaux Montaigne, Pessac, France.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Grandjeat, YC (reprint author), Univ Bordeaux Montaigne, Pessac, France.</td>
</tr>

<tr>
<td valign="top">EM </td><td>Yves.Grandjeat@u-bordeaux3.fr</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2015</td>
</tr>

<tr>
<td valign="top">IS </td><td>11</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.4000/miranda.7024</td>
</tr>

<tr>
<td valign="top">SC </td><td>Arts &amp; Humanities - Other Topics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000218789000017</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Iranmanesh, SH
   <br>Rastegar, H
   <br>Mokhtarani, MH</td>
</tr>

<tr>
<td valign="top">AF </td><td>Iranmanesh, Seyed H.
   <br>Rastegar, Hamid
   <br>Mokhtarani, Mohammad H.</td>
</tr>

<tr>
<td valign="top">TI </td><td>An intelligent fuzzy logic-based system to support quality function
   deployment analysis</td>
</tr>

<tr>
<td valign="top">SO </td><td>CONCURRENT ENGINEERING-RESEARCH AND APPLICATIONS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Quality function deployment; house of quality; expert system; fuzzy
   inference system; technical characteristics</td>
</tr>

<tr>
<td valign="top">ID </td><td>ENGINEERING CHARACTERISTICS; QFD; MODEL; METHODOLOGY; INDUSTRY</td>
</tr>

<tr>
<td valign="top">AB </td><td>Quality function deployment is an efficient and powerful tool in design, development, and planning of products. The main function of quality function deployment is conversion of VOC to technical characteristics or voice of designer. However, it is not always easy to prioritize and assess technical characteristics during the total mass of information from the different customer attitudes. This article provides a methodology for the development of an intelligent quality function deployment based on fuzzy inference system in order to capture information through house of quality. As quality function deployment integrates different components of design and development, intelligent quality function deployment would be the best replacement for human expertise and can support decision-makers in wide range of design and development. This methodology applied on a classic sample for the design of a new undergraduate curriculum in an engineering department of a university as an illustrative example shows capability of this method. This article is composed of the background of quality function deployment, review of related research works, and representation of an intelligent system for analyzing it.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Iranmanesh, Seyed H.; Rastegar, Hamid; Mokhtarani, Mohammad H.] Univ
   Tehran, Dept Ind Engn, Tehran, Iran.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Rastegar, H (reprint author), Univ Tehran, Dept Ind Engn, Fac Engn, POB 11155-4563, Tehran, Iran.</td>
</tr>

<tr>
<td valign="top">EM </td><td>hamidrastegar93@ut.ac.ir</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">VL </td><td>22</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>106</td>
</tr>

<tr>
<td valign="top">EP </td><td>122</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1177/1063293X14522080</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering; Operations Research &amp; Management Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000336265000002</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Adiga, N
   <br>Govind, D
   <br>Prasanna, SRM</td>
</tr>

<tr>
<td valign="top">AF </td><td>Adiga, Nagaraj
   <br>Govind, D.
   <br>Prasanna, S. R. Mahadeva</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Significance of Epoch Identification Accuracy for Prosody Modification</td>
</tr>

<tr>
<td valign="top">SO </td><td>2014 INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING AND COMMUNICATIONS
   (SPCOM)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Signal Processing and Communications (SPCOM)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUL 22-25, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Banaglore, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>speech prosody; epoch; epoch identification accuracy; pitch; duration;
   telephone speech</td>
</tr>

<tr>
<td valign="top">ID </td><td>SIGNIFICANT EXCITATION; VOICE CONVERSION; INSTANTS; EXTRACTION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Epoch refers to instant of significant excitation in speech [1]. Prosody modification is the process of manipulating the pitch and duration of speech by fixed or dynamic modification factors. In epoch based prosody modification, the prosodic features of the speech signal are modified by anchoring around the epochs location in speech. The objective of the present work is to demonstrate the significance of epoch identification accuracy for prosody modification. Epoch identification accuracy is defined as standard deviation of identification timing error between estimated epochs with the reference epochs. Initially, the epochs location of the original speech are randomly varied for arbitrary time factors and corresponding prosody modified speech is generated. The perceptual quality of the prosody modified speech is evaluated from the mean opinion scores (MOS) and objective measure. The issues in the prosody modification of telephonic speech signals are also presented.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Adiga, Nagaraj; Prasanna, S. R. Mahadeva] Indian Inst Technol Guwahati,
   Dept Elect &amp; Elect Engn, Gauhati, Assam, India.
   <br>[Govind, D.] Amrita Vishwa Vidyapeetham, Ctr Computat Engn &amp; Networking,
   Coimbatore, Tamil Nadu, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Adiga, N (reprint author), Indian Inst Technol Guwahati, Dept Elect &amp; Elect Engn, Gauhati, Assam, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>nagaraj@iitg.ernet.in; govinddmenon@gmail.com; prasanna@iitg.ernet.in</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>D, Govind</display_name>&nbsp;</font></td><td><font size="3">C-4343-2018&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>D, Govind</display_name>&nbsp;</font></td><td><font size="3">0000-0002-8172-0370&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000364936400097</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Liang, HS
   <br>Wang, HB
   <br>Hou, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Liang, Hongshuo
   <br>Wang, Hongbo
   <br>Hou, Juan</td>
</tr>

<tr>
<td valign="top">BE </td><td>Liu, DL
   <br>Zhu, XB
   <br>Xu, KL
   <br>Fang, DM</td>
</tr>

<tr>
<td valign="top">TI </td><td>Designation And Realization Of The Interworking Gateway Between IMS And
   PSTN</td>
</tr>

<tr>
<td valign="top">SO </td><td>APPLIED SCIENCE, MATERIALS SCIENCE AND INFORMATION TECHNOLOGIES IN
   INDUSTRY</td>
</tr>

<tr>
<td valign="top">SE </td><td>Applied Mechanics and Materials</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Advances in Materials Science and
   Information Technologies in Industry (AMSITI)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JAN 11-12, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Xian, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>IP Multimedia Subsystem; Public Switched Telephone Network; Signaling
   System 7(SS7); Media Gateway; Signaling Gateway; CODEC</td>
</tr>

<tr>
<td valign="top">AB </td><td>The interworking with the PSTN network is a network integration issues which must be solved. M2PA, M2UA and M3UA were compared and M3UA was selected because of its stronger flexibility. For the convenience of equipment expansion, IP switch was used in the hardware designation of PSTN Gateway. The software designation was based on independent entity. Socket communication mode which is loosely coupled was used between different entities. The software was divided into the SIP Proxy Module, the Signal Adapter Module, the Service Adapter Module, the Code and Decode Resource Control Module, and so on. The conversion of format, code, identification between SIP and No. 7 signaling was realized. Multi-channel controller was used in the conversion of voice stream between the E1 interface circuit mode and RTP/RTCP mode. The interworking between PSTN and IMS was realized through the gateway and the old equipments can be used in the new network. This work is helpful to promote the network integration work.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Liang, Hongshuo; Hou, Juan] Shijiazhuang Vocat Technol Inst,
   Shijiazhuang 050081, HeBei, Peoples R China.
   <br>[Wang, Hongbo] China Elect Technol Grp Corp, Res Inst 54, Shijiazhuang
   050081, HeBei, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Liang, HS (reprint author), Shijiazhuang Vocat Technol Inst, Shijiazhuang 050081, HeBei, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>lianghongshuo@163.com; wang-hongbo@tom.com; 35586267@qq.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">VL </td><td>513-517</td>
</tr>

<tr>
<td valign="top">BP </td><td>2542</td>
</tr>

<tr>
<td valign="top">EP </td><td>2547</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.4028/www.scientific.net/AMM.513-517.2542</td>
</tr>

<tr>
<td valign="top">SC </td><td>Construction &amp; Building Technology; Engineering; Mechanics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000349668903112</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Blok, M
   <br>Drozda, P</td>
</tr>

<tr>
<td valign="top">AF </td><td>Blok, Marek
   <br>Drozda, Piotr</td>
</tr>

<tr>
<td valign="top">TI </td><td>Variable Ratio Sample Rate Conversion Based on Fractional Delay Filter</td>
</tr>

<tr>
<td valign="top">SO </td><td>ARCHIVES OF ACOUSTICS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>sample rate conversion; offset window method; variable fractional delay
   filter; variable resampling ratio</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper a sample rate conversion algorithm which allows for continuously changing resampling ratio has been presented. The proposed implementation is based on a variable fractional delay filter which is implemented by means of a Farrow structure. Coefficients of this structure are computed on the basis of fractional delay filters which are designed using the offset window method. The proposed approach allows us to freely change the instantaneous resampling ratio during processing. Using such an algorithm we can simulate recording of audio on magnetic tape with nonuniform velocity as well as remove such distortions. We have demonstrated capabilities of the proposed approach based on the example of speech signal processing with a resampling ratio which was computed on the basis of estimated fundamental frequency of voiced speech segments.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Blok, Marek] Gdansk Univ Technol, Fac Elect Telecommun &amp; Informat,
   PL-80233 Gdansk, Poland.
   <br>[Drozda, Piotr] ADVA Opt Networking Sp Zoo, PL-81310 Gdynia, Poland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Blok, M (reprint author), Gdansk Univ Technol, Fac Elect Telecommun &amp; Informat, G Narutowicza 11-12, PL-80233 Gdansk, Poland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>mblok@eti.pg.gda.pl</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Blok, Marek</display_name>&nbsp;</font></td><td><font size="3">0000-0002-8793-1697&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">VL </td><td>39</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>231</td>
</tr>

<tr>
<td valign="top">EP </td><td>242</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.2478/aoa-2014-0027</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000339645600009</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Liu, WW
   <br>Zhang, SS</td>
</tr>

<tr>
<td valign="top">AF </td><td>Liu, Wenwen
   <br>Zhang, Shasha</td>
</tr>

<tr>
<td valign="top">BE </td><td>Lee, G</td>
</tr>

<tr>
<td valign="top">TI </td><td>Application of ZigBee Technology in Traffic Intersection</td>
</tr>

<tr>
<td valign="top">SO </td><td>2014 4TH INTERNATIONAL CONFERENCE ON EDUCATION AND EDUCATION MANAGEMENT
   (EEM 2014), PT 6</td>
</tr>

<tr>
<td valign="top">SE </td><td>Advances in Education Research</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>4th International Conference on Education and Education Management (EEM
   2014)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 08-09, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Singapore, SINGAPORE</td>
</tr>

<tr>
<td valign="top">DE </td><td>Identification of traffic lights; ZigBee technology; Voice conversion
   module</td>
</tr>

<tr>
<td valign="top">AB </td><td>In order to solve the problems about difficult acquisition and identification of traffic lights in complex environment, and to realize real-time acquisition of traffic lights, a traffic signal recognition system based on ZigBee technology is proposed, constructed its hardware by CC2530 chip and peripheral interface, compiled its software by using the protocol stack of Z-Stack and added ATmega128L microcontroller and LD3320 speech recognition and processing module. All wireless communication modules in the system constitute a star network and transmit real-time traffic light signal. Test results indicated that the proposed method had high detection accuracy for multi-type traffic lights in different scenarios and provided the driver with timely and accurate traffic information.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Liu, Wenwen] Great Wall Motor Co, R&amp;D Ctr, Automot Engn Tech Ctr Hebei,
   Baoding 071000, Peoples R China.
   <br>[Zhang, Shasha] Chongqing Jiaotong Univ, Sch Mechatron &amp; Automobile
   Engn, Chongqing 400074, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Liu, WW (reprint author), Great Wall Motor Co, R&amp;D Ctr, Automot Engn Tech Ctr Hebei, Baoding 071000, Peoples R China.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">VL </td><td>68</td>
</tr>

<tr>
<td valign="top">BP </td><td>154</td>
</tr>

<tr>
<td valign="top">EP </td><td>159</td>
</tr>

<tr>
<td valign="top">SC </td><td>Education &amp; Educational Research; Social Sciences - Other Topics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000375964000031</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Tanaka, K
   <br>Toda, T
   <br>Neubig, G
   <br>Sakti, S
   <br>Nakamura, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Tanaka, Kou
   <br>Toda, Tomoki
   <br>Neubig, Graham
   <br>Sakti, Sakriani
   <br>Nakamura, Satoshi</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>An Inter-Speaker Evaluation through Simulation of Electrolarynx Control
   based on Statistical F-0 Prediction</td>
</tr>

<tr>
<td valign="top">SO </td><td>2014 ASIA-PACIFIC SIGNAL AND INFORMATION PROCESSING ASSOCIATION ANNUAL
   SUMMIT AND CONFERENCE (APSIPA)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Asia-Pacific Signal and Information Processing Association Annual Summit
   and Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Annual Summit and Conference of
   Asia-Pacific-Signal-and-Information-Processing-Association (APSIPA)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 09-12, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Angkor, CAMBODIA</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>An electrolarynx is a device that artificially generates excitation sounds to produce electrolaryngeal (EL) speech. Although proficient laryngectomees can produce intelligible EL speech by using this device, it sounds quite unnatural due to the mechanical excitation. To address this issue, we have proposed several EL speech enhancement methods using statistical voice conversion and showed that statistical prediction of excitation parameters, such as F-0 patterns, was essential to significantly improve naturalness of EL speech. Based on this result, we have also proposed a direct control method of F-0 patterns of excitation sounds generated from the electrolarynx based on the statistical excitation prediction, which may allow EL speech enhancement to be applied to face-to-face conversation. In our previous work, this direct control method was evaluated through simulation using only a single laryngectomee's EL speech and it was demonstrated that this method allows for improved naturalness of EL speech while preserving listenability. However, because quality of EL speech highly depends on the proficiency of each laryngectomee, it is still not clear whether these methods will generalize to other speakers. In addition, while previous work only evaluated the naturalness and listenability, intelligibility is also an important factor that has not been evaluated. In this paper, we apply the direct control method to multiple speakers consisting of two real laryngectomees and one non-laryngectomee and evaluate its performance through simulations in terms of naturalness, listenability, and intelligibility. The experimental results demonstrate that the proposed method yields significant improvements in naturalness of EL speech for multiple laryngectomees while maintaining listenability and intelligibility.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Tanaka, Kou; Toda, Tomoki; Neubig, Graham; Sakti, Sakriani; Nakamura,
   Satoshi] Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Ikoma, Nara,
   Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Tanaka, K (reprint author), Nara Inst Sci &amp; Technol, Grad Sch Informat Sci, Ikoma, Nara, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ko-t@is.naist.jp; tomoki@is.naist.jp; neubig@is.naist.jp;
   ssakti@is.naist.jp; s-nakamura@is.naist.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000392861900081</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Shanmugapriya, R
   <br>RajaMohammed, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Shanmugapriya, R.
   <br>RajaMohammed, S.</td>
</tr>

<tr>
<td valign="top">BE </td><td>Porkumaran, K
   <br>Chinnaiyan, VK</td>
</tr>

<tr>
<td valign="top">TI </td><td>SPEECH RECOGNITION OPEN SOURCE TOOLS FOR THE SEMANTIC IDENTIFICATION OF
   THE SENTENCE</td>
</tr>

<tr>
<td valign="top">SO </td><td>2014 INTERNATIONAL CONFERENCE ON GREEN COMPUTING COMMUNICATION AND
   ELECTRICAL ENGINEERING (ICGCCEE)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Green Computing Communication and Electrical
   Engineering (ICGCCEE)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAR 06-08, 2014</td>
</tr>

<tr>
<td valign="top">CL </td><td>Coimbatore, INDIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speech Recognition; Speech to text; learning Disabilities; Natural
   Language Processing; knowledge</td>
</tr>

<tr>
<td valign="top">AB </td><td>In the NLP world the recognizing speech as in the voice recognition and also allowing voice to serve as the "main interface between the human and the computer. This paper mainly focuses on the tools that are used to deliver how current speech recognition technology facilitates the learning part of the students, and also in what way the technology will be help to developing the advance learning system for future. The tools like speech to text conversion are to be discussed in this paper and also focused some attributes for the semantic identification. Although speech recognition has a potential benefit for students with physical disabilities and harsh learning disabilities, these are the technology in which it has been implemented inconsistently in the classroom above the years. By means of this the knowledge continues to develop, on the other hand, various issues are being addressed.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Shanmugapriya, R.; RajaMohammed, S.] Kalaignar Karunanidhi Inst
   Technol, Dept Comp Sci &amp; Engn, Coimbatore, Tamil Nadu, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Shanmugapriya, R (reprint author), Kalaignar Karunanidhi Inst Technol, Dept Comp Sci &amp; Engn, Coimbatore, Tamil Nadu, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>coolshanmugapriya@gmail.com; raja_delip@yahoo.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2014</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000365616800024</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Li, F
   <br>Tang, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Li, Frank
   <br>Tang, Alan</td>
</tr>

<tr>
<td valign="top">TI </td><td>High Performance Payload Conversion Method</td>
</tr>

<tr>
<td valign="top">SO </td><td>BELL LABS TECHNICAL JOURNAL</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">AB </td><td>Virtualization technology is becoming more and more popular, and when applicable, telecom equipment is being migrated to virtualized platforms. While virtualization technology provides many benefits, it also presents significant challenges. For example, the special hardware used for audio payload conversion can no longer be used. In legacy telecom equipment, the user plane, which processes voice over Internet Protocol (IP), was designed to use special hardware such as field programmable gate array/complex programmable logic devices (FPGA/CPLD) for optimum performance. However, in virtualized platforms the only choice is a general-purpose processor and therefore software-based payload conversion algorithms have to be used accordingly. This introduces a major performance issue in that a software-only solution is needed to reach the performance levels already possible with legacy equipment, and the prospect of reaching this goal using a software-based algorithm is a very challenging task. This letter describes our work to use a virtualized machine to replace the CPLD board used to convert the VoIP payload packets from the Real Time Transport Protocol (RTP) to the Transcoder and Rate Adaptation Unit over IP (TRAUP) protocol and vice versa. We also describe a conversion method that we developed to improve performance. (c) 2013 Alcatel-Lucent.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Li, Frank] Alcatel Lucent, Qingdao, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Li, F (reprint author), Alcatel Lucent, Qingdao, Peoples R China.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>SEP</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">VL </td><td>18</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">SI </td><td>SI</td>
</tr>

<tr>
<td valign="top">BP </td><td>135</td>
</tr>

<tr>
<td valign="top">EP </td><td>142</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1002/bltj.21609</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000323661400009</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sridhar, R
   <br>Madhavan, KV
   <br>Nagarajan, S
   <br>Nishanth, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sridhar, Rajeswari
   <br>Madhavan, K., V
   <br>Nagarajan, S.
   <br>Nishanth, S.</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>INCREMENTAL LANGUAGE MODEL AND DYNAMIC DECODER FOR TAMIL CHAT SYSTEM</td>
</tr>

<tr>
<td valign="top">SO </td><td>2013 INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTING, COMMUNICATIONS
   AND INFORMATICS (ICACCI)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>2nd International Conference on Advances in Computing, Communications
   and Informatics (ICACCI)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 22-25, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Sri Jayachamarajendra Coll Engn, Mysore, INDIA</td>
</tr>

<tr>
<td valign="top">HO </td><td>Sri Jayachamarajendra Coll Engn</td>
</tr>

<tr>
<td valign="top">DE </td><td>Language Model; Segmentation; Decoder; Tri phone effect;
   Transliteration; Bilingual phoneme mapping</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH RECOGNITION</td>
</tr>

<tr>
<td valign="top">AB </td><td>The paper gives an overview about a newly designed chat system "Tamil Vayadi" that recognizes Tamil voice input from the sender, converts it to text and transfers to the other end in a serialized form. This is received by the receiver in text form and presented in a new chat window. The system tries to eliminate some of the problems that are profound to Speech processing systems. The Input speech is split into words and each word is pre-processed (Normalization and Noise Removal). This is then fed into segmentation algorithm which splits the words into phonemes and PLP values of these phonemes are extracted which is stored for comparison during recognition. During recognition the PLP features of the extracted phonemes are compared with the PLP values of the available phoneme for converting the speech to text. A Bigram Language Model and a Decoder are designed for this purpose of conversion. We have introduced and incorporated the Tri phone effect for effective detection of phonemes. A learning module is also incorporated to effectively identify the phoneme. Using these improvements, we have achieved a word identification rate of 70%.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Sridhar, Rajeswari; Madhavan, K., V; Nagarajan, S.; Nishanth, S.] Anna
   Univ, Dept Comp Sci &amp; Engn, Madras 600025, Tamil Nadu, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sridhar, R (reprint author), Anna Univ, Dept Comp Sci &amp; Engn, Madras 600025, Tamil Nadu, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>rajisridhar@gmail.com; madhavan.varadan@gmail.com; naga4042@gmail.com;
   nishanthss@live.com</td>
</tr>

<tr xmlns:date="http://exslt.org/dates-and-times">
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Sridhar, Rajeswari</display_name>&nbsp;</font></td><td><font size="3">0000-0003-1204-1056&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>2057</td>
</tr>

<tr>
<td valign="top">EP </td><td>2062</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000343771500358</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Chen, XQ
   <br>Zhao, HM
   <br>Yu, YB
   <br>Wu, HW</td>
</tr>

<tr>
<td valign="top">AF </td><td>Chen, Xueqin
   <br>Zhao, Heming
   <br>Yu, Yibiao
   <br>Wu, Hongwei</td>
</tr>

<tr>
<td valign="top">BE </td><td>Wang, H
   <br>Yuen, SY
   <br>Wang, L
   <br>Shao, L
   <br>Wang, X</td>
</tr>

<tr>
<td valign="top">TI </td><td>Speech Feature Analysis and Spectrum Conversion From Children to Young
   Adults</td>
</tr>

<tr>
<td valign="top">SO </td><td>2013 NINTH INTERNATIONAL CONFERENCE ON NATURAL COMPUTATION (ICNC)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th International Conference on Natural Computation (ICNC)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUL 23-25, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Shenyang, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Age speech conversion; Gaussian mixture model; Linear predictive
   cepstral coefficients; Feature matching alignment; Gender-dependent</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOCAL-TRACT; NORMALIZATION; AGE</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, the short time spectral envelope differences are analyzed and compared between children and young adults speech. Based on the analysis, a feature matching alignment Gaussian mixture model (FMA-GMM) is proposed to achieve the voice conversion from children to young adults. The model is gender-dependent and feature parallel training. In FMA-GMM, the F0 track matching degree is computed between several children and young adult speakers. Then a child and a young adult who have the best matching degree are chose to making feature warping alignment. The test speech is produced by twelve young people who provide the recordings in childhood. Experimental results show that the proposed method can achieve better performance than GMM and piece-wise linear warping function.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Chen, Xueqin; Zhao, Heming; Yu, Yibiao; Wu, Hongwei] Soochow Univ, Sch
   Elect &amp; Informat Engn, Suzhou, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Chen, XQ (reprint author), Soochow Univ, Sch Elect &amp; Informat Engn, Suzhou, Peoples R China.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>1444</td>
</tr>

<tr>
<td valign="top">EP </td><td>1448</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Mathematical &amp; Computational Biology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000341627900259</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Huang, YC
   <br>Wu, CH
   <br>Lin, SL</td>
</tr>

<tr>
<td valign="top">AF </td><td>Huang, Yi-Chin
   <br>Wu, Chung-Hsien
   <br>Lin, Shih-Lun</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>PERSONALIZED NATURAL SPEECH SYNTHESIS BASED ON RETRIEVAL OF PITCH
   PATTERNS USING HIERARCHICAL FUJISAKI MODEL</td>
</tr>

<tr>
<td valign="top">SO </td><td>2013 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 26-31, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Vancouver, CANADA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Fujisaki Model; Hierarchical Prosodic Structure; Pattern Retrieval;
   Personalized Speech Synthesis</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION; INFORMATION; GENERATION</td>
</tr>

<tr>
<td valign="top">AB </td><td>In recent years, speech synthesis base don Hidden Markov Model (HMM) has been developed, which can synthesize stable and intelligible speech with flexibility and small footprint. However, synthesized prosodic features are still incapable to convey personalization and natural property. Previous prosody models, mainly constructed from the clustered prosodic features, are unable to characterize personalized prosodic information as the linguistic cues of the input sentence are indistinguishable for all speakers. An approach to retrieval of personalized pitch patterns from the real speech corpus of the target speaker, is proposed, incorporating with the HMM-based speech synthesizer, to generate a personalized natural pitch contour. The modified Fujisaki model is adopted to depict the hierarchical pitch patterns, aiming to model local pitch contour variation and global intonation of utterances in the corpus. The codeword sequences of utterances in the training and the synthesized corpora are constructed and used to obtain the relationship of pitch patterns between the real and synthesized speech. Finally, a language model of pitch pattern is constructed to obtain an optimal pitch pattern sequence of the input sentence. The experimental results using subjective and objective evaluations demonstrated the proposed approach can substantially outperform the conventional statistical synthesis methods, in terms of naturalness and speaker similarity.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Huang, Yi-Chin; Wu, Chung-Hsien; Lin, Shih-Lun] Natl Cheng Kung Univ,
   Dept Comp Sci &amp; Informat Engn, Tainan 70101, Taiwan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Huang, YC (reprint author), Natl Cheng Kung Univ, Dept Comp Sci &amp; Informat Engn, Tainan 70101, Taiwan.</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>7844</td>
</tr>

<tr>
<td valign="top">EP </td><td>7848</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000329611508001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Chen, CP
   <br>Yeh, BF</td>
</tr>

<tr>
<td valign="top">AF </td><td>Chen, Chia-Ping
   <br>Yeh, Bing-Feng</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>YET ANOTHER GAUSSIAN MIXTURE MODEL-BASED FEATURE COMPENSATION METHOD FOR
   ROBUST NOISY-DIGIT RECOGNITION</td>
</tr>

<tr>
<td valign="top">SO </td><td>2013 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 26-31, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Vancouver, CANADA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Gaussian mixture model; Aurora 2.0; noise-robust speech recognition</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH RECOGNITION; SPEAKER VERIFICATION; VOICE CONVERSION;
   REPRESENTATION; GMM</td>
</tr>

<tr>
<td valign="top">AB </td><td>We propose yet another Gaussian mixture model (YGMM) for robust speech recognition in noisy environments. The main difference between the proposed method and previously proposed GMM-based methods is that we estimate the noise features instead of the clean-speech features. In the implemented system, a condition classifier, incidentally based on GMM, is used to decide the noise type and level, and the corresponding GMM is employed to compensate for the noise-corrupted features. The proposed method and the implemented system are evaluated with the well-documented Aurora 2.0 noisy digit corpus. The results are promising. Specifically, it achieves a relative improvement in word error rate of 52.4% over the standard baseline, and 24.9% over a better baseline based on a traditional GMM-based feature compensation method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Chen, Chia-Ping; Yeh, Bing-Feng] Natl Sun Yat Sen Univ, Dept Comp Sci &amp;
   Engn, Kaohsiung 804, Taiwan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Chen, CP (reprint author), Natl Sun Yat Sen Univ, Dept Comp Sci &amp; Engn, 70 Lien Hai Rd, Kaohsiung 804, Taiwan.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>8051</td>
</tr>

<tr>
<td valign="top">EP </td><td>8055</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000329611508043</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sadeque, FY
   <br>Yasar, S
   <br>Islam, MM</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sadeque, Farig Yousuf
   <br>Yasar, Samin
   <br>Islam, Md. Monirul</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Bangla Text to Speech Conversion: a Syllabic Unit Selection Approach</td>
</tr>

<tr>
<td valign="top">SO </td><td>2013 INTERNATIONAL CONFERENCE ON INFORMATICS, ELECTRONICS &amp; VISION
   (ICIEV)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Informatics, Electronics and Vision (ICIEV)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 17-18, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Univ Dhaka, Dhaka, BANGLADESH</td>
</tr>

<tr>
<td valign="top">HO </td><td>Univ Dhaka</td>
</tr>

<tr>
<td valign="top">AB </td><td>Text to speech conversion, abbreviated as TTS, is an artificial voice tool for text documents that converts written text into synthesized sounds. There are quite a number of methods of synthesizing speech for TTS and all of them have their own significant issues like complicated physical model, non-intelligibility, limited usability etc. The best one available is Unit Selection Synthesis, which provides with optimal naturalness and efficiency based on which unit is used- for example, phoneme, diphone, syllable etc. This paper describes a syllabic method of unit selection synthesis for converting text into speech for Bangla language, the sixth most spoken language in the world. The development process of text collection, text analysis and speech synthesis is also described in this paper.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Sadeque, Farig Yousuf; Yasar, Samin; Islam, Md. Monirul] Bangladesh
   Univ Engn &amp; Technol, Dept Comp Sci &amp; Engn, Dhaka 1000, Bangladesh.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sadeque, FY (reprint author), Bangladesh Univ Engn &amp; Technol, Dept Comp Sci &amp; Engn, Dhaka 1000, Bangladesh.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000326546900069</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>McLoughlin, IV
   <br>Li, JJ
   <br>Song, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>McLoughlin, Ian Vince
   <br>Li, Jingjie
   <br>Song, Yan</td>
</tr>

<tr>
<td valign="top">BE </td><td>Bimbot, F
   <br>Cerisara, C
   <br>Fougeron, C
   <br>Gravier, G
   <br>Lamel, L
   <br>Pellegrino, F
   <br>Perrier, P</td>
</tr>

<tr>
<td valign="top">TI </td><td>Reconstruction of continuous voiced speech from whispers</td>
</tr>

<tr>
<td valign="top">SO </td><td>14TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2013), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>14th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2013)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 25-29, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Lyon, FRANCE</td>
</tr>

<tr>
<td valign="top">DE </td><td>whispers; speech reconstruction; whisper-to-speech conversion</td>
</tr>

<tr>
<td valign="top">ID </td><td>ENHANCEMENT</td>
</tr>

<tr>
<td valign="top">AB </td><td>Whispers are an important secondary vocal communications mechanism, that can be necessary for communicating private information and which are an integral aspect of natural human to-human dialogue. Furthermore, they may be the primary communications method of those suffering from certain forms of aphonia, such as laryngectomees. This paper considers the conversion of continuous whispers to natural-sounding speech, and proposes a new reconstruction method based upon the synthesis of individual formants as excitation source, followed by artificial glottal modulation. Early results show that the proposed method can improve quality and intelligibility over the original whispers when evaluated using continuous speech. It requires neither a priori nor speaker-dependent information, is of relatively low-complexity and suitable for real-time processing.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[McLoughlin, Ian Vince; Li, Jingjie; Song, Yan] Univ Sci &amp; Technol
   China, Natl Engn Lab Speech &amp; Language Informat Proc, Hefei, Peoples R
   China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>McLoughlin, IV (reprint author), Univ Sci &amp; Technol China, Natl Engn Lab Speech &amp; Language Informat Proc, Hefei, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ivm@ustc.edu.cn; jingjie@mail.ustc.edu.cn; songy@ustc.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>1021</td>
</tr>

<tr>
<td valign="top">EP </td><td>1025</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000395050000217</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kameoka, H
   <br>Yoshizato, K
   <br>Ishihara, T
   <br>Ohishi, Y
   <br>Kashino, K
   <br>Sagayama, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kameoka, Hirokazu
   <br>Yoshizato, Kota
   <br>Ishihara, Tatsuma
   <br>Ohishi, Yasunori
   <br>Kashino, Kunio
   <br>Sagayama, Shigeki</td>
</tr>

<tr>
<td valign="top">BE </td><td>Bimbot, F
   <br>Cerisara, C
   <br>Fougeron, C
   <br>Gravier, G
   <br>Lamel, L
   <br>Pellegrino, F
   <br>Perrier, P</td>
</tr>

<tr>
<td valign="top">TI </td><td>Generative modeling of speech F-0 contours</td>
</tr>

<tr>
<td valign="top">SO </td><td>14TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2013), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>14th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2013)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 25-29, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Lyon, FRANCE</td>
</tr>

<tr>
<td valign="top">DE </td><td>speech F-0 contour; Fujisaki model; generative model; hidden Markov
   model; EM algorithm</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper introduces our ongoing work on generative modeling of speech fundamental frequency (F-0) contours for estimating prosodic features from raw speech data. The present F-0 contour model is formulated by translating the Fujisaki model, a well-founded mathematical model representing the control mechanism of vocal fold vibration, into a probabilistic model described as a discrete-time stochastic process. The motivation behind this formulation is two fold. One is to derive a general parameter estimation framework for the Fujisaki model, allowing for the introduction of powerful statistical methods. The other is to construct an automatically trainable version of the Fujisaki model so that in future it can be used to develop a statistical speaking style conversion system or incorporated into existing text-to-speech synthesis systems to improve the naturalness and intelligibility of computer-generated speech. We also briefly introduce a generative model of F-0 contours of singing voice developed under the same spirit.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kameoka, Hirokazu; Yoshizato, Kota; Ishihara, Tatsuma; Sagayama,
   Shigeki] Univ Tokyo, Grad Sch Informat Sci &amp; Technol, Tokyo, Japan.
   <br>[Kameoka, Hirokazu; Ohishi, Yasunori; Kashino, Kunio] NTT Corp, NTT
   Commun Sci Labs, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kameoka, H (reprint author), Univ Tokyo, Grad Sch Informat Sci &amp; Technol, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>kameoka@hil.t.u-tokyo.ac.jp; yoshizato@hil.t.u-tokyo.ac.jp;
   ishihara@hil.t.u-tokyo.ac.jp; ohishi.yasunori@lab.ntt.co.jp;
   kashino.kunio@lab.ntt.co.jp; sagayama@hil.t.u-tokyo.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>1825</td>
</tr>

<tr>
<td valign="top">EP </td><td>1829</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000395050000381</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Turan, MAT
   <br>Erzin, E</td>
</tr>

<tr>
<td valign="top">AF </td><td>Turan, M. A. Tugtekin
   <br>Erzin, Engin</td>
</tr>

<tr>
<td valign="top">BE </td><td>Bimbot, F
   <br>Cerisara, C
   <br>Fougeron, C
   <br>Gravier, G
   <br>Lamel, L
   <br>Pellegrino, F
   <br>Perrier, P</td>
</tr>

<tr>
<td valign="top">TI </td><td>A New Statistical Excitation Mapping for Enhancement of Throat
   Microphone Recordings</td>
</tr>

<tr>
<td valign="top">SO </td><td>14TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2013), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">SE </td><td>Interspeech</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>14th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2013)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 25-29, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Lyon, FRANCE</td>
</tr>

<tr>
<td valign="top">DE </td><td>throat-microphone; speech enhancement; spectral envelope mapping;
   excitation mapping; GMM mapping</td>
</tr>

<tr>
<td valign="top">ID </td><td>ARTIFICIAL BANDWIDTH EXTENSION; SPEECH RECOGNITION; VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper we investigate a new statistical excitation mapping technique to enhance throat-microphone speech using joint analysis of throat- and acoustic-microphone recordings. In a recent study we employed source-filter decomposition to enhance spectral envelope of the throat-microphone recordings. In the source-filter decomposition framework we observed that the spectral envelope difference of the excitation signals of throat and acoustic-microphone recordings is an important source of the degradation in the throat-microphone voice quality. In this study we model spectral envelope difference of the excitation signals as a spectral tilt vector, and we propose a new phone-dependent GMM-based spectral tilt mapping scheme to enhance throat excitation signal. Experiments are performed to evaluate the proposed excitation mapping scheme in comparison with the state-of-the-art throat-microphone speech enhancement techniques using both objective and subjective evaluations. Objective evaluations are performed with the wide band perceptual evaluation of speech quality (ITU-PESQ) metric. Subjective evaluations are performed with the AB pair comparison listening test. Both objective and subjective evaluations yield that the proposed statistical excitation mapping consistently delivers higher improvements than the statistical mapping of the spectral envelope to enhance the throat-microphone recordings.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Turan, M. A. Tugtekin; Erzin, Engin] Koc Univ, Coll Engn, Multimedia
   Vis &amp; Graph Lab, Istanbul, Turkey.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Turan, MAT (reprint author), Koc Univ, Coll Engn, Multimedia Vis &amp; Graph Lab, Istanbul, Turkey.</td>
</tr>

<tr>
<td valign="top">EM </td><td>mturan@ku.edu.tr; eerzin@ku.edu.tr</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>3243</td>
</tr>

<tr>
<td valign="top">EP </td><td>3247</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000395050001193</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Takanashi, H
   <br>Mimura, R
   <br>Mimuro, T
   <br>Kodama, H
   <br>Ito, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Takanashi, Hiroyuki
   <br>Mimura, Ryosuke
   <br>Mimuro, Tetsushi
   <br>Kodama, Hiroyuki
   <br>Ito, Takeshi</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Development and Performance Evaluation of Portable Braille Scanner Using
   Simple Plate Spring Sensor</td>
</tr>

<tr>
<td valign="top">SO </td><td>2013 IEEE SENSORS</td>
</tr>

<tr>
<td valign="top">SE </td><td>IEEE Sensors</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>12th IEEE Sensors Conference</td>
</tr>

<tr>
<td valign="top">CY </td><td>NOV 03-06, 2013</td>
</tr>

<tr>
<td valign="top">CL </td><td>Baltimore, MD</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper proposes a portable Braille reading sensor which is consisting of three plate springs and strain gauges. A Braille letter is composed of six dots of 3 rows and 2 columns. Each plate spring corresponds to each row of the dot. The detection and conversion process are as follows: With the plate spring passing on a dot, the bending strain of the plate springs are detected through the strain gauge. The detected strain signals are converted to 0-1 normalized sequences, in which 0 means no dot and 1 corresponds to a dot. The three normalized sequences are assigned to 0-1 array of 3 times 2. Further, the array is converted to a corresponding letter, and the sequence of letters is transmitted to the voice signal. The performance of the portable Braille reading system is examined through Braille reading test.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Takanashi, Hiroyuki; Mimura, Ryosuke; Mimuro, Tetsushi] Akita
   Prefectural Univ, Dept Machine Intelligence &amp; Syst Engn, 84-4 Tsuchiya,
   Akita 0150055, Japan.
   <br>[Kodama, Hiroyuki; Ito, Takeshi] Akita Techno Design Co Ltd, Akita,
   Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Takanashi, H (reprint author), Akita Prefectural Univ, Dept Machine Intelligence &amp; Syst Engn, 84-4 Tsuchiya, Akita 0150055, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>takanashi@akita-pu.ac.jp; h-kodama@akita-techno.co.jp;
   t-ito@akita-techno.co.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2013</td>
</tr>

<tr>
<td valign="top">BP </td><td>1223</td>
</tr>

<tr>
<td valign="top">EP </td><td>1225</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Remote Sensing</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000379846100293</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Huang, YC
   <br>Wu, CH
   <br>Weng, ST</td>
</tr>

<tr>
<td valign="top">AF </td><td>Huang, Yi-Chin
   <br>Wu, Chung-Hsien
   <br>Weng, Sz-Ting</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>HIERARCHICAL PROSODIC PATTERN SELECTION BASED ON FUJISAKI MODEL FOR
   NATURAL MANDARIN SPEECH SYNTHESIS</td>
</tr>

<tr>
<td valign="top">SO </td><td>2012 8TH INTERNATIONAL SYMPOSIUM ON CHINESE SPOKEN LANGUAGE PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>8th International Symposium on Chinese Spoken Language Processing
   (ISCSLP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 05-08, 2012</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hong Kong, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Fujisaki Model; Hierarchical Prosodic Structure; Pattern Retrieval; Unit
   Selection</td>
</tr>

<tr>
<td valign="top">ID </td><td>CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, a novel hierarchical prosodic unit selection method is proposed based on pitch contour pattern retrieval, in order to obtained natural pitch contour of the personalized synthetic voice. In this framework, a hierarchical prosodic unit based on Fujisaki model is used to take local pitch contour variation and global intonation of utterance into account. Furthermore, novel ways of integrating pitch contour pattern of prosodic units in the prosodic model are invents in order to improve the selection mechanism of the appropriate pitch contour. A novel prosodic unit selection method is proposed based on sentence retrieval, which not only uses the traditional linguistic cue as selection criterion, but also the shape of the pitch contour. Also, the codewords of pitch patterns in the training corpus and synthesized corpus were constructed by the proposed method and were used to map the relation between training codeword and synthesized corpus. Finally, the language model of pitch pattern is adopted to find the proper pitch pattern sequence of input text. The evaluation results demonstrate that the proposed prosodic model substantially improves naturalness of the intonation of the synthesized speech compared to that of model-based method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Huang, Yi-Chin; Wu, Chung-Hsien; Weng, Sz-Ting] Natl Cheng Kung Univ,
   Dept Comp Sci &amp; Informat Engn, Tainan 70101, Taiwan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Huang, YC (reprint author), Natl Cheng Kung Univ, Dept Comp Sci &amp; Informat Engn, Tainan 70101, Taiwan.</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">BP </td><td>79</td>
</tr>

<tr>
<td valign="top">EP </td><td>83</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000316984700024</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Susakulchai, S
   <br>Kaewprapan, W
   <br>Chaisanit, S</td>
</tr>

<tr>
<td valign="top">AF </td><td>Susakulchai, Surachai
   <br>Kaewprapan, Wacheerapan
   <br>Chaisanit, Settachai</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Making of DAISY Talking book on Mobile Phone: a Concept and Architecture</td>
</tr>

<tr>
<td valign="top">SO </td><td>2012 INTERNATIONAL CONFERENCE FOR INTERNET TECHNOLOGY AND SECURED
   TRANSACTIONS</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference for Internet Technology and Secured
   Transactions</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>7th International Conference for Internet Technology and Secured
   Transactions (ICITST)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 10-12, 2012</td>
</tr>

<tr>
<td valign="top">CL </td><td>London, ENGLAND</td>
</tr>

<tr>
<td valign="top">DE </td><td>Making DAISY Talking Book; Mobile technology</td>
</tr>

<tr>
<td valign="top">AB </td><td>The DAISY Talking Book, like analog talking books, renders the audio in human or synthetic voice. Additionally, the DAISY Talking Book can contain image files, text files, and navigator capability that offering a wide range of features in order to provide services to a broader audience, including deaf and hearing impaired people. However, the cost associated with making the DAISY Talking Book is very high and there are also other problems in that it is time consuming to develop and requires a number of volunteers. The impact of these factors led the researchers to develop the DAISY talking book development application on mobile phone. The development application describes a mobile application that enables the interactive online preparation and presentation of Daisy talking books. It can interact with users as well as recognize and record voices for conversion of human voices to use with the DAISY Talking Book. The system features and user friendly interface comprise an environment for creating DAISY talking books easily. This is a novel application design for usability without requiring extensive computer skills and utilizes ubiquitous mobile platforms. These aspects combine to improve access to and convenient, inexpensive participation in Daisy talking book development.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Susakulchai, Surachai; Kaewprapan, Wacheerapan] King Mongkuts Univ
   Technol Thonburi, Fac Ind Educ &amp; Technol, Bangkok, Thailand.
   <br>[Chaisanit, Settachai] Sripatum Univ Chonburi Campus, Sch Informat
   Technol, Chon Buri, Thailand.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Susakulchai, S (reprint author), King Mongkuts Univ Technol Thonburi, Fac Ind Educ &amp; Technol, Bangkok, Thailand.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">BP </td><td>791</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000317120000130</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Passila, A
   <br>Uotila, T
   <br>Melkas, H</td>
</tr>

<tr>
<td valign="top">AF </td><td>Passila, Anne
   <br>Uotila, Tuomo
   <br>Melkas, Helina</td>
</tr>

<tr>
<td valign="top">BE </td><td>Schiuma, G
   <br>Spender, JC
   <br>Yigitcanlar, T</td>
</tr>

<tr>
<td valign="top">TI </td><td>Facilitating collaborative knowledge creation by using 'Research-Based
   Theatre' in organizational innovation: Experiences from a Finnish
   wood-processing company</td>
</tr>

<tr>
<td valign="top">SO </td><td>IFKAD - KCWS 2012: 7TH INTERNATIONAL FORUM ON KNOWLEDGE ASSET DYNAMICS,
   5TH KNOWLEDGE CITIES WORLD SUMMIT: KNOWLEDGE, INNOVATION AND
   SUSTAINABILITY: INTEGRATING MICRO &amp; MACRO PERSPECTIVES</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>7th International Forum on Knowledge Asset Dynamics (IFKAD) / 5th
   Knowledge Cities World Summit (KCWS)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUN 13-15, 2012</td>
</tr>

<tr>
<td valign="top">CL </td><td>Matera, ITALY</td>
</tr>

<tr>
<td valign="top">DE </td><td>Collaborative Knowledge Creation; Research-Based Theatre; Innovation</td>
</tr>

<tr>
<td valign="top">ID </td><td>MANAGEMENT; FINLAND; LAHTI</td>
</tr>

<tr>
<td valign="top">AB </td><td>Purpose - In this study, methods of artistic mediation are linked to innovation and organizational development via the concept of knowledge creation. We first discuss an extended SECI model of knowledge creation, and then link that model to methods of artistic mediation - i.e., Research-Based Theatre (RBT). This results in a framework for processing innovation in organizational settings in a new way. In light of a practical company case we discuss how knowledge may be created, focusing particularly on different modes of knowledge, and how that may be aided in practice with artistic mediation.
   <br>Design/methodology/approach - Scharmer's (2001) concept of self-transcending knowledge is intriguing in innovation research. It implies the ability to sense the presence of potential, to see what does not yet exist. The concept was later incorporated into an extended SECI/ba model (the 'rye-bread model') by adding two phases of knowledge conversion: imagination ba and futurizing ba. In our study, this model is turned into a concrete framework by including appropriate actions for the different bas - i.e., methods of artistic mediation. Our research orientation represents a specific artistic orientation of action research, Research-Based Theatre (RBT); a research strategy that includes theatre as a way to conduct and represent scholarly research methods through the process of theatricalizing data into an integrated script, and then rehearsing and performing this material. This theoretical framework thus created is reflected on using empirical data collected during an organizational development process in a large Finnish wood-processing company.
   <br>Originality/value - In recent years, there has been a growing interest among researchers and developers to find new kinds of methods and approaches to organizational development and innovation. Using different kinds of artistic methods in this new context has opened up interesting possibilities. However, there is still a great need for concrete examples of how artistic methods - in this case, Research-Based Theatre - are used in 'real life situations' to facilitate collaborative knowledge creation and common understanding. This study demonstrates one such concrete example, which is made even more interesting by the fact that the case company is operating in a very traditional industrial sector - mechanical wood-processing.
   <br>Practical implications - Through the process of research-based theatre, RBT, different types of methods of artistic mediation are utilized so that not only explicit and codified knowledge, but also embodied and not yet embodied tacit knowledge as well as self-transcending knowledge are taken into account. Participants open up the procedures related to their own work and organizational practices. At the case company, employees and managers were confused in the midst of major changes, but with the help of RBT, they started to interpret what is happening, what the organisational change means to each of them, how they could learn together in terms of raising their awareness about change and by doing so, they actually created common and shared knowing together. All this ought to be organized with ethics in mind; participation ought to cherish dialogue, and dialogue cannot be controlled by managers. A participative process includes different voices; it is polyphony.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Passila, Anne; Uotila, Tuomo; Melkas, Helina] Lappeenranta Univ
   Technol, Lahti Sch Innovat, FI-15140 Lahti, Finland.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">BP </td><td>737</td>
</tr>

<tr>
<td valign="top">EP </td><td>755</td>
</tr>

<tr>
<td valign="top">SC </td><td>Business &amp; Economics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000313556000039</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Song, P
   <br>Bao, YQ
   <br>Zhao, L</td>
</tr>

<tr>
<td valign="top">AF </td><td>Song, Peng
   <br>Bao, Yongqiang
   <br>Zhao, Li</td>
</tr>

<tr>
<td valign="top">BE </td><td>Liu, CL
   <br>Zhang, CS
   <br>Wang, L</td>
</tr>

<tr>
<td valign="top">TI </td><td>Voice Conversion Using Improved Spectral and F0 Transformation Methods</td>
</tr>

<tr>
<td valign="top">SO </td><td>PATTERN RECOGNITION</td>
</tr>

<tr>
<td valign="top">SE </td><td>Communications in Computer and Information Science</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Chinese Conference on Pattern Recognition</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 24-26, 2012</td>
</tr>

<tr>
<td valign="top">CL </td><td>Beijing, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>spectral transformation; clustering and regression; F0 prediction;
   Gaussian normalization; speaker identification</td>
</tr>

<tr>
<td valign="top">ID </td><td>REGRESSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, two main shortcomings of the traditional Gaussian mixture model (GMM) based spectral transformation method are addressed. One is the over-smoothing caused by the statistic averaging of the model, the other is the discontinuities because of the frame-wise conversion. Aiming at compensating for these two problems, a novel spectral transformation method based on clustering and regression is proposed to solve the over-smoothing. Meanwhile, a novel averaging strategy is adopted to reduce the discontinuities. In order to further improve the perceptual speech quality, being different from the traditional methods, a novel F0 transformation method combining the F0 prediction with the Gaussian normalization is presented. Objective and subjective experiments are carried out to demonstrate the efficiency of the proposed method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Song, Peng; Zhao, Li] Southeast Univ, Minist Educ, Key Lab Underwater
   Acoust Signal Proc, Nanjing 210096, Jiangsu, Peoples R China.
   <br>[Bao, Yongqiang] Nanjing Inst Technol, Sch Commun Engn, Nanjing 211167,
   Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Song, P (reprint author), Southeast Univ, Minist Educ, Key Lab Underwater Acoust Signal Proc, Nanjing 210096, Jiangsu, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>pengsongseu@gmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">VL </td><td>321</td>
</tr>

<tr>
<td valign="top">BP </td><td>589</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000312434700072</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wang, ZH
   <br>Huang, BH</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wang, Zhaohai
   <br>Huang, Binghua</td>
</tr>

<tr>
<td valign="top">BE </td><td>Sung, WP
   <br>Kao, JCM
   <br>Chen, R</td>
</tr>

<tr>
<td valign="top">TI </td><td>The Research and Development of an Automotive Fault Real-time Alarming
   System</td>
</tr>

<tr>
<td valign="top">SO </td><td>FRONTIERS OF MECHANICAL ENGINEERING AND MATERIALS ENGINEERING, PTS 1 AND
   2</td>
</tr>

<tr>
<td valign="top">SE </td><td>Applied Mechanics and Materials</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Frontiers of Mechanical Engineering and
   Materials Engineering (MEME 2012)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUL 27-29, 2012</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hong Kong, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>automotive fault diagnosis; real-time alarming; OBD-II; trouble code;
   data stream; communication protocol; expert system</td>
</tr>

<tr>
<td valign="top">AB </td><td>The authors have researched on the communication protocols and the diagnostic modes used by the second generation of On-Board-Diagnosis (OBD-II) system and put forward a project scheme of automotive fault real-time alarming system based on OBD-II system. The Total Control Unit (TCU) of the alarming system, the signal conversion module between TCU and OBD-II system, the preliminary expert system of automotive fault diagnosis, LCD display system and phonic broadcast system have been developed. Test results on sample machines prove that the alarming system can successfully acquires data stream &amp; trouble codes from automotive electronic control unit, so the expert system can comprehensively analyze potential fault causes and provide the driver with solutions. The fault causes and solutions can be displayed on LCD and converted into voice by text-to-speech module. The wide spread of the alarming system would effectively reduce the costs of vehicle operation and maintenance and greatly improve the vehicle road safety.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Wang, Zhaohai] Shenzhen Polytech, Sch Automot &amp; Transportat Engn,
   Shenzhen 518055, Guangdong, Peoples R China.
   <br>[Huang, Binghua] Shenzhen Polytech, Sch Training, Shenzhen 518055,
   Guangdong, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Wang, ZH (reprint author), Shenzhen Polytech, Sch Automot &amp; Transportat Engn, Shenzhen 518055, Guangdong, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>szptwzh@szpt.edu.cn; bhhuang@szpt.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">VL </td><td>184-185</td>
</tr>

<tr>
<td valign="top">BP </td><td>1578</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.4028/www.scientific.net/AMM.184-185.1578</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Materials Science; Mechanics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000310257500329</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Debby, NB</td>
</tr>

<tr>
<td valign="top">AF </td><td>Debby, Nirit Ben-Aryeh</td>
</tr>

<tr>
<td valign="top">TI </td><td>VISUAL RHETORIC: IMAGES OF SARACENS IN FLORENTINE CHURCHES</td>
</tr>

<tr>
<td valign="top">SO </td><td>ANUARIO DE ESTUDIOS MEDIEVALES</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Santa Maria Novella; Santa Croce; Saracens; crusade propaganda;
   Benedetto da Maiano; Andrea da Firenze; preaching; art</td>
</tr>

<tr>
<td valign="top">ID </td><td>SANTA-MARIA-NOVELLA; FRESCOES</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper focuses on the encounter between the Christian and the Islamic worlds as it appears in Florentine churches. It explores images of Muslims connected to the ideas of mission, conversion and crusade as they appear in the oral and visual traditions. Crusading sympathy in Tuscany, particularly in Florence, had a long history, going back to the twelfth century. The role of the mendicant orders, established in the great convents of Santa Croce and Santa Maria Novella, was crucial in winning sympathy for the crusades. This tradition continued in the fifteenth century, after the fall of Constantinople, when Florence openly voiced support for papal crusading efforts and participated in fund-raising for the crusade. The main supporters of crusade propaganda in Florence were the Franciscan and Dominican preachers, who acted as virtual papal envoys, continuing a tradition of mendicant crusade sermons. These movements also developed special types of artworks, either painting or sculptures in order to disseminate their religious ideals. The usage of rhetoric and preaching, the interrelations between word and image, the artistic and literary traditions, artworks and sermons will be a central focus of essay.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Ben Gurion Univ Negev, Dept Arts, IL-84105 Beer Sheva, Israel.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Debby, NB (reprint author), Ben Gurion Univ Negev, Dept Arts, POB 653, IL-84105 Beer Sheva, Israel.</td>
</tr>

<tr>
<td valign="top">EM </td><td>nbad@bgu.ac.il</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Debby Ben Aryeh NiritDebby, NIRIT</display_name>&nbsp;</font></td><td><font size="3">F-1482-2012&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Ben-Aryeh Debby, Nirit</display_name>&nbsp;</font></td><td><font size="3">0000-0003-1791-491X&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PD </td><td>JAN-JUN</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">VL </td><td>42</td>
</tr>

<tr>
<td valign="top">IS </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>7</td>
</tr>

<tr>
<td valign="top">EP </td><td>28</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.3989/aem.2012.42.1.01</td>
</tr>

<tr>
<td valign="top">SC </td><td>History; Arts &amp; Humanities - Other Topics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000307320000002</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Chen, YJ
   <br>Jiang, XM
   <br>Yang, GY
   <br>Cai, Y</td>
</tr>

<tr>
<td valign="top">AF </td><td>Chen, Yunjun
   <br>Jiang, Xiuming
   <br>Yang, Gongyuan
   <br>Cai, Yan</td>
</tr>

<tr>
<td valign="top">BE </td><td>Zhang, CS</td>
</tr>

<tr>
<td valign="top">TI </td><td>Design and Implementation of Real-Time Audio Transmission System</td>
</tr>

<tr>
<td valign="top">SO </td><td>MATERIALS SCIENCE AND INFORMATION TECHNOLOGY, PTS 1-8</td>
</tr>

<tr>
<td valign="top">SE </td><td>Advanced Materials Research</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Materials Science and Information Technology
   (MSIT 2011)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 16-18, 2011</td>
</tr>

<tr>
<td valign="top">CL </td><td>Singapore, SINGAPORE</td>
</tr>

<tr>
<td valign="top">DE </td><td>Producer/consumer design pattern; Multi-channel digital audio;
   Dual-Buffer technique; Multithread technology</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>Continuous Multi-channel digital audio signals system not only guarantees the continuity of signal acquisition, but has the real-time control ability in the process of signal acquisition. This paper proposes the producer/consumer design pattern which can make program designing quicker, simpler and more efficient. Through the example of continuous sound signal acquisition, the designing idea for the Producer/consumer design pattern is described in details and the design process of this program on the Delphi platform is given. The result shows that the introduction of the producer/consumer design pattern in the use of program design which has serious request in real-time and continuous sound signal acquisition and playing can make the processes response faster and more efficient.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Chen, Yunjun; Yang, Gongyuan; Cai, Yan] Tianjin Polytech Univ, Sch
   Elect Engn &amp; Automat, Tianjin, Peoples R China.
   <br>[Chen, Yunjun; Jiang, Xiuming] Tianjin Polytech Univ, Sch Mech &amp; Elect
   Engn, Tianjin, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Chen, YJ (reprint author), Tianjin Polytech Univ, Sch Elect Engn &amp; Automat, Tianjin, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>chenyj@tjpu.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2012</td>
</tr>

<tr>
<td valign="top">VL </td><td>433-440</td>
</tr>

<tr>
<td valign="top">BP </td><td>2887</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.4028/www.scientific.net/AMR.433-440.2887</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering; Materials Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000302092001002</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Schuster, JP
   <br>Mouchabac, S
   <br>Le Strat, Y
   <br>Limosin, F</td>
</tr>

<tr>
<td valign="top">AF </td><td>Schuster, J. -P.
   <br>Mouchabac, S.
   <br>Le Strat, Y.
   <br>Limosin, F.</td>
</tr>

<tr>
<td valign="top">TI </td><td>Hysterical mutism</td>
</tr>

<tr>
<td valign="top">SO </td><td>ENCEPHALE-REVUE DE PSYCHIATRIE CLINIQUE BIOLOGIQUE ET THERAPEUTIQUE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>Mutism; Conversion disorder; Hysteria</td>
</tr>

<tr>
<td valign="top">ID </td><td>FUNCTIONAL DYSPHONIA; PSYCHOGENIC APHONIA; DIFFERENTIAL-DIAGNOSIS; VOICE
   DISORDERS; HYPNOSIS; THERAPY; ANXIETY</td>
</tr>

<tr>
<td valign="top">AB </td><td>Background. - Conversion disorders comprise many clinical pictures, including hysterical mutism. Hysterical mutism has emerged as a clinical entity that remains difficult to diagnose, and whose treatment is poorly codified. Hysterical mutism is a disorder of the vocal function without changing the integrity of the body, resulting in loss of voice. Identified at all times, hysterical mutism entered the medical field in the late nineteenth century, under the direction of Jean-Martin Charcot (Salpetriere School). Since then, although the disorder has emerged as a clinical entity, it remains little known.
   <br>Method. - A systematic review of the literature. We performed electronic literatures search of relevant studies using Medline, SUDOC, and BIUM. Search terms used were mutism, functional aphonia, conversion disorder, hysteria.
   <br>Results. - The epidemiology of hysterical mutism is difficult to assess. The first limitation is the lack of consensensual diagnostic criteria. An estimate of its frequency may be advanced through registries consultation of otolaryngology-head and neck surgery. Through a literature review, emerges a rare disorder, about 5% of functional dysphonia. The sex-ratio is in favour of women. Regarding age of onset of disorder, functional aphonia mainly concerns adults with an average around the age of 30-40 years. The onset of the disorder typically involves a sudden onset and a recent stressful event. The duration of the disorder is difficult to specify. It appears that this dysfunction is rapidly reversible and that the majority of patients are in remission of this disorder within three months. The recurrence of dysfunction seems to be frequent. The existence of psychiatric comorbidity did not appear to be the rule. The natural history of this disorder is not known making it tricky to evaluate the efficiency of therapeutic approaches.
   <br>Conclusion. - Today the term hysterical mutism does not appear as an entity in either international classification. It belongs to the category of conversion disorder in the Diagnostic and Statistical Manual of Mental Disorders (DSM-IV-TR). Identified as a medical entity described by the school of the Salpetriere, this disorder has raised little interest. The medicalization of the condition remains difficult because of the importance of stigma associated with it, which contributes to the rejection rather than support of patients with mutism. To better understand this disorder and improve the care of patients who suffer, renewed interest is warranted. (C) L'Encephale, Paris, 2010.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Schuster, J. -P.; Limosin, F.] Univ Paris 05, Hop Corentin Celton, AP
   HP, Serv Univ Psychiat, F-92130 Issy Les Moulineaux, France.
   <br>[Mouchabac, S.] CHU St Antoine, Dept Psychiat &amp; Psychol Med, F-75012
   Paris, France.
   <br>[Le Strat, Y.; Limosin, F.] Hop St Anne, INSERM, Ctr Psychiat &amp;
   Neurosci, U894, F-75014 Paris, France.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Schuster, JP (reprint author), Univ Paris 05, Hop Corentin Celton, AP HP, Serv Univ Psychiat, 4 Parvis Corentin Celton, F-92130 Issy Les Moulineaux, France.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jean-pierre.schuster@ccl.aphp.fr</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>OCT</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">VL </td><td>37</td>
</tr>

<tr>
<td valign="top">IS </td><td>5</td>
</tr>

<tr>
<td valign="top">BP </td><td>339</td>
</tr>

<tr>
<td valign="top">EP </td><td>344</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.encep.2010.12.006</td>
</tr>

<tr>
<td valign="top">SC </td><td>Neurosciences &amp; Neurology; Psychiatry</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000297395200001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>He, P
   <br>Sun, HQ
   <br>Shang, W
   <br>Li, P</td>
</tr>

<tr>
<td valign="top">AF </td><td>He, Ping
   <br>Sun, Huiqi
   <br>Shang, Wei
   <br>Li, Pan</td>
</tr>

<tr>
<td valign="top">BE </td><td>Zeng, D</td>
</tr>

<tr>
<td valign="top">TI </td><td>The Principle and Algorithm of Earthquake Alarm System Designed for
   Families</td>
</tr>

<tr>
<td valign="top">SO </td><td>APPLIED INFORMATICS AND COMMUNICATION, PT 2</td>
</tr>

<tr>
<td valign="top">SE </td><td>Communications in Computer and Information Science</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Applied Informatics and Communication (ICAIC
   2011)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 20-21, 2011</td>
</tr>

<tr>
<td valign="top">CL </td><td>Xian, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>51SCM; Seismic alarm; digital-to-analog conversion chip; Acceleration
   measurement</td>
</tr>

<tr>
<td valign="top">AB </td><td>Combined with a 51 SCM, AT89LV55, a three-axis accelerometer, ADXL335, and a digital-to-analog conversion chip, AD7708, a seismic alarm system has been designed, which can be placed and used in residents' homes or some public places. ADXL335 is used to measure the acceleration of three axis, X axis, Y axis and Z axis. The design, a combination of performance chip AT89LV55 and other interface functions, functioned with the relationship between seismic magnitude and acceleration, so that we can resort to the software enquiry to figure out the seismic magnitude. In addition, large variation on Z axis would trigger the alarm that earthquake is coming. The procedure has been simulated and debugged on the actual Printed Circuit Board and a summary was made. In fact, the system delivers the seismic message promptly with high accuracy and provides some additional functions such as voice prompt and calendar.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[He, Ping; Sun, Huiqi; Shang, Wei; Li, Pan] Harbin Inst Technol, Dept
   Control Sci &amp; Engn, Harbin 150006, Heilongjiang Pr, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>He, P (reprint author), Harbin Inst Technol, Dept Control Sci &amp; Engn, Harbin 150006, Heilongjiang Pr, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ping_he116@163.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">VL </td><td>225</td>
</tr>

<tr>
<td valign="top">BP </td><td>361</td>
</tr>

<tr>
<td valign="top">EP </td><td>368</td>
</tr>

<tr>
<td valign="top">PN </td><td>II</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000307481100046</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Bartosek, J
   <br>Hanzl, V</td>
</tr>

<tr>
<td valign="top">AF </td><td>Bartosek, Jan
   <br>Hanzl, Vaclav</td>
</tr>

<tr>
<td valign="top">BE </td><td>Pinker, J</td>
</tr>

<tr>
<td valign="top">TI </td><td>Exploring Abilities of Merged Normalized Forward-Backward Correlation
   for Speech Pitch Analysis</td>
</tr>

<tr>
<td valign="top">SO </td><td>2011 INTERNATIONAL CONFERENCE ON APPLIED ELECTRONICS (AE)</td>
</tr>

<tr>
<td valign="top">SE </td><td>Applied Electronics</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Applied Electronics</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 07-08, 2011</td>
</tr>

<tr>
<td valign="top">CL </td><td>Pilsen, CZECH REPUBLIC</td>
</tr>

<tr>
<td valign="top">AB </td><td>The article deals with usage of time-domain merged normalized forward-backward correlation (MNFBC) for pitch estimation of speech signals. This method should prevent from shortcomings of other methods commonly used in pitch detection algorithms (PDA). The text also presents comparison of possible improvements for voicing decision stage of MNFBC and also puts mind to final fundamental frequency (F0) smoothing with Viterbi algorithm. The precision and voiced-unvoiced (VUV) decision was compared against pitch reference database (part of Spanish Speecon). Results show that F0 estimate precision of MNFBC in connection with Viterbi smoothing using cents conversion in transition probability function is comparable to PRAAT cross-correlation. Although with additional signal energy thresholding unvoiced errors for close-talk channel 0 are lowered, the results are still better in PRAAT algorithm, but the difference gets even for channel 1 (lavaliere microphone). Noise robustness of the algorithm could be improved by pre-ordering a noise reduction block.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Bartosek, Jan; Hanzl, Vaclav] FEE CTU Prague, Dept Circuit Theory,
   Prague, Czech Republic.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Bartosek, J (reprint author), FEE CTU Prague, Dept Circuit Theory, Prague, Czech Republic.</td>
</tr>

<tr>
<td valign="top">EM </td><td>bartoj11@fel.cvut.cz; hanzl@fel.cvut.cz</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000305136600007</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Pribil, J
   <br>Pribilova, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Pribil, J.
   <br>Pribilova, A.</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>COMPARISON OF SPECTRAL AND PROSODIC PARAMETERS OF MALE AND FEMALE
   EMOTIONAL SPEECH IN CZECH AND SLOVAK</td>
</tr>

<tr>
<td valign="top">SO </td><td>2011 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING</td>
</tr>

<tr>
<td valign="top">SE </td><td>International Conference on Acoustics Speech and Signal Processing
   ICASSP</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing (ICASSP)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 22-27, 2011</td>
</tr>

<tr>
<td valign="top">CL </td><td>Prague Congress Ctr, Prague, CZECH REPUBLIC</td>
</tr>

<tr>
<td valign="top">HO </td><td>Prague Congress Ctr</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speech processing; speech analysis; statistics</td>
</tr>

<tr>
<td valign="top">ID </td><td>CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper analyzes and compares spectral properties (first three formants position and spectral flatness measure values) and prosodic parameters (F0 and energy, microintonation and jitter) of male and female acted emotional speech in Czech and Slovak languages. Statistical results and values of parameter ratios will be used for modification of the text-to-speech (TTS) system enabling expressive speech production with male / female voices, based on cepstral speech description.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Pribil, J.] Acad Sci CR, Vvi, Inst Photon &amp; Elect, Chaberska 57,
   CZ-18251 Prague 8, Czech Republic.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Pribil, J (reprint author), Acad Sci CR, Vvi, Inst Photon &amp; Elect, Chaberska 57, CZ-18251 Prague 8, Czech Republic.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">BP </td><td>4720</td>
</tr>

<tr>
<td valign="top">EP </td><td>4723</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering; Imaging Science &amp; Photographic Technology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000296062405082</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hirose, K
   <br>Ochi, K
   <br>Mihara, R
   <br>Hashimoto, H
   <br>Saito, D
   <br>Minematsu, N</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hirose, Keikichi
   <br>Ochi, Keiko
   <br>Mihara, Ryusuke
   <br>Hashimoto, Hiroya
   <br>Saito, Daisuke
   <br>Minematsu, Nobuaki</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Adaptation of Prosody in Speech Synthesis by Changing Command Values of
   the Generation Process Model of Fundamental Frequency</td>
</tr>

<tr>
<td valign="top">SO </td><td>12TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION 2011 (INTERSPEECH 2011), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>12th Annual Conference of the
   International-Speech-Communication-Association 2011 (INTERSPEECH 2011)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 27-31, 2011</td>
</tr>

<tr>
<td valign="top">CL </td><td>Florence, ITALY</td>
</tr>

<tr>
<td valign="top">DE </td><td>prosody adaptation; generation process model; speech synthesis</td>
</tr>

<tr>
<td valign="top">ID </td><td>PARAMETERS; CONTOURS</td>
</tr>

<tr>
<td valign="top">AB </td><td>A method was developed to adapt prosody to a new speaker/style in speech synthesis. It is based on predicting differences between target and original speakers/styles and applying them to the original one. Differences in fundamental frequency (F-0) contours are represented in the framework of the generation process model; differences in the command magnitudes/amplitudes. While the original one requires a certain amount of training corpus, while corpus for training command differences can be small. Furthermore, in the case of style adaptation, it is not necessarily the corpus being uttered by the same speaker of the original style. Speech synthesis was conducted using HMM-based speech synthesis system, where prosody was controlled by the method. Listening experiments on synthetic speech with style adaptation and voice conversion both showed the validity of the method.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Hirose, Keikichi; Ochi, Keiko; Mihara, Ryusuke; Hashimoto, Hiroya;
   Saito, Daisuke; Minematsu, Nobuaki] Univ Tokyo, Dept Informat &amp; Commun
   Engn, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hirose, K (reprint author), Univ Tokyo, Dept Informat &amp; Commun Engn, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>hirose@gavo.t.u-tokyo.ac.jp; ochi@gavo.t.u-tokyo.ac.jp;
   mihara@gavo.t.u-tokyo.ac.jp; hiroya@gavo.t.u-tokyo.ac.jp;
   dsk_saito@gavo.t.u-tokyo.ac.jp; mine@gavo.t.u-tokyo.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">BP </td><td>2804</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000316502201190</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kunikoshi, A
   <br>Qiao, Y
   <br>Saito, D
   <br>Minematsu, N
   <br>Hirose, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kunikoshi, Aki
   <br>Qiao, Yu
   <br>Saito, Daisuke
   <br>Minematsu, Nobuaki
   <br>Hirose, Keikichi</td>
</tr>

<tr>
<td valign="top">GP </td><td>Int Speech Commun Assoc</td>
</tr>

<tr>
<td valign="top">TI </td><td>Gesture Design of Hand-to-Speech Converter Derived from Speech-to-Hand
   Converter Based on Probabilistic Integration Model</td>
</tr>

<tr>
<td valign="top">SO </td><td>12TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION 2011 (INTERSPEECH 2011), VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>12th Annual Conference of the
   International-Speech-Communication-Association 2011 (INTERSPEECH 2011)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 27-31, 2011</td>
</tr>

<tr>
<td valign="top">CL </td><td>Florence, ITALY</td>
</tr>

<tr>
<td valign="top">DE </td><td>Dysarthria; speech production; hand motions; media conversion;
   arrangement of gestures and vowels</td>
</tr>

<tr>
<td valign="top">AB </td><td>When dysarthrics, individuals with speaking disabilities, try to communicate using speech, they often have no choice but to use speech synthesizers which require them to type word symbols or sound symbols. Input by this method often makes real-time communication troublesome and dysarthric users struggle to have smooth flowing conversations. In this study, we are developing a novel speech synthesizer where speech is generated through hand motions rather than symbol input. In recent years, statistical voice conversion techniques have been proposed based on space mapping between given parallel utterances. By applying these methods, a hand space was mapped to a vowel space and a converter from hand motions to vowel transitions was developed. It reported that the proposed method is effective enough to generate the five Japanese vowels. In this paper, we discuss the expansion of this system to consonant generation. In order to create the gestures for consonants, a Speech-to-Hand conversion system is firstly developed using parallel data for vowels, in which consonants are not included. Then, we are able to automatically search for candidates for consonant gestures for a Hand-to-Speech system.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kunikoshi, Aki; Saito, Daisuke; Minematsu, Nobuaki; Hirose, Keikichi]
   Univ Tokyo, Tokyo 1138654, Japan.
   <br>[Qiao, Yu] Shenzhen Inst Adv Technol, Shenzhen, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kunikoshi, A (reprint author), Univ Tokyo, Tokyo 1138654, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>kunikoshi@gavo.t.u-tokyo.ac.jp; yu.qiao@sub.siat.ac.cn;
   dsk_saito@gavo.t.u-tokyo.ac.jp; mine@gavo.t.u-tokyo.ac.jp;
   hirose@gavo.t.u-tokyo.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">BP </td><td>3032</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000316502201247</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Chan, PY
   <br>Dong, MH
   <br>Lee, SW
   <br>Cen, L</td>
</tr>

<tr>
<td valign="top">AF </td><td>Chan, Paul Y.
   <br>Dong, Minghui
   <br>Lee, S. W.
   <br>Cen, Ling</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>SOLO TO A CAPELLA CONVERSION - SYNTHESIZING VOCAL HARMONY FROM LEAD
   VOCALS</td>
</tr>

<tr>
<td valign="top">SO </td><td>2011 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME)</td>
</tr>

<tr>
<td valign="top">SE </td><td>IEEE International Conference on Multimedia and Expo</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Multimedia and Expo (ICME)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUL 11-15, 2011</td>
</tr>

<tr>
<td valign="top">CL </td><td>Univ Ramon Llull, La Salle, Barcelona, SPAIN</td>
</tr>

<tr>
<td valign="top">HO </td><td>Univ Ramon Llull, La Salle</td>
</tr>

<tr>
<td valign="top">DE </td><td>Singing synthesis; vocal harmony; accompaniment; pitch interpretation;
   pitch alignment; A Capella</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents our work in the automatic synthesis of vocal harmony. Existing innovations either allow for dissonances (i.e. non-harmonious or clashing intervals) at various locations or require some musical ability of the user. We have developed a method that is able to automatically synthesize vocal harmony even for ordinary singers with a poor sense of harmony and rhythm. We have evaluated our method by means of spectrogram comparison as well as subjective listening tests. A spectrogram comparison of our method and two popular existing methods against that of the human voice shows that our method is least dissonant and most similar to natural human vocals. Subjective listening tests conducted separately for experts and non-experts in the field confirm that the vocal harmony synthesized using our method sounds the best in terms of consonance, inter-syllable transition, as well as naturalness and appeal.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ychan@i2r.a-star.edu.sg; mhdong@i2r.a-star.edu.sg;
   swylee@i2r.a-star.edu.sg; lcen@i2r.a-star.edu.sg</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000304354700034</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Yang, DQ
   <br>Yan, WP</td>
</tr>

<tr>
<td valign="top">AF </td><td>Yang, Deqin
   <br>Yan, Weiping</td>
</tr>

<tr>
<td valign="top">BE </td><td>Zhou, XJ</td>
</tr>

<tr>
<td valign="top">TI </td><td>Flexible Ergonomic Principle of Construction System</td>
</tr>

<tr>
<td valign="top">SO </td><td>ADVANCES IN STRUCTURAL ENGINEERING, PTS 1-3</td>
</tr>

<tr>
<td valign="top">SE </td><td>Applied Mechanics and Materials</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Civil Engineering and Transportation (ICCET
   2011)</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 14-16, 2011</td>
</tr>

<tr>
<td valign="top">CL </td><td>Jinan, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Construction System; Ergonomics; System Flexibility; Factors Mutual
   Fitness; System Performance</td>
</tr>

<tr>
<td valign="top">AB </td><td>Idea of element co-adaptation of construction ergonomics claims the elements of construction ergonomic system mutual fitness. Flexibility of construction system embodies the element co-adaptation. Flexible ergonomic principle indicates that construction system design and update must make the system to have and keep the function of fit in with and conversion of noise to meet the needs of flexibility of construction system. Principle of factors fit in with system expresses the relations between construction ergonomic system and its factors. The principle claims any one of the factors should co-ordinate the system, and that is the precondition of system flexibility. Principle of factors mutual fitness expresses the relations between the factor and factor of system. It voices that any one of the factors (and fine factors) should adapt to other ones and keep on the adaptation. The paper names the last two principles combined as "fit flexible principle" and proofs it by using system evolutionary equation through defining the conception of adaptable coefficient of factors. Findings of the research have corrected the ergonomic system design idea that based on cybernetics as well as the optimizing idea of construction resource that based on static thinking.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Yang, Deqin] Zhengzhou Inst Aeronaut Ind Management, Sch Civil Engn,
   Zhengzhou 450015, Henan, Peoples R China.
   <br>[Yan, Weiping] Henan Zhongyuan Construct Supervis Ctr, Zhengzhou 450003,
   Henan, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Yang, DQ (reprint author), Zhengzhou Inst Aeronaut Ind Management, Sch Civil Engn, Zhengzhou 450015, Henan, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>yd123@zzia.edu.cn; jzgcglx@126.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">VL </td><td>94-96</td>
</tr>

<tr>
<td valign="top">BP </td><td>1235</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.4028/www.scientific.net/AMM.94-96.1235</td>
</tr>

<tr>
<td valign="top">PN </td><td>1-3</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000307366600232</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Donnelly, K
   <br>Deuchar, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Donnelly, Kevin
   <br>Deuchar, Margaret</td>
</tr>

<tr>
<td valign="top">BE </td><td>Cunningham, S
   <br>Grout, V
   <br>Houlden, N
   <br>Oram, D
   <br>Picking, R</td>
</tr>

<tr>
<td valign="top">TI </td><td>THE ANGOR AUTOGLOSSER: A MULTILINGUAL TAGGER FOR CONVERSATIONAL TEXT</td>
</tr>

<tr>
<td valign="top">SO </td><td>PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON INTERNET
   TECHNOLOGIES AND APPLICATIONS (ITA 11)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>4th International Conference on Internet Technologies and Applications
   (ITA 11)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 06-09, 2011</td>
</tr>

<tr>
<td valign="top">CL </td><td>Glyndwr Univ, Ctr Appl Internet Res, Wrexham, UNITED KINGDOM</td>
</tr>

<tr>
<td valign="top">HO </td><td>Glyndwr Univ, Ctr Appl Internet Res</td>
</tr>

<tr>
<td valign="top">DE </td><td>bilingual; speech; tagging; corpora</td>
</tr>

<tr>
<td valign="top">AB </td><td>We describe software which tags multilingual transcriptions of spoken texts in Welsh, Spanish and English to a high degree of accuracy. This is believed to be the first application to handle the tagging of Welsh text. The tagger is easily extensible to other languages, and may be of interest to researchers in natural language processing in minority languages, as well as to those working on the informal language used in speech-to-text conversion, voice recognition software, and so on.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Donnelly, Kevin; Deuchar, Margaret] ESRC Ctr Res Bilingualism, Bangor,
   Gwynedd, Wales.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Donnelly, K (reprint author), ESRC Ctr Res Bilingualism, Bangor, Gwynedd, Wales.</td>
</tr>

<tr>
<td valign="top">EM </td><td>k.donnelly@bangor.ac.uk; m.deuchar@bangor.ac.uk</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2011</td>
</tr>

<tr>
<td valign="top">BP </td><td>317</td>
</tr>

<tr>
<td valign="top">EP </td><td>324</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000391342300039</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Hayashi, T
   <br>Nankaku, Y
   <br>Lee, A
   <br>Tokuda, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Hayashi, Toyohiro
   <br>Nankaku, Yoshihiko
   <br>Lee, Akinobu
   <br>Tokuda, Keiichi</td>
</tr>

<tr>
<td valign="top">GP </td><td>INST SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Speaker Adaptation Based on Nonlinear Spectral Transform for Speech
   Recognition</td>
</tr>

<tr>
<td valign="top">SO </td><td>11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>11th Annual Conference of the
   International-Speech-Communication-Association 2010</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 26-30, 2010</td>
</tr>

<tr>
<td valign="top">CL </td><td>Makuhari, JAPAN</td>
</tr>

<tr>
<td valign="top">DE </td><td>Speech Recognition; Speaker Adaptation; Nonlinear Spectral
   Transformation</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper proposes a speaker adaptation technique using a nonlinear spectral transform based on GMMs. One of the most popular forms of speaker adaptation is based on linear transforms, e.g., MLLR. Although MLLR uses multiple transforms according to regression classes, only a single linear transform is applied to each state. The proposed method performs nonlinear speaker adaptation based on a new likelihood function combining HMMs for recognition with GMMs for spectral transform. Moreover, the dependency of transforms on context can also be estimated in an integrated ML fashion. The proposed technique outperformed conventional approaches in phoneme-recognition experiments.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Hayashi, Toyohiro; Nankaku, Yoshihiko; Lee, Akinobu; Tokuda, Keiichi]
   Nagoya Inst Technol, Dept Comp Sci &amp; Engn, Nagoya, Aichi, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hayashi, T (reprint author), Nagoya Inst Technol, Dept Comp Sci &amp; Engn, Nagoya, Aichi, Japan.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">BP </td><td>542</td>
</tr>

<tr>
<td valign="top">EP </td><td>545</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000294382400132</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Teutenberg, J
   <br>Watson, CI</td>
</tr>

<tr>
<td valign="top">AF </td><td>Teutenberg, Jonathan
   <br>Watson, Catherine I.</td>
</tr>

<tr>
<td valign="top">GP </td><td>INST SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>The Effect of Audience Familiarity on the Perception of Modified Accent</td>
</tr>

<tr>
<td valign="top">SO </td><td>11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 3 AND 4</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>11th Annual Conference of the
   International-Speech-Communication-Association 2010</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 26-30, 2010</td>
</tr>

<tr>
<td valign="top">CL </td><td>Makuhari, JAPAN</td>
</tr>

<tr>
<td valign="top">DE </td><td>accent perception; voice conversion; localisation; evaluation</td>
</tr>

<tr>
<td valign="top">ID </td><td>F0</td>
</tr>

<tr>
<td valign="top">AB </td><td>Evaluating the efficacy of accent transformation is important when localising speech-enabled software. However, perceived accent is an attribute assigned by a listener, and the apparent success of accent transformation will vary with the audience. Here we show the extent to which evaluations can be affected by audience familiarity with an accent. A perceptual study comparing two approaches to accent transformation is presented to two audiences with differing familiarity with the target accents. For mean opinion score style evaluations, we quantify the approximate change in perception, and show that this can be sufficient to alter relative successfulness of such systems.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Teutenberg, Jonathan] Univ Teesside, Sch Comp, Middlesbrough,
   Cleveland, England.
   <br>[Watson, Catherine I.] Univ Auckland, Dept Elect &amp; Comp Engn, Auckland,
   New Zealand.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Teutenberg, J (reprint author), Univ Teesside, Sch Comp, Middlesbrough, Cleveland, England.</td>
</tr>

<tr>
<td valign="top">EM </td><td>j.teutenberg@tees.ac.uk; c.watson@auckland.ac.nz</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">BP </td><td>1970</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000313086500105</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Tamura, M
   <br>Kagoshima, T
   <br>Akamine, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Tamura, Masatsune
   <br>Kagoshima, Takehiko
   <br>Akamine, Masami</td>
</tr>

<tr>
<td valign="top">GP </td><td>INST SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Sub-band Basis Spectrum Model for Pitch-synchronous Log-spectrum and
   Phase Based on Approximation of Sparse Coding</td>
</tr>

<tr>
<td valign="top">SO </td><td>11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 3 AND 4</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>11th Annual Conference of the
   International-Speech-Communication-Association 2010</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 26-30, 2010</td>
</tr>

<tr>
<td valign="top">CL </td><td>Makuhari, JAPAN</td>
</tr>

<tr>
<td valign="top">DE </td><td>spectrum parameter; sparse coding; sub-band basis spectrum model; speech
   synthesis; voice adaptation</td>
</tr>

<tr>
<td valign="top">ID </td><td>SPEECH</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper, we propose a sub-band basis spectrum model which is a new spectrum representation model based on a linear combination of sub-band basis vectors. We apply sparse coding to the pitch-synchronously analyzed log-spectra. Based on the approximation of the resulting basis, we obtain sub-band basis vectors with 1-cycle sinusoidal shapes that have mel-scale for lower frequencies and equally spaced scale for higher frequencies. Parameters of the sub-band basis spectrum model representing the log spectrum and the phase spectrum are calculated by fitting the basis to the spectrum. Since the parameters represent the shape of a spectrum, it can be easily used for voice adaptation, interpolation and conversion. Experimental results show that the analysis synthesis speech based on the proposed model is close to original speech and that there is no significant difference between the synthetic speech using analysis-synthesis database and those using original database for unit-fusion based TTS[1].</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Tamura, Masatsune; Kagoshima, Takehiko; Akamine, Masami] Toshiba Co
   Ltd, Corp Res &amp; Dev Ctr, Knowledge Media Lab, Otawara, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Tamura, M (reprint author), Toshiba Co Ltd, Corp Res &amp; Dev Ctr, Knowledge Media Lab, Otawara, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>masatsune.tamura@toshiba.co.jp; takehiko.kagoshima@toshiba.co.jp;
   masa.akamine@toshiba.co.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">BP </td><td>2406</td>
</tr>

<tr>
<td valign="top">EP </td><td>2409</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000313086500214</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Vychodil, J
   <br>Tomala, K
   <br>Voznak, M
   <br>Rozhon, J
   <br>Rezac, F</td>
</tr>

<tr>
<td valign="top">AF </td><td>Vychodil, Jiri
   <br>Tomala, Karel
   <br>Voznak, Miroslav
   <br>Rozhon, Jan
   <br>Rezac, Filip</td>
</tr>

<tr>
<td valign="top">BE </td><td>Voznak, M
   <br>Skapa, J</td>
</tr>

<tr>
<td valign="top">TI </td><td>Approach to converting WAV container into PCAP</td>
</tr>

<tr>
<td valign="top">SO </td><td>12TH INTERNATIONAL CONFERENCE ON RESEARCH IN TELECOMMUNICATION
   TECHNOLOGIES (RTT 2010)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>12th International Conference on Research in Telecommunication
   Technologies (RTT)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 08-10, 2010</td>
</tr>

<tr>
<td valign="top">CL </td><td>Velke Losiny, CZECH REPUBLIC</td>
</tr>

<tr>
<td valign="top">DE </td><td>audio converting; WAV; PCAP; TTS</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper deals with the conversion of a WAV container (Waveform Audio File Format) into a PCAP format, using different codecs. The PCAP format is used by packet analysers as well as by SIPp. The converted files are fed into a system for communicating danger alerts created as a part of research at the Department of Telecommunications of the VSB Technical University of Ostrava. The aim of the system is to distribute pre-recorded voice messages in order to alert the called party to danger.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Vychodil, Jiri; Tomala, Karel; Voznak, Miroslav; Rozhon, Jan; Rezac,
   Filip] VSB Tech Univ Ostrava, Dept Telecommun, Ostrava, Czech Republic.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Vychodil, J (reprint author), VSB Tech Univ Ostrava, Dept Telecommun, Ostrava, Czech Republic.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jiri.vychodil@vsb.cz; karel.tomala@vsb.cz; miroslav.voznak@vsb.cz;
   jan.rozhon@vsb.cz; filip.rezac@vsb.cz</td>
</tr>

<tr xmlns:date="http://exslt.org/dates-and-times">
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Voznak, Miroslav</display_name>&nbsp;</font></td><td><font size="3">E-6448-2016&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Voznak, Miroslav</display_name>&nbsp;</font></td><td><font size="3">0000-0001-5135-7980&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">BP </td><td>165</td>
</tr>

<tr>
<td valign="top">EP </td><td>167</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000395658200034</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>He, P
   <br>Sun, HQ
   <br>Shang, W
   <br>Li, P</td>
</tr>

<tr>
<td valign="top">AF </td><td>He, Ping
   <br>Sun, Huiqi
   <br>Shang, Wei
   <br>Li, Pan</td>
</tr>

<tr>
<td valign="top">BE </td><td>Zhang, Y
   <br>Tan, H</td>
</tr>

<tr>
<td valign="top">TI </td><td>The Principle and Algorithm of Earthquake Alarm System Designed for
   Families</td>
</tr>

<tr>
<td valign="top">SO </td><td>2010 THE 3RD INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND
   INDUSTRIAL APPLICATION (PACIIA2010), VOL II</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>3rd International Conference on Computational Intelligence and
   Industrial Application (PACIIA2010)</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 04-05, 2010</td>
</tr>

<tr>
<td valign="top">CL </td><td>Wuhan, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>51SCM; Seismic alarm; digital-to-analog conversion chip; Acceleration
   measurement</td>
</tr>

<tr>
<td valign="top">AB </td><td>Combined with a 51 SCM, AT89LV55, a three axis accelerometer, ADXL335, and a digital-to-analog conversion chip, AD7708, a seismic alarm system has been designed, which can be placed and used in residents' homes or some public places. ADXL335 is used to measure the acceleration of three axis, X axis, Y axis and Z axis. The design, a combination of performance chip AT89LV55 and other interface functions, functioned with the relationship between seismic magnitude and acceleration, so that we can resort to the software enquiry to figure out the seismic magnitude. In addition, large variation on Z axis would trigger the alarm that earthquake is coming. The procedure has been simulated and debugged on the actual Printed Circuit Board and a summary was made. In fact, the system delivers the seismic message promptly with high accuracy and provides some additional functions such as voice prompt and calendar.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[He, Ping; Sun, Huiqi; Shang, Wei; Li, Pan] Harbin Inst Technol, Dept
   Control Sci &amp; Engn, Harbin, Heilongjiang Pr, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>He, P (reprint author), Harbin Inst Technol, Dept Control Sci &amp; Engn, Harbin, Heilongjiang Pr, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>sunhuiqihit@126.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2010</td>
</tr>

<tr>
<td valign="top">BP </td><td>220</td>
</tr>

<tr>
<td valign="top">EP </td><td>224</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000398765400056</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kunikoshi, A
   <br>Qiao, Y
   <br>Minematsu, N
   <br>Hirose, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kunikoshi, Aki
   <br>Qiao, Yu
   <br>Minematsu, Nobuaki
   <br>Hirose, Keikichi</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INST SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Speech Generation from Hand Gestures Based on Space Mapping</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2009: 10TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH
   COMMUNICATION ASSOCIATION 2009, VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>10th INTERSPEECH 2009 Conference</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 06-10, 2009</td>
</tr>

<tr>
<td valign="top">CL </td><td>Brighton, ENGLAND</td>
</tr>

<tr>
<td valign="top">DE </td><td>Dysarthria; speech production; hand motions; media conversion;
   arrangement of gestures and vowels</td>
</tr>

<tr>
<td valign="top">AB </td><td>Individuals with speaking disabilities, particularly people suffering from dysarthria, often use a TTS synthesizer for speech communication. Since users always have to type sound symbols and the synthesizer reads them out in a monotonous style, the use of the current synthesizers usually renders real-time operation and lively communication difficult. This is why dysarthric users often fail to control the flow of conversation. In this paper, we propose a novel speech generation framework which makes use of hand gestures as input. People usually use tongue gesture transitions for speech generation but we develop a special glove, by wearing which, speech sounds are generated from hand gesture transitions. For development, GMM-based voice conversion techniques (mapping techniques) are applied to estimate a mapping function between a space of hand gestures and another space of speech sounds. In this paper, as an initial trial, a mapping between hand gestures and Japanese vowel sounds is estimated so that topological features of the selected gestures in a feature space and those of the five Japanese vowels in a cepstrum space are equalized. Experiments show that the special glove can generate good Japanese vowel transitions with voluntary control of duration and articulation.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kunikoshi, Aki; Qiao, Yu; Minematsu, Nobuaki; Hirose, Keikichi] Univ
   Tokyo, Bunkyo Ku, Tokyo 1138656, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kunikoshi, A (reprint author), Univ Tokyo, Bunkyo Ku, 7-3-1 Hongo, Tokyo 1138656, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>kunikoshi@gavo.t.u-tokyo.ac.jp; qiao@gavo.t.u-tokyo.ac.jp;
   mine@gavo.t.u-tokyo.ac.jp; hirose@gavo.t.u-tokyo.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">BP </td><td>260</td>
</tr>

<tr>
<td valign="top">EP </td><td>263</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000276842800063</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Morinaka, R
   <br>Tamura, M
   <br>Morita, M
   <br>Kagoshima, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Morinaka, Ryo
   <br>Tamura, Masatsune
   <br>Morita, Masahiro
   <br>Kagoshima, Takehiko</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA-INST SPEECH COMMUN ASSOC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Speech synthesis based on the plural unit selection and fusion method
   using FWF model</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2009: 10TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH
   COMMUNICATION ASSOCIATION 2009, VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>10th INTERSPEECH 2009 Conference</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 06-10, 2009</td>
</tr>

<tr>
<td valign="top">CL </td><td>Brighton, ENGLAND</td>
</tr>

<tr>
<td valign="top">DE </td><td>plural unit selection and fusion method; periodic/aperiodic component;
   FWF model; formant parameter</td>
</tr>

<tr>
<td valign="top">AB </td><td>For speech synthesizers, enhanced diversity and improved quality of synthesized speech are required. Speaker interpolation and voice conversion are the techniques that enhance diversity. The PUSF (plural unit selection and fusion) method, which we have proposed, generates synthesized waveforms using pitch-cycle waveforms. However, it is difficult to modify its spectral features while keeping naturalness of synthesized speech. In the present work, we investigated how best to represent speech waveforms. Firstly, we introduce a method that decomposes a pitch waveform in a voiced portion into a periodic component, which is excited by vocal sound source, and an aperiodic component, which is excited by noise source. Moreover, we introduce the FWF (formant waveform) model to represent the periodic component. Because the FWF model represents the pitch waveform in accordance with formant parameters, it can control the formant parameters independently. We realized a method that can easily be applied to the diversity-enhancing techniques in the PUSF-based method because this model is based on vocal tract features.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ryo.morinaka@toshiba.co.jp; masatsune.tamura@toshiba.co.jp;
   masahiro.morita@toshiba.co.jp; takehiko.kagoshima@toshiba.co.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">BP </td><td>2019</td>
</tr>

<tr>
<td valign="top">EP </td><td>2022</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000276842801046</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Lee, SN
   <br>Hwang, IS</td>
</tr>

<tr>
<td valign="top">AF </td><td>Lee, San-Nan
   <br>Hwang, I-Shyan</td>
</tr>

<tr>
<td valign="top">TI </td><td>STOCHASTIC GENERATIVE MODEL OF COST EFFECTIVE OADM USING THE SOM NEURAL
   NETWORK IN A WDM ACCESS NETWORK</td>
</tr>

<tr>
<td valign="top">SO </td><td>JOURNAL OF THE CHINESE INSTITUTE OF ENGINEERS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>WDM access network; OADM; SOM neural network; data aggregation</td>
</tr>

<tr>
<td valign="top">ID </td><td>WAVELENGTH ASSIGNMENT; RING NETWORKS; MESH NETWORKS</td>
</tr>

<tr>
<td valign="top">AB </td><td>All-optical networks have matured enormously in the recent years. The WDM access network is a high performance network that provides different channels to serve different purposes and increases bandwidth by using multi-channel techniques. The heterogeneous format traffic is supported by the symbiosis of electronic and optical WDM networking functions that enable efficient gigabit-per-second user access in next-generation Internet networks. By decreasing the transmission time, the performance in the WDM access network can be increased. In general, applications of packet transmissions between several nodes, such as multicast communication, voice and video, usually have a property in common: same destination. In this paper, the Self-Organized Map (SOM) neural network is used to generate a stochastic generative model to assign a proper channel to the packet and achieve packet aggregation for cost-effective OADM. The simulation results show that our stochastic model is better than traditional ones in terms of the mean frequency of conversion and transmission time.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Hwang, I-Shyan] Yuan Ze Univ, Dept Comp Engn &amp; Sci, Chungli 32026,
   Taiwan.
   <br>[Lee, San-Nan] Vanung Univ, Dept Comp Sci &amp; Informat Engn, Chungli
   32061, Taiwan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Hwang, IS (reprint author), Yuan Ze Univ, Dept Comp Engn &amp; Sci, Chungli 32026, Taiwan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>ishwang@saturn.yzu.edu.tw</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">VL </td><td>32</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">BP </td><td>457</td>
</tr>

<tr>
<td valign="top">EP </td><td>467</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1080/02533839.2009.9671528</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000267475300003</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>McConky, KT
   <br>McLaughlin, P
   <br>Rose, W
   <br>Sudit, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>McConky, Katie T.
   <br>McLaughlin, Pat
   <br>Rose, William
   <br>Sudit, Moises</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>AUTOMATING BATTLEFIELD EVENT REPORTING USING CONCEPTUAL SPACES AND FUZZY
   LOGIC FOR PASSIVE SPEECH INTERPRETATION</td>
</tr>

<tr>
<td valign="top">SO </td><td>MILCOM 2009 - 2009 IEEE MILITARY COMMUNICATIONS CONFERENCE, VOLS 1-4</td>
</tr>

<tr>
<td valign="top">SE </td><td>IEEE Military Communications Conference</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE Military Communications Conference (MILCOM 2009)</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 18-21, 2009</td>
</tr>

<tr>
<td valign="top">CL </td><td>Boston, MA</td>
</tr>

<tr>
<td valign="top">AB </td><td>This research explores the feasibility of performing passive information capture on voice data in order to analyze and classify the contents of interpersonal communication. The general form of this problem is very difficult as fully automated speech understanding technology does not exist. This is further complicated by battlefield realities including: noise, jargon and unstructured speech. However, when specific topics are isolated for extraction, the challenge becomes manageable. Conceptual Spaces is used as a fusion framework to classify data passively captured by traditional speech recognition software coupled with fuzzy logic to provide matching of phonetics to jargon. Together these technologies prove to be a valuable fusion framework because of their ability to mitigate the high levels of errors inherent in speech recognition. An initial study focused on recognizing important topics in communications between commanders and field personnel amidst background chatter. Results indicate the Conceptual Spaces model is flexible enough to define "spaces" for military events, and the underlying optimization model used for classification was robust and fast enough to quickly and accurately classify the noisy scenario data. This technology enables a new and more general class of automation, permitting conversion of passive speech into structured data.
   <br>The authors gratefully acknowledge the support provided by the Defense Advanced Research Projects Agency (DARPA).</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[McConky, Katie T.] CUBRC, Buffalo, NY 14225 USA.
   <br>[McLaughlin, Pat; Rose, William] Lockheed Martin IS&amp;GS, Laguna Hills, CA
   USA.
   <br>[Sudit, Moises] Ctr Multisource Informat Fus, Buffalo, NY USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>McConky, KT (reprint author), CUBRC, Buffalo, NY 14225 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>mcconky@cubrc.org; pat.mclaughlin@lmco.com; william.j.rose@lmco.com;
   sudit@cubrc.org</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2009</td>
</tr>

<tr>
<td valign="top">BP </td><td>2023</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000280509901097</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Kuo, CT
   <br>Wang, HC</td>
</tr>

<tr>
<td valign="top">AF </td><td>Kuo, Chih-Ting
   <br>Wang, Hsiao-Chuan</td>
</tr>

<tr>
<td valign="top">BE </td><td>Meng, H
   <br>Jiang, H
   <br>Tao, JH
   <br>Wang, R</td>
</tr>

<tr>
<td valign="top">TI </td><td>A Pitch Synchronous Method for Speech Modification</td>
</tr>

<tr>
<td valign="top">SO </td><td>2008 6TH INTERNATIONAL SYMPOSIUM ON CHINESE SPOKEN LANGUAGE PROCESSING,
   PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>6th International Symposium on Chinese Spoken Language Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 16-19, 2008</td>
</tr>

<tr>
<td valign="top">CL </td><td>Kunming, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>speech modification; pitch extraction; pitch trajectory change; pitch
   synchronous linear prediction; Mandarin perception</td>
</tr>

<tr>
<td valign="top">AB </td><td>The speech modification is a mechanism of changing speech characteristics and prosody for some specific applications. It is used in voice conversion, pronunciation correction, tone perception, and language learning. The most important part is the change of pitch in an utterance. Pitch extraction is an essential process for speech modification. This paper presents an efficient pitch extraction algorithm based on the normalized second standard deviation function (NSSDF) of magnitude difference. A pitch synchronous method for modifying speaking rate and pitch trajectory is proposed. The speaking rate is modified by inserting or deleting pitch periods in voiced segments. The pitch trajectory change is accomplished by modifying the pitch period of residual signal obtained from pitch synchronous linear prediction (LP) analysis and reconstructing speech signal by LP filter. A speech modification system is developed for Mandarin perception which is used to help hearing impaired students in pronunciation learning.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Kuo, Chih-Ting; Wang, Hsiao-Chuan] Natl Tsing Hua Univ, Dept Elect
   Engn, Hsinchu, Taiwan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Kuo, CT (reprint author), Natl Tsing Hua Univ, Dept Elect Engn, Hsinchu, Taiwan.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">BP </td><td>245</td>
</tr>

<tr>
<td valign="top">EP </td><td>248</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000264234600062</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Arslan, H
   <br>Gorcin, A</td>
</tr>

<tr>
<td valign="top">AF </td><td>Arslan, Hueseyin
   <br>Goercin, Ali</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Cognitive Radio and Software Defined Radio: Signal Processing
   Perspectives</td>
</tr>

<tr>
<td valign="top">SO </td><td>2008 IEEE 16TH SIGNAL PROCESSING, COMMUNICATION AND APPLICATIONS
   CONFERENCE, VOLS 1 AND 2</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE 16th Signal Processing and Communications Applications Conference</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 20-22, 2008</td>
</tr>

<tr>
<td valign="top">CL </td><td>Aydin, TURKEY</td>
</tr>

<tr>
<td valign="top">AB </td><td>Wireless communications technologies have evolved tremendously throughout the last three decades. Rapidly increasing number of users and conversion of the voice oriented applications to multimedia applications lead to the need for efficient and flexible communications systems. The evolution of wireless systems addressed this need with adaptive radios until now. The future evolutions are expected to continue this trend with a more structured and sophisticated manner, leading to software defined radio and cognitive radio concepts with the capabilities of sensing, awareness, learning, decision making, and advanced adaptation. These next generation communication technologies require effective, reliable and intelligent signal processing techniques to sustain these features. In this paper, signal intelligence concept including signal identification, multi dimensional signal analysis, and location and environment awareness will be discussed following introductory information about radio spectrum sensing and monitoring.(1)</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Arslan, Hueseyin; Goercin, Ali] Univ S Florida, Dept Elect Engn, Tampa,
   FL 33620 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Arslan, H (reprint author), Univ S Florida, Dept Elect Engn, 4202 E Fowler Ave,ENB-118, Tampa, FL 33620 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>arslan@eng.usf.edu; agorcin@mail.usf.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">BP </td><td>873</td>
</tr>

<tr>
<td valign="top">EP </td><td>877</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Imaging Science &amp; Photographic Technology;
   Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000261359200216</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ghai, S
   <br>Sinha, R</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ghai, Shweta
   <br>Sinha, Rohit</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>An Investigation into the Effect of Pitch Transformation on Children
   Speech Recognition</td>
</tr>

<tr>
<td valign="top">SO </td><td>2008 IEEE REGION 10 CONFERENCE: TENCON 2008, VOLS 1-4</td>
</tr>

<tr>
<td valign="top">SE </td><td>TENCON IEEE Region 10 Conference Proceedings</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE Region 10 Conference (TENCON 2008)</td>
</tr>

<tr>
<td valign="top">CY </td><td>NOV 19-21, 2008</td>
</tr>

<tr>
<td valign="top">CL </td><td>Hyderabad, INDIA</td>
</tr>

<tr>
<td valign="top">AB </td><td>The degradation in the automatic speech recognition performance of the adult speech trained models for children speech data is a well known problem. In this work, motivated by the voice conversion approaches for addressing the acoustic mismatch between the adult and children speech, we investigated the effect of pitch transformation on children speech on telephone-based connected digit recognition task. Our preliminary results indicate that the effect of pitch transformation on the recognition performance of the children speech varies with their average pitch values. With the reduction of pitch, an improvement of 10% was observed in the speech recognition performance for children having pitch values more than 300 Hz. We have also proposed an explanation for this performance improvement based on the study of filter-bank smoothing in front-end signal processing.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ghai, Shweta; Sinha, Rohit] Indian Inst Technol Guwahati, Dept Elect &amp;
   Commun Engn, Gauhati 781039, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ghai, S (reprint author), Indian Inst Technol Guwahati, Dept Elect &amp; Commun Engn, Gauhati 781039, India.</td>
</tr>

<tr>
<td valign="top">EM </td><td>shweta@iitg.ernet.in; rsinha@iitg.ernet.in</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">BP </td><td>1731</td>
</tr>

<tr>
<td valign="top">EP </td><td>1736</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000266545101076</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Bourbakis, N</td>
</tr>

<tr>
<td valign="top">AF </td><td>Bourbakis, Nikolaos</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE Computer Society</td>
</tr>

<tr>
<td valign="top">TI </td><td>Automatic Image-to-Text-to-Voice Conversion for Interactively Locating
   Objects in Home Environments</td>
</tr>

<tr>
<td valign="top">SO </td><td>20TH IEEE INTERNATIONAL CONFERENCE ON TOOLS WITH ARTIFICIAL
   INTELLIGENCE, VOL 2, PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">SE </td><td>Proceedings-International Conference on Tools With Artificial
   Intelligence</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>20th IEEE International Conference on Tools with Artificial Intelligence</td>
</tr>

<tr>
<td valign="top">CY </td><td>NOV 03-05, 2008</td>
</tr>

<tr>
<td valign="top">CL </td><td>Dayton, OH</td>
</tr>

<tr>
<td valign="top">DE </td><td>Converting Images to NL-Text; Image Analysis and Representation; Graphs;
   Recognizing Objects</td>
</tr>

<tr>
<td valign="top">ID </td><td>PATTERN-RECOGNITION; SEGMENTATION; REPRESENTATION; SIMILARITY; CURVES</td>
</tr>

<tr>
<td valign="top">AB </td><td>The efficient processing and association of different multimodal information is a very important research field with a great variety of applications, such as human computer interaction, knowledge discovery, document understanding, etc. A good approach to this important issue is the development of a common platform for converting different modalities (such as images, text, etc) into the same medium and associating them for efficient processing and understanding. Thus, this paper here presents the development of a novel methodology based on Local-Global (LG) graphs capable for automatically converting image context into natural language text sentences and then into speech for serving as an interactive model for locating missing objects in home environments. Simple illustrative examples are provided for proving the concept proposed here.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">BP </td><td>49</td>
</tr>

<tr>
<td valign="top">EP </td><td>55</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/ICTAI.2008.123</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000262215500008</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Minematsu, N
   <br>Saito, D
   <br>Hirose, K</td>
</tr>

<tr>
<td valign="top">AF </td><td>Minematsu, Nobuaki
   <br>Saito, Daisuke
   <br>Hirose, Keikichi</td>
</tr>

<tr>
<td valign="top">BE </td><td>Yuan, BZ
   <br>Ruan, QQ
   <br>Tang, XF</td>
</tr>

<tr>
<td valign="top">TI </td><td>Experimental Study of Structure to Speech Conversion - An implementation
   of Infant-like Vocal Imitation on a Machine</td>
</tr>

<tr>
<td valign="top">SO </td><td>ICSP: 2008 9TH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING, VOLS 1-5,
   PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th International Conference on Signal Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 26-29, 2008</td>
</tr>

<tr>
<td valign="top">CL </td><td>Beijing, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">AB </td><td>Most of the speech synthesizers have been developed as text (phoneme sequence) to speech converters and, in this framework, text input is a precondition for speech production. However, we can say that no child acquires spoken language by reading a given text out. Children are explained to acquire spoken language by imitating the utterances of their parents but they never imitate the voices of their parents. Developmental psychology claims that they extract a holistic and speaker-invariant sound pattern embedded in a given utterance, called word Gestalt, and realize the pattern acoustically using their short vocal tubes. In our previous studies, we mathematically defined this holistic and speaker-invariant pattern and used it for ASR [1,2,3,4]. Here, we experimentally implement its inverse process, i.e. Gestalt-to-utterance conversion, on a computer.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Minematsu, Nobuaki; Saito, Daisuke; Hirose, Keikichi] Univ Tokyo, Tokyo
   1138654, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Minematsu, N (reprint author), Univ Tokyo, Tokyo 1138654, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>mine@gavo.t.u-tokyo.ac.jp; dsk_saito@gavo.t.u-tokyo.ac.jp;
   hirose@gavo.t.u-tokyo.ac.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">BP </td><td>651</td>
</tr>

<tr>
<td valign="top">EP </td><td>654</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/ICOSP.2008.4697215</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000270665400158</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Wszolek, J
   <br>Danda, J</td>
</tr>

<tr>
<td valign="top">AF </td><td>Wszolek, Jacek
   <br>Danda, Jacek</td>
</tr>

<tr>
<td valign="top">BE </td><td>Misra, SC
   <br>Revetria, R
   <br>Sztandera, LM
   <br>Iliescu, M
   <br>Zaharim, A
   <br>Parsiani, H</td>
</tr>

<tr>
<td valign="top">TI </td><td>Using Direct Sequence Spread Spectrum in Marine Radio Communication</td>
</tr>

<tr>
<td valign="top">SO </td><td>PROCEEDINGS OF THE 8TH WSEAS INTERNATIONAL CONFERENCE ON APPLIED
   COMPUTER SCIENCE (ACS'08): RECENT ADVANCES ON APPLIED COMPUTER SCIENCE</td>
</tr>

<tr>
<td valign="top">SE </td><td>Recent Advances in Computer Engineering</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>8th WSEAS International Conference on Applied Computer Science (ACS 08)</td>
</tr>

<tr>
<td valign="top">CY </td><td>NOV 21-23, 2008</td>
</tr>

<tr>
<td valign="top">CL </td><td>Venice, ITALY</td>
</tr>

<tr>
<td valign="top">DE </td><td>CDMA; marine communications; DSSS; radiotelephone; spreading;
   transmission security</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper proposes application of the DSSS CDMA method (Direct Sequence Spread Spectrum Code Division Multiplexing Access) in marine voice and data transmission. The spread spectrum method is based oil conversion of signal into the form of wider band, while signal power remains on the same level. This conversion causes decrease of power spectral density, which is proportional to the spreading factor. Spectrum spreading is achieved by multiplying the input signal by a code sequence (spreading code). Simultaneous transmission can be achieved by spreading multiple signals by uncorrelated and orthogonal sequences. Power spectral density of CDMA is usually lower than noise power, therefore the transmission remains transparent for other receivers and does not affect other systems. Another advantage of the CDMA is its robustness against the narrowband noise.
   <br>For a correct reception of the CDMA signal, a precise synchronization is necessary. This results from a method of despreading, which is based on another multiplication of signal and the spreading code, This operation should be performed in-phase. If not, on the input of detector appears additional noise.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Wszolek, Jacek; Danda, Jacek] AGH Univ Sci &amp; Technol, Dept Telecommun,
   Al Mickiewicza 30, PL-30059 Krakow, Poland.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Wszolek, J (reprint author), AGH Univ Sci &amp; Technol, Dept Telecommun, Al Mickiewicza 30, PL-30059 Krakow, Poland.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jwszolek@agh.edu.pl; danda@agh.edu.pl</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Danda, Jacek</display_name>&nbsp;</font></td><td><font size="3">A-1191-2013&nbsp;</font></td>
</tr>
<tr class="fr_data_row">
<td><font size="3">
<display_name>Wszolek, Jacek</display_name>&nbsp;</font></td><td><font size="3">A-1460-2013&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Danda, Jacek</display_name>&nbsp;</font></td><td><font size="3">0000-0002-9204-9615&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2008</td>
</tr>

<tr>
<td valign="top">BP </td><td>317</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000264170900053</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ali, HQ
   <br>Ahmed, J
   <br>Siyal, MY</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ali, Hamida Qunber
   <br>Ahmed, Jameel
   <br>Siyal, Mohammed Yakoob</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Delay computation for real-time synchronization of speech and its
   converted text</td>
</tr>

<tr>
<td valign="top">SO </td><td>2007 6TH INTERNATIONAL CONFERENCE ON INFORMATION, COMMUNICATIONS &amp;
   SIGNAL PROCESSING, VOLS 1-4</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>6th International Conference on Information, Communications and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 10-13, 2007</td>
</tr>

<tr>
<td valign="top">CL </td><td>Singapore, SINGAPORE</td>
</tr>

<tr>
<td valign="top">DE </td><td>component; formatting; style; styling; insert</td>
</tr>

<tr>
<td valign="top">AB </td><td>Transmission of real-time text data integrated with other multimedia applications such as audio and video has raised the issues of compatibility and synchronization among these applications since a stringent quality of service (QoS) guarantee is especially critical for real-time traffic. In order to meet the real-time properties, text must be produced efficiently to integrate its transmission with other multimedia applications. literature survey shows that the text for the real-time transmission can be produced by different input sources such as from handwriting recognition, voice recognition or it can be entered by human users from a keyboard or any other input method [1]. This paper presents an efficient way of producing text which to the best of our knowledge has not been previously explored. We propose to generate text from the recognition of the real-time voice in the source machine. We calculate the delay in the speech-recognition or speech-to-text conversion. Based on these statistics we suggest a buffer size to store the voice data until its respective text is generated. This enables us to transmit both voice and its converted text synchronously. We find, and show it graphically, that this delay is almost negligible and there is almost no queue formation in the buffer. Hence both of the applications can be transmitted instantly that is as they are available. This research is a reasonable advancement in the subject area.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Ali, Hamida Qunber] Iqra Univ, Dept Comp Sci, Karachi, Pakistan.
   <br>[Ahmed, Jameel] NFC IET, Dept Elect &amp; Comp Engn, Multan, Pakistan.
   <br>[Siyal, Mohammed Yakoob] Nanyang Technol Univ, Sch Elect &amp; Elect Engn,
   Singapore 639798, Singapore.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ali, HQ (reprint author), Iqra Univ, Dept Comp Sci, Karachi, Pakistan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>hameeda-ali@yahoo.co.uk; jameel@nfciet.edu.pk; eyakoob@ntu.edu.sg</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">BP </td><td>233</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000256699500049</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Nicodem, MV
   <br>Seara, IC
   <br>Seara, R
   <br>dos Anjos, D</td>
</tr>

<tr>
<td valign="top">AF </td><td>Nicodem, Monique V.
   <br>Seara, Izabel C.
   <br>Seara, Rui
   <br>dos Anjos, Daiana</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Recording script design for a Brazilian Portuguese TTS system aiming at
   a higher phonetic and prosodic variability</td>
</tr>

<tr>
<td valign="top">SO </td><td>2007 9TH INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND ITS
   APPLICATIONS, VOLS 1-3</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th International Symposium on Signal Processing and its Applications</td>
</tr>

<tr>
<td valign="top">CY </td><td>FEB 12-15, 2007</td>
</tr>

<tr>
<td valign="top">CL </td><td>Sharjah, U ARAB EMIRATES</td>
</tr>

<tr>
<td valign="top">ID </td><td>GENETIC ALGORITHMS</td>
</tr>

<tr>
<td valign="top">AB </td><td>The naturalness of intonation of the speech produced by corpus-based synthesis systems is strongly dependent on factors such as prosodic modeling algorithms, quality of the speech corpus, speaker's voice characteristics, and recording script. In order to improve naturalness, the recording script must present a considerable amount of variants of a given phone in distinct prosodic contexts, i.e., this corpus must own an appropriate phonetic as well as prosodic variability. Aiming to improve the variability of such a corpus, this work proposes a procedure to design the recording script for a concatenative speech synthesis (text-to-speech - TTS) system developed for the Brazilian Portuguese (BP) language. In this design procedure, four stages are considered: grapheme-to-phoneme conversion, prosodic annotation, feature vector representation, and selection itself. The procedure of prosodic annotation is an original contribution of the current research work. The selection itself is carried out by using an approach based on genetic algorithms. In addition, a set of 3000 prosodically rich BP sentences will be made available as a research tool for speech processing applications.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Nicodem, Monique V.; Seara, Izabel C.; Seara, Rui; dos Anjos, Daiana]
   Univ Fed Santa Catarina, Dept Elect Engn, Circuits &amp; Signal Proc Lab,
   LINSE, BR-88040900 Florianopolis, SC, Brazil.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Nicodem, MV (reprint author), Univ Fed Santa Catarina, Dept Elect Engn, Circuits &amp; Signal Proc Lab, LINSE, BR-88040900 Florianopolis, SC, Brazil.</td>
</tr>

<tr>
<td valign="top">EM </td><td>monique@linse.ufsc.br; izabels@linse.ufsc.br; seara@linse.ufsc.br;
   daiana@linse.ufsc.br</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Seara, Rui</display_name>&nbsp;</font></td><td><font size="3">H-1356-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">BP </td><td>939</td>
</tr>

<tr>
<td valign="top">EP </td><td>942</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000259439900236</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Matthews, JW
   <br>Michalson, WR</td>
</tr>

<tr>
<td valign="top">AF </td><td>Matthews, James W.
   <br>Michalson, William R.</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Distributed digital radios and WLAN interoperability</td>
</tr>

<tr>
<td valign="top">SO </td><td>2007 IEEE CONFERENCE ON TECHNOLOGIES FOR HOMELAND SECURITY: ENHANCING
   CRITICAL INFRASTRUCTURE DEPENDABILITY</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE Conference on Technologies for Homeland Security - Enhancing
   Critical Infrastructure Dependability</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 16-17, 2007</td>
</tr>

<tr>
<td valign="top">CL </td><td>Woburn, MA</td>
</tr>

<tr>
<td valign="top">DE </td><td>digital radio; land mobile radio; communication; systems; public safety</td>
</tr>

<tr>
<td valign="top">AB </td><td>In addition to the speaker and the microphone, the radio handset contains the baseband electronics for a digital radio. This includes an Ethernet jack, a full network protocol stack, an OFDM baseband modem and a user interface to place calls: one-to-one (private line) or one-to-many (talk groups). The handset plugs into an ordinary land mobile radio that is used only for up conversion to RF and power amplification. No modification to the radio is needed. Using existing radios means the existing infrastructure and existing channel frequencies are also used. This is a significant cost savings to achieve interoperabitity, and it immediately puts the technology into the marketplace.
   <br>A distributed digital radio (DDR) is a 6-way radio: voice/RF, voice/Ethernet and Ethernet/RF. In computer network terminology, a DDR is a hybrid router-hybrid is used because the PHY layer at each of the ports is different: audio, RF or Ethernet.
   <br>A public safety radio network with DDRs becomes a wireless local area network (WLAN). The WLAN is not WiFi; it is a client mesh with an Ethernet backplane. The DDR provides 2G communications and cost effective IP-based interoperability to all first responders-even to the local and rural public safety agencies with conventional analog radio networks. This paper discusses WLAN interoperability architectures, and it traces the development of the DDR.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Matthews, James W.] Worcester Polytech Inst, Bioengn Inst, Worcester,
   MA 01609 USA.
   <br>[Michalson, William R.] Worcester Polytech Inst, Dept Elect Engn,
   Worcester, MA 01609 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Matthews, JW (reprint author), Worcester Polytech Inst, Bioengn Inst, Worcester, MA 01609 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jwm@wpi.edu; wnn@ece.wpi.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">BP </td><td>107</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/THS.2007.370029</td>
</tr>

<tr>
<td valign="top">SC </td><td>Remote Sensing; Imaging Science &amp; Photographic Technology;
   Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000248535800020</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Gao, YQ
   <br>Yang, Z</td>
</tr>

<tr>
<td valign="top">AF </td><td>Gao, Yinqiu
   <br>Yang, Zhen</td>
</tr>

<tr>
<td valign="top">BE </td><td>Li, K
   <br>Xiang, Y
   <br>Jin, H
   <br>Qu, WY
   <br>Cao, ZY</td>
</tr>

<tr>
<td valign="top">TI </td><td>Pitch modification based on syllable units for voice morphing system</td>
</tr>

<tr>
<td valign="top">SO </td><td>2007 IFIP INTERNATIONAL CONFERENCE ON NETWORK AND PARALLEL COMPUTING
   WORKSHOPS, PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">SE </td><td>IFIP International Conference on Network and Parallel Computing NPC</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IFIP International Conference on Network and Parallel Computing</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 18-21, 2007</td>
</tr>

<tr>
<td valign="top">CL </td><td>Dalian, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">AB </td><td>An innovative scheme of voice morphing is proposed to make the speech of a source speaker sound like uttered by a target speaker. The morphing technique can hide people's identity, age, gender while chatting and doing other things related to the transformation of speech streams online, which can ensure the privacy on the prevalent internet. Speaker individuality transformation is achieved by altering the spectral envelope and estimating the excitation signal by modifying the fundamental pitch frequency in syllable units of the residual signal of the source speech based on linear prediction coding (LPC) model. The main advantage of this scheme relies in the aspect of having considered the dynamic characteristic of the pitch frequency, not just focusing on the average level, which enhances the performance of the whole conversion system compared with general concepts such as discrete pitch frequency mapping and so on. Moreover, in the aspect of the alignment of line spectral frequencies (LSFs) vectors, an advanced technique based on isolated syllables rather than the general dynamic time warping algorithm (DTW) is introduced. The experimental results show that the system is capable of effectively transforming speaker identity whilst the converted speech maintains high quality.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Gao, Yinqiu; Yang, Zhen] Nanjing Univ Posts &amp; Telecommun, Inst Signal &amp;
   Informat Proc, Nanjing 210003, Peoples R China.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Gao, YQ (reprint author), Nanjing Univ Posts &amp; Telecommun, Inst Signal &amp; Informat Proc, POB 59,66 XinMofan St, Nanjing 210003, Peoples R China.</td>
</tr>

<tr>
<td valign="top">EM </td><td>y050920@njupt.edu.cn; yangz@njupt.edu.cn</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">BP </td><td>135</td>
</tr>

<tr>
<td valign="top">EP </td><td>139</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/NPC.2007.11</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000251390000024</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Rao, KS
   <br>Koolagudi, SG</td>
</tr>

<tr>
<td valign="top">AF </td><td>Rao, K. Sreenivasa
   <br>Koolagudi, Shashidhar G.</td>
</tr>

<tr>
<td valign="top">TI </td><td>Transformation of speaker characteristics in speech using support vector
   machines</td>
</tr>

<tr>
<td valign="top">SO </td><td>ADCOM 2007: PROCEEDINGS OF THE 15TH INTERNATIONAL CONFERENCE ON ADVANCED
   COMPUTING AND COMMUNICATIONS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>15th International Conference on Advanced Computing and Communications</td>
</tr>

<tr>
<td valign="top">CY </td><td>DEC 18-21, 2007</td>
</tr>

<tr>
<td valign="top">CL </td><td>Indian Inst Technol Guwahati, Guwahati, INDIA</td>
</tr>

<tr>
<td valign="top">HO </td><td>Indian Inst Technol Guwahati</td>
</tr>

<tr>
<td valign="top">ID </td><td>VOICE CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>In this paper we propose Support Vector Machines (SVM) for transforming the speaker characteristics of the speech. Speaker characteristics are mainly influenced by the behavioural characteristics (prosody) of the speaker, characteristics of the vocal tract system and the excitation source. In this work speaker transformation indicates, modifying the speaker characteristics of the speech according to the desired speaker, and preserving the underlying message (sequence of sound units, i.e., text) same as in the original speech. This is performed by deriving the mapping functions for transforming the vocal tract characteristics and prosodic characteristics. SVMs are explored for deriving these mapping functions. The prosodic parameters and the characteristics of the vocal tract system and the excitation source of the target speaker are obtained from the output of the mapping functions. ne manipulations of the prosodic parameters (durational characteristics, pitch contour (intonation pattern) and intensity patterns) are achieved by manipulating the Linear Prediction (LP) residual with the help of the knowledge of the instants of significant excitation. ne modified LP residual is used to excite the time varying filter. The filter parameters are updated according to the desired vocal tract characteristics. The target speaker's speech is synthesized and evaluated using listening tests. The results of the listening tests indicate that the proposed mapping functions using SVMs provide the better speaker transformation compared to the earlier methods proposed by the author [1].</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Rao, K. Sreenivasa; Koolagudi, Shashidhar G.] Indian Inst Technol, Sch
   Informat Technol, Kharagpur 721302, W Bengal, India.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Rao, KS (reprint author), Indian Inst Technol, Sch Informat Technol, Kharagpur 721302, W Bengal, India.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">BP </td><td>660</td>
</tr>

<tr>
<td valign="top">EP </td><td>665</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000252883700101</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Glass, K
   <br>Bangay, S
   <br>Alcock, B</td>
</tr>

<tr>
<td valign="top">AF </td><td>Glass, Kevin
   <br>Bangay, Shaun
   <br>Alcock, Bruce</td>
</tr>

<tr>
<td valign="top">BE </td><td>Spencer, SN</td>
</tr>

<tr>
<td valign="top">TI </td><td>Mechanisms for Multimodality: Taking Fiction to Another Dimension</td>
</tr>

<tr>
<td valign="top">SO </td><td>AFRIGRAPH 2007: 5TH INTERNATIONAL CONFERENCE ON VIRTUAL REALITY,
   COMPUTER GRAPHICS, VISUALIZATION AND INTERACTION IN AFRICA</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>5th International Conference on Virtual Reality, Computer Graphics,
   Visualization and Interaction in Africa</td>
</tr>

<tr>
<td valign="top">CY </td><td>OCT 29-31, 2007</td>
</tr>

<tr>
<td valign="top">CL </td><td>Grahamstown, SOUTH AFRICA</td>
</tr>

<tr>
<td valign="top">DE </td><td>multimodality; text-to-scene conversion; constraint solving</td>
</tr>

<tr>
<td valign="top">AB </td><td>We present methods for automatically constructing representations of fiction books in a range of modalities: audibly, graphically and as 3D virtual environments. The correspondence between the sequential ordering of events against the order of events presented in the text is used to correctly resolve the dynamic interactions for each representation. Synthesised audio created from the fiction text is used to calibrate the base time-line against which the other forms of media are correctly aligned. The audio stream is based on speech synthesis using the text of the book, and is enhanced using distinct voices for the different characters in a book. Sound effects are included automatically. The graphical representation represents the text (as subtitles), identifies active characters and provides visual feedback of the content of the story. Dynamic virtual environments conform to the constraints implied by the story, and are used as a source of further visual content. These representations are all aligned to a common time-line, and combined using sequencing facilities to provide a multimodal version of the original text.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Glass, Kevin; Bangay, Shaun; Alcock, Bruce] Rhodes Univ, Dept Comp Sci,
   ZA-6140 Grahamstown, South Africa.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Glass, K (reprint author), Rhodes Univ, Dept Comp Sci, ZA-6140 Grahamstown, South Africa.</td>
</tr>

<tr>
<td valign="top">EM </td><td>k.glass@ru.ac.za; s.bangay@ru.ac.za; bruce.alcock@gmail.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2007</td>
</tr>

<tr>
<td valign="top">BP </td><td>135</td>
</tr>

<tr>
<td valign="top">EP </td><td>144</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Imaging Science &amp; Photographic Technology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000266432000017</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<style type="text/css">
        table.FR_table_borders {-webkit-box-sizing: border-box; -moz-box-sizing: border-box;box-sizing: border-box; margin-top: 2px; outline: 1px solid #bababa; border-collapse: collapse;}
        table.FR_table_noborders .fr_address_row2:last-child {width: 100%} 
        table.FR_table_borders th {vertical-align: middle; padding: 9px 10px 9px 9px;background: #f3f3f3; text-align: left;font-weight: bold;}
        table.FR_table_borders td {vertical-align: middle; padding: 5px;}
        table.FR_table_borders th, table.FR_table_borders td {border: 1px solid #CCCCCC; }
    </style><table xmlns:bean="http://ts.thomson.com/ua/bean" xmlns:exsl="http://exslt.org/common">
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Zheng, Q
   <br>Mohan, G</td>
</tr>

<tr>
<td valign="top">AF </td><td>Zheng, Q.
   <br>Mohan, G.</td>
</tr>

<tr>
<td valign="top">TI </td><td>LSP protection for delay-differentiated dynamic traffic in IP-over-WDM
   networks with port constraints</td>
</tr>

<tr>
<td valign="top">SO </td><td>COMPUTER COMMUNICATIONS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>wavelength division multiplexing; multi-protocol label switching; label
   switched path; optical-electrical-optical (OEO) conversion;
   port-independent routing; port-dependent routing</td>
</tr>

<tr>
<td valign="top">AB </td><td>We consider the problem of label switched path (LSP) protection for dynamic traffic with differentiated delay requirements in IP-over-WDM networks with limited port resources. A pair of link-disjoint primary LSP and backup LSP is provided for each connection to enable guaranteed and timely recovery in the event of a single link failure. To support delay sensitive traffic such as voice, the primary and backup LSPs must traverse a limited number of optical-electrical-optical (OEO) conversions (equivalently, to traverse limited number of electronic routers and hence reduced electrical processing), besides the bandwidth requirement. This OEO constraint can be specified by users in service level agreement (SLA) or determined by service providers based on the end-to-end delay requirement. We propose two integrated routing algorithms to route traffic with or with no OEO conversion requirements, respectively. These two algorithms can compute primary LSPs and backup LSPs in polynomial time. We consider the case where limited ports are provided at each node in the network and develop two routing approaches called port-independent routing and port-dependent routing. In the port-independent routing, paths are selected first and then port availabilities are checked to setup the path. While this approach is simple to implement, it leads to connection blocking if ports required on the chosen path are not available. In the port-dependent routing, port information is incorporated in the path selection process. It guarantees that a path can be setup once it is found. We evaluate the effectiveness of LSP protection using the proposed algorithms on the NSFNET network. (C) 2005 Elsevier B.V. All rights reserved.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Natl Univ Singapore, Dept Elect &amp; Comp Engn, Singapore 117576, Singapore.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Mohan, G (reprint author), Natl Univ Singapore, Dept Elect &amp; Comp Engn, Singapore 117576, Singapore.</td>
</tr>

<tr>
<td valign="top">EM </td><td>engp1752@nus.edu.sg</td>
</tr>

<tr xmlns:date="http://exslt.org/dates-and-times">
<td valign="top"><span class="FR_label">OI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Gurusamy, Mohan</display_name>&nbsp;</font></td><td><font size="3">0000-0001-6764-268X&nbsp;&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAY 31</td>
</tr>

<tr>
<td valign="top">PY </td><td>2006</td>
</tr>

<tr>
<td valign="top">VL </td><td>29</td>
</tr>

<tr>
<td valign="top">IS </td><td>9</td>
</tr>

<tr>
<td valign="top">SI </td><td>SI</td>
</tr>

<tr>
<td valign="top">BP </td><td>1402</td>
</tr>

<tr>
<td valign="top">EP </td><td>1412</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1016/j.comcom.2005.09.013</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000237994600016</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Jang, YG</td>
</tr>

<tr>
<td valign="top">AF </td><td>Jang, Young-Gun</td>
</tr>

<tr>
<td valign="top">BE </td><td>Cheung, YM
   <br>Wang, Y
   <br>Lium, H</td>
</tr>

<tr>
<td valign="top">TI </td><td>Intelligent HTML to VXML conversion using automatic object extraction
   and prior structural knowledge</td>
</tr>

<tr>
<td valign="top">SO </td><td>2006 International Conference on Computational Intelligence and
   Security, Pts 1 and 2, Proceedings</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Conference on Computational-Intelligence and Security</td>
</tr>

<tr>
<td valign="top">CY </td><td>NOV 03-06, 2006</td>
</tr>

<tr>
<td valign="top">CL </td><td>Guangzhou, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">ID </td><td>WEB</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper is a presentation a new intelligent agent which converts HTML contents to VXML contents for voice services via web. In this paper I propose an interactive hybrid sequential contents selection method to select desired contents fast and robustly from known web pages. It uses real time structural features as well as included text and/or prior structural knowledge such as link symbol position. To verify its effectiveness, a full agent system is implemented and tested for Korean web sites. The method reflects user intention more accurately than conventional selections using structural features and is more robust to variations of HTML programming techniques. The agent is fast and has less of computational burden than methods using conventional XML or XHTML conversion as intermediate stage.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Chongju Univ, Dept Comp Informat Engn, Chonju, South Korea.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Jang, YG (reprint author), Chongju Univ, Dept Comp Informat Engn, 36,Naedok Dong, Chonju, South Korea.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2006</td>
</tr>

<tr>
<td valign="top">BP </td><td>1446</td>
</tr>

<tr>
<td valign="top">EP </td><td>1451</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/ICCIAS.2006.295299</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000243679800309</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Matthews, B
   <br>Bakis, R
   <br>Eide, E</td>
</tr>

<tr>
<td valign="top">AF </td><td>Matthews, Brett
   <br>Bakis, Raimo
   <br>Eide, Ellen</td>
</tr>

<tr>
<td valign="top">GP </td><td>ISCA</td>
</tr>

<tr>
<td valign="top">TI </td><td>Synthesizing Breathiness in Natural Speech with Sinusoidal Modelling</td>
</tr>

<tr>
<td valign="top">SO </td><td>INTERSPEECH 2006 AND 9TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE
   PROCESSING, VOLS 1-5</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th International Conference on Spoken Language Processing/INTERSPEECH
   2006</td>
</tr>

<tr>
<td valign="top">CY </td><td>2006</td>
</tr>

<tr>
<td valign="top">CL </td><td>Pittsburgh, PA</td>
</tr>

<tr>
<td valign="top">DE </td><td>speech modification; voice conversion; sinusoidal modelling;
   concatenative TTS</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper discusses recent work in synthesizing a breathy quality in pre-recorded speech, which has applications in voice morphing and concatenative TTS. Previous work has shown that the breathy quality in speech is characterized in part by the presence of random noise in the upper region of the spectrum [1]. The sinusoidal modelling representation of speech facilitates making high-quality modifications to speech signals as well as modifying regions of the spectrum independently. We use sinusoidal modelling, along with techniques borrowed from analog communication systems to simulate aspiration noise in wideband speech signals above some lower cutoff frequency. Specifically, we use techniques based on amplitude modulation (AM) and phase modulation (PM), with the harmonics from the sinusoidal model of speech as carriers and lowpass random noise as the message signal. Formal listening tests were conducted and listeners rated the synthesized effect as "breathy" more often than in natural non-breathy speech, but significantly less often than in naturally breathy speech.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Matthews, Brett; Bakis, Raimo; Eide, Ellen] IBM Corp, Thomas J Watson
   Res Ctr, Yorktown Hts, NY 10598 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Matthews, B (reprint author), IBM Corp, Thomas J Watson Res Ctr, Yorktown Hts, NY 10598 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>brett@ece.gatech.edu; bakis@us.ibm.com; eeide@us.ibm.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2006</td>
</tr>

<tr>
<td valign="top">BP </td><td>1790</td>
</tr>

<tr>
<td valign="top">EP </td><td>1793</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000269965901185</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Smith, DWK
   <br>Hunt, RA</td>
</tr>

<tr>
<td valign="top">AF </td><td>Smith, Dean W. K.
   <br>Hunt, Robert A.</td>
</tr>

<tr>
<td valign="top">BE </td><td>Zhao, X
   <br>Liu, B</td>
</tr>

<tr>
<td valign="top">TI </td><td>Assisting the australian furniture manufacturing industry using QFD:
   Voice of the customer collection and preparation</td>
</tr>

<tr>
<td valign="top">SO </td><td>PROCEEDINGS OF THE 11TH ANNUAL CONFERENCE OF ASIA PACIFIC DECISION
   SCIENCES INSTITUTE: INNOVATION &amp; SERVICE EXCELLENCE FOR COMPETITIVE
   ADVANTAGE IN THE GLOBAL ENVIRONMENT</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>11th Annual Conference of the Asia-Pacific-Decision-Sciences-Institute</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUN 14-18, 2006</td>
</tr>

<tr>
<td valign="top">CL </td><td>Kowloon, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>quality function deployment (QFD); voice of the Customer(VOC); wood
   industry</td>
</tr>

<tr>
<td valign="top">AB </td><td>The Australian furniture manufacturing industry has experienced a substantial loss of local market share due to globalisation. A typical response for domestic firms encountering intense global competition is to seek export markets that yield competitive advantages. However, with a lack of export experience and small average firm size, the challenges faced by Australian furniture producers to enter foreign markets are great.
   <br>Being successful in foreign markets requires a strategic approach to advanced understanding of the needs of the target market and how to meet those needs. As a process specifically designed to accomplish this, quality function deployment (QFD) has the potential to provide a formalised framework for the Australian furniture manufacturing industry.
   <br>The input to QFD is the Voice of the Customer (VOC). This paper uses a case study to demonstrate the VOC phase of QFD to Australian furniture producers. A structured list of customer needs is developed and then a quantitative customer survey is conducted on German furniture producers to ascertain and importance rank needs when purchasing wooden furniture components. These needs are structured using an affinity and tree diagram. The resulting structured and prioritised needs are analysed for input to the house of quality (HOQ) for conversion into quantified product specifications.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>[Smith, Dean W. K.] Univ Melbourne, CRC Wood Innovat, Parkville, Vic
   3052, Australia.
   <br>[Hunt, Robert A.] Macquarie Univ, Ctr Management Innovat &amp; Technol, Grad
   Sch Management, Sydney, NSW, Australia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Smith, DWK (reprint author), Univ Melbourne, CRC Wood Innovat, Parkville, Vic 3052, Australia.</td>
</tr>

<tr>
<td valign="top">EM </td><td>d.smith3@pgrad.unimeIb.edu.au; rhunt@work.gsm.mq.edu.au</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2006</td>
</tr>

<tr>
<td valign="top">BP </td><td>58</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Business &amp; Economics; Computer Science; Engineering; Operations Research
   &amp; Management Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000239342800016</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Pezzini, D</td>
</tr>

<tr>
<td valign="top">AF </td><td>Pezzini, D</td>
</tr>

<tr>
<td valign="top">TI </td><td>The prophetic voice in St Birgitta's 'Revelations': An analysis of
   'Incominciano Certi Capitoli', a late fifteenth century Italian
   compilation</td>
</tr>

<tr>
<td valign="top">SO </td><td>AEVUM-RASSEGNA DI SCIENZE STORICHE LINGUISTICHE E FILOLOGICHE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article; Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>International Symposium on 700 Years of Birgittine Spirituality and
   Culture</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 04-09, 2003</td>
</tr>

<tr>
<td valign="top">CL </td><td>Vadstena, SWEDEN</td>
</tr>

<tr>
<td valign="top">AB </td><td>It seems that in Italy St Birgitta of Sweden was received mainly as a prophetess, both as one who could forecast coming events (prophecies of political content were falsely attributed to her) and, more properly, as a preacher speaking in the name of God to call people to conversion. This second aspect of prophecy is the guiding principle of an Italian compilation, a sort of 'thematic book' consisting of selected chapters (113 in all) from the Revelations. This work exists in a manuscript from the end of the fifteenth century (Florence, Bibl. naz. centrale, 11, 11, 391), and in a book printed in Mondovi in 1518, of which only three copies appear to be extant in public libraries (Turin, Stockholm and Uppsala). A comparison shows that the two versions go back to the same source, although they have many differences both in wording and in layout. The content of the work is first summarily described, and the main themes of Birgitta's preaching on conversion pointed out as they were perceived by the compiler. Then the section drawn from the Third Book of the Revelations (fifteen chapters out of thirty-three), containing particularly harsh criticism of clerical behaviour, is examined in order to find out criteria followed in the selection.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>SEP-DEC</td>
</tr>

<tr>
<td valign="top">PY </td><td>2005</td>
</tr>

<tr>
<td valign="top">VL </td><td>79</td>
</tr>

<tr>
<td valign="top">IS </td><td>3</td>
</tr>

<tr>
<td valign="top">BP </td><td>591</td>
</tr>

<tr>
<td valign="top">EP </td><td>+</td>
</tr>

<tr>
<td valign="top">SC </td><td>Arts &amp; Humanities - Other Topics</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000235079100001</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Jeong, JH
   <br>Yoo, C</td>
</tr>

<tr>
<td valign="top">AF </td><td>Jeong, JH
   <br>Yoo, C</td>
</tr>

<tr>
<td valign="top">TI </td><td>Software-based video/audio processing for cellular phones</td>
</tr>

<tr>
<td valign="top">SO </td><td>TELECOMMUNICATION SYSTEMS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>video/audio processing; cellular phone; adaptation</td>
</tr>

<tr>
<td valign="top">ID </td><td>FAST ALGORITHMS; DCT-DOMAIN; STANDARD</td>
</tr>

<tr>
<td valign="top">AB </td><td>Nowadays, most cellular phones are used beyond voice communication. Although the processing power of cellular phones is sufficient for most data applications, it is difficult to play video and audio contents in software because of their computational complexity and lack of basic tools for multimedia processing. so software-based multimedia processing on cellular phones is a challenging issue. Several transcoding methods are introduced to address this issue, but they are mainly of the DCT-domain conversion. Hence, they are only applicable to high-end cellular phones. To develop a solution for low-end and mid-tier cellular phones, we begin this paper by analyzing the complexity of existing video standards to see if it is possible to play them on cellular phones by software. Next, various coding profiles as combinations of subalgorithms are studied, and we select a profile that adapts its complexity to the processing power of cellular phones. Also, an efficient dithering algorithm called out-of-order dithering is developed. We implement the profile with out-of-order dithering in an actual cellular phone software environment and present the performance results. The performance results show that software based video/audio processing is indeed possible on low-end cellular phones.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Korea Univ, Seoul 136701, South Korea.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Jeong, JH (reprint author), Korea Univ, Anam 5 Ga, Seoul 136701, South Korea.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jhjeong@os.korea.ac.kr</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PD </td><td>FEB</td>
</tr>

<tr>
<td valign="top">PY </td><td>2005</td>
</tr>

<tr>
<td valign="top">VL </td><td>28</td>
</tr>

<tr>
<td valign="top">IS </td><td>2</td>
</tr>

<tr>
<td valign="top">BP </td><td>185</td>
</tr>

<tr>
<td valign="top">EP </td><td>210</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1007/s11235-004-5016-y</td>
</tr>

<tr>
<td valign="top">SC </td><td>Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000226977400005</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Modegi, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Modegi, T</td>
</tr>

<tr>
<td valign="top">BE </td><td>Khosla, R
   <br>Howlett, RJ
   <br>Jain, LC</td>
</tr>

<tr>
<td valign="top">TI </td><td>Development of nearly lossless embedding technology of contactless
   sensible watermarks for audio signals</td>
</tr>

<tr>
<td valign="top">SO </td><td>KNOWLEDGE-BASED INTELLIGENT INFORMATION AND ENGINEERING SYSTEMS, PT 2,
   PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">SE </td><td>LECTURE NOTES IN ARTIFICIAL INTELLIGENCE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article; Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th International Conference on Knowledge-Based Intelligent Information
   and Engineering Systems</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 14-16, 2005</td>
</tr>

<tr>
<td valign="top">CL </td><td>La Trobe Univ, Melbourne, AUSTRALIA</td>
</tr>

<tr>
<td valign="top">HO </td><td>La Trobe Univ</td>
</tr>

<tr>
<td valign="top">AB </td><td>We have proposed a novel audio watermarking technology, which embeds a set of bitstream data by changing two-channel stereo locations of lower frequency components in an embedding target audio signal. This method features nearly lossless embedding, robustness against lossy data compression or analogue conversion, and enables contactless asynchronous detection of embedded watermarks through speaker and microphone devices without the original audio signals. In this paper, we propose an extended monaural audio watermark embedding method, which supports both monaural and stereo audio signals, and enables watermark detection by a single monaural microphone such as cell phone voice receiver devices. We describe an abstract of our proposed watermark embedding and extracting algorithm, and experimental results of watermark extraction precision.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Dai Nippon Printing Co Ltd, Media Technol Res Ctr, Shinjuku Ku, Tokyo
   1620066, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Modegi, T (reprint author), Dai Nippon Printing Co Ltd, Media Technol Res Ctr, Shinjuku Ku, Ichigaya Daitoh Bldg,6-3,Ichigaya Daimachi, Tokyo 1620066, Japan.</td>
</tr>

<tr>
<td valign="top">EM </td><td>Modegi-T@mail.dnp.co.jp</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2005</td>
</tr>

<tr>
<td valign="top">VL </td><td>3682</td>
</tr>

<tr>
<td valign="top">BP </td><td>1122</td>
</tr>

<tr>
<td valign="top">EP </td><td>1128</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Imaging Science &amp; Photographic Technology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000232722200155</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Yu, JJ
   <br>Chang, GK</td>
</tr>

<tr>
<td valign="top">AF </td><td>Yu, JJ
   <br>Chang, GK</td>
</tr>

<tr>
<td valign="top">BE </td><td>Cheung, KW
   <br>Chang, GK
   <br>Li, G
   <br>Sato, KI</td>
</tr>

<tr>
<td valign="top">TI </td><td>Recent progress in broadband optical access and packet switched core
   networking technologies</td>
</tr>

<tr>
<td valign="top">SO </td><td>NETWORK ARCHITECTURES, MANAGEMENT, AND APPLICATIONS III, PTS 1 AND 2</td>
</tr>

<tr>
<td valign="top">SE </td><td>Proceedings of SPIE</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Conference on Architectures, Management, and Applications III</td>
</tr>

<tr>
<td valign="top">CY </td><td>NOV 07-10, 2005</td>
</tr>

<tr>
<td valign="top">CL </td><td>Shanghai, PEOPLES R CHINA</td>
</tr>

<tr>
<td valign="top">DE </td><td>Optical Internet; Gigabit Ethernet; passive optical network; GPON;
   WDM-PON; optical label switching network; IP over WDM; GMPLS; optical
   packet switching; core networks; wavelength conversion; optical buffer;
   clock recovery; optical label generation and swapping</td>
</tr>

<tr>
<td valign="top">ID </td><td>40GBIT/S WAVELENGTH CONVERSION; CARRIER SUPPRESSION; ELECTROABSORPTION
   MODULATOR; WDM NETWORK; LOOP MIRROR; HIGH-SPEED; LABEL; DELAY; SYSTEMS;
   SIGNAL</td>
</tr>

<tr>
<td valign="top">AB </td><td>The combination of broadband optical access and core networks is considered to be one of the most promising solutions for end-to-end transportation of high bit-rate data, video, and voice signals across optical networks of the future. Optical label switching technology (OLS) is an important aspect of optical packet switching and it involves the extraction and processing of the labels so that the packets can be routed to their destinations. OLS enables the routing and forwarding of the ultra-high bit rate payloads from source to destination entirely in the optical domain, thus alleviating the need for expensive optical-to-electrical conversions for processing. We have developed key enabling technologies for merged core and access networks including optical label generation, label swapping, optical buffering, clock recovery and wavelength conversion. We have experimentally demonstrated that these enabling techniques that can provide efficient broadband services in the future access and core networks.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Georgia Inst Technol, Sch Elect &amp; Comp Engn, Atlanta, GA 30332 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Yu, JJ (reprint author), Georgia Inst Technol, Sch Elect &amp; Comp Engn, Atlanta, GA 30332 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>jianjun@ece.gatech.edu</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2005</td>
</tr>

<tr>
<td valign="top">VL </td><td>6022</td>
</tr>

<tr>
<td valign="top">AR </td><td>60221O</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1117/12.639936</td>
</tr>

<tr>
<td valign="top">PN </td><td>1&amp;2</td>
</tr>

<tr>
<td valign="top">SC </td><td>Optics; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000235303900050</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Stokes, JW
   <br>Malvar, HS</td>
</tr>

<tr>
<td valign="top">AF </td><td>Stokes, JW
   <br>Malvar, HS</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Acoustic echo cancellation with arbitrary playback sampling rate</td>
</tr>

<tr>
<td valign="top">SO </td><td>2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING, VOL IV, PROCEEDINGS: AUDIO AND ELECTROACOUSTICS SIGNAL
   PROCESSING FOR COMMUNICATIONS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE International Conference on Acoustics, Speech, and Signal
   Processing</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 17-21, 2004</td>
</tr>

<tr>
<td valign="top">CL </td><td>Montreal, CANADA</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper introduces a new architecture for implementing subband acoustic echo cancellation (AEC) with arbitrary playback sampling rate. Typically, in AEC algorithms for audio or video conferencing, the sampling rates for the signals played through the speakers and captured from the microphones are identical. For speech recognition while playing CD-quality music and Internet gaming with voice chat, the playback sampling rate is usually higher than the capture rate. A direct solution is to apply a sampling rate converter to the playback signal before feeding it to the AEC, but that is complicated if many sampling frequencies must be supported. We propose a more efficient solution for subband AEC: we perform the sampling rate conversion as a frequency-domain interpolation that matches the transform lengths of the playback and capture signals. Results show that the new AEC architecture has a small computational cost and only a minimal reduction in echo attenuation.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Microsoft Res, Redmond, WA 98052 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Stokes, JW (reprint author), Microsoft Res, 1 Microsoft Way, Redmond, WA 98052 USA.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2004</td>
</tr>

<tr>
<td valign="top">BP </td><td>153</td>
</tr>

<tr>
<td valign="top">EP </td><td>156</td>
</tr>

<tr>
<td valign="top">SC </td><td>Acoustics; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000222179500039</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Okumoto, H</td>
</tr>

<tr>
<td valign="top">AF </td><td>Okumoto, H</td>
</tr>

<tr>
<td valign="top">BE </td><td>Cantoni, L
   <br>McLoughlin, C</td>
</tr>

<tr>
<td valign="top">TI </td><td>Japanese transitive and intransitive verbs: e-learning system with
   speech recognition and video</td>
</tr>

<tr>
<td valign="top">SO </td><td>ED-MEDIA 2004: World Conference on Educational Multimedia, Hypermedia &amp;
   Telecommunications, Vols. 1-7</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>World Conference on Educational Multimedia, Hypermedia and
   Telecommunications</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUN 21-26, 2004</td>
</tr>

<tr>
<td valign="top">CL </td><td>Lugano, SWITZERLAND</td>
</tr>

<tr>
<td valign="top">AB </td><td>Learning a foreign language requires diverse knowledge and skills, including reading and writing, conversation skills, and grammar. So we developed a Web-based Japanese learning system with speech recognition capability in order to provide assistance to teachers and an effective learning tool to learners. This system is equipped with a speaker-independent speech recognition applet, so learners can use voice input by just accessing the system, without installing any additional software. This case of using speech recognition system is one of the advantages of this system. The speech recognition system uses formant frequency, logarithmic power and spectral power ratio as phonetic feature, and analyzes them by dynamic time warping. The system currently offers such exercises as conversion between corresponding transitive and intransitive verbs and filling in conversation between characters in video footages. We had international students at the International Division of Waseda University use the exercise to convert between transitive and intransitive verbs and collected their evaluation of the system using a questionnaire. 65% of them replied that this system was helpful in learning transitive and intransitive verbs. It was also found that nearly 70% of learners were interested in a learning tool with speech recognition feature and more than 90% thought that it was important to pay attention to pronunciation. From these results, we reached the conclusion that voice-based learning tool is beneficial to improve both learners' motivation and learning effectiveness.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Waseda Univ, Tokyo, Japan.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2004</td>
</tr>

<tr>
<td valign="top">BP </td><td>1902</td>
</tr>

<tr>
<td valign="top">EP </td><td>1907</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Education &amp; Educational Research</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000229602302055</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Castaneda, M
   <br>Ostrosky-Solis, F
   <br>Reyna, JCG
   <br>Marin, JAG
   <br>Gutierrez, LM
   <br>Medina, V
   <br>Martin, GH</td>
</tr>

<tr>
<td valign="top">AF </td><td>Castaneda, M
   <br>Ostrosky-Solis, F
   <br>Reyna, JCG
   <br>Marin, JAG
   <br>Gutierrez, LM
   <br>Medina, V
   <br>Martin, GH</td>
</tr>

<tr>
<td valign="top">TI </td><td>Mild cognitive impairment: a neuropsychological and brain activation
   characterization</td>
</tr>

<tr>
<td valign="top">SO </td><td>SALUD MENTAL</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>dementia; mild cognitive impairment; SPECT; activation; Alzheimer's
   disease; functional neuroimaging</td>
</tr>

<tr>
<td valign="top">ID </td><td>CEREBRAL BLOOD-FLOW; ALZHEIMERS-DISEASE; VERBAL MEMORY; SPECT; DEMENTIA;
   PREDICTORS; TOMOGRAPHY; DIAGNOSIS</td>
</tr>

<tr>
<td valign="top">AB </td><td>Mild cognitive impairment (MCI) is a borderline condition between normal aging and dementia and is characterized by subjective complains of memory impairments that go beyond those expected considering age and education. Yet subjects are not demented, and their functional status remains intact (Peterson et al. 1999). It is suspected that this group includes a substantial number of patients with pre-clinical Alzheimer's Disease(AD), since the follow-up of this group indicates a conversion rate from MCI to AD of 12 to 25% per year and 50% convert in 5 years. Normal controls by contrast, convert to AD on an average of 1-5% per year, depending on age. The identification of people at potential risk of dementia, could be helped by an early therapeutic intervention and also, it may lessen distress for both patient and family, minimize the risk of accidents, and perhaps even prevent the onset of the dementig process itself.
   <br>Neuropsychological assessment is relevant to make a differential diagnosis between normal and pathological aging, to distinguish between different types of dementia, to define the patterns of strength and weakness and to suggest the likely pattern of underlying cerebral pathology The Single Photon Emission Computerized Tomography (SPECT) is one of the recent neuroimaging techniques that contribute with information about the regional blood flow. Several studies of regional blood flow in AD patients demonstrate relative temporal and parietal hipoperfusion in AD. SPECT imaging conducted while the patient is engaged in a cognitive task or under sensory stimulation are referred to as activation studies. Activation studies afford unique opportunities to explore brain metabolic changes related to specific cognitive operations and to establish hypothesis of the neural networks supporting very discrete cognitive functions. Recent functional neuromiaging studies used during cognitive tasks have added to our understanding of the neural anatomy of cognition in both normal and pathological states; therefore the application of this technique to the study of patients with mild cognitive impairment could provide additional information for the earl), identification of this disease. Although activation studies have been used with EA, there are very few studies that have used activation methods to study MCI. Using SPECT, Riddle et al. (1993) studied ten patients with EA and nine age-matched normal controls with a verbal memory activation task and found significant differences between the groups only during the activation task.
   <br>Since the differential diagnosis between normal aging, MCI and depression is still a matter of controversy, activation studies could provide objective data for the early and objective diagnosis of this group. We performed SPECT perfusion imaging during a basal and during a recognition verbal memory task in a group of normal and MCI subjects. Twenty-three subjects were studied ten controls and thirteen with MCI, matched by age and education. Patients and controls were not receiving psychotropic drugs. The Clinical Memory Unit and the Geriatric Service, of the Instituto Nacional de Nutricion Salvador Zubiran of Mexico City referred the subjects.
   <br>The diagnosis of MCI was made if the patients met the following criteria: 1) memory complaint confirmed by a relative, 2) normal activities of daily living, 3) normal general cognitive function confirmed by Neuropsychological Testing, with scores within 1.00 to 1.5 deviation below the norms on the Brief Neuropsychological Test for Spanish Speaking Subjects (NEUROPSI) and on the NEUROPSI Attention and Memory Test, 4) not demented according to the Diagnostic and Statistical Manual of Mental Disorders, Revised Fourth Edition, and the National Institute of Neurological and Communicative Disorders and Stroke-Alzheimer's Disease and Related Disorders of Association criteria respectively. Control subjects were chosen from the community population of individuals receiving general medical examination or among those who were related to patients. Patients and controls gave written consent.
   <br>Neuropsychological assessment was carried out one week before the scanning. SPECT studies were carried out in the Department of Nuclear Medicine of The National Institute of Psychiatry with a Single Slice, multi-detector, three-head scanner (Siemmens) with 70 multiplexors per detector. Tc99m-ECD was used as tracer.
   <br>Following the paradigm proposed by Riddle et al. (1993) a verbal memory task was implemented. Stimuli included:
   <br>a) list of stimuli with "yes" "no", previously recorded by a feminine voice, with a rate of presentation of I each 5 seconds,
   <br>b) a list of 10 high frequency words for Spanish language was presented for as many trials as required for five out of ten words to be recalled. The recognition component of the test was performed 5 minutes after,
   <br>c) a recognition list which comprised 50 words -10 target-words repeated twice and at random, and 30 semantically and phonetically similar distractor words. Subjects were asked to respond "yes" or "no" according to whether they recognized the word. The recognition score was calculated by awarding +1 for a correct recognition, -1 for an incorrect recognition and 0 for a failed recognition. Maximum score was +20, the minimum -30.
   <br>Each of the subjects was scanned on the same morning using a split-dose of Tc99m-ECD tracer. An in-dwelling intravenous catheter was inserted in an arm vein 15-20 minutes before the first injection of tracer (250 MBq). During injection and for five minutes afterwards, patients were reclining with eyes covered with patches and cars unplugged; background noise was minimal.
   <br>For the baseline condition, the first injection of tracer was given over 30 seconds while subject repeated "yes" or "no" after the voice previously recorded, at a rate of I per second over 5 minutes beginning 1 minute before the injection. The subject was then scanned.
   <br>For the activation condition, before the second injection of a tracer (again 250MBq) all subjects were given up to 5 presentations of the 10 word Est, until 5 words had been recalled. Beginning 15 seconds before the second injection of tracer, recognition of the 10 words from the 50 word recognition list was tested over 4 minutes at a rate of 1 word per 5 seconds. Subjects again responded "yes" (for a target word) or "no"(for a non-target word), so that simple verbal output was identical between the comparison conditions. Duration of each condition was approximately 15 minutes. State anxiety levels were assessed by administration of the Alderly Park State Anxiety Questionnaire (Walker, 1990) 5 minutes after each injection.
   <br>The regions of interest that were analyzed included: In the lower cut: anterior cingulate, frontal, superior temporal, mid-temporal, posterior cingulate and occipital. In the upper cut: Anterior cingulate, frontal parietal, posterior cingulate and occipital, and the subcortical regions analyzed were: caudate, putamen and thalamus. The cerebellum was chosen for normalization. In each region pixel counts were performed. Activation indexes were calculated according to the following formula: Basal R=basal count/ basal cerebellum. Activation R=activation count-basal activation/basal cerebellum. Activation Index =(Activation R-Basal R)/(Activation R+ Basal R).
   <br>The significance of changes in tracer uptake between baseline and activation conditions for individual regions of interest within each group was assessed using paired Student's t-tests. With the index obtain with the formula, a Student-T test for independent samples was used to compare MCI and control subjects.
   <br>During the SPECT studies three cases with multinfarts were detected, and therefore were eliminated from the MCI sample.
   <br>No significant differences were found in the learning of the word list, the control group had an average of 9.2 (1.2) out of 10 words, and the MCI obtained an average of 9.0 (1.0) out of 10 words. The control group achieved a recall of five words within five presentations and the MCI group within the seventh presentation. Nevertheless no significant differences were found in the recognition list. Performance list was more variable and with higher rate of errors in the MCI group than controls.
   <br>Control subjects showed significant increases in uptake during the recognition tasks in several regions, including: In the lower cut: right anterior cingulate, right and left superior temporal, mid-temporal, left posterior cingulate. In the upper cut: right parietal, right posterior cingulate, right and left putamen, right thalamus. The MCI subjects showed differences in several subcortical structures including the right and left thalamus and left caudate nucleus in the lower cut.
   <br>A great deal of interest has been generated concerning the topic of a boundary or transitional state between normal aging and dementia of the Alzheimer type. This condition has received several descriptors including mild cognitive impairment, incipient dementia, and isolated memory impairment. Although the criteria for MCI have been accepted, the operalization of these criteria can be challenging. In the present study, although no significant differences were found in the total learning of the word list, and differences were related to the rate of learning, the control group achieved a recall of five words within five presentations and the MCI group within the seventh presentation. During the SPECT activation task, significant differences between groups were found specifically in subcortical regions. Regional blood flow is tightly coupled to local neural demands for glucose. Changes in the pattern of regional cerebral perfusion thus provide an index of cerebral metabolic activity. At baseline, uptake tracer was almost identical in MCI subjects and controls whereas during the activation task the control subjects showed significant activation effects while MCI showed hipoperfusion of several subcortical structures including the left thalamus and caudate nucleus. Similar findings were reported by Riddle et al. (1993) who found bilateral activation of the frontal cortex, anterior cingulate and left parietal in normal controls but not in patients with AD during the same recognition memory task. In previous studies of regional glucose metabolism with Positron Emission Tomography (PET), subcortical abnormalities appear to precede cognitive impairment. It suggests that a gradual phase of neuronal degeneration can be detected and preced clinical diagnosis by many years. It has been reported that there exists a period of several years between detectable disease and clinical diagnosis; this offers the possibility of therapeutic intervention at a stage when most cognitive functions are still preserved. Therefore it opens the possibility for disease onset to be postponed or perhaps even avoided. The dividing line between normal physiological evolution and disease will continue to be difficult to define, and although this is a transversal study with a very small sample, results are encouraging since by performing it together with the neuropsychological testing SPECT studies, it might help to identify preclinical stages of dementia.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Univ Nacl Autonoma Mexico, Fac Psicol, Lab Neuropsicol &amp; Psicofisiol,
   Mexico City 11930, DF, Mexico.
   <br>Inst Nacl Psiquiatria Ramon Fuente, Mexico City, DF, Mexico.
   <br>Univ Autonoma Metropolitana Iztapalapa, Plantel Iztapalapa, Mexico City
   09340, DF, Mexico.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ostrosky-Solis, F (reprint author), Univ Nacl Autonoma Mexico, Fac Psicol, Lab Neuropsicol &amp; Psicofisiol, Rivera de Cupia 110-71, Mexico City 11930, DF, Mexico.</td>
</tr>

<tr>
<td valign="top">TC </td><td>3</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>5</td>
</tr>

<tr>
<td valign="top">PD </td><td>AUG</td>
</tr>

<tr>
<td valign="top">PY </td><td>2003</td>
</tr>

<tr>
<td valign="top">VL </td><td>26</td>
</tr>

<tr>
<td valign="top">IS </td><td>4</td>
</tr>

<tr>
<td valign="top">BP </td><td>30</td>
</tr>

<tr>
<td valign="top">EP </td><td>39</td>
</tr>

<tr>
<td valign="top">SC </td><td>Psychiatry</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000186133600004</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Serizawa, M
   <br>Ito, H
   <br>Nomura, T</td>
</tr>

<tr>
<td valign="top">AF </td><td>Serizawa, M
   <br>Ito, H
   <br>Nomura, T</td>
</tr>

<tr>
<td valign="top">TI </td><td>A silence compression algorithm for the multi-rate dual-bandwidth MPEG-4
   CELP standard</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEICE TRANSACTIONS ON INFORMATION AND SYSTEMS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>speech coding; CELP; silence compression; VAD</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper proposes a silence compression algorithm operating at multi-rates (MR) and with dual-bandwidths (DB), a narrowband and a wideband, for the MPEG (Moving Picture Experts Croup)-4 CELP (Code Excited Linear Prediction) standard. The MR/DB operations are implemented by a Variable-Frame-size/Dual-Bandwidth Voice Activity Detection (VF/DB-VAD) module with bandwidth conversions of the input signal, and a Variable-Frame-size Comfort Noise Generator (VF-CNG) module. The CNG module adaptively smoothes the Root Mean Square (RMS) value of the input signal to improve the coding quality during transition periods. The algorithm also employs a Dual-Rate Discontinuous Transmission (DR-DTX) module to reduce an average transmission bitrate during silence periods. Subjective test results show that the proposed silence compression algorithm gives no degradation in coding quality for clean and noisy speech signals. These signals include about 20 to 30% non-speech frames and the average transmission bitrates are reduced by 20 to 40%. The proposed algorithm has been adopted as a part of the ISO/IEC MPEG-4 CELP version 2 standard.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>NEC Corp Ltd, Kawasaki, Kanagawa 2168555, Japan.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Serizawa, M (reprint author), NEC Corp Ltd, Kawasaki, Kanagawa 2168555, Japan.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>MAR</td>
</tr>

<tr>
<td valign="top">PY </td><td>2003</td>
</tr>

<tr>
<td valign="top">VL </td><td>E86D</td>
</tr>

<tr>
<td valign="top">IS </td><td>3</td>
</tr>

<tr>
<td valign="top">BP </td><td>412</td>
</tr>

<tr>
<td valign="top">EP </td><td>417</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000181421800006</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Castello, FC
   <br>Balbinot, R
   <br>Silveira, JG
   <br>Santos, PM</td>
</tr>

<tr>
<td valign="top">AF </td><td>Castello, FC
   <br>Balbinot, R
   <br>Silveira, JG
   <br>Santos, PM</td>
</tr>

<tr>
<td valign="top">BE </td><td>Gebali, F</td>
</tr>

<tr>
<td valign="top">TI </td><td>A robust architecture for IP telephony systems interconnection</td>
</tr>

<tr>
<td valign="top">SO </td><td>2003 IEEE PACIFIC RIM CONFERENCE ON COMMUNICATIONS, COMPUTERS, AND
   SIGNAL PROCESSING, VOLS 1 AND 2, CONFERENCE PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>IEEE Pacific Rim Conference on Communications Computers and Signal
   Processing (PACRIM 2003)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 28-30, 2003</td>
</tr>

<tr>
<td valign="top">CL </td><td>VICTORIA, CANADA</td>
</tr>

<tr>
<td valign="top">DE </td><td>voip; telephony IP; robust gateway interconnection; modular architecture</td>
</tr>

<tr>
<td valign="top">AB </td><td>This work presents a robust system's architecture that allows the interoperability between traditional telephony calls and voice over IP originated calls. The architecture here described is modular and flexible, allowing an easy addition of new functionalities. It also generates a scalable and robust final system, as we describe in this paper. The main system's modules are responsible for: signaling conversion, media transmission, address resolution, user location, authorization, authentication and accounting (AAA) and remote management (via SNMP and command fine).</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Pontifical Catholic Univ Rio Grande Sul, Dept Elect Engn, Porto Alegre,
   RS, Brazil.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Castello, FC (reprint author), Pontifical Catholic Univ Rio Grande Sul, Dept Elect Engn, Porto Alegre, RS, Brazil.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2003</td>
</tr>

<tr>
<td valign="top">BP </td><td>593</td>
</tr>

<tr>
<td valign="top">EP </td><td>596</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering; Imaging Science &amp; Photographic
   Technology; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000186424200146</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Yoo, HK
   <br>Kang, BR</td>
</tr>

<tr>
<td valign="top">AF </td><td>Yoo, HK
   <br>Kang, BR</td>
</tr>

<tr>
<td valign="top">BE </td><td>Ismail, M
   <br>Azhar, H
   <br>Abdalla, AGE
   <br>Anuar, K</td>
</tr>

<tr>
<td valign="top">TI </td><td>A media stream processing of VoIP media gateway</td>
</tr>

<tr>
<td valign="top">SO </td><td>APCC 2003: 9TH ASIA-PACIFIC CONFERENCE ON COMMUNICATION, VOLS 1-3,
   PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>9th Asia-Pacific Conference on Communications held in conjunction with
   the 6th Malaysia International Conference on Communications (MICC 2003)</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 21-24, 2003</td>
</tr>

<tr>
<td valign="top">CL </td><td>Penang, MALAYSIA</td>
</tr>

<tr>
<td valign="top">DE </td><td>VoIP; NGcN; media gateway; transcoding; media processing</td>
</tr>

<tr>
<td valign="top">AB </td><td>The converged network consisted of PSTN, IP, 2G and 3G-mobile will allow many different communications system's to interoperates so that users will be able to share data and voice services. One of the key elements is the media gateway which provides conversion of streamed media formats such as voice and manages the transfer of information between the different networks. The VoIP media gateway supports transcodec that converts between speech codecs of each network and can improve the speech quality of intercommunication among different networks. Our VoIP media gateway adopted direct network interface to eliminate bottlenecks between host processor and DSPs and need not use packet processor because DSP is in charge of network interface. This paper described the architecture of converged network applied to VoIP media gateway, the architecture and the functions of media gateway and the DSP's media processing scheme.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>ETRI, Network Engn Technol Team, Taejon 305350, South Korea.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Yoo, HK (reprint author), ETRI, Network Engn Technol Team, 161 Gajeong Dong, Taejon 305350, South Korea.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2003</td>
</tr>

<tr>
<td valign="top">BP </td><td>91</td>
</tr>

<tr>
<td valign="top">EP </td><td>94</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1109/APCC.2003.1274318</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000189415600021</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Ramsey, DA
   <br>Basavanhally, NR
   <br>Low, YL</td>
</tr>

<tr>
<td valign="top">AF </td><td>Ramsey, DA
   <br>Basavanhally, NR
   <br>Low, YL</td>
</tr>

<tr>
<td valign="top">GP </td><td>SPIE
   SPIE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Telecom MEMS packaging in ceramics</td>
</tr>

<tr>
<td valign="top">SO </td><td>CERAMIC INTERCONNECT TECHNOLOGY: NEXT GENERATION</td>
</tr>

<tr>
<td valign="top">SE </td><td>PROCEEDINGS OF THE SOCIETY OF PHOTO-OPTICAL INSTRUMENTATION ENGINEERS
   (SPIE)</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>Ceramic Interconnect Technology Conference and Exhibition of the IMAPS</td>
</tr>

<tr>
<td valign="top">CY </td><td>APR 07-09, 2003</td>
</tr>

<tr>
<td valign="top">CL </td><td>DENVER, CO</td>
</tr>

<tr>
<td valign="top">DE </td><td>MEMS packaging; ceramic packaging</td>
</tr>

<tr>
<td valign="top">AB </td><td>Due to its enormous bandwidth, optical fiber has become the medium of choice for the high-speed, distant transmission of voice and data. Thus, a large portion of a modern telecommunications network is optical in nature. A single fiber might carry light at a single wavelength-modulated at rates of 2.5, 10, or even 40 Gbit/s-for hundreds of kilometers without regeneration. Alternatively, a single fiber might carry dozens of independent optical signals, all at slightly different wavelengths-so-called Dense Wavelength Division Multiplexing (DWDM).
   <br>Historically, any manipulation of such optical data streams first required conversion to the electrical domain. Here, signals could be switched, added, dropped or re-shaped as needed, and then re-converted to their optical counterparts. But such optical-to-electrical and electrical-to-optical conversions require expensive hardware and can create a bandwidth bottleneck if the electronics become the limiting factor.
   <br>Recently, MEMS (Micro-Electro-Mechanical-Systems) devices have become key elements in real components and subsystems used for construction of state-of-the-art DWDM optical networks. Here, tiny bits of sculpted silicon interact directly with the optical signals. Microscopic mirrors can switch light from one fiber to another. Other simple structures can be used to add, drop, or attenuate individual wavelengths at will. All while signals remain in the optical domain.
   <br>With such novel devices come some novel packaging challenges. In our AEMS packaging efforts to date, we have generally resorted to ceramic structures to provide hermeticity and mechanical stability. Windows have been incorporated to allow interaction of light and MEMS devices. And electrical I/O counts have ranged from dozens for simple applications, to thousands for large optical cross-connect mirror arrays. In this work, we'll provide a sampling of such MEMS packaging projects, and the rationale behind our design decisions.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Bell Labs, Lucent Technol, Murray Hill, NJ 07974 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Ramsey, DA (reprint author), Bell Labs, Lucent Technol, 600 Mt Ave, Murray Hill, NJ 07974 USA.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2003</td>
</tr>

<tr>
<td valign="top">VL </td><td>5231</td>
</tr>

<tr>
<td valign="top">BP </td><td>53</td>
</tr>

<tr>
<td valign="top">EP </td><td>57</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Materials Science; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000183727600011</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Schneider, B</td>
</tr>

<tr>
<td valign="top">AF </td><td>Schneider, B</td>
</tr>

<tr>
<td valign="top">TI </td><td>Network architectures: Conversion strategies and evolution towards the
   next-generation network</td>
</tr>

<tr>
<td valign="top">SO </td><td>JOURNAL OF THE COMMUNICATIONS NETWORK</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article; Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>41st European Telecommunication Congress</td>
</tr>

<tr>
<td valign="top">CY </td><td>SEP 04-07, 2002</td>
</tr>

<tr>
<td valign="top">CL </td><td>GENOA, ITALY</td>
</tr>

<tr>
<td valign="top">AB </td><td>Today, a transformation is taking place in the traditional telecommunications marketplace. Whenever the question of the next-generation network (NGN) comes up, the answer that comes up is invariably: voice over IP (VoIP). But VoIP cannot be an end in itself. The goals of any operator remain the same: introduce new services to increase revenues and on the other hand reduce costs. In other words operators must maximise their return on assets (RoA).
   <br>This paper looks at the transformation from TDM to NGN; specifically, we look at the changes in the telecommunication market which drive the introduction of an NGN, the transformation from a TDM network structure towards an NGN structure, and the changes in network architecture, operations, etc. which result from such transformation. We show how a large-scale TDM network can be transformed to an NGN. Furthermore, we evaluate the feasibility of such transformation with a real network configuration and give a qualitative reasoning for such transformation.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PD </td><td>JUL-SEP</td>
</tr>

<tr>
<td valign="top">PY </td><td>2002</td>
</tr>

<tr>
<td valign="top">VL </td><td>1</td>
</tr>

<tr>
<td valign="top">BP </td><td>47</td>
</tr>

<tr>
<td valign="top">EP </td><td>51</td>
</tr>

<tr>
<td valign="top">PN </td><td>2</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Telecommunications</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000179180200012</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Chung, JM
   <br>Ramasamy, K
   <br>Kotikalapudi, V
   <br>Mulla, Z
   <br>Thiyagarajan, G
   <br>Weiser, M
   <br>Scheets, G
   <br>Sharda, R</td>
</tr>

<tr>
<td valign="top">AF </td><td>Chung, JM
   <br>Ramasamy, K
   <br>Kotikalapudi, V
   <br>Mulla, Z
   <br>Thiyagarajan, G
   <br>Weiser, M
   <br>Scheets, G
   <br>Sharda, R</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE
   IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Virtual laboratory education for persons with vision disabilities</td>
</tr>

<tr>
<td valign="top">SO </td><td>2002 45TH MIDWEST SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL II, CONFERENCE
   PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>45th Midwest Symposium on Circuits and Systems (MWSCAS 2002)</td>
</tr>

<tr>
<td valign="top">CY </td><td>AUG 04-07, 2002</td>
</tr>

<tr>
<td valign="top">CL </td><td>TULSA, OK</td>
</tr>

<tr>
<td valign="top">AB </td><td>The Virtual Laboratory (VLab) is focused on providing same-time different-place group-interactions, allowing full real-time virtual-interaction of voice/video/data information of the Internet, communication systems, and multimedia equipment and facilities for vision disabled persons. The VLab interactive-monitor uses piezoelectric technology for graphical display and includes a novel braille markup language (BML) interface with the wireless application protocol (WAP) and the hyper-text markup language (HTML). The BML interface enables conversion of all types of existing web sites and facility/equipment control information to be displayed on the VLab interactive Braille monitor.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Oklahoma State Univ, Stillwater, OK 74078 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Chung, JM (reprint author), Oklahoma State Univ, Stillwater, OK 74078 USA.</td>
</tr>

<tr>
<td valign="top"><span class="FR_label">RI</span></td><td><span style="display" name="show_resc_blurb" id="show_resc_blurb">
<table class="FR_table_borders" border="0" cellpadding="0" cellspacing="0">
<tr class="fr_data_row">
<td><font size="3">
<display_name>Sharda, Ramesh</display_name>&nbsp;</font></td><td><font size="3">R-3954-2019&nbsp;</font></td>
</tr>
</table>
</span></td>
</tr>
<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2002</td>
</tr>

<tr>
<td valign="top">BP </td><td>617</td>
</tr>

<tr>
<td valign="top">EP </td><td>620</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering; Imaging Science &amp; Photographic Technology</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000181336800156</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>B</td>
</tr>

<tr>
<td valign="top">AU </td><td>Sef, T
   <br>Gams, M</td>
</tr>

<tr>
<td valign="top">AF </td><td>Sef, T
   <br>Gams, M</td>
</tr>

<tr>
<td valign="top">BE </td><td>Callaos, N
   <br>Zhou, J
   <br>Nobesawa, S</td>
</tr>

<tr>
<td valign="top">TI </td><td>Govorec (Speaker) - Slovenian text-to-speech synthesizer for various
   applications</td>
</tr>

<tr>
<td valign="top">SO </td><td>6TH WORLD MULTICONFERENCE ON SYSTEMICS, CYBERNETICS AND INFORMATICS, VOL
   III, PROCEEDINGS: IMAGE, ACOUSTIC, SPEECH AND SIGNAL PROCESSING I</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>6th World Multi-Conference on Systemics, Cybernetics and Informatics
   (SCI 2002)/8th International Conference on Information Systems Analysis
   and Synthesis (ISAS 2002)</td>
</tr>

<tr>
<td valign="top">CY </td><td>JUL 14-18, 2002</td>
</tr>

<tr>
<td valign="top">CL </td><td>ORLANDO, FL</td>
</tr>

<tr>
<td valign="top">DE </td><td>text-to-speech system; natural language processing; intelligent systems;
   telecommunication applications; voice portals</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a new text-to-speech (TTS) system called Speaker (Govorec) that is capable of automatic conversion of any Slovenian text into speech. The different phases of the synthesis task are performed by several sequentially operating independent modules (text analysis, prosody generation and segmental concatenation), which are pipelined together. With enhancements to the first module the weakest point of previous synthesizer has been eliminated, that is the correct lexical stress assignment of words. Higher naturalness and agitation of synthetic speech is achieved mainly with different transformations between labelled speech corpus and concrete text, which is synthesised.
   <br>The system is used by members of the Slovenian Foundation for the Blind and Visually impaired and was awarded with tile first price for innovation in the field of life improvements for handicapped people. Currently, several leading Slovenian telecommunication companies are testing the system for providing information (e-mail, SMS, weather reports, traffic information) through mobile phones.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Jozef Stefan Inst, Dept Intelligent Syst, SI-1000 Ljubljana, Slovenia.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Sef, T (reprint author), Jozef Stefan Inst, Dept Intelligent Syst, Jamova 39, SI-1000 Ljubljana, Slovenia.</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2002</td>
</tr>

<tr>
<td valign="top">BP </td><td>270</td>
</tr>

<tr>
<td valign="top">EP </td><td>275</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000178869100055</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>S</td>
</tr>

<tr>
<td valign="top">AU </td><td>Lee, DJY</td>
</tr>

<tr>
<td valign="top">AF </td><td>Lee, DJY</td>
</tr>

<tr>
<td valign="top">GP </td><td>IEEE
   IEEE
   IEEE</td>
</tr>

<tr>
<td valign="top">TI </td><td>Tunneling wireless voice with software defined vocoder</td>
</tr>

<tr>
<td valign="top">SO </td><td>IEEE 55TH VEHICULAR TECHNOLOGY CONFERENCE, VTC SPRING 2002, VOLS 1-4,
   PROCEEDINGS</td>
</tr>

<tr>
<td valign="top">SE </td><td>IEEE VTS Vehicular Technology Conference Proceedings</td>
</tr>

<tr>
<td valign="top">DT </td><td>Proceedings Paper</td>
</tr>

<tr>
<td valign="top">CT </td><td>55th IEEE Vehicular Technology Conference (VTC 2002)</td>
</tr>

<tr>
<td valign="top">CY </td><td>MAY 06-09, 2002</td>
</tr>

<tr>
<td valign="top">CL </td><td>BIRMINGHAM, AL</td>
</tr>

<tr>
<td valign="top">AB </td><td>The objective of these new algorithms is to specify means of reducing the vocoding conversions among different PLMN, PSTN and IP networks. This invention is one of the key enabler to the wireless VoIP application.
   <br>VoIP is getting more and more momentum and is viewed as the future way of transporting voice. This new algorithm uses the integrated signaling among wireline and wireless (PSTN/PLMN, TDMA, GSM, CDMA, AMPS) networks so that wireless voice can be tunneled through different wireless system without any vocoding (except at the handset). This provides a mean of efficient bandwidth, vocoding resources and potential delay management for wireless VoIP application. After the calling party dialed the digits, the signaling will be set up to notify the called party network that the call is on which wireless technology and how the network and handset will be handling the call. The called party network can have local storage media to load the vocoder driver if the software defined handset concept is adopted.
   <br>As 3G right around the corner, some features that could be supported by 3G are also discussed. For example, more efficient and intelligent interaction between user and handset regarding can be a reality now. The fundamental channel can be used to control the air-link resources and become more flexible and efficient.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Cisco Syst, San Jose, CA 95134 USA.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Lee, DJY (reprint author), Cisco Syst, 270 W Tasman Dr, San Jose, CA 95134 USA.</td>
</tr>

<tr>
<td valign="top">EM </td><td>djylee@cisco.com</td>
</tr>

<tr>
<td valign="top">TC </td><td>0</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>0</td>
</tr>

<tr>
<td valign="top">PY </td><td>2002</td>
</tr>

<tr>
<td valign="top">BP </td><td>1623</td>
</tr>

<tr>
<td valign="top">EP </td><td>1625</td>
</tr>

<tr>
<td valign="top">SC </td><td>Engineering; Transportation</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000177471200336</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr><table>
<tr>
<td valign="top">PT </td><td>J</td>
</tr>

<tr>
<td valign="top">AU </td><td>Jorgensen, IHH
   <br>Bogason, G</td>
</tr>

<tr>
<td valign="top">AF </td><td>Jorgensen, IHH
   <br>Bogason, G</td>
</tr>

<tr>
<td valign="top">TI </td><td>Optimization and design of a low power switched current A/D-sigma
   delta-modulator for voice band applications</td>
</tr>

<tr>
<td valign="top">SO </td><td>ANALOG INTEGRATED CIRCUITS AND SIGNAL PROCESSING</td>
</tr>

<tr>
<td valign="top">DT </td><td>Article</td>
</tr>

<tr>
<td valign="top">DE </td><td>sigma delta-modulator; switched current (SI); optimization methodology
   for SI; low power</td>
</tr>

<tr>
<td valign="top">ID </td><td>DB DYNAMIC-RANGE; SI CIRCUITS; CONVERTER; CONVERSION</td>
</tr>

<tr>
<td valign="top">AB </td><td>This paper presents a third order switched current sigma delta-modulator. The modulator is optimized at the system level for minimum power consumption by careful design of the noise transfer function. A thorough noise analysis of the cascode type current copiers used to implement the modulator, together with a new methodology for evaluating the nonlinear settling behavior is presented. This leads to a new optimization methodology that minimize the power consumption in switched current circuits for given design parameters. The optimization methodology takes process variations into account. The modulator is implemented in a standard 2.4 mu m CMOS process only using MOS capacitors. For a power supply of 3.3 V the power consumption is approximately 2.5 mW when operating at a sampling rate of 600 kHz. Under these condition the peak SNR it measured to 74.5 dB with a signal band width of 5.5 kHz. Due to internal clamping in the integrators and proper scaling the modulator shows excellent stability properties. In order to compare the performance of the modulator presented in this paper to other sigma delta-modulators two figure-of-merits (FOMs) are proposed. From these figure-of-merits it is found that the performance of the modulator presented in this paper is significantely higher than the perforamce of other switched current sigma delta-modulators reported. Also, the figure-of-merits show that the performance is comparable to the performance of reported switched capacitor sigma delta-modulators.</td>
</tr>

<tr>
<td valign="top">C1 </td><td>Tech Univ Denmark, Dept Informat Technol, DK-2800 Lyngby, Denmark.
   <br>OTICON AS, DK-2900 Copenhagen, Denmark.</td>
</tr>

<tr>
<td valign="top">RP </td><td>Jorgensen, IHH (reprint author), Tech Univ Denmark, Dept Informat Technol, DK-2800 Lyngby, Denmark.</td>
</tr>

<tr>
<td valign="top">TC </td><td>1</td>
</tr>

<tr>
<td valign="top">Z9 </td><td>1</td>
</tr>

<tr>
<td valign="top">PD </td><td>NOV</td>
</tr>

<tr>
<td valign="top">PY </td><td>1998</td>
</tr>

<tr>
<td valign="top">VL </td><td>17</td>
</tr>

<tr>
<td valign="top">IS </td><td>3</td>
</tr>

<tr>
<td valign="top">BP </td><td>221</td>
</tr>

<tr>
<td valign="top">EP </td><td>247</td>
</tr>

<tr>
<td valign="top">DI </td><td>10.1023/A:1008335730991</td>
</tr>

<tr>
<td valign="top">SC </td><td>Computer Science; Engineering</td>
</tr>

<tr>
<td valign="top">UT </td><td>WOS:000076948400003</td>
</tr>

<tr>
<td>ER</td><td></td>
</tr>


</table><hr>
<tr><td>EF</td><td></td></tr></table>